{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.metrics\n",
    "import re\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import hashlib\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Issue ID                                              Title  \\\n",
      "0     35832            Upgrade to go1.21 and make: Fix tidying   \n",
      "1     35825  [Bug]: LimitExceededException: Certificate has...   \n",
      "2     35823  [Enhancement]: Enable RoutingConfig argument i...   \n",
      "3     35822                       td/shield: fix various tests   \n",
      "4     35821  Fix update operation for authorizer id on apig...   \n",
      "\n",
      "                                          Issue Body             Closed At  \n",
      "0  <!---\\r\\nSee what makes a good Pull Request at...  2024-02-15T17:58:18Z  \n",
      "1  ### Terraform Core Version\\n\\n1.7.3\\n\\n### AWS...  2024-02-15T16:43:01Z  \n",
      "2  ### Description\\n\\nIn resource `aws_sagemaker_...  2024-02-15T01:32:58Z  \n",
      "3  <!---\\r\\nSee what makes a good Pull Request at...  2024-02-15T14:39:44Z  \n",
      "4  ### Description\\r\\n\\r\\nThe change from\\r\\n```\\...  2024-02-15T17:03:14Z  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "df = pd.read_csv(\"../crawled_issue/data_terraform-provider-aws.csv\")\n",
    "df_unique = df.drop_duplicates(subset='Issue ID', keep='first')\n",
    "df = df_unique\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install openpyxl \n",
    "excel_data = pd.read_excel('../dataset/Secret-Regular-Expression.xlsx')\n",
    "\n",
    "# Read the values of the file in the dataframe\n",
    "regex = pd.DataFrame(excel_data, columns=[\n",
    "'Pattern_ID','Secret Type',\t'Regular Expression','Source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue ID</th>\n",
       "      <th>Issue Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35832</td>\n",
       "      <td>\\r\\n### Description\\r\\n\\r\\n\\r\\n\\r\\n### Relatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35825</td>\n",
       "      <td>### Terraform Core Version\\n\\n\\n\\n### AWS Prov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35823</td>\n",
       "      <td>### Description\\n\\nIn resource `aws_sagemaker_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35822</td>\n",
       "      <td>\\r\\n### Description\\r\\n\\r\\n\\r\\n```\\r\\n------- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35821</td>\n",
       "      <td>### Description\\r\\n\\r\\nThe change from\\r\\n```\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31533</th>\n",
       "      <td>5</td>\n",
       "      <td>(Migrating from hashicorp#13246)\\r\\n\\r\\nAWS ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31534</th>\n",
       "      <td>4</td>\n",
       "      <td>Fixes configuration public ip for spot_instanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31535</th>\n",
       "      <td>3</td>\n",
       "      <td>Adds support for tags to `aws_ebs_snapshot` re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31536</th>\n",
       "      <td>2</td>\n",
       "      <td>testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31537</th>\n",
       "      <td>1</td>\n",
       "      <td>testing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31537 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Issue ID                                         Issue Body\n",
       "0         35832  \\r\\n### Description\\r\\n\\r\\n\\r\\n\\r\\n### Relatio...\n",
       "1         35825  ### Terraform Core Version\\n\\n\\n\\n### AWS Prov...\n",
       "2         35823  ### Description\\n\\nIn resource `aws_sagemaker_...\n",
       "3         35822  \\r\\n### Description\\r\\n\\r\\n\\r\\n```\\r\\n------- ...\n",
       "4         35821  ### Description\\r\\n\\r\\nThe change from\\r\\n```\\...\n",
       "...         ...                                                ...\n",
       "31533         5  (Migrating from hashicorp#13246)\\r\\n\\r\\nAWS ha...\n",
       "31534         4  Fixes configuration public ip for spot_instanc...\n",
       "31535         3  Adds support for tags to `aws_ebs_snapshot` re...\n",
       "31536         2                                            testing\n",
       "31537         1                                            testing\n",
       "\n",
       "[31537 rows x 2 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_dict={}\n",
    "for j in df.index:\n",
    "        # if df[\"id\"][j] != \"1165939311\":\n",
    "        #         continue\n",
    "        input_string =    str(df[\"Issue Body\"][j])    \n",
    "        input_string = re.sub(r'[\\'\"\\│]', '', input_string)\n",
    "        dir_list_clean = re.sub(r'drwx[-\\s]*\\d+\\s+\\w+\\s+\\w+\\s+\\d+\\s+\\w+\\s+\\d+\\s+[0-9a-fA-F-]+.*','',input_string)\n",
    "        shell_code_free_text = re.sub(r'```shell([^`]+)```','',dir_list_clean,flags=re.IGNORECASE)\n",
    "        shell_code_free_text = re.sub(r'```Shell\\s*\"([^\"]*)\"\\s*```','',shell_code_free_text,flags=re.IGNORECASE)\n",
    "        # saved_game_free_text = re.sub(r'```([^`]+)```','',shell_code_free_text) #etay jhamela hobe\n",
    "        saved_game_free_text = re.sub(r'<details><summary>Saved game</summary>\\n\\n```(.*?)```', '', shell_code_free_text)\n",
    "        remove_packages = re.sub(r'(\\w+\\.)+\\w+','',saved_game_free_text)\n",
    "        java_exp_free_text = re.sub(r'at\\s[\\w.$]+\\.([\\w]+)\\(([^:]+:\\d+)\\)','',remove_packages)\n",
    "        # url_free_text= re.sub(https?://[^\\s#]+#[A-Za-z0-9\\-]+,'', java_exp_free_text, flags=re.IGNORECASE)\n",
    "        url_with_fragment_text= re.sub(r'https?://[^\\s#]+#[A-Za-z0-9\\-\\=\\+]+','', java_exp_free_text, flags=re.IGNORECASE)\n",
    "        url_free_text= re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '',url_with_fragment_text)\n",
    "        commit_free_text= re.sub(r'commit[ ]?(?:id)?[ ]?[:]?[ ]?([0-9a-f]{40})\\b', '', url_free_text, flags=re.IGNORECASE)\n",
    "        file_path_free_text = re.sub(r\"/[\\w/. :-]+\",'',commit_free_text)\n",
    "        file_path_free_text = re.sub( r'(/[^/\\s]+)+','',file_path_free_text)\n",
    "        sha256_free_text = re.sub(r'sha256\\s*[:]?[=]?\\s*[a-fA-F0-9]{64}','',file_path_free_text)\n",
    "        sha1_free_text = re.sub(r'git-tree-sha1\\s*=\\s*[a-fA-F0-9]+','',sha256_free_text)\n",
    "        build_id_free_text = re.sub(r'build-id\\s*[:]?[=]?\\s*([a-fA-F0-9]+)','',sha1_free_text)\n",
    "        guids_free_text = re.sub(r'GUIDs:\\s+([0-9a-fA-F-]+\\s+[0-9a-fA-F-]+\\s+[0-9a-fA-F-]+)','',build_id_free_text)\n",
    "        uuids_free_text = re.sub(r'([0-9a-fA-F-]+\\s*,\\s*[0-9a-fA-F-]+\\s*,\\s*[0-9a-fA-F-]+)','',guids_free_text)\n",
    "        event_id_free_text = re.sub(r'<([^>]+)>','',uuids_free_text)\n",
    "        UUID_free_text = re.sub(r'(?:UUID|GUID|version|id)[\\\\=:\"\\'\\s]*\\b[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}\\b'\n",
    ",'',event_id_free_text,flags=re.IGNORECASE) ##without the prefix so many false positives can be omitted\n",
    "        hex_free_text = re.sub(r'(?:data|address|id)[\\\\=:\"\\'\\s]*\\b0x[0-9a-fA-F]+\\b','',UUID_free_text,flags=re.IGNORECASE) ## deleting hex ids directly can cause issues\n",
    "        ss_free_text = re.sub(r'Screenshot_(\\d{4}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\w+)','',hex_free_text,flags=re.IGNORECASE)\n",
    "        cleaned_text = ss_free_text\n",
    "        # file_path = \"output.txt\"\n",
    "\n",
    "        # with open(file_path, 'w') as file:\n",
    "        #                 file.write(cleaned_text)\n",
    "        data_dict[j] = {'Issue ID':df['Issue ID'][j],'Issue Body':cleaned_text}\n",
    "        # idx = idx+1\n",
    "    \n",
    "\n",
    "\n",
    "cleaned_text_data = pd.DataFrame.from_dict(data_dict, \"index\")\n",
    "cleaned_text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate String Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_3756\\3600390447.py:11: FutureWarning: Possible nested set at position 35\n",
      "  p = re.compile(regex['Regular Expression'][i])\n",
      "C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_3756\\3600390447.py:11: DeprecationWarning: Flags not at the start of the expression '/(?i)-----\\\\s*?BEGIN[' (truncated) but at position 1\n",
      "  p = re.compile(regex['Regular Expression'][i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97177, 3)\n"
     ]
    }
   ],
   "source": [
    "###recovered\n",
    "    \n",
    "idx = 0\n",
    "data_dict={}\n",
    "# start = iter*100000\n",
    "# end = (iter+1)*100000\n",
    "for i in regex.index:\n",
    "    #print(i,regex['Secret Type'][i]) #, regex['Regular Expression'][i])\n",
    "    # if i%100==0:\n",
    "    #     print(\"checkpoint\")\n",
    "    p = re.compile(regex['Regular Expression'][i])\n",
    "    \n",
    "    # print(\"=====================================================================\")\n",
    "    \n",
    "    for j in df.index:\n",
    "        \n",
    "        cleaned_text = cleaned_text_data.loc[j, 'Issue Body']\n",
    "            # Now you can use 'cleaned_text' for further processing\n",
    "       \n",
    "        matches = re.findall(p,cleaned_text)\n",
    "        for match in set(matches):\n",
    "                data_dict[idx] = {'Type': regex['Secret Type'][i], 'Issue ID':df['Issue ID'][j],'Candidate String':match} #,'Entropy':shannon_entropy(match)}\n",
    "                idx = idx+1\n",
    "    \n",
    "\n",
    "\n",
    "data = pd.DataFrame.from_dict(data_dict, \"index\")\n",
    "data=data.drop_duplicates(subset=[\"Issue ID\", \"Candidate String\"], keep='first')\n",
    "print(data.shape)\n",
    "data.to_csv('../crawled_issue/issues-with-candidate-strings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97177, 3)\n",
      "           Type  Issue_id   Candidate String\n",
      "0  Airtable API     28491  key-consolepolicy\n",
      "1  Airtable API     23663  keyspaces-service\n",
      "2  Airtable API     21200  keypair-lightsail\n",
      "3  Airtable API     20665  key-consolepolicy\n",
      "4  Airtable API     20081  key-group-staging\n",
      "(97177, 7)\n",
      "Index(['Issue ID', 'Title', 'Issue Body', 'Closed At', 'Type',\n",
      "       'Candidate String'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = data.rename(columns={'Issue ID': 'Issue_id'})\n",
    "print(data.shape)\n",
    "print(data.head())\n",
    "merged_df = df.merge(data, left_on='Issue ID', right_on='Issue_id')\n",
    "print(merged_df.shape)\n",
    "columns_to_remove = ['Issue_id']\n",
    "merged_df.drop(columns=columns_to_remove, inplace=True)\n",
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context Window Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97177, 7)\n",
      "   Issue ID                                    Title  \\\n",
      "0     35832  Upgrade to go1.21 and make: Fix tidying   \n",
      "1     35832  Upgrade to go1.21 and make: Fix tidying   \n",
      "2     35832  Upgrade to go1.21 and make: Fix tidying   \n",
      "3     35832  Upgrade to go1.21 and make: Fix tidying   \n",
      "4     35832  Upgrade to go1.21 and make: Fix tidying   \n",
      "\n",
      "                                          Issue Body             Closed At  \\\n",
      "0  <!---\\r\\nSee what makes a good Pull Request at...  2024-02-15T17:58:18Z   \n",
      "1  <!---\\r\\nSee what makes a good Pull Request at...  2024-02-15T17:58:18Z   \n",
      "2  <!---\\r\\nSee what makes a good Pull Request at...  2024-02-15T17:58:18Z   \n",
      "3  <!---\\r\\nSee what makes a good Pull Request at...  2024-02-15T17:58:18Z   \n",
      "4  <!---\\r\\nSee what makes a good Pull Request at...  2024-02-15T17:58:18Z   \n",
      "\n",
      "              Type                          Candidate String  \\\n",
      "0  Generic Pattern            TestAccECSTaskDefinition_basic   \n",
      "1  Generic Pattern  TestAccSecretsManagerSecret_basicReplica   \n",
      "2  Generic Pattern            TestAccIAMInstanceProfile_tags   \n",
      "3  Generic Pattern                    TestAccLogsGroup_basic   \n",
      "4  Generic Pattern      TestAccVPCSecurityGroup_vpcAllEgress   \n",
      "\n",
      "                                       modified_text  \n",
      "0  _basic\\r\\n=== RUN   TestAccECSService_basicImp...  \n",
      "1  === RUN   TestAccSecretsManagerSecret_basic\\r\\...  \n",
      "2  y.\\r\\n=== RUN   TestAccIAMInstanceProfile_basi...  \n",
      "3  1s)\\r\\nPASS\\r\\nok  \\tgithub.com/hashicorp/terr...  \n",
      "4  = RUN   TestAccVPCSecurityGroup_egressMode\\r\\n...  \n"
     ]
    }
   ],
   "source": [
    "def create_context_window(text, target_string, window_size=100):\n",
    "\n",
    "    target_index = text.find(target_string)\n",
    "    #print(target_index)\n",
    "\n",
    "    if target_index != -1:\n",
    "        start_index = max(0, target_index - window_size)\n",
    "        end_index = min(len(text), target_index + len(target_string) + window_size)\n",
    "        context_window = text[start_index:end_index]\n",
    "        return context_window\n",
    "\n",
    "    return None\n",
    "\n",
    "# Apply the create_context_window function to each row in the DataFrame\n",
    "merged_df['modified_text'] = merged_df.apply(lambda row: create_context_window(row['Issue Body'], row['Candidate String']), axis=1)\n",
    "print(merged_df.shape)\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch: 489\n",
      "Ok: 96688\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "inverse_count=0\n",
    "\n",
    "for i in range(merged_df.shape[0]):\n",
    "  #print(i)\n",
    "  main_string=merged_df['Issue Body'][i]\n",
    "  substring=merged_df['Candidate String'][i]\n",
    "  #print(main_string.find(substring))\n",
    "  if main_string.find(substring)!=-1:\n",
    "    count+=1\n",
    "  else:\n",
    "    inverse_count+=1\n",
    "print(\"Mismatch: \"+str(inverse_count))\n",
    "print(\"Ok: \"+str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_issue_ids = merged_df['Issue ID'].tolist()\n",
    "X_text_test = merged_df['Issue Body'].tolist()  # Convert the 'text' column to a list of strings\n",
    "X_candidate_test = merged_df['Candidate String'].tolist()  # Convert the 'candidate_string' column to a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts(texts):\n",
    "    encodings = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    return encodings\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text_encodings, candidate_encodings, labels):\n",
    "        self.text_encodings = text_encodings\n",
    "        self.candidate_encodings = candidate_encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx): #it works fine for training\n",
    "        text_input_ids = self.text_encodings['input_ids'][idx]\n",
    "        text_attention_mask = self.text_encodings['attention_mask'][idx]\n",
    "        candidate_input_ids = self.candidate_encodings['input_ids'][idx]\n",
    "        candidate_attention_mask = self.candidate_encodings['attention_mask'][idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "\n",
    "        return text_input_ids, text_attention_mask, candidate_input_ids, candidate_attention_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model_path = \"../models/rmsprop_cntxt100_data25k_pre.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()  # Set the model to evaluation mode for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_body_encodings_test = encode_texts(X_text_test)\n",
    "candidate_encodings_test = encode_texts(X_candidate_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_text_test))\n",
    "Y_labels = [0] * len(X_text_test)\n",
    "Y = np.array(Y_labels)\n",
    "Y_ =Y.astype(int)\n",
    "print(Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(text_body_encodings_test, candidate_encodings_test, Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()  # Set the model in evaluation mode\n",
    "c=0\n",
    "predicted_labels_list = []\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in test_loader:\n",
    "        print(\"Batch %d\"%c)\n",
    "        c+=1\n",
    "\n",
    "        text_input_ids, text_attention_mask, candidate_input_ids, candidate_attention_mask, labels = batch\n",
    "\n",
    "        # Move tensors to the device\n",
    "        text_input_ids, text_attention_mask, candidate_input_ids, candidate_attention_mask, labels = (\n",
    "            text_input_ids.to(device),\n",
    "            text_attention_mask.to(device),\n",
    "            candidate_input_ids.to(device),\n",
    "            candidate_attention_mask.to(device),\n",
    "            labels.to(device)\n",
    "        )\n",
    "\n",
    "        # Perform inference\n",
    "\n",
    "        outputs = model(input_ids=text_input_ids.type(torch.LongTensor).cuda(), attention_mask=text_attention_mask.type(torch.LongTensor).cuda())\n",
    "        predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        # print(f\"predicted_labels: {predicted_labels}\")\n",
    "        predicted_labels_list.append(predicted_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_list_output = [f.cpu().numpy().tolist() for f in predicted_labels_list]\n",
    "print(predicted_labels_list_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flagged CSV Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repolink=\"https://github.com/hashicorp/terraform-provider-aws\"\n",
    "data_res={}\n",
    "indx=0\n",
    "for i in range(len(predicted_labels_list_output)):\n",
    "    if predicted_labels_list_output[i] == 1 :\n",
    "        data_res[indx]={\"Issue ID\":df[df['Issue ID'] == X_issue_ids[i]]['Issue ID'].values[0],\"Issue Body\":df[df['Issue ID'] == X_issue_ids[i]]['Issue Body'].values[0],\"Candidate String\":X_candidate_test[i],\"Repolink\":repolink}\n",
    "        indx=indx+1\n",
    "if(indx==0):\n",
    "    data_res[indx]={\"Issue ID\":\"N/A\",\"Issue Body\":\"N/A\",\"Candidate String\":\"N/A\",\"Repolink\":repolink}\n",
    "\n",
    "print(data_res)\n",
    "data = pd.DataFrame.from_dict(data_res, \"index\")\n",
    "data=data.drop_duplicates(subset=[\"Issue ID\", \"Candidate String\"], keep='first')\n",
    "print(data.shape)\n",
    "data.to_csv('../results/flagged_terraform-provider-aws.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "secret_indexes = []\n",
    "for i in range(len(predicted_labels_list_output)):\n",
    "    if predicted_labels_list_output[i] == 1 :\n",
    "        secret_indexes.append(X_issue_ids[i])\n",
    "secret_indexes = list(set(secret_indexes))\n",
    "print(secret_indexes)\n",
    "print(len(secret_indexes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
