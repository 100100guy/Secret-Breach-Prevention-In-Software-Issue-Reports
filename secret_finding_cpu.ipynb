{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.metrics\n",
    "import re\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import hashlib\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate String Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Issue ID                                              Title  \\\n",
      "0       252  \"Property null is not supported\" on valid resp...   \n",
      "1       249       cognitect aws-api dependency security issues   \n",
      "2       246                                    Support Bedrock   \n",
      "3       245  Vulnerability in org.eclipse.jetty:jetty-http@...   \n",
      "4       240  Breaking change in `c7e21d2` - HTTP 304 is not...   \n",
      "\n",
      "                                          Issue Body             Closed At  \n",
      "0  ## Dependencies\\r\\n\\r\\n``` clojure\\r\\n:deps {c...  2024-02-02T14:20:41Z  \n",
      "1  Current version of the aws-api have started th...  2024-01-31T14:59:11Z  \n",
      "2  Hi!\\r\\n\\r\\nThanks for the awesome library! :)\\...  2024-01-16T15:50:16Z  \n",
      "3  There is a critical vulnerability in a depende...  2023-11-13T20:38:44Z  \n",
      "4  ## Dependencies\\r\\n\\r\\n``` clojure\\r\\ncom.cogn...  2023-06-16T14:44:32Z  \n",
      "(191, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "df = pd.read_csv(\"crawled_issue/data.csv\")\n",
    "df_unique = df.drop_duplicates(subset='Issue ID', keep='first')\n",
    "df = df_unique\n",
    "print(df.head())\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install openpyxl \n",
    "excel_data = pd.read_excel('dataset/Secret-Regular-Expression.xlsx')\n",
    "\n",
    "# Read the values of the file in the dataframe\n",
    "regex = pd.DataFrame(excel_data, columns=[\n",
    "'Pattern_ID','Secret Type',\t'Regular Expression','Source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue ID</th>\n",
       "      <th>Issue Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>252</td>\n",
       "      <td>## Dependencies\\r\\n\\r\\n``` clojure\\r\\n:deps {{...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>249</td>\n",
       "      <td>Current version of the aws-api have started th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>246</td>\n",
       "      <td>Hi!\\r\\n\\r\\nThanks for the awesome library! :)\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>245</td>\n",
       "      <td>There is a critical vulnerability in a depende...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>240</td>\n",
       "      <td>## Dependencies\\r\\n\\r\\n``` clojure\\r\\n{:mvn}\\r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>7</td>\n",
       "      <td>```\\r\\n  ;; {:mvn}\\r\\n  ;; {:mvn}\\r\\n  ;; {:mv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>6</td>\n",
       "      <td>```\\r\\n  ;; {:mvn}\\r\\n  ;; {:mvn}\\r\\n  ;; {:m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>3</td>\n",
       "      <td>Use the credentials file specified by the enva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>2</td>\n",
       "      <td>When specifying an AWS_PROFILE environment var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1</td>\n",
       "      <td># Reproduce Undesirable Behavior\\r\\n\\r\\n``` cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Issue ID                                         Issue Body\n",
       "0         252  ## Dependencies\\r\\n\\r\\n``` clojure\\r\\n:deps {{...\n",
       "1         249  Current version of the aws-api have started th...\n",
       "2         246  Hi!\\r\\n\\r\\nThanks for the awesome library! :)\\...\n",
       "3         245  There is a critical vulnerability in a depende...\n",
       "4         240  ## Dependencies\\r\\n\\r\\n``` clojure\\r\\n{:mvn}\\r...\n",
       "..        ...                                                ...\n",
       "186         7  ```\\r\\n  ;; {:mvn}\\r\\n  ;; {:mvn}\\r\\n  ;; {:mv...\n",
       "187         6   ```\\r\\n  ;; {:mvn}\\r\\n  ;; {:mvn}\\r\\n  ;; {:m...\n",
       "188         3  Use the credentials file specified by the enva...\n",
       "189         2  When specifying an AWS_PROFILE environment var...\n",
       "190         1  # Reproduce Undesirable Behavior\\r\\n\\r\\n``` cl...\n",
       "\n",
       "[191 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_dict={}\n",
    "for j in df.index:\n",
    "        # if df[\"id\"][j] != \"1165939311\":\n",
    "        #         continue\n",
    "        input_string =    str(df[\"Issue Body\"][j])    \n",
    "        input_string = re.sub(r'[\\'\"\\│]', '', input_string)\n",
    "        dir_list_clean = re.sub(r'drwx[-\\s]*\\d+\\s+\\w+\\s+\\w+\\s+\\d+\\s+\\w+\\s+\\d+\\s+[0-9a-fA-F-]+.*','',input_string)\n",
    "        shell_code_free_text = re.sub(r'```shell([^`]+)```','',dir_list_clean,flags=re.IGNORECASE)\n",
    "        shell_code_free_text = re.sub(r'```Shell\\s*\"([^\"]*)\"\\s*```','',shell_code_free_text,flags=re.IGNORECASE)\n",
    "        # saved_game_free_text = re.sub(r'```([^`]+)```','',shell_code_free_text) #etay jhamela hobe\n",
    "        saved_game_free_text = re.sub(r'<details><summary>Saved game</summary>\\n\\n```(.*?)```', '', shell_code_free_text)\n",
    "        remove_packages = re.sub(r'(\\w+\\.)+\\w+','',saved_game_free_text)\n",
    "        java_exp_free_text = re.sub(r'at\\s[\\w.$]+\\.([\\w]+)\\(([^:]+:\\d+)\\)','',remove_packages)\n",
    "        # url_free_text= re.sub(https?://[^\\s#]+#[A-Za-z0-9\\-]+,'', java_exp_free_text, flags=re.IGNORECASE)\n",
    "        url_with_fragment_text= re.sub(r'https?://[^\\s#]+#[A-Za-z0-9\\-\\=\\+]+','', java_exp_free_text, flags=re.IGNORECASE)\n",
    "        url_free_text= re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '',url_with_fragment_text)\n",
    "        commit_free_text= re.sub(r'commit[ ]?(?:id)?[ ]?[:]?[ ]?([0-9a-f]{40})\\b', '', url_free_text, flags=re.IGNORECASE)\n",
    "        file_path_free_text = re.sub(r\"/[\\w/. :-]+\",'',commit_free_text)\n",
    "        file_path_free_text = re.sub( r'(/[^/\\s]+)+','',file_path_free_text)\n",
    "        sha256_free_text = re.sub(r'sha256\\s*[:]?[=]?\\s*[a-fA-F0-9]{64}','',file_path_free_text)\n",
    "        sha1_free_text = re.sub(r'git-tree-sha1\\s*=\\s*[a-fA-F0-9]+','',sha256_free_text)\n",
    "        build_id_free_text = re.sub(r'build-id\\s*[:]?[=]?\\s*([a-fA-F0-9]+)','',sha1_free_text)\n",
    "        guids_free_text = re.sub(r'GUIDs:\\s+([0-9a-fA-F-]+\\s+[0-9a-fA-F-]+\\s+[0-9a-fA-F-]+)','',build_id_free_text)\n",
    "        uuids_free_text = re.sub(r'([0-9a-fA-F-]+\\s*,\\s*[0-9a-fA-F-]+\\s*,\\s*[0-9a-fA-F-]+)','',guids_free_text)\n",
    "        event_id_free_text = re.sub(r'<([^>]+)>','',uuids_free_text)\n",
    "        UUID_free_text = re.sub(r'(?:UUID|GUID|version|id)[\\\\=:\"\\'\\s]*\\b[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}\\b'\n",
    ",'',event_id_free_text,flags=re.IGNORECASE) ##without the prefix so many false positives can be omitted\n",
    "        hex_free_text = re.sub(r'(?:data|address|id)[\\\\=:\"\\'\\s]*\\b0x[0-9a-fA-F]+\\b','',UUID_free_text,flags=re.IGNORECASE) ## deleting hex ids directly can cause issues\n",
    "        ss_free_text = re.sub(r'Screenshot_(\\d{4}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\w+)','',hex_free_text,flags=re.IGNORECASE)\n",
    "        cleaned_text = ss_free_text\n",
    "        # file_path = \"output.txt\"\n",
    "\n",
    "        # with open(file_path, 'w') as file:\n",
    "        #                 file.write(cleaned_text)\n",
    "        data_dict[j] = {'Issue ID':df['Issue ID'][j],'Issue Body':cleaned_text}\n",
    "        # idx = idx+1\n",
    "    \n",
    "\n",
    "\n",
    "cleaned_text_data = pd.DataFrame.from_dict(data_dict, \"index\")\n",
    "cleaned_text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_10860\\2071773696.py:11: FutureWarning: Possible nested set at position 35\n",
      "  p = re.compile(regex['Regular Expression'][i])\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_10860\\2071773696.py:11: DeprecationWarning: Flags not at the start of the expression '/(?i)-----\\\\s*?BEGIN[' (truncated) but at position 1\n",
      "  p = re.compile(regex['Regular Expression'][i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 3)\n"
     ]
    }
   ],
   "source": [
    "###recovered\n",
    "    \n",
    "idx = 0\n",
    "data_dict={}\n",
    "# start = iter*100000\n",
    "# end = (iter+1)*100000\n",
    "for i in regex.index:\n",
    "    #print(i,regex['Secret Type'][i]) #, regex['Regular Expression'][i])\n",
    "    # if i%100==0:\n",
    "    #     print(\"checkpoint\")\n",
    "    p = re.compile(regex['Regular Expression'][i])\n",
    "    \n",
    "    # print(\"=====================================================================\")\n",
    "    \n",
    "    for j in df.index:\n",
    "        \n",
    "        cleaned_text = cleaned_text_data.loc[j, 'Issue Body']\n",
    "            # Now you can use 'cleaned_text' for further processing\n",
    "       \n",
    "        matches = re.findall(p,cleaned_text)\n",
    "        for match in set(matches):\n",
    "                data_dict[idx] = {'Type': regex['Secret Type'][i], 'Issue ID':df['Issue ID'][j],'Candidate String':match} #,'Entropy':shannon_entropy(match)}\n",
    "                idx = idx+1\n",
    "    \n",
    "\n",
    "\n",
    "data = pd.DataFrame.from_dict(data_dict, \"index\")\n",
    "data=data.drop_duplicates(subset=[\"Issue ID\", \"Candidate String\"], keep='first')\n",
    "print(data.shape)\n",
    "data.to_csv('crawled_issue/issues-with-candidate-strings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context Window and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 3)\n",
      "             Type  Issue_id                          Candidate String\n",
      "0     Alibaba API        74            MissingParameterValueException\n",
      "1    Anypoint API       155      3d9ef9cb-e1eb-4aee-8d57-87f9ef9936be\n",
      "2  AWS API Secret       221  e47fe94049db196fc7f988f6bbfd7fbb0a20a508\n",
      "3  AWS API Secret       192  PutBucketIntelligentTieringConfiguration\n",
      "4  AWS API Secret       192  GetBucketIntelligentTieringConfiguration\n",
      "(49, 7)\n",
      "Index(['Issue ID', 'Title', 'Issue Body', 'Closed At', 'Type',\n",
      "       'Candidate String'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = data.rename(columns={'Issue ID': 'Issue_id'})\n",
    "print(data.shape)\n",
    "print(data.head())\n",
    "merged_df = df.merge(data, left_on='Issue ID', right_on='Issue_id')\n",
    "print(merged_df.shape)\n",
    "columns_to_remove = ['Issue_id']\n",
    "merged_df.drop(columns=columns_to_remove, inplace=True)\n",
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 7)\n",
      "   Issue ID                                              Title  \\\n",
      "0       234  S3 GetObjectAttributes - meeting operation spe...   \n",
      "1       234  S3 GetObjectAttributes - meeting operation spe...   \n",
      "2       225  Creds fail to load (from env) when Java11 snap...   \n",
      "3       221  .clj files in `examples` are scripts, not name...   \n",
      "4       220  ByteBuffer.hasArray should be called before ca...   \n",
      "\n",
      "                                          Issue Body             Closed At  \\\n",
      "0  ## Dependencies\\r\\n\\r\\n``` clojure\\r\\n{:deps {...  2023-07-06T17:20:36Z   \n",
      "1  ## Dependencies\\r\\n\\r\\n``` clojure\\r\\n{:deps {...  2023-07-06T17:20:36Z   \n",
      "2  Thank you for your interest in helping to impr...  2022-12-28T17:42:35Z   \n",
      "3  ## Dependencies\\r\\n\\r\\nI'm working directly wi...  2022-10-11T14:45:36Z   \n",
      "4  In Javadoc, `ByteBuffer.array` may return `Uns...  2022-09-23T15:37:23Z   \n",
      "\n",
      "              Type                          Candidate String  \\\n",
      "0  Generic Pattern                          ObjectAttributes   \n",
      "1  Generic Pattern                         SSECustomerKeyMD5   \n",
      "2  Generic Pattern                       CredentialsProvider   \n",
      "3   AWS API Secret  e47fe94049db196fc7f988f6bbfd7fbb0a20a508   \n",
      "4  Generic Pattern             UnsupportedOperationException   \n",
      "\n",
      "                                       modified_text  \n",
      "0  8.652\"}\\r\\n        com.cognitect.aws/endpoints...  \n",
      "1  ----------------------\\r\\n;;Request\\r\\n;;\\r\\n;...  \n",
      "2                 :endpoint-override    {:hostnam...  \n",
      "3  ## Dependencies\\r\\n\\r\\nI'm working directly wi...  \n",
      "4  In Javadoc, `ByteBuffer.array` may return `Uns...  \n"
     ]
    }
   ],
   "source": [
    "def create_context_window(text, target_string, window_size=200):\n",
    "\n",
    "    target_index = text.find(target_string)\n",
    "    #print(target_index)\n",
    "\n",
    "    if target_index != -1:\n",
    "        start_index = max(0, target_index - window_size)\n",
    "        end_index = min(len(text), target_index + len(target_string) + window_size)\n",
    "        context_window = text[start_index:end_index]\n",
    "        return context_window\n",
    "\n",
    "    return None\n",
    "\n",
    "# Apply the create_context_window function to each row in the DataFrame\n",
    "merged_df['modified_text'] = merged_df.apply(lambda row: create_context_window(row['Issue Body'], row['Candidate String']), axis=1)\n",
    "print(merged_df.shape)\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch: 6\n",
      "Ok: 43\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "inverse_count=0\n",
    "\n",
    "for i in range(merged_df.shape[0]):\n",
    "  #print(i)\n",
    "  main_string=merged_df['Issue Body'][i]\n",
    "  substring=merged_df['Candidate String'][i]\n",
    "  #print(main_string.find(substring))\n",
    "  if main_string.find(substring)!=-1:\n",
    "    count+=1\n",
    "  else:\n",
    "    inverse_count+=1\n",
    "print(\"Mismatch: \"+str(inverse_count))\n",
    "print(\"Ok: \"+str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_issue_ids = merged_df['Issue ID'].tolist()\n",
    "X_text_test = merged_df['Issue Body'].tolist()  # Convert the 'text' column to a list of strings\n",
    "X_candidate_test = merged_df['Candidate String'].tolist()  # Convert the 'candidate_string' column to a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts(texts):\n",
    "    encodings = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    return encodings\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text_encodings, candidate_encodings, labels):\n",
    "        self.text_encodings = text_encodings\n",
    "        self.candidate_encodings = candidate_encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx): #it works fine for training\n",
    "        text_input_ids = self.text_encodings['input_ids'][idx]\n",
    "        text_attention_mask = self.text_encodings['attention_mask'][idx]\n",
    "        candidate_input_ids = self.candidate_encodings['input_ids'][idx]\n",
    "        candidate_attention_mask = self.candidate_encodings['attention_mask'][idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "\n",
    "        return text_input_ids, text_attention_mask, candidate_input_ids, candidate_attention_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "model_path = \"models/adamW_cntxt200_data25k_pre.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "model.eval()  # Set the model to evaluation mode for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_body_encodings_test = encode_texts(X_text_test)\n",
    "candidate_encodings_test = encode_texts(X_candidate_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(len(X_text_test))\n",
    "Y_labels = [0] * len(X_text_test)\n",
    "Y = np.array(Y_labels)\n",
    "Y_ =Y.astype(int)\n",
    "print(Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(text_body_encodings_test, candidate_encodings_test, Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Batch 2\n",
      "Batch 3\n",
      "Batch 4\n",
      "Batch 5\n",
      "Batch 6\n",
      "Batch 7\n",
      "Batch 8\n",
      "Batch 9\n",
      "Batch 10\n",
      "Batch 11\n",
      "Batch 12\n",
      "Batch 13\n",
      "Batch 14\n",
      "Batch 15\n",
      "Batch 16\n",
      "Batch 17\n",
      "Batch 18\n",
      "Batch 19\n",
      "Batch 20\n",
      "Batch 21\n",
      "Batch 22\n",
      "Batch 23\n",
      "Batch 24\n",
      "Batch 25\n",
      "Batch 26\n",
      "Batch 27\n",
      "Batch 28\n",
      "Batch 29\n",
      "Batch 30\n",
      "Batch 31\n",
      "Batch 32\n",
      "Batch 33\n",
      "Batch 34\n",
      "Batch 35\n",
      "Batch 36\n",
      "Batch 37\n",
      "Batch 38\n",
      "Batch 39\n",
      "Batch 40\n",
      "Batch 41\n",
      "Batch 42\n",
      "Batch 43\n",
      "Batch 44\n",
      "Batch 45\n",
      "Batch 46\n",
      "Batch 47\n",
      "Batch 48\n"
     ]
    }
   ],
   "source": [
    "\n",
    "c=0\n",
    "predicted_labels_list = []\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in test_loader:\n",
    "        print(\"Batch %d\"%c)\n",
    "        c+=1\n",
    "\n",
    "        text_input_ids, text_attention_mask, candidate_input_ids, candidate_attention_mask, labels = batch\n",
    "\n",
    "        # Move tensors to the device\n",
    "        text_input_ids, text_attention_mask, candidate_input_ids, candidate_attention_mask, labels = (\n",
    "            text_input_ids,\n",
    "            text_attention_mask,\n",
    "            candidate_input_ids,\n",
    "            candidate_attention_mask,\n",
    "            labels.to\n",
    "        )\n",
    "\n",
    "        # Perform inference\n",
    "\n",
    "        outputs = model(input_ids=text_input_ids.type(torch.LongTensor), attention_mask=text_attention_mask.type(torch.LongTensor))\n",
    "        predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        # print(f\"predicted_labels: {predicted_labels}\")\n",
    "        predicted_labels_list.append(predicted_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "predicted_labels_list_output = [f.cpu().numpy().tolist() for f in predicted_labels_list]\n",
    "print(predicted_labels_list_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
