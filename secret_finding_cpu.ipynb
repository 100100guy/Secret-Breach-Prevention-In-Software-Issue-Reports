{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.metrics\n",
    "import re\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import hashlib\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate String Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Issue ID                                         Issue Body  \\\n",
      "0           0     26405  ### Before reporting an issue\\r\\n\\r\\n- [X] I h...   \n",
      "3           3     26082  ## Description\\r\\nA weakness was found in Keyc...   \n",
      "4           4     25078  ### Before reporting an issue\\n\\n- [X] I have ...   \n",
      "6           6     12438  ### Describe the bug\\n\\nI have followed direct...   \n",
      "9           9     12162  ### Describe the bug\\r\\n\\r\\nOIDC client authen...   \n",
      "\n",
      "                                    Candidate String  \n",
      "0               78b4fb0f-781f-4686-91e8-58ead1f797ff  \n",
      "3  console&redirect_uri=%00&state=32128ce4-0b7f-4...  \n",
      "4                                   -base-keycloak-1  \n",
      "6                                 fine-grained-authz  \n",
      "9                                grant_type=password  \n",
      "(7, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "df = pd.read_csv(\"crawled_issue/flagged_keycloak.csv\")\n",
    "df_unique = df.drop_duplicates(subset='Issue ID', keep='first')\n",
    "df = df_unique\n",
    "print(df.head())\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install openpyxl \n",
    "excel_data = pd.read_excel('dataset/Secret-Regular-Expression.xlsx')\n",
    "\n",
    "# Read the values of the file in the dataframe\n",
    "regex = pd.DataFrame(excel_data, columns=[\n",
    "'Pattern_ID','Secret Type',\t'Regular Expression','Source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue ID</th>\n",
       "      <th>Issue Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26405</td>\n",
       "      <td>### Before reporting an issue\\r\\n\\r\\n- [X] I h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26082</td>\n",
       "      <td>## Description\\r\\nA weakness was found in Keyc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25078</td>\n",
       "      <td>### Before reporting an issue\\n\\n- [X] I have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12438</td>\n",
       "      <td>### Describe the bug\\n\\nI have followed direct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12162</td>\n",
       "      <td>### Describe the bug\\r\\n\\r\\nOIDC client authen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11561</td>\n",
       "      <td>### Describe the bug\\n\\nI am trying to migrate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11284</td>\n",
       "      <td>### Describe the bug\\r\\n\\r\\nThe policy endpoin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Issue ID                                         Issue Body\n",
       "0      26405  ### Before reporting an issue\\r\\n\\r\\n- [X] I h...\n",
       "3      26082  ## Description\\r\\nA weakness was found in Keyc...\n",
       "4      25078  ### Before reporting an issue\\n\\n- [X] I have ...\n",
       "6      12438  ### Describe the bug\\n\\nI have followed direct...\n",
       "9      12162  ### Describe the bug\\r\\n\\r\\nOIDC client authen...\n",
       "13     11561  ### Describe the bug\\n\\nI am trying to migrate...\n",
       "14     11284  ### Describe the bug\\r\\n\\r\\nThe policy endpoin..."
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_dict={}\n",
    "for j in df.index:\n",
    "        # if df[\"id\"][j] != \"1165939311\":\n",
    "        #         continue\n",
    "        input_string =    str(df[\"Issue Body\"][j])    \n",
    "        input_string = re.sub(r'[\\'\"\\â”‚]', '', input_string)\n",
    "        dir_list_clean = re.sub(r'drwx[-\\s]*\\d+\\s+\\w+\\s+\\w+\\s+\\d+\\s+\\w+\\s+\\d+\\s+[0-9a-fA-F-]+.*','',input_string)\n",
    "        shell_code_free_text = re.sub(r'```shell([^`]+)```','',dir_list_clean,flags=re.IGNORECASE)\n",
    "        shell_code_free_text = re.sub(r'```Shell\\s*\"([^\"]*)\"\\s*```','',shell_code_free_text,flags=re.IGNORECASE)\n",
    "        # saved_game_free_text = re.sub(r'```([^`]+)```','',shell_code_free_text) #etay jhamela hobe\n",
    "        saved_game_free_text = re.sub(r'<details><summary>Saved game</summary>\\n\\n```(.*?)```', '', shell_code_free_text)\n",
    "        remove_packages = re.sub(r'(\\w+\\.)+\\w+','',saved_game_free_text)\n",
    "        java_exp_free_text = re.sub(r'at\\s[\\w.$]+\\.([\\w]+)\\(([^:]+:\\d+)\\)','',remove_packages)\n",
    "        # url_free_text= re.sub(https?://[^\\s#]+#[A-Za-z0-9\\-]+,'', java_exp_free_text, flags=re.IGNORECASE)\n",
    "        url_with_fragment_text= re.sub(r'https?://[^\\s#]+#[A-Za-z0-9\\-\\=\\+]+','', java_exp_free_text, flags=re.IGNORECASE)\n",
    "        url_free_text= re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '',url_with_fragment_text)\n",
    "        commit_free_text= re.sub(r'commit[ ]?(?:id)?[ ]?[:]?[ ]?([0-9a-f]{40})\\b', '', url_free_text, flags=re.IGNORECASE)\n",
    "        file_path_free_text = re.sub(r\"/[\\w/. :-]+\",'',commit_free_text)\n",
    "        file_path_free_text = re.sub( r'(/[^/\\s]+)+','',file_path_free_text)\n",
    "        sha256_free_text = re.sub(r'sha256\\s*[:]?[=]?\\s*[a-fA-F0-9]{64}','',file_path_free_text)\n",
    "        sha1_free_text = re.sub(r'git-tree-sha1\\s*=\\s*[a-fA-F0-9]+','',sha256_free_text)\n",
    "        build_id_free_text = re.sub(r'build-id\\s*[:]?[=]?\\s*([a-fA-F0-9]+)','',sha1_free_text)\n",
    "        guids_free_text = re.sub(r'GUIDs:\\s+([0-9a-fA-F-]+\\s+[0-9a-fA-F-]+\\s+[0-9a-fA-F-]+)','',build_id_free_text)\n",
    "        uuids_free_text = re.sub(r'([0-9a-fA-F-]+\\s*,\\s*[0-9a-fA-F-]+\\s*,\\s*[0-9a-fA-F-]+)','',guids_free_text)\n",
    "        event_id_free_text = re.sub(r'<([^>]+)>','',uuids_free_text)\n",
    "        UUID_free_text = re.sub(r'(?:UUID|GUID|version|id)[\\\\=:\"\\'\\s]*\\b[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}\\b'\n",
    ",'',event_id_free_text,flags=re.IGNORECASE) ##without the prefix so many false positives can be omitted\n",
    "        hex_free_text = re.sub(r'(?:data|address|id)[\\\\=:\"\\'\\s]*\\b0x[0-9a-fA-F]+\\b','',UUID_free_text,flags=re.IGNORECASE) ## deleting hex ids directly can cause issues\n",
    "        ss_free_text = re.sub(r'Screenshot_(\\d{4}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\w+)','',hex_free_text,flags=re.IGNORECASE)\n",
    "        cleaned_text = ss_free_text\n",
    "        # file_path = \"output.txt\"\n",
    "\n",
    "        # with open(file_path, 'w') as file:\n",
    "        #                 file.write(cleaned_text)\n",
    "        data_dict[j] = {'Issue ID':df['Issue ID'][j],'Issue Body':cleaned_text}\n",
    "        # idx = idx+1\n",
    "    \n",
    "\n",
    "\n",
    "cleaned_text_data = pd.DataFrame.from_dict(data_dict, \"index\")\n",
    "cleaned_text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_12780\\2071773696.py:11: FutureWarning: Possible nested set at position 35\n",
      "  p = re.compile(regex['Regular Expression'][i])\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_12780\\2071773696.py:11: DeprecationWarning: Flags not at the start of the expression '/(?i)-----\\\\s*?BEGIN[' (truncated) but at position 1\n",
      "  p = re.compile(regex['Regular Expression'][i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 3)\n"
     ]
    }
   ],
   "source": [
    "###recovered\n",
    "    \n",
    "idx = 0\n",
    "data_dict={}\n",
    "# start = iter*100000\n",
    "# end = (iter+1)*100000\n",
    "for i in regex.index:\n",
    "    #print(i,regex['Secret Type'][i]) #, regex['Regular Expression'][i])\n",
    "    # if i%100==0:\n",
    "    #     print(\"checkpoint\")\n",
    "    p = re.compile(regex['Regular Expression'][i])\n",
    "    \n",
    "    # print(\"=====================================================================\")\n",
    "    \n",
    "    for j in df.index:\n",
    "        \n",
    "        cleaned_text = cleaned_text_data.loc[j, 'Issue Body']\n",
    "            # Now you can use 'cleaned_text' for further processing\n",
    "       \n",
    "        matches = re.findall(p,cleaned_text)\n",
    "        for match in set(matches):\n",
    "                data_dict[idx] = {'Type': regex['Secret Type'][i], 'Issue ID':df['Issue ID'][j],'Candidate String':match} #,'Entropy':shannon_entropy(match)}\n",
    "                idx = idx+1\n",
    "    \n",
    "\n",
    "\n",
    "data = pd.DataFrame.from_dict(data_dict, \"index\")\n",
    "data=data.drop_duplicates(subset=[\"Issue ID\", \"Candidate String\"], keep='first')\n",
    "print(data.shape)\n",
    "data.to_csv('crawled_issue/issues-with-candidate-strings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context Window and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 3)\n",
      "                            Type  Issue_id  \\\n",
      "0                   Anypoint API     26405   \n",
      "1      Auth0 OAuth Client Secret     26405   \n",
      "2  Contentful API Personal Token     26405   \n",
      "3                Generic Pattern     26082   \n",
      "4                Generic Pattern     25078   \n",
      "\n",
      "                                    Candidate String  \n",
      "0               78b4fb0f-781f-4686-91e8-58ead1f797ff  \n",
      "1  b2d3f76c-a0fb-41e4-a39c--781f-4686-91e8--03a1-...  \n",
      "2  a39c--781f-4686-91e8--03a1-4b3f-8ec9-fb56c093f909  \n",
      "3  console&redirect_uri=%00&state=32128ce4-0b7f-4...  \n",
      "4                         auth_method=openid-connect  \n",
      "(17, 7)\n",
      "Index(['Unnamed: 0', 'Issue ID', 'Issue Body', 'Candidate String_x', 'Type',\n",
      "       'Candidate String_y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = data.rename(columns={'Issue ID': 'Issue_id'})\n",
    "print(data.shape)\n",
    "print(data.head())\n",
    "merged_df = df.merge(data, left_on='Issue ID', right_on='Issue_id')\n",
    "print(merged_df.shape)\n",
    "columns_to_remove = ['Issue_id']\n",
    "merged_df.drop(columns=columns_to_remove, inplace=True)\n",
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Candidate String'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3361\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3362\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:76\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:108\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Candidate String'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Apply the create_context_window function to each row in the DataFrame\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodified_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_context_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIssue Body\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCandidate String\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(merged_df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(merged_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:8740\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   8729\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m   8731\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m   8732\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   8733\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   8738\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   8739\u001b[0m )\n\u001b[1;32m-> 8740\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:688\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    686\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[1;32m--> 688\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:812\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 812\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:828\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m    827\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 828\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    829\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    830\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    831\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    832\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[107], line 15\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Apply the create_context_window function to each row in the DataFrame\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodified_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m merged_df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: create_context_window(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIssue Body\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCandidate String\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(merged_df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(merged_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:942\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    945\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:1051\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1048\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1051\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3363\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3362\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3363\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_scalar(key) \u001b[38;5;129;01mand\u001b[39;00m isna(key) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhasnans:\n\u001b[0;32m   3366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Candidate String'"
     ]
    }
   ],
   "source": [
    "def create_context_window(text, target_string, window_size=200):\n",
    "\n",
    "    target_index = text.find(target_string)\n",
    "    #print(target_index)\n",
    "\n",
    "    if target_index != -1:\n",
    "        start_index = max(0, target_index - window_size)\n",
    "        end_index = min(len(text), target_index + len(target_string) + window_size)\n",
    "        context_window = text[start_index:end_index]\n",
    "        return context_window\n",
    "\n",
    "    return None\n",
    "\n",
    "# Apply the create_context_window function to each row in the DataFrame\n",
    "merged_df['modified_text'] = merged_df.apply(lambda row: create_context_window(row['Issue Body'], row['Candidate String']), axis=1)\n",
    "print(merged_df.shape)\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch: 2\n",
      "Ok: 1\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "inverse_count=0\n",
    "\n",
    "for i in range(merged_df.shape[0]):\n",
    "  #print(i)\n",
    "  main_string=merged_df['Issue Body'][i]\n",
    "  substring=merged_df['Candidate String'][i]\n",
    "  #print(main_string.find(substring))\n",
    "  if main_string.find(substring)!=-1:\n",
    "    count+=1\n",
    "  else:\n",
    "    inverse_count+=1\n",
    "print(\"Mismatch: \"+str(inverse_count))\n",
    "print(\"Ok: \"+str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_issue_ids = merged_df['Issue ID'].tolist()\n",
    "X_text_test = merged_df['Issue Body'].tolist()  # Convert the 'text' column to a list of strings\n",
    "X_candidate_test = merged_df['Candidate String'].tolist()  # Convert the 'candidate_string' column to a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts(texts):\n",
    "    encodings = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    return encodings\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text_encodings, candidate_encodings, labels):\n",
    "        self.text_encodings = text_encodings\n",
    "        self.candidate_encodings = candidate_encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx): #it works fine for training\n",
    "        text_input_ids = self.text_encodings['input_ids'][idx]\n",
    "        text_attention_mask = self.text_encodings['attention_mask'][idx]\n",
    "        candidate_input_ids = self.candidate_encodings['input_ids'][idx]\n",
    "        candidate_attention_mask = self.candidate_encodings['attention_mask'][idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "\n",
    "        return text_input_ids, text_attention_mask, candidate_input_ids, candidate_attention_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "model_path = \"models/adamW_cntxt200_data25k_pre.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "model.eval()  # Set the model to evaluation mode for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_body_encodings_test = encode_texts(X_text_test)\n",
    "candidate_encodings_test = encode_texts(X_candidate_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(len(X_text_test))\n",
    "Y_labels = [0] * len(X_text_test)\n",
    "Y = np.array(Y_labels)\n",
    "Y_ =Y.astype(int)\n",
    "print(Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(text_body_encodings_test, candidate_encodings_test, Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Batch 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "c=0\n",
    "predicted_labels_list = []\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in test_loader:\n",
    "        print(\"Batch %d\"%c)\n",
    "        c+=1\n",
    "\n",
    "        text_input_ids, text_attention_mask, candidate_input_ids, candidate_attention_mask, labels = batch\n",
    "\n",
    "        # Move tensors to the device\n",
    "        text_input_ids, text_attention_mask, candidate_input_ids, candidate_attention_mask, labels = (\n",
    "            text_input_ids,\n",
    "            text_attention_mask,\n",
    "            candidate_input_ids,\n",
    "            candidate_attention_mask,\n",
    "            labels.to\n",
    "        )\n",
    "\n",
    "        # Perform inference\n",
    "\n",
    "        outputs = model(input_ids=text_input_ids.type(torch.LongTensor), attention_mask=text_attention_mask.type(torch.LongTensor))\n",
    "        predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        # print(f\"predicted_labels: {predicted_labels}\")\n",
    "        predicted_labels_list.append(predicted_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_list_output = [f.cpu().numpy().tolist() for f in predicted_labels_list]\n",
    "print(predicted_labels_list_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "secret_indexes = []\n",
    "for i in range(len(predicted_labels_list_output)):\n",
    "    if predicted_labels_list_output[i] == 1 :\n",
    "        secret_indexes.append(X_issue_ids[i])\n",
    "secret_indexes = list(set(secret_indexes))\n",
    "print(secret_indexes)\n",
    "print(len(secret_indexes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
