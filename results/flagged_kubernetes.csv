,Issue ID,Issue Body,Candidate String,Repolink
0,119059,"#### What type of PR is this?
/kind bug
#### What this PR does / why we need it:

#### Which issue(s) this PR fixes:
xref #118866

#### Special notes for your reviewer:
With  #118866, the migrate print old kubeadm version. With `--allow-experimental-api`, I think we should print v1beta4 instead.
```
[root@daocloud ~]# ./kubeadm config   migrate --allow-experimental-api --old-config v1beta3.yaml
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: heb765.i3xq246pv7fwvhc0
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.6.177.40
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock
  imagePullPolicy: IfNotPresent
  name: daocloud
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/control-plane
...
```

#### Does this PR introduce a user-facing change?
```release-note
None
```
",kubeadm:default-node-token,https://github.com/kubernetes/kubernetes
1,110161,"### What happened?

[root@k8s-master02 ~]# kubeadm join k8s-vip:16443 --token 5amux5.4feptm57mlep9kgo \
> --discovery-token-ca-cert-hash sha256:815d82daf3a963b3cbdd969fd9da10f7d36747c726e21d268d64c11bba0b4f3c \
> --control-plane --certificate-key 698b0866289fc24f5bf587f47e145f0111e4732874d51c9ee8f3994555eae405 \
> --apiserver-advertise-address 192.168.11.117
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[preflight] Running pre-flight checks before initializing the new control plane instance
        [WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret ""kubeadm-certs"" in the ""kube-system"" Namespace
[certs] Using certificateDir folder ""/etc/kubernetes/pki""
[certs] Generating ""etcd/healthcheck-client"" certificate and key
[certs] Generating ""apiserver-etcd-client"" certificate and key
[certs] Generating ""etcd/server"" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master02 localhost] and IPs [192.168.11.117 127.0.0.1 ::1]
[certs] Generating ""etcd/peer"" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master02 localhost] and IPs [192.168.11.117 127.0.0.1 ::1]
[certs] Generating ""apiserver-kubelet-client"" certificate and key
[certs] Generating ""apiserver"" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master02 k8s-vip kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.10.0.1 192.168.11.117]
[certs] Generating ""front-proxy-client"" certificate and key
[certs] Valid certificates and keys now exist in ""/etc/kubernetes/pki""
[certs] Using the existing ""sa"" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder ""/etc/kubernetes""
W0521 10:33:01.494054   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""admin.conf"" kubeconfig file
W0521 10:33:01.548809   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""controller-manager.conf"" kubeconfig file
W0521 10:33:01.776664   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""scheduler.conf"" kubeconfig file
[control-plane] Using manifest folder ""/etc/kubernetes/manifests""
[control-plane] Creating static Pod manifest for ""kube-apiserver""
[control-plane] Creating static Pod manifest for ""kube-controller-manager""
[control-plane] Creating static Pod manifest for ""kube-scheduler""
[check-etcd] Checking that the etcd cluster is healthy
[kubelet-start] Writing kubelet configuration to file ""/var/lib/kubelet/config.yaml""
[kubelet-start] Writing kubelet environment file with flags to file ""/var/lib/kubelet/kubeadm-flags.env""
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[etcd] Announced new etcd member joining to the existing etcd cluster
[etcd] Creating static Pod manifest for ""etcd""
[etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s
[kubelet-check] Initial timeout of 40s passed.
error execution phase control-plane-join/etcd: error creating local etcd static pod manifest file: timeout waiting for etcd cluster to be available
To see the stack trace of this error execute with --v=5 or higher


### What did you expect to happen?

Another etcd join master succeeded

### How can we reproduce it (as minimally and precisely as possible)?

just refer to  #https://kubernetes.io/docs/setup/production-environment/


### Anything else we need to know?

1. user containerd not docker for runtime container
2. user calico for pod nework
3. use centos8


### Kubernetes version

<details>

Client Version: version.Info{Major:""1"", Minor:""24"", GitVersion:""v1.24.0"", GitCommit:""4ce5a8954017644c5420bae81d72b09b735c21f0"", GitTreeState:""clean"", BuildDate:""2022-05-03T13:46:05Z"", GoVersion:""go1.18.1"", Compiler:""gc"", Platform:""linux/amd64""}
Kustomize Version: v4.5.4



</details>


### Cloud provider

<details>
VM 16
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
NAME=""CentOS Stream""
VERSION=""8""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""8""
PLATFORM_ID=""platform:el8""
PRETTY_NAME=""CentOS Stream 8""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:8""
HOME_URL=""https://centos.org/""
BUG_REPORT_URL=""https://bugzilla.redhat.com/""
REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux 8""
REDHAT_SUPPORT_PRODUCT_VERSION=""CentOS Stream""

$ uname -a
# paste output here
Linux k8s-master02 4.18.0-373.el8.x86_64 #1 SMP Tue Mar 22 15:11:47 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kubeadm init/join
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd 
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico
</details>
",698b0866289fc24f5bf587f47e145f0111e4732874d51c9ee8f3994555eae405,https://github.com/kubernetes/kubernetes
2,110161,"### What happened?

[root@k8s-master02 ~]# kubeadm join k8s-vip:16443 --token 5amux5.4feptm57mlep9kgo \
> --discovery-token-ca-cert-hash sha256:815d82daf3a963b3cbdd969fd9da10f7d36747c726e21d268d64c11bba0b4f3c \
> --control-plane --certificate-key 698b0866289fc24f5bf587f47e145f0111e4732874d51c9ee8f3994555eae405 \
> --apiserver-advertise-address 192.168.11.117
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[preflight] Running pre-flight checks before initializing the new control plane instance
        [WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret ""kubeadm-certs"" in the ""kube-system"" Namespace
[certs] Using certificateDir folder ""/etc/kubernetes/pki""
[certs] Generating ""etcd/healthcheck-client"" certificate and key
[certs] Generating ""apiserver-etcd-client"" certificate and key
[certs] Generating ""etcd/server"" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master02 localhost] and IPs [192.168.11.117 127.0.0.1 ::1]
[certs] Generating ""etcd/peer"" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master02 localhost] and IPs [192.168.11.117 127.0.0.1 ::1]
[certs] Generating ""apiserver-kubelet-client"" certificate and key
[certs] Generating ""apiserver"" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master02 k8s-vip kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.10.0.1 192.168.11.117]
[certs] Generating ""front-proxy-client"" certificate and key
[certs] Valid certificates and keys now exist in ""/etc/kubernetes/pki""
[certs] Using the existing ""sa"" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder ""/etc/kubernetes""
W0521 10:33:01.494054   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""admin.conf"" kubeconfig file
W0521 10:33:01.548809   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""controller-manager.conf"" kubeconfig file
W0521 10:33:01.776664   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""scheduler.conf"" kubeconfig file
[control-plane] Using manifest folder ""/etc/kubernetes/manifests""
[control-plane] Creating static Pod manifest for ""kube-apiserver""
[control-plane] Creating static Pod manifest for ""kube-controller-manager""
[control-plane] Creating static Pod manifest for ""kube-scheduler""
[check-etcd] Checking that the etcd cluster is healthy
[kubelet-start] Writing kubelet configuration to file ""/var/lib/kubelet/config.yaml""
[kubelet-start] Writing kubelet environment file with flags to file ""/var/lib/kubelet/kubeadm-flags.env""
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[etcd] Announced new etcd member joining to the existing etcd cluster
[etcd] Creating static Pod manifest for ""etcd""
[etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s
[kubelet-check] Initial timeout of 40s passed.
error execution phase control-plane-join/etcd: error creating local etcd static pod manifest file: timeout waiting for etcd cluster to be available
To see the stack trace of this error execute with --v=5 or higher


### What did you expect to happen?

Another etcd join master succeeded

### How can we reproduce it (as minimally and precisely as possible)?

just refer to  #https://kubernetes.io/docs/setup/production-environment/


### Anything else we need to know?

1. user containerd not docker for runtime container
2. user calico for pod nework
3. use centos8


### Kubernetes version

<details>

Client Version: version.Info{Major:""1"", Minor:""24"", GitVersion:""v1.24.0"", GitCommit:""4ce5a8954017644c5420bae81d72b09b735c21f0"", GitTreeState:""clean"", BuildDate:""2022-05-03T13:46:05Z"", GoVersion:""go1.18.1"", Compiler:""gc"", Platform:""linux/amd64""}
Kustomize Version: v4.5.4



</details>


### Cloud provider

<details>
VM 16
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
NAME=""CentOS Stream""
VERSION=""8""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""8""
PLATFORM_ID=""platform:el8""
PRETTY_NAME=""CentOS Stream 8""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:8""
HOME_URL=""https://centos.org/""
BUG_REPORT_URL=""https://bugzilla.redhat.com/""
REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux 8""
REDHAT_SUPPORT_PRODUCT_VERSION=""CentOS Stream""

$ uname -a
# paste output here
Linux k8s-master02 4.18.0-373.el8.x86_64 #1 SMP Tue Mar 22 15:11:47 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kubeadm init/join
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd 
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico
</details>
",token-ca-cert-hash,https://github.com/kubernetes/kubernetes
3,110161,"### What happened?

[root@k8s-master02 ~]# kubeadm join k8s-vip:16443 --token 5amux5.4feptm57mlep9kgo \
> --discovery-token-ca-cert-hash sha256:815d82daf3a963b3cbdd969fd9da10f7d36747c726e21d268d64c11bba0b4f3c \
> --control-plane --certificate-key 698b0866289fc24f5bf587f47e145f0111e4732874d51c9ee8f3994555eae405 \
> --apiserver-advertise-address 192.168.11.117
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[preflight] Running pre-flight checks before initializing the new control plane instance
        [WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret ""kubeadm-certs"" in the ""kube-system"" Namespace
[certs] Using certificateDir folder ""/etc/kubernetes/pki""
[certs] Generating ""etcd/healthcheck-client"" certificate and key
[certs] Generating ""apiserver-etcd-client"" certificate and key
[certs] Generating ""etcd/server"" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master02 localhost] and IPs [192.168.11.117 127.0.0.1 ::1]
[certs] Generating ""etcd/peer"" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master02 localhost] and IPs [192.168.11.117 127.0.0.1 ::1]
[certs] Generating ""apiserver-kubelet-client"" certificate and key
[certs] Generating ""apiserver"" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master02 k8s-vip kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.10.0.1 192.168.11.117]
[certs] Generating ""front-proxy-client"" certificate and key
[certs] Valid certificates and keys now exist in ""/etc/kubernetes/pki""
[certs] Using the existing ""sa"" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder ""/etc/kubernetes""
W0521 10:33:01.494054   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""admin.conf"" kubeconfig file
W0521 10:33:01.548809   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""controller-manager.conf"" kubeconfig file
W0521 10:33:01.776664   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""scheduler.conf"" kubeconfig file
[control-plane] Using manifest folder ""/etc/kubernetes/manifests""
[control-plane] Creating static Pod manifest for ""kube-apiserver""
[control-plane] Creating static Pod manifest for ""kube-controller-manager""
[control-plane] Creating static Pod manifest for ""kube-scheduler""
[check-etcd] Checking that the etcd cluster is healthy
[kubelet-start] Writing kubelet configuration to file ""/var/lib/kubelet/config.yaml""
[kubelet-start] Writing kubelet environment file with flags to file ""/var/lib/kubelet/kubeadm-flags.env""
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[etcd] Announced new etcd member joining to the existing etcd cluster
[etcd] Creating static Pod manifest for ""etcd""
[etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s
[kubelet-check] Initial timeout of 40s passed.
error execution phase control-plane-join/etcd: error creating local etcd static pod manifest file: timeout waiting for etcd cluster to be available
To see the stack trace of this error execute with --v=5 or higher


### What did you expect to happen?

Another etcd join master succeeded

### How can we reproduce it (as minimally and precisely as possible)?

just refer to  #https://kubernetes.io/docs/setup/production-environment/


### Anything else we need to know?

1. user containerd not docker for runtime container
2. user calico for pod nework
3. use centos8


### Kubernetes version

<details>

Client Version: version.Info{Major:""1"", Minor:""24"", GitVersion:""v1.24.0"", GitCommit:""4ce5a8954017644c5420bae81d72b09b735c21f0"", GitTreeState:""clean"", BuildDate:""2022-05-03T13:46:05Z"", GoVersion:""go1.18.1"", Compiler:""gc"", Platform:""linux/amd64""}
Kustomize Version: v4.5.4



</details>


### Cloud provider

<details>
VM 16
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
NAME=""CentOS Stream""
VERSION=""8""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""8""
PLATFORM_ID=""platform:el8""
PRETTY_NAME=""CentOS Stream 8""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:8""
HOME_URL=""https://centos.org/""
BUG_REPORT_URL=""https://bugzilla.redhat.com/""
REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux 8""
REDHAT_SUPPORT_PRODUCT_VERSION=""CentOS Stream""

$ uname -a
# paste output here
Linux k8s-master02 4.18.0-373.el8.x86_64 #1 SMP Tue Mar 22 15:11:47 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kubeadm init/join
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd 
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico
</details>
",control-plane-join,https://github.com/kubernetes/kubernetes
4,110161,"### What happened?

[root@k8s-master02 ~]# kubeadm join k8s-vip:16443 --token 5amux5.4feptm57mlep9kgo \
> --discovery-token-ca-cert-hash sha256:815d82daf3a963b3cbdd969fd9da10f7d36747c726e21d268d64c11bba0b4f3c \
> --control-plane --certificate-key 698b0866289fc24f5bf587f47e145f0111e4732874d51c9ee8f3994555eae405 \
> --apiserver-advertise-address 192.168.11.117
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[preflight] Running pre-flight checks before initializing the new control plane instance
        [WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret ""kubeadm-certs"" in the ""kube-system"" Namespace
[certs] Using certificateDir folder ""/etc/kubernetes/pki""
[certs] Generating ""etcd/healthcheck-client"" certificate and key
[certs] Generating ""apiserver-etcd-client"" certificate and key
[certs] Generating ""etcd/server"" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master02 localhost] and IPs [192.168.11.117 127.0.0.1 ::1]
[certs] Generating ""etcd/peer"" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master02 localhost] and IPs [192.168.11.117 127.0.0.1 ::1]
[certs] Generating ""apiserver-kubelet-client"" certificate and key
[certs] Generating ""apiserver"" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master02 k8s-vip kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.10.0.1 192.168.11.117]
[certs] Generating ""front-proxy-client"" certificate and key
[certs] Valid certificates and keys now exist in ""/etc/kubernetes/pki""
[certs] Using the existing ""sa"" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder ""/etc/kubernetes""
W0521 10:33:01.494054   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""admin.conf"" kubeconfig file
W0521 10:33:01.548809   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""controller-manager.conf"" kubeconfig file
W0521 10:33:01.776664   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""scheduler.conf"" kubeconfig file
[control-plane] Using manifest folder ""/etc/kubernetes/manifests""
[control-plane] Creating static Pod manifest for ""kube-apiserver""
[control-plane] Creating static Pod manifest for ""kube-controller-manager""
[control-plane] Creating static Pod manifest for ""kube-scheduler""
[check-etcd] Checking that the etcd cluster is healthy
[kubelet-start] Writing kubelet configuration to file ""/var/lib/kubelet/config.yaml""
[kubelet-start] Writing kubelet environment file with flags to file ""/var/lib/kubelet/kubeadm-flags.env""
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[etcd] Announced new etcd member joining to the existing etcd cluster
[etcd] Creating static Pod manifest for ""etcd""
[etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s
[kubelet-check] Initial timeout of 40s passed.
error execution phase control-plane-join/etcd: error creating local etcd static pod manifest file: timeout waiting for etcd cluster to be available
To see the stack trace of this error execute with --v=5 or higher


### What did you expect to happen?

Another etcd join master succeeded

### How can we reproduce it (as minimally and precisely as possible)?

just refer to  #https://kubernetes.io/docs/setup/production-environment/


### Anything else we need to know?

1. user containerd not docker for runtime container
2. user calico for pod nework
3. use centos8


### Kubernetes version

<details>

Client Version: version.Info{Major:""1"", Minor:""24"", GitVersion:""v1.24.0"", GitCommit:""4ce5a8954017644c5420bae81d72b09b735c21f0"", GitTreeState:""clean"", BuildDate:""2022-05-03T13:46:05Z"", GoVersion:""go1.18.1"", Compiler:""gc"", Platform:""linux/amd64""}
Kustomize Version: v4.5.4



</details>


### Cloud provider

<details>
VM 16
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
NAME=""CentOS Stream""
VERSION=""8""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""8""
PLATFORM_ID=""platform:el8""
PRETTY_NAME=""CentOS Stream 8""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:8""
HOME_URL=""https://centos.org/""
BUG_REPORT_URL=""https://bugzilla.redhat.com/""
REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux 8""
REDHAT_SUPPORT_PRODUCT_VERSION=""CentOS Stream""

$ uname -a
# paste output here
Linux k8s-master02 4.18.0-373.el8.x86_64 #1 SMP Tue Mar 22 15:11:47 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kubeadm init/join
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd 
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico
</details>
",etcd[k8s-master02,https://github.com/kubernetes/kubernetes
5,110161,"### What happened?

[root@k8s-master02 ~]# kubeadm join k8s-vip:16443 --token 5amux5.4feptm57mlep9kgo \
> --discovery-token-ca-cert-hash sha256:815d82daf3a963b3cbdd969fd9da10f7d36747c726e21d268d64c11bba0b4f3c \
> --control-plane --certificate-key 698b0866289fc24f5bf587f47e145f0111e4732874d51c9ee8f3994555eae405 \
> --apiserver-advertise-address 192.168.11.117
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[preflight] Running pre-flight checks before initializing the new control plane instance
        [WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret ""kubeadm-certs"" in the ""kube-system"" Namespace
[certs] Using certificateDir folder ""/etc/kubernetes/pki""
[certs] Generating ""etcd/healthcheck-client"" certificate and key
[certs] Generating ""apiserver-etcd-client"" certificate and key
[certs] Generating ""etcd/server"" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master02 localhost] and IPs [192.168.11.117 127.0.0.1 ::1]
[certs] Generating ""etcd/peer"" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master02 localhost] and IPs [192.168.11.117 127.0.0.1 ::1]
[certs] Generating ""apiserver-kubelet-client"" certificate and key
[certs] Generating ""apiserver"" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master02 k8s-vip kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.10.0.1 192.168.11.117]
[certs] Generating ""front-proxy-client"" certificate and key
[certs] Valid certificates and keys now exist in ""/etc/kubernetes/pki""
[certs] Using the existing ""sa"" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder ""/etc/kubernetes""
W0521 10:33:01.494054   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""admin.conf"" kubeconfig file
W0521 10:33:01.548809   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""controller-manager.conf"" kubeconfig file
W0521 10:33:01.776664   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""scheduler.conf"" kubeconfig file
[control-plane] Using manifest folder ""/etc/kubernetes/manifests""
[control-plane] Creating static Pod manifest for ""kube-apiserver""
[control-plane] Creating static Pod manifest for ""kube-controller-manager""
[control-plane] Creating static Pod manifest for ""kube-scheduler""
[check-etcd] Checking that the etcd cluster is healthy
[kubelet-start] Writing kubelet configuration to file ""/var/lib/kubelet/config.yaml""
[kubelet-start] Writing kubelet environment file with flags to file ""/var/lib/kubelet/kubeadm-flags.env""
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[etcd] Announced new etcd member joining to the existing etcd cluster
[etcd] Creating static Pod manifest for ""etcd""
[etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s
[kubelet-check] Initial timeout of 40s passed.
error execution phase control-plane-join/etcd: error creating local etcd static pod manifest file: timeout waiting for etcd cluster to be available
To see the stack trace of this error execute with --v=5 or higher


### What did you expect to happen?

Another etcd join master succeeded

### How can we reproduce it (as minimally and precisely as possible)?

just refer to  #https://kubernetes.io/docs/setup/production-environment/


### Anything else we need to know?

1. user containerd not docker for runtime container
2. user calico for pod nework
3. use centos8


### Kubernetes version

<details>

Client Version: version.Info{Major:""1"", Minor:""24"", GitVersion:""v1.24.0"", GitCommit:""4ce5a8954017644c5420bae81d72b09b735c21f0"", GitTreeState:""clean"", BuildDate:""2022-05-03T13:46:05Z"", GoVersion:""go1.18.1"", Compiler:""gc"", Platform:""linux/amd64""}
Kustomize Version: v4.5.4



</details>


### Cloud provider

<details>
VM 16
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
NAME=""CentOS Stream""
VERSION=""8""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""8""
PLATFORM_ID=""platform:el8""
PRETTY_NAME=""CentOS Stream 8""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:8""
HOME_URL=""https://centos.org/""
BUG_REPORT_URL=""https://bugzilla.redhat.com/""
REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux 8""
REDHAT_SUPPORT_PRODUCT_VERSION=""CentOS Stream""

$ uname -a
# paste output here
Linux k8s-master02 4.18.0-373.el8.x86_64 #1 SMP Tue Mar 22 15:11:47 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kubeadm init/join
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd 
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico
</details>
",698b0866289fc24f5bf587f47e145f0111e4732874d51c9ee8,https://github.com/kubernetes/kubernetes
6,108933,"### What happened?

created secret with regular command but due space after password it generates different base64 string.
e.g

`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

`apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system`


`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajWg**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system




NAME                 STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION                CONTAINER-RUNTIME
kind-control-plane   Ready    control-plane,master   14h   v1.21.1   172.20.0.2    <none>        Ubuntu 21.04   3.10.0-1160.59.1.el7.x86_64   containerd://1.5.2

### What did you expect to happen?

it should ignore the space after password with single quote 
echo -n 'MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5' | openssl base64
**TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**

### How can we reproduce it (as minimally and precisely as possible)?

kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml


kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.4"", GitCommit:""e6c093d87ea4cbb530a7b2ae91e54c0842d8308a"", GitTreeState:""clean"", BuildDate:""2022-02-16T12:38:05Z"", GoVersion:""go1.17.7"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""21"", GitVersion:""v1.21.1"", GitCommit:""5e58841cce77d4bc13713ad2b91fa0d961e69192"", GitTreeState:""clean"", BuildDate:""2021-05-21T23:01:33Z"", GoVersion:""go1.16.4"", Compiler:""gc"", Platform:""linux/amd64""}
WARNING: version difference between client (1.23) and server (1.21) exceeds the supported minor version skew of +/-1
</details>


### Cloud provider

<details>
kind
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
$ uname -a
Linux internalkind78 3.10.0-1160.59.1.el7.x86_64 #1 SMP Wed Feb 23 16:47:03 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kind-cluster
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.5.2
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",creationTimestamp,https://github.com/kubernetes/kubernetes
7,108933,"### What happened?

created secret with regular command but due space after password it generates different base64 string.
e.g

`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

`apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system`


`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajWg**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system




NAME                 STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION                CONTAINER-RUNTIME
kind-control-plane   Ready    control-plane,master   14h   v1.21.1   172.20.0.2    <none>        Ubuntu 21.04   3.10.0-1160.59.1.el7.x86_64   containerd://1.5.2

### What did you expect to happen?

it should ignore the space after password with single quote 
echo -n 'MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5' | openssl base64
**TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**

### How can we reproduce it (as minimally and precisely as possible)?

kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml


kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.4"", GitCommit:""e6c093d87ea4cbb530a7b2ae91e54c0842d8308a"", GitTreeState:""clean"", BuildDate:""2022-02-16T12:38:05Z"", GoVersion:""go1.17.7"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""21"", GitVersion:""v1.21.1"", GitCommit:""5e58841cce77d4bc13713ad2b91fa0d961e69192"", GitTreeState:""clean"", BuildDate:""2021-05-21T23:01:33Z"", GoVersion:""go1.16.4"", Compiler:""gc"", Platform:""linux/amd64""}
WARNING: version difference between client (1.23) and server (1.21) exceeds the supported minor version skew of +/-1
</details>


### Cloud provider

<details>
kind
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
$ uname -a
Linux internalkind78 3.10.0-1160.59.1.el7.x86_64 #1 SMP Wed Feb 23 16:47:03 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kind-cluster
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.5.2
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",MTQyMzM1MTE0MzE5Ouep7a7j=client,https://github.com/kubernetes/kubernetes
8,108933,"### What happened?

created secret with regular command but due space after password it generates different base64 string.
e.g

`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

`apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system`


`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajWg**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system




NAME                 STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION                CONTAINER-RUNTIME
kind-control-plane   Ready    control-plane,master   14h   v1.21.1   172.20.0.2    <none>        Ubuntu 21.04   3.10.0-1160.59.1.el7.x86_64   containerd://1.5.2

### What did you expect to happen?

it should ignore the space after password with single quote 
echo -n 'MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5' | openssl base64
**TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**

### How can we reproduce it (as minimally and precisely as possible)?

kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml


kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.4"", GitCommit:""e6c093d87ea4cbb530a7b2ae91e54c0842d8308a"", GitTreeState:""clean"", BuildDate:""2022-02-16T12:38:05Z"", GoVersion:""go1.17.7"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""21"", GitVersion:""v1.21.1"", GitCommit:""5e58841cce77d4bc13713ad2b91fa0d961e69192"", GitTreeState:""clean"", BuildDate:""2021-05-21T23:01:33Z"", GoVersion:""go1.16.4"", Compiler:""gc"", Platform:""linux/amd64""}
WARNING: version difference between client (1.23) and server (1.21) exceeds the supported minor version skew of +/-1
</details>


### Cloud provider

<details>
kind
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
$ uname -a
Linux internalkind78 3.10.0-1160.59.1.el7.x86_64 #1 SMP Wed Feb 23 16:47:03 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kind-cluster
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.5.2
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajWg,https://github.com/kubernetes/kubernetes
9,108933,"### What happened?

created secret with regular command but due space after password it generates different base64 string.
e.g

`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

`apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system`


`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajWg**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system




NAME                 STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION                CONTAINER-RUNTIME
kind-control-plane   Ready    control-plane,master   14h   v1.21.1   172.20.0.2    <none>        Ubuntu 21.04   3.10.0-1160.59.1.el7.x86_64   containerd://1.5.2

### What did you expect to happen?

it should ignore the space after password with single quote 
echo -n 'MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5' | openssl base64
**TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**

### How can we reproduce it (as minimally and precisely as possible)?

kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml


kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.4"", GitCommit:""e6c093d87ea4cbb530a7b2ae91e54c0842d8308a"", GitTreeState:""clean"", BuildDate:""2022-02-16T12:38:05Z"", GoVersion:""go1.17.7"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""21"", GitVersion:""v1.21.1"", GitCommit:""5e58841cce77d4bc13713ad2b91fa0d961e69192"", GitTreeState:""clean"", BuildDate:""2021-05-21T23:01:33Z"", GoVersion:""go1.16.4"", Compiler:""gc"", Platform:""linux/amd64""}
WARNING: version difference between client (1.23) and server (1.21) exceeds the supported minor version skew of +/-1
</details>


### Cloud provider

<details>
kind
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
$ uname -a
Linux internalkind78 3.10.0-1160.59.1.el7.x86_64 #1 SMP Wed Feb 23 16:47:03 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kind-cluster
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.5.2
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",literal=username,https://github.com/kubernetes/kubernetes
10,108933,"### What happened?

created secret with regular command but due space after password it generates different base64 string.
e.g

`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

`apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system`


`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajWg**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system




NAME                 STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION                CONTAINER-RUNTIME
kind-control-plane   Ready    control-plane,master   14h   v1.21.1   172.20.0.2    <none>        Ubuntu 21.04   3.10.0-1160.59.1.el7.x86_64   containerd://1.5.2

### What did you expect to happen?

it should ignore the space after password with single quote 
echo -n 'MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5' | openssl base64
**TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**

### How can we reproduce it (as minimally and precisely as possible)?

kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml


kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.4"", GitCommit:""e6c093d87ea4cbb530a7b2ae91e54c0842d8308a"", GitTreeState:""clean"", BuildDate:""2022-02-16T12:38:05Z"", GoVersion:""go1.17.7"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""21"", GitVersion:""v1.21.1"", GitCommit:""5e58841cce77d4bc13713ad2b91fa0d961e69192"", GitTreeState:""clean"", BuildDate:""2021-05-21T23:01:33Z"", GoVersion:""go1.16.4"", Compiler:""gc"", Platform:""linux/amd64""}
WARNING: version difference between client (1.23) and server (1.21) exceeds the supported minor version skew of +/-1
</details>


### Cloud provider

<details>
kind
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
$ uname -a
Linux internalkind78 3.10.0-1160.59.1.el7.x86_64 #1 SMP Wed Feb 23 16:47:03 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kind-cluster
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.5.2
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU,https://github.com/kubernetes/kubernetes
11,108933,"### What happened?

created secret with regular command but due space after password it generates different base64 string.
e.g

`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

`apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system`


`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajWg**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system




NAME                 STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION                CONTAINER-RUNTIME
kind-control-plane   Ready    control-plane,master   14h   v1.21.1   172.20.0.2    <none>        Ubuntu 21.04   3.10.0-1160.59.1.el7.x86_64   containerd://1.5.2

### What did you expect to happen?

it should ignore the space after password with single quote 
echo -n 'MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5' | openssl base64
**TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**

### How can we reproduce it (as minimally and precisely as possible)?

kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml


kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.4"", GitCommit:""e6c093d87ea4cbb530a7b2ae91e54c0842d8308a"", GitTreeState:""clean"", BuildDate:""2022-02-16T12:38:05Z"", GoVersion:""go1.17.7"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""21"", GitVersion:""v1.21.1"", GitCommit:""5e58841cce77d4bc13713ad2b91fa0d961e69192"", GitTreeState:""clean"", BuildDate:""2021-05-21T23:01:33Z"", GoVersion:""go1.16.4"", Compiler:""gc"", Platform:""linux/amd64""}
WARNING: version difference between client (1.23) and server (1.21) exceeds the supported minor version skew of +/-1
</details>


### Cloud provider

<details>
kind
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
$ uname -a
Linux internalkind78 3.10.0-1160.59.1.el7.x86_64 #1 SMP Wed Feb 23 16:47:03 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kind-cluster
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.5.2
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",MTQyMzM1MTE0MzE5Ouep7a7j,https://github.com/kubernetes/kubernetes
12,108933,"### What happened?

created secret with regular command but due space after password it generates different base64 string.
e.g

`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

`apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system`


`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajWg**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system




NAME                 STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION                CONTAINER-RUNTIME
kind-control-plane   Ready    control-plane,master   14h   v1.21.1   172.20.0.2    <none>        Ubuntu 21.04   3.10.0-1160.59.1.el7.x86_64   containerd://1.5.2

### What did you expect to happen?

it should ignore the space after password with single quote 
echo -n 'MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5' | openssl base64
**TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**

### How can we reproduce it (as minimally and precisely as possible)?

kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml


kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.4"", GitCommit:""e6c093d87ea4cbb530a7b2ae91e54c0842d8308a"", GitTreeState:""clean"", BuildDate:""2022-02-16T12:38:05Z"", GoVersion:""go1.17.7"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""21"", GitVersion:""v1.21.1"", GitCommit:""5e58841cce77d4bc13713ad2b91fa0d961e69192"", GitTreeState:""clean"", BuildDate:""2021-05-21T23:01:33Z"", GoVersion:""go1.16.4"", Compiler:""gc"", Platform:""linux/amd64""}
WARNING: version difference between client (1.23) and server (1.21) exceeds the supported minor version skew of +/-1
</details>


### Cloud provider

<details>
kind
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
$ uname -a
Linux internalkind78 3.10.0-1160.59.1.el7.x86_64 #1 SMP Wed Feb 23 16:47:03 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kind-cluster
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.5.2
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=,https://github.com/kubernetes/kubernetes
13,108933,"### What happened?

created secret with regular command but due space after password it generates different base64 string.
e.g

`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

`apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system`


`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajWg**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system




NAME                 STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION                CONTAINER-RUNTIME
kind-control-plane   Ready    control-plane,master   14h   v1.21.1   172.20.0.2    <none>        Ubuntu 21.04   3.10.0-1160.59.1.el7.x86_64   containerd://1.5.2

### What did you expect to happen?

it should ignore the space after password with single quote 
echo -n 'MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5' | openssl base64
**TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**

### How can we reproduce it (as minimally and precisely as possible)?

kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml


kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.4"", GitCommit:""e6c093d87ea4cbb530a7b2ae91e54c0842d8308a"", GitTreeState:""clean"", BuildDate:""2022-02-16T12:38:05Z"", GoVersion:""go1.17.7"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""21"", GitVersion:""v1.21.1"", GitCommit:""5e58841cce77d4bc13713ad2b91fa0d961e69192"", GitTreeState:""clean"", BuildDate:""2021-05-21T23:01:33Z"", GoVersion:""go1.16.4"", Compiler:""gc"", Platform:""linux/amd64""}
WARNING: version difference between client (1.23) and server (1.21) exceeds the supported minor version skew of +/-1
</details>


### Cloud provider

<details>
kind
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
$ uname -a
Linux internalkind78 3.10.0-1160.59.1.el7.x86_64 #1 SMP Wed Feb 23 16:47:03 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kind-cluster
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.5.2
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",different,https://github.com/kubernetes/kubernetes
14,108773,"### What happened?

I'm willing to set up OIDC connection to kubernetes via an SSO tool (authentik) using kube-login.
So, I Installed kube-login with krew. Then, I added `oidc-groups-claim` and `oidc-username-claim` to kube-apiserver in the service file of k3s:
```yaml
ExecStart=/usr/local/bin/k3s \
    server \
    '--kube-apiserver-arg' 'oidc-username-claim=email'
    '--kube-apiserver-arg' 'oidc-groups-claim=groups'
```
Then I configured the kubeconfig:
```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CR***********************************S0tLS0K
    server: https://127.0.0.1:6443
  name: default
contexts:
- context:
    cluster: default
    user: default
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: oidc
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      command: kubectl
      args:
      - oidc-login
      - get-token
      - --oidc-issuer-url=http://sso.server.company.com/application/o/kubernetes/
      - --oidc-client-id=79cdxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx72
      - --oidc-client-secret=301e4a2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx093edb62
      - --oidc-extra-scope=email
      - --oidc-extra-scope=profile
      - --oidc-extra-scope=groups
      - --oidc-extra-scope=openid
      - --listen-address=0.0.0.0:8000
      - --oidc-redirect-url-hostname=cluster.server.company.com
      - --skip-open-browser
      command: kubectl
      env: null
      provideClusterInfo: false
```
Afterward, I deployed RBAC linked with groups in Authentik.
I got inspired by this [article](https://developer.okta.com/blog/2021/11/08/k8s-api-server-oidc#what-youll-need-to-get-started)

I did the same thing with a k8s cluster managed by scaleway (a cloud provider).

Unfortunately, I got this error when using oidc user in both clusters: `error: You must be logged in to the server (Unauthorized)`.

### What did you expect to happen?

I expect to be authorized to do anything I'm authorized to do respecting the set RBAC.

### How can we reproduce it (as minimally and precisely as possible)?

I explained it in the above sections

### Anything else we need to know?

_No response_

### Kubernetes version

- kubelogin version: v1.25.1

### Cloud provider

- local machine (k3s)
- scaleway provider (k8s / kapsule)

### OS version

- Debian 11

### Install tools

- kubectl version: v1.21.7
- OpenID Connect provider: [Authentik](https://github.com/goauthentik/authentik) (SSO)

### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",301e4a2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx093edb62,https://github.com/kubernetes/kubernetes
15,108773,"### What happened?

I'm willing to set up OIDC connection to kubernetes via an SSO tool (authentik) using kube-login.
So, I Installed kube-login with krew. Then, I added `oidc-groups-claim` and `oidc-username-claim` to kube-apiserver in the service file of k3s:
```yaml
ExecStart=/usr/local/bin/k3s \
    server \
    '--kube-apiserver-arg' 'oidc-username-claim=email'
    '--kube-apiserver-arg' 'oidc-groups-claim=groups'
```
Then I configured the kubeconfig:
```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CR***********************************S0tLS0K
    server: https://127.0.0.1:6443
  name: default
contexts:
- context:
    cluster: default
    user: default
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: oidc
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      command: kubectl
      args:
      - oidc-login
      - get-token
      - --oidc-issuer-url=http://sso.server.company.com/application/o/kubernetes/
      - --oidc-client-id=79cdxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx72
      - --oidc-client-secret=301e4a2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx093edb62
      - --oidc-extra-scope=email
      - --oidc-extra-scope=profile
      - --oidc-extra-scope=groups
      - --oidc-extra-scope=openid
      - --listen-address=0.0.0.0:8000
      - --oidc-redirect-url-hostname=cluster.server.company.com
      - --skip-open-browser
      command: kubectl
      env: null
      provideClusterInfo: false
```
Afterward, I deployed RBAC linked with groups in Authentik.
I got inspired by this [article](https://developer.okta.com/blog/2021/11/08/k8s-api-server-oidc#what-youll-need-to-get-started)

I did the same thing with a k8s cluster managed by scaleway (a cloud provider).

Unfortunately, I got this error when using oidc user in both clusters: `error: You must be logged in to the server (Unauthorized)`.

### What did you expect to happen?

I expect to be authorized to do anything I'm authorized to do respecting the set RBAC.

### How can we reproduce it (as minimally and precisely as possible)?

I explained it in the above sections

### Anything else we need to know?

_No response_

### Kubernetes version

- kubelogin version: v1.25.1

### Cloud provider

- local machine (k3s)
- scaleway provider (k8s / kapsule)

### OS version

- Debian 11

### Install tools

- kubectl version: v1.21.7
- OpenID Connect provider: [Authentik](https://github.com/goauthentik/authentik) (SSO)

### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",oidc-client-id=79cdxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx72,https://github.com/kubernetes/kubernetes
16,108773,"### What happened?

I'm willing to set up OIDC connection to kubernetes via an SSO tool (authentik) using kube-login.
So, I Installed kube-login with krew. Then, I added `oidc-groups-claim` and `oidc-username-claim` to kube-apiserver in the service file of k3s:
```yaml
ExecStart=/usr/local/bin/k3s \
    server \
    '--kube-apiserver-arg' 'oidc-username-claim=email'
    '--kube-apiserver-arg' 'oidc-groups-claim=groups'
```
Then I configured the kubeconfig:
```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CR***********************************S0tLS0K
    server: https://127.0.0.1:6443
  name: default
contexts:
- context:
    cluster: default
    user: default
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: oidc
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      command: kubectl
      args:
      - oidc-login
      - get-token
      - --oidc-issuer-url=http://sso.server.company.com/application/o/kubernetes/
      - --oidc-client-id=79cdxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx72
      - --oidc-client-secret=301e4a2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx093edb62
      - --oidc-extra-scope=email
      - --oidc-extra-scope=profile
      - --oidc-extra-scope=groups
      - --oidc-extra-scope=openid
      - --listen-address=0.0.0.0:8000
      - --oidc-redirect-url-hostname=cluster.server.company.com
      - --skip-open-browser
      command: kubectl
      env: null
      provideClusterInfo: false
```
Afterward, I deployed RBAC linked with groups in Authentik.
I got inspired by this [article](https://developer.okta.com/blog/2021/11/08/k8s-api-server-oidc#what-youll-need-to-get-started)

I did the same thing with a k8s cluster managed by scaleway (a cloud provider).

Unfortunately, I got this error when using oidc user in both clusters: `error: You must be logged in to the server (Unauthorized)`.

### What did you expect to happen?

I expect to be authorized to do anything I'm authorized to do respecting the set RBAC.

### How can we reproduce it (as minimally and precisely as possible)?

I explained it in the above sections

### Anything else we need to know?

_No response_

### Kubernetes version

- kubelogin version: v1.25.1

### Cloud provider

- local machine (k3s)
- scaleway provider (k8s / kapsule)

### OS version

- Debian 11

### Install tools

- kubectl version: v1.21.7
- OpenID Connect provider: [Authentik](https://github.com/goauthentik/authentik) (SSO)

### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",301e4a2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx,https://github.com/kubernetes/kubernetes
17,105644,"### What happened?

```
/var/log/messages-20211010:Oct  9 16:48:04 iZcr001mb815dp0lyka354Z cloud-init: + kubeadm join --node-name cn-wulan-env212-d01.192.168.155.228 --token o6vnv2.qnbogocntjt8pijx 192.168.153.151:6443 --discovery-token-unsafe-skip-ca-verification
/var/log/messages-20211010:Oct  9 16:48:04 iZcr001mb815dp0lyka354Z cloud-init: [20211009 16:48:04]: wait api server to be ready for ten times in a row, this is10
/var/log/messages-20211010:Oct  9 16:48:04 iZcr001mb815dp0lyka354Z cloud-init: W1009 16:48:04.317944    2124 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
/var/log/messages-20211010:Oct  9 16:48:04 iZcr001mb815dp0lyka354Z cloud-init: [preflight] Running pre-flight checks
/var/log/messages-20211010:Oct  9 16:48:04 iZcr001mb815dp0lyka354Z cloud-init: [WARNING Hostname]: hostname ""cn-wulan-env212-d01.192.168.155.228"" could not be reached
/var/log/messages-20211010:Oct  9 16:48:04 iZcr001mb815dp0lyka354Z cloud-init: [WARNING Hostname]: hostname ""cn-wulan-env212-d01.192.168.155.228"": lookup cn-wulan-env212-d01.192.168.155.228 on 10.212.0.1:53: server misbehaving
/var/log/messages-20211010:Oct  9 16:48:12 iZcr001mb815dp0lyka354Z cloud-init: [preflight] Reading configuration from the cluster...
/var/log/messages-20211010:Oct  9 16:48:12 iZcr001mb815dp0lyka354Z cloud-init: [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
/var/log/messages-20211010:Oct  9 16:48:12 iZcr001mb815dp0lyka354Z cloud-init: error execution phase preflight: unable to fetch the kubeadm-config ConfigMap: failed to get config map: Unauthorized

```

kubeadm failed to retry get `kubeadm-config` configmap when apiserver occasionally failed responsed on massive scale node condition.

This is caused by sharing retry backoff among multiple GetConfigMapWithRetry call. Global DefaultBackoff should not be used unless you known what you r doing.





### What did you expect to happen?

retry get kubeadm-config configmap when failed.

### How can we reproduce it (as minimally and precisely as possible)?

kubeadm join 300 node at the seem time or simply block the traffic between node and apiserver

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: version.Info{Major:""1"", Minor:""21"", GitVersion:""v1.21.1"", GitCommit:""5e58841cce77d4bc13713ad2b91fa0d961e69192"", GitTreeState:""clean"", BuildDate:""2021-05-12T14:18:45Z"", GoVersion:""go1.16.4"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""20+"", GitVersion:""v1.20.4-aliyun.1"", GitCommit:""66492d8"", GitTreeState:"""", BuildDate:""2021-08-23T11:21:01Z"", GoVersion:""go1.15.8"", Compiler:""gc"", Platform:""linux/amd64""}
(base) ➜  ~
</details>


### Cloud provider

<details>
alibaba cloudprovider
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",#NAME?,https://github.com/kubernetes/kubernetes
18,103978,"Hello all,

I'm new in this group and in the kubernets topic.
May be you can help me with the first steps. I got a error if I try to add a additional master to the cluster
For me it looks like a security-problem

It would be nice if you could help me

Let me know If I did any wrong with open of question 

----------------------------------------------------------------
""kubectl version

Client Version: version.Info{Major:""1"", Minor:""20"", GitVersion:""v1.20.1"", 

GitCommit:""c4d752765b3bbac2237bf87cf0b1c2e307844666"", GitTreeState:""clean"", BuildDate:""2020-12-18T12:09:25Z"", 

GoVersion:""go1.15.5"", Compiler:""gc"", Platform:""linux/amd64""}

Server Version: version.Info{Major:""1"", Minor:""20"", GitVersion:""v1.20.9"", 

GitCommit:""7a576bc3935a6b555e33346fd73ad77c925e9e4a"", GitTreeState:""clean"", BuildDate:""2021-07-15T20:56:38Z"", 

GoVersion:""go1.15.14"", Compiler:""gc"", Platform:""linux/amd64""}

""
----------------------------------------------------------------

Command Join a second master

[node5 ~]$ kubeadm join 192.168.0.33:6443 --token mput08.a67ftptgrmv80t7l     --discovery-token-ca-cert-hash sha256:79f87231358600410ed5f16764bd6256d58b1b833aa0e0d5cf57f4606266cc70 --control-plane --certificate-key 86fa3895828decbdbe6445c901641d8e051c3b8dccf02080bacb2fb28c51f8f1

----------------------------------------------------------------
Error
Downloading the certificates in Secret ""kubeadm-certs"" in the ""kube-system"" Namespace
error execution phase control-plane-prepare/download-certs: error downloading certs: error downloading the secret: secrets ""kubeadm-certs"" is forbidden: User ""system:bootstrap:mput08"" cannot get resource ""secrets"" in API group """" in the namespace ""kube-system""
To see the stack trace of this error execute with --v=5 or higher

----------------------------------------------------------------

Many thanks
Norman",86fa3895828decbdbe6445c901641d8e051c3b8dccf02080bacb2fb28c51f8f1,https://github.com/kubernetes/kubernetes
19,103978,"Hello all,

I'm new in this group and in the kubernets topic.
May be you can help me with the first steps. I got a error if I try to add a additional master to the cluster
For me it looks like a security-problem

It would be nice if you could help me

Let me know If I did any wrong with open of question 

----------------------------------------------------------------
""kubectl version

Client Version: version.Info{Major:""1"", Minor:""20"", GitVersion:""v1.20.1"", 

GitCommit:""c4d752765b3bbac2237bf87cf0b1c2e307844666"", GitTreeState:""clean"", BuildDate:""2020-12-18T12:09:25Z"", 

GoVersion:""go1.15.5"", Compiler:""gc"", Platform:""linux/amd64""}

Server Version: version.Info{Major:""1"", Minor:""20"", GitVersion:""v1.20.9"", 

GitCommit:""7a576bc3935a6b555e33346fd73ad77c925e9e4a"", GitTreeState:""clean"", BuildDate:""2021-07-15T20:56:38Z"", 

GoVersion:""go1.15.14"", Compiler:""gc"", Platform:""linux/amd64""}

""
----------------------------------------------------------------

Command Join a second master

[node5 ~]$ kubeadm join 192.168.0.33:6443 --token mput08.a67ftptgrmv80t7l     --discovery-token-ca-cert-hash sha256:79f87231358600410ed5f16764bd6256d58b1b833aa0e0d5cf57f4606266cc70 --control-plane --certificate-key 86fa3895828decbdbe6445c901641d8e051c3b8dccf02080bacb2fb28c51f8f1

----------------------------------------------------------------
Error
Downloading the certificates in Secret ""kubeadm-certs"" in the ""kube-system"" Namespace
error execution phase control-plane-prepare/download-certs: error downloading certs: error downloading the secret: secrets ""kubeadm-certs"" is forbidden: User ""system:bootstrap:mput08"" cannot get resource ""secrets"" in API group """" in the namespace ""kube-system""
To see the stack trace of this error execute with --v=5 or higher

----------------------------------------------------------------

Many thanks
Norman",token-ca-cert-hash,https://github.com/kubernetes/kubernetes
20,103978,"Hello all,

I'm new in this group and in the kubernets topic.
May be you can help me with the first steps. I got a error if I try to add a additional master to the cluster
For me it looks like a security-problem

It would be nice if you could help me

Let me know If I did any wrong with open of question 

----------------------------------------------------------------
""kubectl version

Client Version: version.Info{Major:""1"", Minor:""20"", GitVersion:""v1.20.1"", 

GitCommit:""c4d752765b3bbac2237bf87cf0b1c2e307844666"", GitTreeState:""clean"", BuildDate:""2020-12-18T12:09:25Z"", 

GoVersion:""go1.15.5"", Compiler:""gc"", Platform:""linux/amd64""}

Server Version: version.Info{Major:""1"", Minor:""20"", GitVersion:""v1.20.9"", 

GitCommit:""7a576bc3935a6b555e33346fd73ad77c925e9e4a"", GitTreeState:""clean"", BuildDate:""2021-07-15T20:56:38Z"", 

GoVersion:""go1.15.14"", Compiler:""gc"", Platform:""linux/amd64""}

""
----------------------------------------------------------------

Command Join a second master

[node5 ~]$ kubeadm join 192.168.0.33:6443 --token mput08.a67ftptgrmv80t7l     --discovery-token-ca-cert-hash sha256:79f87231358600410ed5f16764bd6256d58b1b833aa0e0d5cf57f4606266cc70 --control-plane --certificate-key 86fa3895828decbdbe6445c901641d8e051c3b8dccf02080bacb2fb28c51f8f1

----------------------------------------------------------------
Error
Downloading the certificates in Secret ""kubeadm-certs"" in the ""kube-system"" Namespace
error execution phase control-plane-prepare/download-certs: error downloading certs: error downloading the secret: secrets ""kubeadm-certs"" is forbidden: User ""system:bootstrap:mput08"" cannot get resource ""secrets"" in API group """" in the namespace ""kube-system""
To see the stack trace of this error execute with --v=5 or higher

----------------------------------------------------------------

Many thanks
Norman",86fa3895828decbdbe6445c901641d8e051c3b8dccf02080ba,https://github.com/kubernetes/kubernetes
21,101960,"
#### What type of PR is this?

/kind cleanup
/kind flake

#### What this PR does / why we need it:

According to the logs in this test https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/101960/pull-kubernetes-e2e-gce-ubuntu-containerd/1392808375145730048/:
```
I0513 12:14:41.521] I0513 12:14:37.277313   84013 metrics_proxy.go:150] [DEBUG] metrics-proxy nginx config: 
I0513 12:14:41.521] server {
I0513 12:14:41.521] 	listen 10257;
I0513 12:14:41.521] 	server_name _;
I0513 12:14:41.522] 	proxy_set_header Authorization ""Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6InZDZEkxeTZLbjNKUGswNk5QeWdwa3ROT0p3M244NnFfRjBza3lpQWdxVncifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJtZXRyaWNzLXByb3h5LXRva2VuLWd0cDZoIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6Im1ldHJpY3MtcHJveHkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIyZmE0NDA5Yy0zMGE2LTRlMWQtODJiMS1jMzhlMTFiZjM2MjUiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06bWV0cmljcy1wcm94eSJ9.D28Lvn8lmqHH21r_vfWJMFSp0uRXDxhX0m-6mNA7RdX-2S202kL1GqU7RUIV7F3uqPJB_8de6yzLqDu3kaHmpNB1FN68LQcgTCsV5A-V8axwXWCwlaCzb-40ig0UvasyaxOx4wg2TMWK3HxnViSEDD3B2t0KOefbHugeexImbYdsmUJkp_6Nctj2DSK3EH67NwmONYR61Xphnyh9a_CCP3rI6qvmgGD3pbtCaGuiupoBcHW1QniwkM4SlvOoov_g5Sx8l8vLWfaFbLhjC71tRGmH2DEyoXjjvJxe8j4An7bHk5ByGyZAJ3w1kQJlRUU7N9rHUM3kag_Z8tfS4QBEjw"";
I0513 12:14:41.522] 	proxy_ssl_verify off;
I0513 12:14:41.522] 	location /metrics {
I0513 12:14:41.523] 		proxy_pass https://10.40.0.2:10257;
I0513 12:14:41.523] 	}
I0513 12:14:41.523] }
I0513 12:14:41.523] 
I0513 12:14:41.523] I0513 12:14:41.473977   84013 metrics_proxy.go:198] Successfully setup metrics-proxy
...

I0513 12:19:27.432] [It] should grab all metrics from a Scheduler.
...

I0513 12:19:27.433] 
I0513 12:19:27.433] May 13 12:19:19.525: FAIL: Unexpected error:
I0513 12:19:27.433]     <*errors.StatusError | 0xc001a726e0>: {
I0513 12:19:27.434]         ErrStatus: {
I0513 12:19:27.434]             TypeMeta: {Kind: """", APIVersion: """"},
I0513 12:19:27.434]             ListMeta: {
I0513 12:19:27.434]                 SelfLink: """",
I0513 12:19:27.434]                 ResourceVersion: """",
I0513 12:19:27.434]                 Continue: """",
I0513 12:19:27.434]                 RemainingItemCount: nil,
I0513 12:19:27.434]             },
I0513 12:19:27.434]             Status: ""Failure"",
I0513 12:19:27.435]             Message: ""the server is currently unable to handle the request (get pods metrics-proxy:10259)"",
I0513 12:19:27.435]             Reason: ""ServiceUnavailable"",
I0513 12:19:27.435]             Details: {
I0513 12:19:27.435]                 Name: ""metrics-proxy:10259"",
I0513 12:19:27.435]                 Group: """",
I0513 12:19:27.435]                 Kind: ""pods"",
I0513 12:19:27.435]                 UID: """",
I0513 12:19:27.435]                 Causes: [
I0513 12:19:27.436]                     {
I0513 12:19:27.436]                         Type: ""UnexpectedServerResponse"",
I0513 12:19:27.436]                         Message: ""unknown"",
I0513 12:19:27.436]                         Field: """",
I0513 12:19:27.436]                     },
I0513 12:19:27.436]                 ],
I0513 12:19:27.436]                 RetryAfterSeconds: 0,
I0513 12:19:27.436]             },
I0513 12:19:27.436]             Code: 503,
I0513 12:19:27.437]         },
I0513 12:19:27.437]     }
I0513 12:19:27.437]     the server is currently unable to handle the request (get pods metrics-proxy:10259)
```

It seems that the [scheduler pod had not shown up](https://github.com/kubernetes/kubernetes/blob/09268c16853b233ebaedcd6a877eac23690b5190/test/e2e/framework/metrics/metrics_proxy.go#L43-L50) when we [setup e2e test suite](https://github.com/kubernetes/kubernetes/blob/master/test/e2e/e2e.go#L312), so the nginx config for the forwarder pod was incomplete.

This PR make `SetupMetricsProxy` function wait for desired component pods to show up first.

#### Which issue(s) this PR fixes:
<!--
*Automatically closes linked issue when PR is merged.
Usage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.
_If PR is about `failing-tests or flakes`, please post the related issues/tests in a comment and do not use `Fixes`_*
-->
Fixes #

#### Special notes for your reviewer:

#### Does this PR introduce a user-facing change?
<!--
If no, just write ""NONE"" in the release-note block below.
If yes, a release note is required:
Enter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string ""action required"".

For more information on release notes see: https://git.k8s.io/community/contributors/guide/release-notes.md
-->
```release-note
NONE
```

#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:

<!--
This section can be blank if this pull request does not require a release note.

When adding links which point to resources within git repositories, like
KEPs or supporting documentation, please reference a specific commit and avoid
linking directly to the master branch. This ensures that links reference a
specific point in time, rather than a document that may change over time.

See here for guidance on getting permanent links to files: https://help.github.com/en/articles/getting-permanent-links-to-files

Please use the following format for linking documentation:
- [KEP]: <link>
- [Usage]: <link>
- [Other doc]: <link>
-->
```docs

```
",6mNA7RdX-2S202kL1GqU7RUIV7F3uqPJB_8de6yzLqDu3kaHmpNB1FN68LQcgTCsV5A-V8axwXWCwlaCzb-40ig0UvasyaxOx4wg2TMWK3HxnViSEDD3B2t0KOefbHugeexImbYdsmUJkp_6Nctj2DSK3EH67NwmONYR61Xphnyh9a_CCP3rI6qvmgGD3pbtCaGuiupoBcHW1QniwkM4SlvOoov_g5Sx8l8vLWfaFbLhjC71tRGmH2DEyoXjjvJxe8j4An7bHk5ByGyZAJ3w1kQJlRUU7N9rHUM3kag_Z8tfS4QBEjw,https://github.com/kubernetes/kubernetes
22,101761,"When I try to execute kubectl apply -f mongo-secret.yaml, I get this error ""Error converting from YAML to JSON: mapping values are not allowed in this context"".Can anyone help?


apiVersion: v1
kind: Secret
metadata:
    name: mongodb-secret
type: Opaque
data:
    mongo-root-username: dXNlcm5hbWU=
    mongo-root-password: cGFzc3dvcmQ=
",mongo-root-username,https://github.com/kubernetes/kubernetes
23,101761,"When I try to execute kubectl apply -f mongo-secret.yaml, I get this error ""Error converting from YAML to JSON: mapping values are not allowed in this context"".Can anyone help?


apiVersion: v1
kind: Secret
metadata:
    name: mongodb-secret
type: Opaque
data:
    mongo-root-username: dXNlcm5hbWU=
    mongo-root-password: cGFzc3dvcmQ=
",cGFzc3dvcmQ=,https://github.com/kubernetes/kubernetes
24,98474,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

```
I0120 17:35:46.590403       1 trace.go:116] Trace[1835007712]: ""Delete"" url:/api/v1/namespaces/admin/secrets/k8sconfig-sa-token-6c8hz,user-agent:kube-controller-manager/v1.17.9 (linux/amd64) kubernetes/59603c6/tokens-controller,client:1111::d15f (started: 2021-01-20 17:35:45.193170835 +0800 CST m=+196.151374014) (total time: 1.397188005s):
Trace[1835007712]: [1.397148638s] [1.397101146s] Object deleted from database
```

```
E0120 17:35:47.026101   70870 nestedpendingoperations.go:290] Operation for ""\""kubernetes.io/secret/ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8-k8sconfig-sa-token-6c8hz\"" (\""ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8\"")"" failed. No retries permitted until 2021-01-20 17:35:48.02606438 +0800 CST m=+183.395522570 (durationBeforeRetry 1s). Error: ""MountVolume.SetUp failed for volume \""k8sconfig-sa-token-6c8hz\"" (UniqueName: \""kubernetes.io/secret/ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8-k8sconfig-sa-token-6c8hz\"") pod \""op-containers-k8sconfig-1-jz7pb\"" (UID: \""ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8\"") : secret \""k8sconfig-sa-token-6c8hz\"" not found""
E0120 17:37:55.849529   70870 secret.go:195] Couldn't get secret admin/k8sconfig-sa-token-6c8hz: secret ""k8sconfig-sa-token-6c8hz"" not found
```

I found in the code of `tokenController` that a secret may be deleted due to various reasons, such as failed  updating the secret reference in service account:

```
		// Try to add a reference to the token
		liveServiceAccount.Secrets = append(liveServiceAccount.Secrets, v1.ObjectReference{Name: secret.Name})
		if _, err := serviceAccounts.Update(context.TODO(), liveServiceAccount, metav1.UpdateOptions{}); err != nil {
			return err
		}

		addedReference = true
		return nil
	})

	if !addedReference {
		// we weren't able to use the token, try to clean it up.
		klog.V(2).Infof(""deleting secret %s/%s because reference couldn't be added (%v)"", secret.Namespace, secret.Name, err)
		deleteOpts := metav1.DeleteOptions{Preconditions: &metav1.Preconditions{UID: &createdToken.UID}}
		if err := e.client.CoreV1().Secrets(createdToken.Namespace).Delete(context.TODO(), createdToken.Name, deleteOpts); err != nil {
			klog.Error(err) // if we fail, just log it
		}
	}
``` 

But the are many scenes, kubeClient create a serviceaccount, and then it watches secrets until it find the first secret passes kube-apiserver's authentication(For example, the `SAControllerClientBuilder.Config(name string) (*restclient.Config, error)` method create a controller's sa and watch its secret to build a sa based controller client). In the tokenController's main goroutine, this secret was deleted after that, this case the newly build client failed with Unauthorized error.


**What you expected to happen**:

Once the secret of a sa is created, we cannot  restrict user use it before tokenController successfully updates the sa's secret reference. 

Is that possible to:

- make the secret failed to authenticate  before tokenController successfully updates the sa's secret reference?
- treat the creation of secret and the update of sa's secret reference as a transaction?
- do not delete the secret until tokenController update sa's secret reference successfully?

**How to reproduce it (as minimally and precisely as possible)**:

This may happens etcd works at a low IO disks or some other processes squeeze the disk IO.

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):  v1.19.4  
- Cloud provider or hardware configuration:
- OS (e.g: `cat /etc/os-release`): Centos 7
- Kernel (e.g. `uname -a`): 3.10.0-693.21.1.el7.x86_64 #1 SMP Sat Dec 12 11:46:01 CST 2020 x86_64 x86_64 x86_64 GNU/Linux
- Install tools:
- Network plugin and version (if this is a network-related bug):
- Others:
",ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8,https://github.com/kubernetes/kubernetes
25,98474,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

```
I0120 17:35:46.590403       1 trace.go:116] Trace[1835007712]: ""Delete"" url:/api/v1/namespaces/admin/secrets/k8sconfig-sa-token-6c8hz,user-agent:kube-controller-manager/v1.17.9 (linux/amd64) kubernetes/59603c6/tokens-controller,client:1111::d15f (started: 2021-01-20 17:35:45.193170835 +0800 CST m=+196.151374014) (total time: 1.397188005s):
Trace[1835007712]: [1.397148638s] [1.397101146s] Object deleted from database
```

```
E0120 17:35:47.026101   70870 nestedpendingoperations.go:290] Operation for ""\""kubernetes.io/secret/ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8-k8sconfig-sa-token-6c8hz\"" (\""ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8\"")"" failed. No retries permitted until 2021-01-20 17:35:48.02606438 +0800 CST m=+183.395522570 (durationBeforeRetry 1s). Error: ""MountVolume.SetUp failed for volume \""k8sconfig-sa-token-6c8hz\"" (UniqueName: \""kubernetes.io/secret/ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8-k8sconfig-sa-token-6c8hz\"") pod \""op-containers-k8sconfig-1-jz7pb\"" (UID: \""ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8\"") : secret \""k8sconfig-sa-token-6c8hz\"" not found""
E0120 17:37:55.849529   70870 secret.go:195] Couldn't get secret admin/k8sconfig-sa-token-6c8hz: secret ""k8sconfig-sa-token-6c8hz"" not found
```

I found in the code of `tokenController` that a secret may be deleted due to various reasons, such as failed  updating the secret reference in service account:

```
		// Try to add a reference to the token
		liveServiceAccount.Secrets = append(liveServiceAccount.Secrets, v1.ObjectReference{Name: secret.Name})
		if _, err := serviceAccounts.Update(context.TODO(), liveServiceAccount, metav1.UpdateOptions{}); err != nil {
			return err
		}

		addedReference = true
		return nil
	})

	if !addedReference {
		// we weren't able to use the token, try to clean it up.
		klog.V(2).Infof(""deleting secret %s/%s because reference couldn't be added (%v)"", secret.Namespace, secret.Name, err)
		deleteOpts := metav1.DeleteOptions{Preconditions: &metav1.Preconditions{UID: &createdToken.UID}}
		if err := e.client.CoreV1().Secrets(createdToken.Namespace).Delete(context.TODO(), createdToken.Name, deleteOpts); err != nil {
			klog.Error(err) // if we fail, just log it
		}
	}
``` 

But the are many scenes, kubeClient create a serviceaccount, and then it watches secrets until it find the first secret passes kube-apiserver's authentication(For example, the `SAControllerClientBuilder.Config(name string) (*restclient.Config, error)` method create a controller's sa and watch its secret to build a sa based controller client). In the tokenController's main goroutine, this secret was deleted after that, this case the newly build client failed with Unauthorized error.


**What you expected to happen**:

Once the secret of a sa is created, we cannot  restrict user use it before tokenController successfully updates the sa's secret reference. 

Is that possible to:

- make the secret failed to authenticate  before tokenController successfully updates the sa's secret reference?
- treat the creation of secret and the update of sa's secret reference as a transaction?
- do not delete the secret until tokenController update sa's secret reference successfully?

**How to reproduce it (as minimally and precisely as possible)**:

This may happens etcd works at a low IO disks or some other processes squeeze the disk IO.

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):  v1.19.4  
- Cloud provider or hardware configuration:
- OS (e.g: `cat /etc/os-release`): Centos 7
- Kernel (e.g. `uname -a`): 3.10.0-693.21.1.el7.x86_64 #1 SMP Sat Dec 12 11:46:01 CST 2020 x86_64 x86_64 x86_64 GNU/Linux
- Install tools:
- Network plugin and version (if this is a network-related bug):
- Others:
",k8sconfig-sa-token-6c8hz,https://github.com/kubernetes/kubernetes
26,98474,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

```
I0120 17:35:46.590403       1 trace.go:116] Trace[1835007712]: ""Delete"" url:/api/v1/namespaces/admin/secrets/k8sconfig-sa-token-6c8hz,user-agent:kube-controller-manager/v1.17.9 (linux/amd64) kubernetes/59603c6/tokens-controller,client:1111::d15f (started: 2021-01-20 17:35:45.193170835 +0800 CST m=+196.151374014) (total time: 1.397188005s):
Trace[1835007712]: [1.397148638s] [1.397101146s] Object deleted from database
```

```
E0120 17:35:47.026101   70870 nestedpendingoperations.go:290] Operation for ""\""kubernetes.io/secret/ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8-k8sconfig-sa-token-6c8hz\"" (\""ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8\"")"" failed. No retries permitted until 2021-01-20 17:35:48.02606438 +0800 CST m=+183.395522570 (durationBeforeRetry 1s). Error: ""MountVolume.SetUp failed for volume \""k8sconfig-sa-token-6c8hz\"" (UniqueName: \""kubernetes.io/secret/ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8-k8sconfig-sa-token-6c8hz\"") pod \""op-containers-k8sconfig-1-jz7pb\"" (UID: \""ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8\"") : secret \""k8sconfig-sa-token-6c8hz\"" not found""
E0120 17:37:55.849529   70870 secret.go:195] Couldn't get secret admin/k8sconfig-sa-token-6c8hz: secret ""k8sconfig-sa-token-6c8hz"" not found
```

I found in the code of `tokenController` that a secret may be deleted due to various reasons, such as failed  updating the secret reference in service account:

```
		// Try to add a reference to the token
		liveServiceAccount.Secrets = append(liveServiceAccount.Secrets, v1.ObjectReference{Name: secret.Name})
		if _, err := serviceAccounts.Update(context.TODO(), liveServiceAccount, metav1.UpdateOptions{}); err != nil {
			return err
		}

		addedReference = true
		return nil
	})

	if !addedReference {
		// we weren't able to use the token, try to clean it up.
		klog.V(2).Infof(""deleting secret %s/%s because reference couldn't be added (%v)"", secret.Namespace, secret.Name, err)
		deleteOpts := metav1.DeleteOptions{Preconditions: &metav1.Preconditions{UID: &createdToken.UID}}
		if err := e.client.CoreV1().Secrets(createdToken.Namespace).Delete(context.TODO(), createdToken.Name, deleteOpts); err != nil {
			klog.Error(err) // if we fail, just log it
		}
	}
``` 

But the are many scenes, kubeClient create a serviceaccount, and then it watches secrets until it find the first secret passes kube-apiserver's authentication(For example, the `SAControllerClientBuilder.Config(name string) (*restclient.Config, error)` method create a controller's sa and watch its secret to build a sa based controller client). In the tokenController's main goroutine, this secret was deleted after that, this case the newly build client failed with Unauthorized error.


**What you expected to happen**:

Once the secret of a sa is created, we cannot  restrict user use it before tokenController successfully updates the sa's secret reference. 

Is that possible to:

- make the secret failed to authenticate  before tokenController successfully updates the sa's secret reference?
- treat the creation of secret and the update of sa's secret reference as a transaction?
- do not delete the secret until tokenController update sa's secret reference successfully?

**How to reproduce it (as minimally and precisely as possible)**:

This may happens etcd works at a low IO disks or some other processes squeeze the disk IO.

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):  v1.19.4  
- Cloud provider or hardware configuration:
- OS (e.g: `cat /etc/os-release`): Centos 7
- Kernel (e.g. `uname -a`): 3.10.0-693.21.1.el7.x86_64 #1 SMP Sat Dec 12 11:46:01 CST 2020 x86_64 x86_64 x86_64 GNU/Linux
- Install tools:
- Network plugin and version (if this is a network-related bug):
- Others:
",containers-k8sconfig-1-jz7pb,https://github.com/kubernetes/kubernetes
27,98474,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

```
I0120 17:35:46.590403       1 trace.go:116] Trace[1835007712]: ""Delete"" url:/api/v1/namespaces/admin/secrets/k8sconfig-sa-token-6c8hz,user-agent:kube-controller-manager/v1.17.9 (linux/amd64) kubernetes/59603c6/tokens-controller,client:1111::d15f (started: 2021-01-20 17:35:45.193170835 +0800 CST m=+196.151374014) (total time: 1.397188005s):
Trace[1835007712]: [1.397148638s] [1.397101146s] Object deleted from database
```

```
E0120 17:35:47.026101   70870 nestedpendingoperations.go:290] Operation for ""\""kubernetes.io/secret/ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8-k8sconfig-sa-token-6c8hz\"" (\""ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8\"")"" failed. No retries permitted until 2021-01-20 17:35:48.02606438 +0800 CST m=+183.395522570 (durationBeforeRetry 1s). Error: ""MountVolume.SetUp failed for volume \""k8sconfig-sa-token-6c8hz\"" (UniqueName: \""kubernetes.io/secret/ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8-k8sconfig-sa-token-6c8hz\"") pod \""op-containers-k8sconfig-1-jz7pb\"" (UID: \""ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8\"") : secret \""k8sconfig-sa-token-6c8hz\"" not found""
E0120 17:37:55.849529   70870 secret.go:195] Couldn't get secret admin/k8sconfig-sa-token-6c8hz: secret ""k8sconfig-sa-token-6c8hz"" not found
```

I found in the code of `tokenController` that a secret may be deleted due to various reasons, such as failed  updating the secret reference in service account:

```
		// Try to add a reference to the token
		liveServiceAccount.Secrets = append(liveServiceAccount.Secrets, v1.ObjectReference{Name: secret.Name})
		if _, err := serviceAccounts.Update(context.TODO(), liveServiceAccount, metav1.UpdateOptions{}); err != nil {
			return err
		}

		addedReference = true
		return nil
	})

	if !addedReference {
		// we weren't able to use the token, try to clean it up.
		klog.V(2).Infof(""deleting secret %s/%s because reference couldn't be added (%v)"", secret.Namespace, secret.Name, err)
		deleteOpts := metav1.DeleteOptions{Preconditions: &metav1.Preconditions{UID: &createdToken.UID}}
		if err := e.client.CoreV1().Secrets(createdToken.Namespace).Delete(context.TODO(), createdToken.Name, deleteOpts); err != nil {
			klog.Error(err) // if we fail, just log it
		}
	}
``` 

But the are many scenes, kubeClient create a serviceaccount, and then it watches secrets until it find the first secret passes kube-apiserver's authentication(For example, the `SAControllerClientBuilder.Config(name string) (*restclient.Config, error)` method create a controller's sa and watch its secret to build a sa based controller client). In the tokenController's main goroutine, this secret was deleted after that, this case the newly build client failed with Unauthorized error.


**What you expected to happen**:

Once the secret of a sa is created, we cannot  restrict user use it before tokenController successfully updates the sa's secret reference. 

Is that possible to:

- make the secret failed to authenticate  before tokenController successfully updates the sa's secret reference?
- treat the creation of secret and the update of sa's secret reference as a transaction?
- do not delete the secret until tokenController update sa's secret reference successfully?

**How to reproduce it (as minimally and precisely as possible)**:

This may happens etcd works at a low IO disks or some other processes squeeze the disk IO.

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):  v1.19.4  
- Cloud provider or hardware configuration:
- OS (e.g: `cat /etc/os-release`): Centos 7
- Kernel (e.g. `uname -a`): 3.10.0-693.21.1.el7.x86_64 #1 SMP Sat Dec 12 11:46:01 CST 2020 x86_64 x86_64 x86_64 GNU/Linux
- Install tools:
- Network plugin and version (if this is a network-related bug):
- Others:
",authentication(For,https://github.com/kubernetes/kubernetes
28,95105,"Centos7, three servers are installed with k8s cluster, one master, other slave1 and slave2. The three nodes are installed with CEPH for data persistence.

the cephfs on master /mnt/cephfs directroy
master:6789,slave1:6789,slave2:6789:/ /mnt/cephfs ceph name=cephfs,secretfile=/etc/ceph/cephfskey,_netdev,noatime 0 0

Create deployment through the python API of kubernetes client, and mount three directories at the same time,
One is to dynamically hang to pod through PVC
Here's secret
apiVersion: v1
kind: Secret
metadata:
name: ceph-secret
namespace: espp-ns
type: ""kubernetes.io/rbd""
data:
key:

storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
name: ceph-espp
provisioner: kubernetes.io/rbd
parameters:
monitors: 10.197.1.11, 10.197.1.12, 10.197.1.13
adminId: admin
adminSecretName: ceph-secret
adminSecretNamespace: espp-ns
pool: rbd 
userId: admin
userSecretName: ceph-secret
reclaimPolicy: Retain

The other two directories are mounted through cephfs.
secret.yml
apiVersion: v1
kind: Secret
metadata:
name: cephfs-secret-fs
namespace: espp-ns
type: ""kubernetes.io/rbd""
data:
key: AQBJrUxfZiH6NxAAljeXDd/DBYBiHx64PJ+shQ==

**Problem**：All created pods are in the master node, not assigned to the other two nodes. At the same time, the created PVC and PV are also mounted on the master

below is on deployment example:
espp-ns-deployment-1-1-2-0-6fc6cd65c4-8lq9m 1/1 Running 0 13h 172.30.22.5 **master**

and the mount information:
172.18.101.10:6789,172.18.101.11:6789,172.18.101.12:6789:/studentenv/dataset_stuenv_2_1/ 1.2T 244M 1.2T 1% /var/lib/kubelet/pods/1d40454b-00fc-4847-8948-7b1d73e1bb98/volumes/kubernetes.iocephfs/espp-ns-deployment-1-1-2-0-pv-fs-stuenv-0-1
172.18.101.10:6789,172.18.101.11:6789,172.18.101.12:6789:/lessonplan/dataset_lp_1_1_19/ 1.2T 244M 1.2T 1% /var/lib/kubelet/pods/1d40454b-00fc-4847-8948-7b1d73e1bb98/volumes/kubernetes.iocephfs/espp-ns-deployment-1-1-2-0-pv-fs-stuenv-0-0
/dev/rbd0 20G 387M 20G 2% /var/lib/kubelet/plugins/kubernetes.io/rbd/mounts/rbd-image-kubernetes-dynamic-pvc-95ce6a24-e7fe-4456-b008-d53428be6352
overlay

thanks",adminSecretNamespace,https://github.com/kubernetes/kubernetes
29,95105,"Centos7, three servers are installed with k8s cluster, one master, other slave1 and slave2. The three nodes are installed with CEPH for data persistence.

the cephfs on master /mnt/cephfs directroy
master:6789,slave1:6789,slave2:6789:/ /mnt/cephfs ceph name=cephfs,secretfile=/etc/ceph/cephfskey,_netdev,noatime 0 0

Create deployment through the python API of kubernetes client, and mount three directories at the same time,
One is to dynamically hang to pod through PVC
Here's secret
apiVersion: v1
kind: Secret
metadata:
name: ceph-secret
namespace: espp-ns
type: ""kubernetes.io/rbd""
data:
key:

storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
name: ceph-espp
provisioner: kubernetes.io/rbd
parameters:
monitors: 10.197.1.11, 10.197.1.12, 10.197.1.13
adminId: admin
adminSecretName: ceph-secret
adminSecretNamespace: espp-ns
pool: rbd 
userId: admin
userSecretName: ceph-secret
reclaimPolicy: Retain

The other two directories are mounted through cephfs.
secret.yml
apiVersion: v1
kind: Secret
metadata:
name: cephfs-secret-fs
namespace: espp-ns
type: ""kubernetes.io/rbd""
data:
key: AQBJrUxfZiH6NxAAljeXDd/DBYBiHx64PJ+shQ==

**Problem**：All created pods are in the master node, not assigned to the other two nodes. At the same time, the created PVC and PV are also mounted on the master

below is on deployment example:
espp-ns-deployment-1-1-2-0-6fc6cd65c4-8lq9m 1/1 Running 0 13h 172.30.22.5 **master**

and the mount information:
172.18.101.10:6789,172.18.101.11:6789,172.18.101.12:6789:/studentenv/dataset_stuenv_2_1/ 1.2T 244M 1.2T 1% /var/lib/kubelet/pods/1d40454b-00fc-4847-8948-7b1d73e1bb98/volumes/kubernetes.iocephfs/espp-ns-deployment-1-1-2-0-pv-fs-stuenv-0-1
172.18.101.10:6789,172.18.101.11:6789,172.18.101.12:6789:/lessonplan/dataset_lp_1_1_19/ 1.2T 244M 1.2T 1% /var/lib/kubelet/pods/1d40454b-00fc-4847-8948-7b1d73e1bb98/volumes/kubernetes.iocephfs/espp-ns-deployment-1-1-2-0-pv-fs-stuenv-0-0
/dev/rbd0 20G 387M 20G 2% /var/lib/kubelet/plugins/kubernetes.io/rbd/mounts/rbd-image-kubernetes-dynamic-pvc-95ce6a24-e7fe-4456-b008-d53428be6352
overlay

thanks",cephfs-secret-fs,https://github.com/kubernetes/kubernetes
30,95105,"Centos7, three servers are installed with k8s cluster, one master, other slave1 and slave2. The three nodes are installed with CEPH for data persistence.

the cephfs on master /mnt/cephfs directroy
master:6789,slave1:6789,slave2:6789:/ /mnt/cephfs ceph name=cephfs,secretfile=/etc/ceph/cephfskey,_netdev,noatime 0 0

Create deployment through the python API of kubernetes client, and mount three directories at the same time,
One is to dynamically hang to pod through PVC
Here's secret
apiVersion: v1
kind: Secret
metadata:
name: ceph-secret
namespace: espp-ns
type: ""kubernetes.io/rbd""
data:
key:

storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
name: ceph-espp
provisioner: kubernetes.io/rbd
parameters:
monitors: 10.197.1.11, 10.197.1.12, 10.197.1.13
adminId: admin
adminSecretName: ceph-secret
adminSecretNamespace: espp-ns
pool: rbd 
userId: admin
userSecretName: ceph-secret
reclaimPolicy: Retain

The other two directories are mounted through cephfs.
secret.yml
apiVersion: v1
kind: Secret
metadata:
name: cephfs-secret-fs
namespace: espp-ns
type: ""kubernetes.io/rbd""
data:
key: AQBJrUxfZiH6NxAAljeXDd/DBYBiHx64PJ+shQ==

**Problem**：All created pods are in the master node, not assigned to the other two nodes. At the same time, the created PVC and PV are also mounted on the master

below is on deployment example:
espp-ns-deployment-1-1-2-0-6fc6cd65c4-8lq9m 1/1 Running 0 13h 172.30.22.5 **master**

and the mount information:
172.18.101.10:6789,172.18.101.11:6789,172.18.101.12:6789:/studentenv/dataset_stuenv_2_1/ 1.2T 244M 1.2T 1% /var/lib/kubelet/pods/1d40454b-00fc-4847-8948-7b1d73e1bb98/volumes/kubernetes.iocephfs/espp-ns-deployment-1-1-2-0-pv-fs-stuenv-0-1
172.18.101.10:6789,172.18.101.11:6789,172.18.101.12:6789:/lessonplan/dataset_lp_1_1_19/ 1.2T 244M 1.2T 1% /var/lib/kubelet/pods/1d40454b-00fc-4847-8948-7b1d73e1bb98/volumes/kubernetes.iocephfs/espp-ns-deployment-1-1-2-0-pv-fs-stuenv-0-0
/dev/rbd0 20G 387M 20G 2% /var/lib/kubelet/plugins/kubernetes.io/rbd/mounts/rbd-image-kubernetes-dynamic-pvc-95ce6a24-e7fe-4456-b008-d53428be6352
overlay

thanks","=,_netdev,noatime",https://github.com/kubernetes/kubernetes
31,95105,"Centos7, three servers are installed with k8s cluster, one master, other slave1 and slave2. The three nodes are installed with CEPH for data persistence.

the cephfs on master /mnt/cephfs directroy
master:6789,slave1:6789,slave2:6789:/ /mnt/cephfs ceph name=cephfs,secretfile=/etc/ceph/cephfskey,_netdev,noatime 0 0

Create deployment through the python API of kubernetes client, and mount three directories at the same time,
One is to dynamically hang to pod through PVC
Here's secret
apiVersion: v1
kind: Secret
metadata:
name: ceph-secret
namespace: espp-ns
type: ""kubernetes.io/rbd""
data:
key:

storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
name: ceph-espp
provisioner: kubernetes.io/rbd
parameters:
monitors: 10.197.1.11, 10.197.1.12, 10.197.1.13
adminId: admin
adminSecretName: ceph-secret
adminSecretNamespace: espp-ns
pool: rbd 
userId: admin
userSecretName: ceph-secret
reclaimPolicy: Retain

The other two directories are mounted through cephfs.
secret.yml
apiVersion: v1
kind: Secret
metadata:
name: cephfs-secret-fs
namespace: espp-ns
type: ""kubernetes.io/rbd""
data:
key: AQBJrUxfZiH6NxAAljeXDd/DBYBiHx64PJ+shQ==

**Problem**：All created pods are in the master node, not assigned to the other two nodes. At the same time, the created PVC and PV are also mounted on the master

below is on deployment example:
espp-ns-deployment-1-1-2-0-6fc6cd65c4-8lq9m 1/1 Running 0 13h 172.30.22.5 **master**

and the mount information:
172.18.101.10:6789,172.18.101.11:6789,172.18.101.12:6789:/studentenv/dataset_stuenv_2_1/ 1.2T 244M 1.2T 1% /var/lib/kubelet/pods/1d40454b-00fc-4847-8948-7b1d73e1bb98/volumes/kubernetes.iocephfs/espp-ns-deployment-1-1-2-0-pv-fs-stuenv-0-1
172.18.101.10:6789,172.18.101.11:6789,172.18.101.12:6789:/lessonplan/dataset_lp_1_1_19/ 1.2T 244M 1.2T 1% /var/lib/kubelet/pods/1d40454b-00fc-4847-8948-7b1d73e1bb98/volumes/kubernetes.iocephfs/espp-ns-deployment-1-1-2-0-pv-fs-stuenv-0-0
/dev/rbd0 20G 387M 20G 2% /var/lib/kubelet/plugins/kubernetes.io/rbd/mounts/rbd-image-kubernetes-dynamic-pvc-95ce6a24-e7fe-4456-b008-d53428be6352
overlay

thanks",AQBJrUxfZiH6NxAAljeXDd+shQ,https://github.com/kubernetes/kubernetes
32,95105,"Centos7, three servers are installed with k8s cluster, one master, other slave1 and slave2. The three nodes are installed with CEPH for data persistence.

the cephfs on master /mnt/cephfs directroy
master:6789,slave1:6789,slave2:6789:/ /mnt/cephfs ceph name=cephfs,secretfile=/etc/ceph/cephfskey,_netdev,noatime 0 0

Create deployment through the python API of kubernetes client, and mount three directories at the same time,
One is to dynamically hang to pod through PVC
Here's secret
apiVersion: v1
kind: Secret
metadata:
name: ceph-secret
namespace: espp-ns
type: ""kubernetes.io/rbd""
data:
key:

storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
name: ceph-espp
provisioner: kubernetes.io/rbd
parameters:
monitors: 10.197.1.11, 10.197.1.12, 10.197.1.13
adminId: admin
adminSecretName: ceph-secret
adminSecretNamespace: espp-ns
pool: rbd 
userId: admin
userSecretName: ceph-secret
reclaimPolicy: Retain

The other two directories are mounted through cephfs.
secret.yml
apiVersion: v1
kind: Secret
metadata:
name: cephfs-secret-fs
namespace: espp-ns
type: ""kubernetes.io/rbd""
data:
key: AQBJrUxfZiH6NxAAljeXDd/DBYBiHx64PJ+shQ==

**Problem**：All created pods are in the master node, not assigned to the other two nodes. At the same time, the created PVC and PV are also mounted on the master

below is on deployment example:
espp-ns-deployment-1-1-2-0-6fc6cd65c4-8lq9m 1/1 Running 0 13h 172.30.22.5 **master**

and the mount information:
172.18.101.10:6789,172.18.101.11:6789,172.18.101.12:6789:/studentenv/dataset_stuenv_2_1/ 1.2T 244M 1.2T 1% /var/lib/kubelet/pods/1d40454b-00fc-4847-8948-7b1d73e1bb98/volumes/kubernetes.iocephfs/espp-ns-deployment-1-1-2-0-pv-fs-stuenv-0-1
172.18.101.10:6789,172.18.101.11:6789,172.18.101.12:6789:/lessonplan/dataset_lp_1_1_19/ 1.2T 244M 1.2T 1% /var/lib/kubelet/pods/1d40454b-00fc-4847-8948-7b1d73e1bb98/volumes/kubernetes.iocephfs/espp-ns-deployment-1-1-2-0-pv-fs-stuenv-0-0
/dev/rbd0 20G 387M 20G 2% /var/lib/kubelet/plugins/kubernetes.io/rbd/mounts/rbd-image-kubernetes-dynamic-pvc-95ce6a24-e7fe-4456-b008-d53428be6352
overlay

thanks",AQBJrUxfZiH6NxAAljeXDd,https://github.com/kubernetes/kubernetes
33,91992,"**What happened**:

Hey everyone! Not sure why this is happening, I have not tried to set up HA k8s before this. Running normal kubeadm init works.... but I am trying to use HA Kubernetes and getting these following errors. Any pointers would be GREAT.

While installing HA kubernetes via command line with kubeadm, I am repeadetly blocked by kubelet message:

kubeadm init --config config.yaml --upload-certs

Contents of config.yaml:
```
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress:
  bindPort: 6444 #Changed this to 6444 per some documents I read through on GitHub issues... also tried with 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: 
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
controlPlaneEndpoint: ""LoadBalancerDNS:6443""
kubernetesVersion: v1.17.3
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
address: 0.0.0.0
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
cgroupDriver: systemd
cgroupsPerQOS: true
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
configMapAndSecretChangeDetectionStrategy: Watch
containerLogMaxFiles: 5
containerLogMaxSize: 10Mi
contentType: application/vnd.kubernetes.protobuf
cpuCFSQuota: true
cpuCFSQuotaPeriod: 100ms
cpuManagerPolicy: none
cpuManagerReconcilePeriod: 10s
enableControllerAttachDetach: true
enableDebuggingHandlers: true
enforceNodeAllocatable:
- pods
eventBurst: 10
eventRecordQPS: 5
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
evictionPressureTransitionPeriod: 5m0s
failSwapOn: false
fileCheckFrequency: 20s
hairpinMode: promiscuous-bridge
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 20s
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
imageMinimumGCAge: 2m0s
iptablesDropBit: 15
iptablesMasqueradeBit: 14
kind: KubeletConfiguration
kubeAPIBurst: 10
kubeAPIQPS: 5
makeIPTablesUtilChains: true
maxOpenFiles: 1000000
maxPods: 110
nodeLeaseDurationSeconds: 80 #changed from 40
nodeStatusReportFrequency: 1m0s
nodeStatusUpdateFrequency: 10s
oomScoreAdj: -999
podPidsLimit: -1
port: 10250
registryBurst: 10
registryPullQPS: 5
resolvConf: /etc/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 4m0s
serializeImagePulls: true
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 4h0m0s
syncFrequency: 1m0s
topologyManagerPolicy: none
volumeStatsAggPeriod: 1m0s
```
Failed to list *v1.Node: Get **https://load-balancer-dns:6443/api/v1/nodes?fieldSelector=metadata.name%3DCurrentMasterNodeName&limit=500&resourceVersion=0:** x509: certificate signed by unknown authority

This is the same for all:
Failed to list *v1beta1.CSIDriver
Failed to list *v1.Service
Failed to list *v1.Node
Failed to get status for pod ""etcd-NodeNameMaster

**For all of them, this is ending with x509: certificate signed by unknown authority**

**What you expected to happen**:

I expected Kubernetes to set up the first control plane node for me and use the F5 load balancer that I provided

**How to reproduce it (as minimally and precisely as possible)**:

Follow all docs here:
- https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/
My set up is for a stacked control plane

**Anything else we need to know?**:

Tried the following and set these params:
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward = 1

Driver for Docker and kubelet: **systemd**



**Environment**:
- Kubernetes version (use `kubectl version`):
kubeadm kubelet kubectl versions: v1.17.3
Docker version 19.03.11
- Cloud provider or hardware configuration:
Bare-metal with an F5 load balancer instead of HAproxy LB
- OS (e.g: `cat /etc/os-release`):
NAME=""Red Hat Enterprise Linux Server""
VERSION=""7.7 (Maipo)""
ID=""rhel""
ID_LIKE=""fedora""
VARIANT=""Server""
VARIANT_ID=""server""
VERSION_ID=""7.7""

- Kernel (e.g. `uname -a`):
Linux MasterNode 3.10.0-1062.18.1.el7.x86_64 #1 SMP Wed Feb 12 14:08:31 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
- Install tools:
yum
- Network plugin and version (if this is a network-related bug):
- Others:

```
Jun 10 11:19:30 MasterNode kubelet[3195]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-
Jun 10 11:19:30 MasterNode kubelet[3195]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964228    3195 server.go:416] Version: v1.17.3
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964703    3195 plugins.go:100] No cloud provider specified.
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964939    3195 server.go:821] Client rotation is on, will bootstrap in background
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.967369    3195 certificate_store.go:129] Loading cert/key pair from ""/var/lib/kubelet/pki/kubelet-client-current.pem"".
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054304    3195 server.go:641] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054653    3195 container_manager_linux.go:265] container manager verified user specified cgroup-root exists: []
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054687    3195 container_manager_linux.go:270] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: Con
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054789    3195 fake_topology_manager.go:29] [fake topologymanager] NewFakeManager
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054794    3195 container_manager_linux.go:305] Creating device plugin manager: true
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054826    3195 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider:  &{kubelet.sock /var/lib/kubelet/device-plugins/ map[] {0 0} <nil> {{
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054872    3195 state_mem.go:36] [cpumanager] initializing new in-memory state store
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054979    3195 state_mem.go:84] [cpumanager] updated default cpuset: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054985    3195 state_mem.go:92] [cpumanager] updated cpuset assignments: ""map[]""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054994    3195 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider:  &{{0 0} 0x6e9bc50 10000000000 0xc000ace6c0 <nil> <nil> <nil> <nil> m
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.055060    3195 kubelet.go:286] Adding pod path: /etc/kubernetes/manifests
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.055115    3195 kubelet.go:311] Watching apiserver
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.069097    3195 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.069444    3195 client.go:104] Start docker client with request timeout=4m0s
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.076542    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.077278    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.077500    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.079822    3195 docker_service.go:563] Hairpin mode set to ""promiscuous-bridge"" but kubenet is not enabled, falling back to ""hairpin-veth""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.079844    3195 docker_service.go:240] Hairpin mode set to ""hairpin-veth""
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.079937    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.083724    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.083778    3195 docker_service.go:255] Docker cri networking managed by cni
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.083836    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.093083    3195 docker_service.go:260] Docker Info: &{ID:CKKA:YUJL:2557:3IQE:7MRG:35J3:B2MN:GC3H:3WMM:FCV4:R2BU:TQW4 Containers:8 ContainersRunning:8 ContainersPaused:
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.093148    3195 docker_service.go:273] Setting cgroupDriver to systemd
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103239    3195 remote_runtime.go:59] parsed scheme: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103260    3195 remote_runtime.go:59] scheme """" not registered, fallback to default scheme
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103297    3195 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock 0  <nil>}] <nil>}
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103321    3195 clientconn.go:577] ClientConn switching balancer to ""pick_first""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103352    3195 remote_image.go:50] parsed scheme: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103358    3195 remote_image.go:50] scheme """" not registered, fallback to default scheme
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103366    3195 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock 0  <nil>}] <nil>}
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103370    3195 clientconn.go:577] ClientConn switching balancer to ""pick_first""
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.083448    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.086800    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.086872    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.090523    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.094073    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.094838    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.097327    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.102600    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.103424    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.108470    3195 aws_credentials.go:77] while getting AWS credentials NoCredentialProviders: no valid providers in chain. Deprecated.
Jun 10 11:19:34 MasterNode kubelet[3195]: For verbose messaging see aws.Config.CredentialsChainVerboseErrors
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.121933    3195 kuberuntime_manager.go:211] Container runtime docker initialized, version: 19.03.11, apiVersion: 1.40.0
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.135482    3195 server.go:1113] Started kubelet
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.135999    3195 kubelet.go:1302] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.136895    3195 fs_resource_analyzer.go:64] Starting FS ResourceAnalyzer
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.138046    3195 server.go:144] Starting to listen on 0.0.0.0:10250
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.140208    3195 server.go:384] Adding debug handlers to kubelet server.
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.151892    3195 volume_manager.go:265] Starting Kubelet Volume Manager
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.152428    3195 desired_state_of_world_populator.go:138] Desired state populator starts to run
```",streamingConnectionIdleTimeout,https://github.com/kubernetes/kubernetes
34,91992,"**What happened**:

Hey everyone! Not sure why this is happening, I have not tried to set up HA k8s before this. Running normal kubeadm init works.... but I am trying to use HA Kubernetes and getting these following errors. Any pointers would be GREAT.

While installing HA kubernetes via command line with kubeadm, I am repeadetly blocked by kubelet message:

kubeadm init --config config.yaml --upload-certs

Contents of config.yaml:
```
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress:
  bindPort: 6444 #Changed this to 6444 per some documents I read through on GitHub issues... also tried with 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: 
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
controlPlaneEndpoint: ""LoadBalancerDNS:6443""
kubernetesVersion: v1.17.3
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
address: 0.0.0.0
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
cgroupDriver: systemd
cgroupsPerQOS: true
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
configMapAndSecretChangeDetectionStrategy: Watch
containerLogMaxFiles: 5
containerLogMaxSize: 10Mi
contentType: application/vnd.kubernetes.protobuf
cpuCFSQuota: true
cpuCFSQuotaPeriod: 100ms
cpuManagerPolicy: none
cpuManagerReconcilePeriod: 10s
enableControllerAttachDetach: true
enableDebuggingHandlers: true
enforceNodeAllocatable:
- pods
eventBurst: 10
eventRecordQPS: 5
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
evictionPressureTransitionPeriod: 5m0s
failSwapOn: false
fileCheckFrequency: 20s
hairpinMode: promiscuous-bridge
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 20s
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
imageMinimumGCAge: 2m0s
iptablesDropBit: 15
iptablesMasqueradeBit: 14
kind: KubeletConfiguration
kubeAPIBurst: 10
kubeAPIQPS: 5
makeIPTablesUtilChains: true
maxOpenFiles: 1000000
maxPods: 110
nodeLeaseDurationSeconds: 80 #changed from 40
nodeStatusReportFrequency: 1m0s
nodeStatusUpdateFrequency: 10s
oomScoreAdj: -999
podPidsLimit: -1
port: 10250
registryBurst: 10
registryPullQPS: 5
resolvConf: /etc/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 4m0s
serializeImagePulls: true
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 4h0m0s
syncFrequency: 1m0s
topologyManagerPolicy: none
volumeStatsAggPeriod: 1m0s
```
Failed to list *v1.Node: Get **https://load-balancer-dns:6443/api/v1/nodes?fieldSelector=metadata.name%3DCurrentMasterNodeName&limit=500&resourceVersion=0:** x509: certificate signed by unknown authority

This is the same for all:
Failed to list *v1beta1.CSIDriver
Failed to list *v1.Service
Failed to list *v1.Node
Failed to get status for pod ""etcd-NodeNameMaster

**For all of them, this is ending with x509: certificate signed by unknown authority**

**What you expected to happen**:

I expected Kubernetes to set up the first control plane node for me and use the F5 load balancer that I provided

**How to reproduce it (as minimally and precisely as possible)**:

Follow all docs here:
- https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/
My set up is for a stacked control plane

**Anything else we need to know?**:

Tried the following and set these params:
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward = 1

Driver for Docker and kubelet: **systemd**



**Environment**:
- Kubernetes version (use `kubectl version`):
kubeadm kubelet kubectl versions: v1.17.3
Docker version 19.03.11
- Cloud provider or hardware configuration:
Bare-metal with an F5 load balancer instead of HAproxy LB
- OS (e.g: `cat /etc/os-release`):
NAME=""Red Hat Enterprise Linux Server""
VERSION=""7.7 (Maipo)""
ID=""rhel""
ID_LIKE=""fedora""
VARIANT=""Server""
VARIANT_ID=""server""
VERSION_ID=""7.7""

- Kernel (e.g. `uname -a`):
Linux MasterNode 3.10.0-1062.18.1.el7.x86_64 #1 SMP Wed Feb 12 14:08:31 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
- Install tools:
yum
- Network plugin and version (if this is a network-related bug):
- Others:

```
Jun 10 11:19:30 MasterNode kubelet[3195]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-
Jun 10 11:19:30 MasterNode kubelet[3195]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964228    3195 server.go:416] Version: v1.17.3
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964703    3195 plugins.go:100] No cloud provider specified.
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964939    3195 server.go:821] Client rotation is on, will bootstrap in background
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.967369    3195 certificate_store.go:129] Loading cert/key pair from ""/var/lib/kubelet/pki/kubelet-client-current.pem"".
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054304    3195 server.go:641] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054653    3195 container_manager_linux.go:265] container manager verified user specified cgroup-root exists: []
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054687    3195 container_manager_linux.go:270] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: Con
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054789    3195 fake_topology_manager.go:29] [fake topologymanager] NewFakeManager
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054794    3195 container_manager_linux.go:305] Creating device plugin manager: true
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054826    3195 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider:  &{kubelet.sock /var/lib/kubelet/device-plugins/ map[] {0 0} <nil> {{
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054872    3195 state_mem.go:36] [cpumanager] initializing new in-memory state store
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054979    3195 state_mem.go:84] [cpumanager] updated default cpuset: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054985    3195 state_mem.go:92] [cpumanager] updated cpuset assignments: ""map[]""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054994    3195 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider:  &{{0 0} 0x6e9bc50 10000000000 0xc000ace6c0 <nil> <nil> <nil> <nil> m
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.055060    3195 kubelet.go:286] Adding pod path: /etc/kubernetes/manifests
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.055115    3195 kubelet.go:311] Watching apiserver
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.069097    3195 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.069444    3195 client.go:104] Start docker client with request timeout=4m0s
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.076542    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.077278    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.077500    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.079822    3195 docker_service.go:563] Hairpin mode set to ""promiscuous-bridge"" but kubenet is not enabled, falling back to ""hairpin-veth""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.079844    3195 docker_service.go:240] Hairpin mode set to ""hairpin-veth""
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.079937    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.083724    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.083778    3195 docker_service.go:255] Docker cri networking managed by cni
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.083836    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.093083    3195 docker_service.go:260] Docker Info: &{ID:CKKA:YUJL:2557:3IQE:7MRG:35J3:B2MN:GC3H:3WMM:FCV4:R2BU:TQW4 Containers:8 ContainersRunning:8 ContainersPaused:
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.093148    3195 docker_service.go:273] Setting cgroupDriver to systemd
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103239    3195 remote_runtime.go:59] parsed scheme: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103260    3195 remote_runtime.go:59] scheme """" not registered, fallback to default scheme
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103297    3195 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock 0  <nil>}] <nil>}
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103321    3195 clientconn.go:577] ClientConn switching balancer to ""pick_first""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103352    3195 remote_image.go:50] parsed scheme: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103358    3195 remote_image.go:50] scheme """" not registered, fallback to default scheme
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103366    3195 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock 0  <nil>}] <nil>}
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103370    3195 clientconn.go:577] ClientConn switching balancer to ""pick_first""
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.083448    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.086800    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.086872    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.090523    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.094073    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.094838    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.097327    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.102600    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.103424    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.108470    3195 aws_credentials.go:77] while getting AWS credentials NoCredentialProviders: no valid providers in chain. Deprecated.
Jun 10 11:19:34 MasterNode kubelet[3195]: For verbose messaging see aws.Config.CredentialsChainVerboseErrors
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.121933    3195 kuberuntime_manager.go:211] Container runtime docker initialized, version: 19.03.11, apiVersion: 1.40.0
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.135482    3195 server.go:1113] Started kubelet
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.135999    3195 kubelet.go:1302] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.136895    3195 fs_resource_analyzer.go:64] Starting FS ResourceAnalyzer
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.138046    3195 server.go:144] Starting to listen on 0.0.0.0:10250
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.140208    3195 server.go:384] Adding debug handlers to kubelet server.
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.151892    3195 volume_manager.go:265] Starting Kubelet Volume Manager
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.152428    3195 desired_state_of_world_populator.go:138] Desired state populator starts to run
```",NoCredentialProviders,https://github.com/kubernetes/kubernetes
35,91992,"**What happened**:

Hey everyone! Not sure why this is happening, I have not tried to set up HA k8s before this. Running normal kubeadm init works.... but I am trying to use HA Kubernetes and getting these following errors. Any pointers would be GREAT.

While installing HA kubernetes via command line with kubeadm, I am repeadetly blocked by kubelet message:

kubeadm init --config config.yaml --upload-certs

Contents of config.yaml:
```
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress:
  bindPort: 6444 #Changed this to 6444 per some documents I read through on GitHub issues... also tried with 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: 
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
controlPlaneEndpoint: ""LoadBalancerDNS:6443""
kubernetesVersion: v1.17.3
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
address: 0.0.0.0
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
cgroupDriver: systemd
cgroupsPerQOS: true
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
configMapAndSecretChangeDetectionStrategy: Watch
containerLogMaxFiles: 5
containerLogMaxSize: 10Mi
contentType: application/vnd.kubernetes.protobuf
cpuCFSQuota: true
cpuCFSQuotaPeriod: 100ms
cpuManagerPolicy: none
cpuManagerReconcilePeriod: 10s
enableControllerAttachDetach: true
enableDebuggingHandlers: true
enforceNodeAllocatable:
- pods
eventBurst: 10
eventRecordQPS: 5
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
evictionPressureTransitionPeriod: 5m0s
failSwapOn: false
fileCheckFrequency: 20s
hairpinMode: promiscuous-bridge
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 20s
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
imageMinimumGCAge: 2m0s
iptablesDropBit: 15
iptablesMasqueradeBit: 14
kind: KubeletConfiguration
kubeAPIBurst: 10
kubeAPIQPS: 5
makeIPTablesUtilChains: true
maxOpenFiles: 1000000
maxPods: 110
nodeLeaseDurationSeconds: 80 #changed from 40
nodeStatusReportFrequency: 1m0s
nodeStatusUpdateFrequency: 10s
oomScoreAdj: -999
podPidsLimit: -1
port: 10250
registryBurst: 10
registryPullQPS: 5
resolvConf: /etc/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 4m0s
serializeImagePulls: true
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 4h0m0s
syncFrequency: 1m0s
topologyManagerPolicy: none
volumeStatsAggPeriod: 1m0s
```
Failed to list *v1.Node: Get **https://load-balancer-dns:6443/api/v1/nodes?fieldSelector=metadata.name%3DCurrentMasterNodeName&limit=500&resourceVersion=0:** x509: certificate signed by unknown authority

This is the same for all:
Failed to list *v1beta1.CSIDriver
Failed to list *v1.Service
Failed to list *v1.Node
Failed to get status for pod ""etcd-NodeNameMaster

**For all of them, this is ending with x509: certificate signed by unknown authority**

**What you expected to happen**:

I expected Kubernetes to set up the first control plane node for me and use the F5 load balancer that I provided

**How to reproduce it (as minimally and precisely as possible)**:

Follow all docs here:
- https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/
My set up is for a stacked control plane

**Anything else we need to know?**:

Tried the following and set these params:
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward = 1

Driver for Docker and kubelet: **systemd**



**Environment**:
- Kubernetes version (use `kubectl version`):
kubeadm kubelet kubectl versions: v1.17.3
Docker version 19.03.11
- Cloud provider or hardware configuration:
Bare-metal with an F5 load balancer instead of HAproxy LB
- OS (e.g: `cat /etc/os-release`):
NAME=""Red Hat Enterprise Linux Server""
VERSION=""7.7 (Maipo)""
ID=""rhel""
ID_LIKE=""fedora""
VARIANT=""Server""
VARIANT_ID=""server""
VERSION_ID=""7.7""

- Kernel (e.g. `uname -a`):
Linux MasterNode 3.10.0-1062.18.1.el7.x86_64 #1 SMP Wed Feb 12 14:08:31 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
- Install tools:
yum
- Network plugin and version (if this is a network-related bug):
- Others:

```
Jun 10 11:19:30 MasterNode kubelet[3195]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-
Jun 10 11:19:30 MasterNode kubelet[3195]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964228    3195 server.go:416] Version: v1.17.3
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964703    3195 plugins.go:100] No cloud provider specified.
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964939    3195 server.go:821] Client rotation is on, will bootstrap in background
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.967369    3195 certificate_store.go:129] Loading cert/key pair from ""/var/lib/kubelet/pki/kubelet-client-current.pem"".
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054304    3195 server.go:641] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054653    3195 container_manager_linux.go:265] container manager verified user specified cgroup-root exists: []
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054687    3195 container_manager_linux.go:270] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: Con
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054789    3195 fake_topology_manager.go:29] [fake topologymanager] NewFakeManager
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054794    3195 container_manager_linux.go:305] Creating device plugin manager: true
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054826    3195 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider:  &{kubelet.sock /var/lib/kubelet/device-plugins/ map[] {0 0} <nil> {{
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054872    3195 state_mem.go:36] [cpumanager] initializing new in-memory state store
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054979    3195 state_mem.go:84] [cpumanager] updated default cpuset: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054985    3195 state_mem.go:92] [cpumanager] updated cpuset assignments: ""map[]""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054994    3195 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider:  &{{0 0} 0x6e9bc50 10000000000 0xc000ace6c0 <nil> <nil> <nil> <nil> m
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.055060    3195 kubelet.go:286] Adding pod path: /etc/kubernetes/manifests
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.055115    3195 kubelet.go:311] Watching apiserver
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.069097    3195 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.069444    3195 client.go:104] Start docker client with request timeout=4m0s
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.076542    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.077278    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.077500    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.079822    3195 docker_service.go:563] Hairpin mode set to ""promiscuous-bridge"" but kubenet is not enabled, falling back to ""hairpin-veth""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.079844    3195 docker_service.go:240] Hairpin mode set to ""hairpin-veth""
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.079937    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.083724    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.083778    3195 docker_service.go:255] Docker cri networking managed by cni
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.083836    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.093083    3195 docker_service.go:260] Docker Info: &{ID:CKKA:YUJL:2557:3IQE:7MRG:35J3:B2MN:GC3H:3WMM:FCV4:R2BU:TQW4 Containers:8 ContainersRunning:8 ContainersPaused:
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.093148    3195 docker_service.go:273] Setting cgroupDriver to systemd
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103239    3195 remote_runtime.go:59] parsed scheme: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103260    3195 remote_runtime.go:59] scheme """" not registered, fallback to default scheme
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103297    3195 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock 0  <nil>}] <nil>}
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103321    3195 clientconn.go:577] ClientConn switching balancer to ""pick_first""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103352    3195 remote_image.go:50] parsed scheme: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103358    3195 remote_image.go:50] scheme """" not registered, fallback to default scheme
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103366    3195 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock 0  <nil>}] <nil>}
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103370    3195 clientconn.go:577] ClientConn switching balancer to ""pick_first""
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.083448    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.086800    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.086872    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.090523    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.094073    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.094838    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.097327    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.102600    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.103424    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.108470    3195 aws_credentials.go:77] while getting AWS credentials NoCredentialProviders: no valid providers in chain. Deprecated.
Jun 10 11:19:34 MasterNode kubelet[3195]: For verbose messaging see aws.Config.CredentialsChainVerboseErrors
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.121933    3195 kuberuntime_manager.go:211] Container runtime docker initialized, version: 19.03.11, apiVersion: 1.40.0
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.135482    3195 server.go:1113] Started kubelet
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.135999    3195 kubelet.go:1302] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.136895    3195 fs_resource_analyzer.go:64] Starting FS ResourceAnalyzer
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.138046    3195 server.go:144] Starting to listen on 0.0.0.0:10250
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.140208    3195 server.go:384] Adding debug handlers to kubelet server.
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.151892    3195 volume_manager.go:265] Starting Kubelet Volume Manager
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.152428    3195 desired_state_of_world_populator.go:138] Desired state populator starts to run
```",containerLogMaxFiles,https://github.com/kubernetes/kubernetes
36,91992,"**What happened**:

Hey everyone! Not sure why this is happening, I have not tried to set up HA k8s before this. Running normal kubeadm init works.... but I am trying to use HA Kubernetes and getting these following errors. Any pointers would be GREAT.

While installing HA kubernetes via command line with kubeadm, I am repeadetly blocked by kubelet message:

kubeadm init --config config.yaml --upload-certs

Contents of config.yaml:
```
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress:
  bindPort: 6444 #Changed this to 6444 per some documents I read through on GitHub issues... also tried with 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: 
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
controlPlaneEndpoint: ""LoadBalancerDNS:6443""
kubernetesVersion: v1.17.3
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
address: 0.0.0.0
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
cgroupDriver: systemd
cgroupsPerQOS: true
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
configMapAndSecretChangeDetectionStrategy: Watch
containerLogMaxFiles: 5
containerLogMaxSize: 10Mi
contentType: application/vnd.kubernetes.protobuf
cpuCFSQuota: true
cpuCFSQuotaPeriod: 100ms
cpuManagerPolicy: none
cpuManagerReconcilePeriod: 10s
enableControllerAttachDetach: true
enableDebuggingHandlers: true
enforceNodeAllocatable:
- pods
eventBurst: 10
eventRecordQPS: 5
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
evictionPressureTransitionPeriod: 5m0s
failSwapOn: false
fileCheckFrequency: 20s
hairpinMode: promiscuous-bridge
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 20s
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
imageMinimumGCAge: 2m0s
iptablesDropBit: 15
iptablesMasqueradeBit: 14
kind: KubeletConfiguration
kubeAPIBurst: 10
kubeAPIQPS: 5
makeIPTablesUtilChains: true
maxOpenFiles: 1000000
maxPods: 110
nodeLeaseDurationSeconds: 80 #changed from 40
nodeStatusReportFrequency: 1m0s
nodeStatusUpdateFrequency: 10s
oomScoreAdj: -999
podPidsLimit: -1
port: 10250
registryBurst: 10
registryPullQPS: 5
resolvConf: /etc/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 4m0s
serializeImagePulls: true
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 4h0m0s
syncFrequency: 1m0s
topologyManagerPolicy: none
volumeStatsAggPeriod: 1m0s
```
Failed to list *v1.Node: Get **https://load-balancer-dns:6443/api/v1/nodes?fieldSelector=metadata.name%3DCurrentMasterNodeName&limit=500&resourceVersion=0:** x509: certificate signed by unknown authority

This is the same for all:
Failed to list *v1beta1.CSIDriver
Failed to list *v1.Service
Failed to list *v1.Node
Failed to get status for pod ""etcd-NodeNameMaster

**For all of them, this is ending with x509: certificate signed by unknown authority**

**What you expected to happen**:

I expected Kubernetes to set up the first control plane node for me and use the F5 load balancer that I provided

**How to reproduce it (as minimally and precisely as possible)**:

Follow all docs here:
- https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/
My set up is for a stacked control plane

**Anything else we need to know?**:

Tried the following and set these params:
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward = 1

Driver for Docker and kubelet: **systemd**



**Environment**:
- Kubernetes version (use `kubectl version`):
kubeadm kubelet kubectl versions: v1.17.3
Docker version 19.03.11
- Cloud provider or hardware configuration:
Bare-metal with an F5 load balancer instead of HAproxy LB
- OS (e.g: `cat /etc/os-release`):
NAME=""Red Hat Enterprise Linux Server""
VERSION=""7.7 (Maipo)""
ID=""rhel""
ID_LIKE=""fedora""
VARIANT=""Server""
VARIANT_ID=""server""
VERSION_ID=""7.7""

- Kernel (e.g. `uname -a`):
Linux MasterNode 3.10.0-1062.18.1.el7.x86_64 #1 SMP Wed Feb 12 14:08:31 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
- Install tools:
yum
- Network plugin and version (if this is a network-related bug):
- Others:

```
Jun 10 11:19:30 MasterNode kubelet[3195]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-
Jun 10 11:19:30 MasterNode kubelet[3195]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964228    3195 server.go:416] Version: v1.17.3
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964703    3195 plugins.go:100] No cloud provider specified.
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964939    3195 server.go:821] Client rotation is on, will bootstrap in background
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.967369    3195 certificate_store.go:129] Loading cert/key pair from ""/var/lib/kubelet/pki/kubelet-client-current.pem"".
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054304    3195 server.go:641] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054653    3195 container_manager_linux.go:265] container manager verified user specified cgroup-root exists: []
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054687    3195 container_manager_linux.go:270] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: Con
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054789    3195 fake_topology_manager.go:29] [fake topologymanager] NewFakeManager
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054794    3195 container_manager_linux.go:305] Creating device plugin manager: true
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054826    3195 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider:  &{kubelet.sock /var/lib/kubelet/device-plugins/ map[] {0 0} <nil> {{
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054872    3195 state_mem.go:36] [cpumanager] initializing new in-memory state store
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054979    3195 state_mem.go:84] [cpumanager] updated default cpuset: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054985    3195 state_mem.go:92] [cpumanager] updated cpuset assignments: ""map[]""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054994    3195 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider:  &{{0 0} 0x6e9bc50 10000000000 0xc000ace6c0 <nil> <nil> <nil> <nil> m
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.055060    3195 kubelet.go:286] Adding pod path: /etc/kubernetes/manifests
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.055115    3195 kubelet.go:311] Watching apiserver
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.069097    3195 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.069444    3195 client.go:104] Start docker client with request timeout=4m0s
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.076542    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.077278    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.077500    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.079822    3195 docker_service.go:563] Hairpin mode set to ""promiscuous-bridge"" but kubenet is not enabled, falling back to ""hairpin-veth""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.079844    3195 docker_service.go:240] Hairpin mode set to ""hairpin-veth""
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.079937    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.083724    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.083778    3195 docker_service.go:255] Docker cri networking managed by cni
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.083836    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.093083    3195 docker_service.go:260] Docker Info: &{ID:CKKA:YUJL:2557:3IQE:7MRG:35J3:B2MN:GC3H:3WMM:FCV4:R2BU:TQW4 Containers:8 ContainersRunning:8 ContainersPaused:
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.093148    3195 docker_service.go:273] Setting cgroupDriver to systemd
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103239    3195 remote_runtime.go:59] parsed scheme: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103260    3195 remote_runtime.go:59] scheme """" not registered, fallback to default scheme
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103297    3195 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock 0  <nil>}] <nil>}
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103321    3195 clientconn.go:577] ClientConn switching balancer to ""pick_first""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103352    3195 remote_image.go:50] parsed scheme: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103358    3195 remote_image.go:50] scheme """" not registered, fallback to default scheme
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103366    3195 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock 0  <nil>}] <nil>}
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103370    3195 clientconn.go:577] ClientConn switching balancer to ""pick_first""
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.083448    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.086800    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.086872    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.090523    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.094073    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.094838    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.097327    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.102600    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.103424    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.108470    3195 aws_credentials.go:77] while getting AWS credentials NoCredentialProviders: no valid providers in chain. Deprecated.
Jun 10 11:19:34 MasterNode kubelet[3195]: For verbose messaging see aws.Config.CredentialsChainVerboseErrors
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.121933    3195 kuberuntime_manager.go:211] Container runtime docker initialized, version: 19.03.11, apiVersion: 1.40.0
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.135482    3195 server.go:1113] Started kubelet
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.135999    3195 kubelet.go:1302] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.136895    3195 fs_resource_analyzer.go:64] Starting FS ResourceAnalyzer
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.138046    3195 server.go:144] Starting to listen on 0.0.0.0:10250
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.140208    3195 server.go:384] Adding debug handlers to kubelet server.
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.151892    3195 volume_manager.go:265] Starting Kubelet Volume Manager
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.152428    3195 desired_state_of_world_populator.go:138] Desired state populator starts to run
```",kubeadm:default-node-token,https://github.com/kubernetes/kubernetes
37,91992,"**What happened**:

Hey everyone! Not sure why this is happening, I have not tried to set up HA k8s before this. Running normal kubeadm init works.... but I am trying to use HA Kubernetes and getting these following errors. Any pointers would be GREAT.

While installing HA kubernetes via command line with kubeadm, I am repeadetly blocked by kubelet message:

kubeadm init --config config.yaml --upload-certs

Contents of config.yaml:
```
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress:
  bindPort: 6444 #Changed this to 6444 per some documents I read through on GitHub issues... also tried with 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: 
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
controlPlaneEndpoint: ""LoadBalancerDNS:6443""
kubernetesVersion: v1.17.3
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
address: 0.0.0.0
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
cgroupDriver: systemd
cgroupsPerQOS: true
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
configMapAndSecretChangeDetectionStrategy: Watch
containerLogMaxFiles: 5
containerLogMaxSize: 10Mi
contentType: application/vnd.kubernetes.protobuf
cpuCFSQuota: true
cpuCFSQuotaPeriod: 100ms
cpuManagerPolicy: none
cpuManagerReconcilePeriod: 10s
enableControllerAttachDetach: true
enableDebuggingHandlers: true
enforceNodeAllocatable:
- pods
eventBurst: 10
eventRecordQPS: 5
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
evictionPressureTransitionPeriod: 5m0s
failSwapOn: false
fileCheckFrequency: 20s
hairpinMode: promiscuous-bridge
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 20s
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
imageMinimumGCAge: 2m0s
iptablesDropBit: 15
iptablesMasqueradeBit: 14
kind: KubeletConfiguration
kubeAPIBurst: 10
kubeAPIQPS: 5
makeIPTablesUtilChains: true
maxOpenFiles: 1000000
maxPods: 110
nodeLeaseDurationSeconds: 80 #changed from 40
nodeStatusReportFrequency: 1m0s
nodeStatusUpdateFrequency: 10s
oomScoreAdj: -999
podPidsLimit: -1
port: 10250
registryBurst: 10
registryPullQPS: 5
resolvConf: /etc/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 4m0s
serializeImagePulls: true
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 4h0m0s
syncFrequency: 1m0s
topologyManagerPolicy: none
volumeStatsAggPeriod: 1m0s
```
Failed to list *v1.Node: Get **https://load-balancer-dns:6443/api/v1/nodes?fieldSelector=metadata.name%3DCurrentMasterNodeName&limit=500&resourceVersion=0:** x509: certificate signed by unknown authority

This is the same for all:
Failed to list *v1beta1.CSIDriver
Failed to list *v1.Service
Failed to list *v1.Node
Failed to get status for pod ""etcd-NodeNameMaster

**For all of them, this is ending with x509: certificate signed by unknown authority**

**What you expected to happen**:

I expected Kubernetes to set up the first control plane node for me and use the F5 load balancer that I provided

**How to reproduce it (as minimally and precisely as possible)**:

Follow all docs here:
- https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/
My set up is for a stacked control plane

**Anything else we need to know?**:

Tried the following and set these params:
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward = 1

Driver for Docker and kubelet: **systemd**



**Environment**:
- Kubernetes version (use `kubectl version`):
kubeadm kubelet kubectl versions: v1.17.3
Docker version 19.03.11
- Cloud provider or hardware configuration:
Bare-metal with an F5 load balancer instead of HAproxy LB
- OS (e.g: `cat /etc/os-release`):
NAME=""Red Hat Enterprise Linux Server""
VERSION=""7.7 (Maipo)""
ID=""rhel""
ID_LIKE=""fedora""
VARIANT=""Server""
VARIANT_ID=""server""
VERSION_ID=""7.7""

- Kernel (e.g. `uname -a`):
Linux MasterNode 3.10.0-1062.18.1.el7.x86_64 #1 SMP Wed Feb 12 14:08:31 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
- Install tools:
yum
- Network plugin and version (if this is a network-related bug):
- Others:

```
Jun 10 11:19:30 MasterNode kubelet[3195]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-
Jun 10 11:19:30 MasterNode kubelet[3195]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964228    3195 server.go:416] Version: v1.17.3
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964703    3195 plugins.go:100] No cloud provider specified.
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964939    3195 server.go:821] Client rotation is on, will bootstrap in background
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.967369    3195 certificate_store.go:129] Loading cert/key pair from ""/var/lib/kubelet/pki/kubelet-client-current.pem"".
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054304    3195 server.go:641] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054653    3195 container_manager_linux.go:265] container manager verified user specified cgroup-root exists: []
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054687    3195 container_manager_linux.go:270] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: Con
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054789    3195 fake_topology_manager.go:29] [fake topologymanager] NewFakeManager
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054794    3195 container_manager_linux.go:305] Creating device plugin manager: true
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054826    3195 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider:  &{kubelet.sock /var/lib/kubelet/device-plugins/ map[] {0 0} <nil> {{
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054872    3195 state_mem.go:36] [cpumanager] initializing new in-memory state store
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054979    3195 state_mem.go:84] [cpumanager] updated default cpuset: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054985    3195 state_mem.go:92] [cpumanager] updated cpuset assignments: ""map[]""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054994    3195 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider:  &{{0 0} 0x6e9bc50 10000000000 0xc000ace6c0 <nil> <nil> <nil> <nil> m
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.055060    3195 kubelet.go:286] Adding pod path: /etc/kubernetes/manifests
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.055115    3195 kubelet.go:311] Watching apiserver
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.069097    3195 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.069444    3195 client.go:104] Start docker client with request timeout=4m0s
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.076542    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.077278    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.077500    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.079822    3195 docker_service.go:563] Hairpin mode set to ""promiscuous-bridge"" but kubenet is not enabled, falling back to ""hairpin-veth""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.079844    3195 docker_service.go:240] Hairpin mode set to ""hairpin-veth""
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.079937    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.083724    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.083778    3195 docker_service.go:255] Docker cri networking managed by cni
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.083836    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.093083    3195 docker_service.go:260] Docker Info: &{ID:CKKA:YUJL:2557:3IQE:7MRG:35J3:B2MN:GC3H:3WMM:FCV4:R2BU:TQW4 Containers:8 ContainersRunning:8 ContainersPaused:
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.093148    3195 docker_service.go:273] Setting cgroupDriver to systemd
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103239    3195 remote_runtime.go:59] parsed scheme: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103260    3195 remote_runtime.go:59] scheme """" not registered, fallback to default scheme
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103297    3195 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock 0  <nil>}] <nil>}
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103321    3195 clientconn.go:577] ClientConn switching balancer to ""pick_first""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103352    3195 remote_image.go:50] parsed scheme: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103358    3195 remote_image.go:50] scheme """" not registered, fallback to default scheme
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103366    3195 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock 0  <nil>}] <nil>}
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103370    3195 clientconn.go:577] ClientConn switching balancer to ""pick_first""
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.083448    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.086800    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.086872    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.090523    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.094073    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.094838    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.097327    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.102600    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.103424    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.108470    3195 aws_credentials.go:77] while getting AWS credentials NoCredentialProviders: no valid providers in chain. Deprecated.
Jun 10 11:19:34 MasterNode kubelet[3195]: For verbose messaging see aws.Config.CredentialsChainVerboseErrors
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.121933    3195 kuberuntime_manager.go:211] Container runtime docker initialized, version: 19.03.11, apiVersion: 1.40.0
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.135482    3195 server.go:1113] Started kubelet
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.135999    3195 kubelet.go:1302] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.136895    3195 fs_resource_analyzer.go:64] Starting FS ResourceAnalyzer
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.138046    3195 server.go:144] Starting to listen on 0.0.0.0:10250
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.140208    3195 server.go:384] Adding debug handlers to kubelet server.
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.151892    3195 volume_manager.go:265] Starting Kubelet Volume Manager
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.152428    3195 desired_state_of_world_populator.go:138] Desired state populator starts to run
```",timeoutForControlPlane,https://github.com/kubernetes/kubernetes
38,90044,"I have configured keycloak for Kubernetes RBAC. 

- user having access to get pods

```
vagrant@haproxy:~/.kube$ kubectl auth can-i get pods --user=oidc
Warning: the server doesn't have a resource type 'pods'
yes
```

```
vagrant@haproxy:~/.kube$ kubectl get pods --user=oidc
error: You must be logged in to the server (Unauthorized)
```

my kubeconfig file for the user looks like below

```yaml
users:
- name: oidc
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - oidc-login
      - get-token
      - --oidc-issuer-url=https://test.example.com/auth/realms/kubernetes
      - --oidc-client-id=kubernetes
      - --oidc-client-secret=e479f74d-d9fd-415b-b1db-fd7946d3ad90
      - --username=test
      - --grant-type=authcode-keyboard
      command: kubectl
```

",e479f74d-d9fd-415b-b1db-fd7946d3ad90,https://github.com/kubernetes/kubernetes
39,90044,"I have configured keycloak for Kubernetes RBAC. 

- user having access to get pods

```
vagrant@haproxy:~/.kube$ kubectl auth can-i get pods --user=oidc
Warning: the server doesn't have a resource type 'pods'
yes
```

```
vagrant@haproxy:~/.kube$ kubectl get pods --user=oidc
error: You must be logged in to the server (Unauthorized)
```

my kubeconfig file for the user looks like below

```yaml
users:
- name: oidc
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - oidc-login
      - get-token
      - --oidc-issuer-url=https://test.example.com/auth/realms/kubernetes
      - --oidc-client-id=kubernetes
      - --oidc-client-secret=e479f74d-d9fd-415b-b1db-fd7946d3ad90
      - --username=test
      - --grant-type=authcode-keyboard
      command: kubectl
```

",oidc-client-id=kubernetes,https://github.com/kubernetes/kubernetes
40,90044,"I have configured keycloak for Kubernetes RBAC. 

- user having access to get pods

```
vagrant@haproxy:~/.kube$ kubectl auth can-i get pods --user=oidc
Warning: the server doesn't have a resource type 'pods'
yes
```

```
vagrant@haproxy:~/.kube$ kubectl get pods --user=oidc
error: You must be logged in to the server (Unauthorized)
```

my kubeconfig file for the user looks like below

```yaml
users:
- name: oidc
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - oidc-login
      - get-token
      - --oidc-issuer-url=https://test.example.com/auth/realms/kubernetes
      - --oidc-client-id=kubernetes
      - --oidc-client-secret=e479f74d-d9fd-415b-b1db-fd7946d3ad90
      - --username=test
      - --grant-type=authcode-keyboard
      command: kubectl
```

",b1db-fd7946d3ad90,https://github.com/kubernetes/kubernetes
41,89181,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

Pods with subPath are stuck on Terminating when deleting

**What you expected to happen**:

Pod succefully removed.

**How to reproduce it (as minimally and precisely as possible)**:

crate:

```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
  - image: docker.io/library/alpine
    name: test
    command: ['sleep', 'infinity']
    volumeMounts:
    - mountPath: /blabla/sss
      name: keys
      subPath: somekey
  volumes:
  - name: keys
    secret:
      secretName: test
---
apiVersion: v1
kind: Secret
metadata:
  name: test
data:
  somekey: R29vZCBhdHRlbXBsdCwgYnV0IG5vdCE=
  someotherkey: QXJlIHlvdSBraWRkaW5nIG1lPw==
```

remove the pod:

```
kubectl delete pod test
```

check the kubelet logs:

```
Mar 17 20:40:31 m1c4 kubelet[13866]: E0317 20:40:31.230957   13866 nestedpendingoperations.go:270] Operation for ""\""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"" (\""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"")"" failed. No retries permitted until 2020-03-17 20:42:33.230929328 +0100 CET m=+1192722.291304745 (durationBeforeRetry 2m2s). Error: ""error cleaning subPath mounts for volume \""keys\"" (UniqueName: \""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"") pod \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"" (UID: \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"") : error processing /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test: error cleaning subpath mount /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0\nOutput: umount: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: not mounted.\n""
Mar 17 20:42:33 m1c4 kubelet[13866]: I0317 20:42:33.234534   13866 reconciler.go:183] operationExecutor.UnmountVolume started for volume ""keys"" (UniqueName: ""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys"") pod ""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238"" (UID: ""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238"")
Mar 17 20:42:33 m1c4 kubelet[13866]: E0317 20:42:33.257819   13866 nestedpendingoperations.go:270] Operation for ""\""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"" (\""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"")"" failed. No retries permitted until 2020-03-17 20:44:35.257684981 +0100 CET m=+1192844.318060559 (durationBeforeRetry 2m2s). Error: ""error cleaning subPath mounts for volume \""keys\"" (UniqueName: \""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"") pod \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"" (UID: \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"") : error processing /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test: error cleaning subpath mount /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0\nOutput: umount: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: not mounted.\n""
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.0"", GitCommit:""70132b0f130acc0bed193d9ba59dd186f0e634cf"", GitTreeState:""clean"", BuildDate:""2019-12-07T21:20:10Z"", GoVersion:""go1.13.4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.0"", GitCommit:""70132b0f130acc0bed193d9ba59dd186f0e634cf"", GitTreeState:""clean"", BuildDate:""2019-12-07T21:12:17Z"", GoVersion:""go1.13.4"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration: Bare Metal
- OS (e.g: `cat /etc/os-release`):
```
NAME=""Ubuntu""
VERSION=""18.04.3 LTS (Bionic Beaver)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 18.04.3 LTS""
VERSION_ID=""18.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic
```
- Kernel (e.g. `uname -a`):
```
Linux m1c4 4.15.18-24-pve #1 SMP PVE 4.15.18-52 (Thu, 05 Dec 2019 10:14:17 +0100) x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools: kubeadm
- Network plugin and version (if this is a network-related bug):
- Others:
docker version: 19.3.5

docker is running with the next config:
```json
{
  ""exec-opts"": [""native.cgroupdriver=systemd""],
  ""iptables"": false,
  ""ip-forward"": false,
  ""bridge"": ""none"",
  ""log-driver"": ""journald"",
  ""storage-driver"": ""overlay2""
}
```

kubelet versions:
```
Kubernetes v1.17.1
Kubernetes v1.17.4
```

created file is empty and not mountpoint, path is not symlink:
```bash
# mountpoint /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 
/var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 is not a mountpoint

# ls -lah
-rw-r----- 1 root root 0 Mar 17 20:37 /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0

# readlink -f /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 
/var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0
```

Not sure, but maybe problem caused by overlayfs root:

```bash
# df -h /var/lib/kubelet/pods/
Filesystem               Size  Used Avail Use% Mounted on
/run/initramfs/ltsp/cow   16G  412M   16G   3% /
```",89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238,https://github.com/kubernetes/kubernetes
42,89181,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

Pods with subPath are stuck on Terminating when deleting

**What you expected to happen**:

Pod succefully removed.

**How to reproduce it (as minimally and precisely as possible)**:

crate:

```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
  - image: docker.io/library/alpine
    name: test
    command: ['sleep', 'infinity']
    volumeMounts:
    - mountPath: /blabla/sss
      name: keys
      subPath: somekey
  volumes:
  - name: keys
    secret:
      secretName: test
---
apiVersion: v1
kind: Secret
metadata:
  name: test
data:
  somekey: R29vZCBhdHRlbXBsdCwgYnV0IG5vdCE=
  someotherkey: QXJlIHlvdSBraWRkaW5nIG1lPw==
```

remove the pod:

```
kubectl delete pod test
```

check the kubelet logs:

```
Mar 17 20:40:31 m1c4 kubelet[13866]: E0317 20:40:31.230957   13866 nestedpendingoperations.go:270] Operation for ""\""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"" (\""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"")"" failed. No retries permitted until 2020-03-17 20:42:33.230929328 +0100 CET m=+1192722.291304745 (durationBeforeRetry 2m2s). Error: ""error cleaning subPath mounts for volume \""keys\"" (UniqueName: \""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"") pod \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"" (UID: \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"") : error processing /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test: error cleaning subpath mount /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0\nOutput: umount: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: not mounted.\n""
Mar 17 20:42:33 m1c4 kubelet[13866]: I0317 20:42:33.234534   13866 reconciler.go:183] operationExecutor.UnmountVolume started for volume ""keys"" (UniqueName: ""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys"") pod ""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238"" (UID: ""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238"")
Mar 17 20:42:33 m1c4 kubelet[13866]: E0317 20:42:33.257819   13866 nestedpendingoperations.go:270] Operation for ""\""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"" (\""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"")"" failed. No retries permitted until 2020-03-17 20:44:35.257684981 +0100 CET m=+1192844.318060559 (durationBeforeRetry 2m2s). Error: ""error cleaning subPath mounts for volume \""keys\"" (UniqueName: \""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"") pod \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"" (UID: \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"") : error processing /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test: error cleaning subpath mount /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0\nOutput: umount: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: not mounted.\n""
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.0"", GitCommit:""70132b0f130acc0bed193d9ba59dd186f0e634cf"", GitTreeState:""clean"", BuildDate:""2019-12-07T21:20:10Z"", GoVersion:""go1.13.4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.0"", GitCommit:""70132b0f130acc0bed193d9ba59dd186f0e634cf"", GitTreeState:""clean"", BuildDate:""2019-12-07T21:12:17Z"", GoVersion:""go1.13.4"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration: Bare Metal
- OS (e.g: `cat /etc/os-release`):
```
NAME=""Ubuntu""
VERSION=""18.04.3 LTS (Bionic Beaver)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 18.04.3 LTS""
VERSION_ID=""18.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic
```
- Kernel (e.g. `uname -a`):
```
Linux m1c4 4.15.18-24-pve #1 SMP PVE 4.15.18-52 (Thu, 05 Dec 2019 10:14:17 +0100) x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools: kubeadm
- Network plugin and version (if this is a network-related bug):
- Others:
docker version: 19.3.5

docker is running with the next config:
```json
{
  ""exec-opts"": [""native.cgroupdriver=systemd""],
  ""iptables"": false,
  ""ip-forward"": false,
  ""bridge"": ""none"",
  ""log-driver"": ""journald"",
  ""storage-driver"": ""overlay2""
}
```

kubelet versions:
```
Kubernetes v1.17.1
Kubernetes v1.17.4
```

created file is empty and not mountpoint, path is not symlink:
```bash
# mountpoint /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 
/var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 is not a mountpoint

# ls -lah
-rw-r----- 1 root root 0 Mar 17 20:37 /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0

# readlink -f /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 
/var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0
```

Not sure, but maybe problem caused by overlayfs root:

```bash
# df -h /var/lib/kubelet/pods/
Filesystem               Size  Used Avail Use% Mounted on
/run/initramfs/ltsp/cow   16G  412M   16G   3% /
```",QXJlIHlvdSBraWRkaW5nIG1lPw,https://github.com/kubernetes/kubernetes
43,89181,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

Pods with subPath are stuck on Terminating when deleting

**What you expected to happen**:

Pod succefully removed.

**How to reproduce it (as minimally and precisely as possible)**:

crate:

```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
  - image: docker.io/library/alpine
    name: test
    command: ['sleep', 'infinity']
    volumeMounts:
    - mountPath: /blabla/sss
      name: keys
      subPath: somekey
  volumes:
  - name: keys
    secret:
      secretName: test
---
apiVersion: v1
kind: Secret
metadata:
  name: test
data:
  somekey: R29vZCBhdHRlbXBsdCwgYnV0IG5vdCE=
  someotherkey: QXJlIHlvdSBraWRkaW5nIG1lPw==
```

remove the pod:

```
kubectl delete pod test
```

check the kubelet logs:

```
Mar 17 20:40:31 m1c4 kubelet[13866]: E0317 20:40:31.230957   13866 nestedpendingoperations.go:270] Operation for ""\""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"" (\""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"")"" failed. No retries permitted until 2020-03-17 20:42:33.230929328 +0100 CET m=+1192722.291304745 (durationBeforeRetry 2m2s). Error: ""error cleaning subPath mounts for volume \""keys\"" (UniqueName: \""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"") pod \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"" (UID: \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"") : error processing /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test: error cleaning subpath mount /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0\nOutput: umount: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: not mounted.\n""
Mar 17 20:42:33 m1c4 kubelet[13866]: I0317 20:42:33.234534   13866 reconciler.go:183] operationExecutor.UnmountVolume started for volume ""keys"" (UniqueName: ""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys"") pod ""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238"" (UID: ""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238"")
Mar 17 20:42:33 m1c4 kubelet[13866]: E0317 20:42:33.257819   13866 nestedpendingoperations.go:270] Operation for ""\""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"" (\""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"")"" failed. No retries permitted until 2020-03-17 20:44:35.257684981 +0100 CET m=+1192844.318060559 (durationBeforeRetry 2m2s). Error: ""error cleaning subPath mounts for volume \""keys\"" (UniqueName: \""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"") pod \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"" (UID: \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"") : error processing /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test: error cleaning subpath mount /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0\nOutput: umount: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: not mounted.\n""
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.0"", GitCommit:""70132b0f130acc0bed193d9ba59dd186f0e634cf"", GitTreeState:""clean"", BuildDate:""2019-12-07T21:20:10Z"", GoVersion:""go1.13.4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.0"", GitCommit:""70132b0f130acc0bed193d9ba59dd186f0e634cf"", GitTreeState:""clean"", BuildDate:""2019-12-07T21:12:17Z"", GoVersion:""go1.13.4"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration: Bare Metal
- OS (e.g: `cat /etc/os-release`):
```
NAME=""Ubuntu""
VERSION=""18.04.3 LTS (Bionic Beaver)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 18.04.3 LTS""
VERSION_ID=""18.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic
```
- Kernel (e.g. `uname -a`):
```
Linux m1c4 4.15.18-24-pve #1 SMP PVE 4.15.18-52 (Thu, 05 Dec 2019 10:14:17 +0100) x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools: kubeadm
- Network plugin and version (if this is a network-related bug):
- Others:
docker version: 19.3.5

docker is running with the next config:
```json
{
  ""exec-opts"": [""native.cgroupdriver=systemd""],
  ""iptables"": false,
  ""ip-forward"": false,
  ""bridge"": ""none"",
  ""log-driver"": ""journald"",
  ""storage-driver"": ""overlay2""
}
```

kubelet versions:
```
Kubernetes v1.17.1
Kubernetes v1.17.4
```

created file is empty and not mountpoint, path is not symlink:
```bash
# mountpoint /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 
/var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 is not a mountpoint

# ls -lah
-rw-r----- 1 root root 0 Mar 17 20:37 /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0

# readlink -f /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 
/var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0
```

Not sure, but maybe problem caused by overlayfs root:

```bash
# df -h /var/lib/kubelet/pods/
Filesystem               Size  Used Avail Use% Mounted on
/run/initramfs/ltsp/cow   16G  412M   16G   3% /
```",4cc6-89a0-2fd9a346e238,https://github.com/kubernetes/kubernetes
44,89181,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

Pods with subPath are stuck on Terminating when deleting

**What you expected to happen**:

Pod succefully removed.

**How to reproduce it (as minimally and precisely as possible)**:

crate:

```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
  - image: docker.io/library/alpine
    name: test
    command: ['sleep', 'infinity']
    volumeMounts:
    - mountPath: /blabla/sss
      name: keys
      subPath: somekey
  volumes:
  - name: keys
    secret:
      secretName: test
---
apiVersion: v1
kind: Secret
metadata:
  name: test
data:
  somekey: R29vZCBhdHRlbXBsdCwgYnV0IG5vdCE=
  someotherkey: QXJlIHlvdSBraWRkaW5nIG1lPw==
```

remove the pod:

```
kubectl delete pod test
```

check the kubelet logs:

```
Mar 17 20:40:31 m1c4 kubelet[13866]: E0317 20:40:31.230957   13866 nestedpendingoperations.go:270] Operation for ""\""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"" (\""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"")"" failed. No retries permitted until 2020-03-17 20:42:33.230929328 +0100 CET m=+1192722.291304745 (durationBeforeRetry 2m2s). Error: ""error cleaning subPath mounts for volume \""keys\"" (UniqueName: \""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"") pod \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"" (UID: \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"") : error processing /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test: error cleaning subpath mount /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0\nOutput: umount: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: not mounted.\n""
Mar 17 20:42:33 m1c4 kubelet[13866]: I0317 20:42:33.234534   13866 reconciler.go:183] operationExecutor.UnmountVolume started for volume ""keys"" (UniqueName: ""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys"") pod ""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238"" (UID: ""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238"")
Mar 17 20:42:33 m1c4 kubelet[13866]: E0317 20:42:33.257819   13866 nestedpendingoperations.go:270] Operation for ""\""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"" (\""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"")"" failed. No retries permitted until 2020-03-17 20:44:35.257684981 +0100 CET m=+1192844.318060559 (durationBeforeRetry 2m2s). Error: ""error cleaning subPath mounts for volume \""keys\"" (UniqueName: \""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"") pod \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"" (UID: \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"") : error processing /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test: error cleaning subpath mount /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0\nOutput: umount: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: not mounted.\n""
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.0"", GitCommit:""70132b0f130acc0bed193d9ba59dd186f0e634cf"", GitTreeState:""clean"", BuildDate:""2019-12-07T21:20:10Z"", GoVersion:""go1.13.4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.0"", GitCommit:""70132b0f130acc0bed193d9ba59dd186f0e634cf"", GitTreeState:""clean"", BuildDate:""2019-12-07T21:12:17Z"", GoVersion:""go1.13.4"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration: Bare Metal
- OS (e.g: `cat /etc/os-release`):
```
NAME=""Ubuntu""
VERSION=""18.04.3 LTS (Bionic Beaver)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 18.04.3 LTS""
VERSION_ID=""18.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic
```
- Kernel (e.g. `uname -a`):
```
Linux m1c4 4.15.18-24-pve #1 SMP PVE 4.15.18-52 (Thu, 05 Dec 2019 10:14:17 +0100) x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools: kubeadm
- Network plugin and version (if this is a network-related bug):
- Others:
docker version: 19.3.5

docker is running with the next config:
```json
{
  ""exec-opts"": [""native.cgroupdriver=systemd""],
  ""iptables"": false,
  ""ip-forward"": false,
  ""bridge"": ""none"",
  ""log-driver"": ""journald"",
  ""storage-driver"": ""overlay2""
}
```

kubelet versions:
```
Kubernetes v1.17.1
Kubernetes v1.17.4
```

created file is empty and not mountpoint, path is not symlink:
```bash
# mountpoint /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 
/var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 is not a mountpoint

# ls -lah
-rw-r----- 1 root root 0 Mar 17 20:37 /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0

# readlink -f /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 
/var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0
```

Not sure, but maybe problem caused by overlayfs root:

```bash
# df -h /var/lib/kubelet/pods/
Filesystem               Size  Used Avail Use% Mounted on
/run/initramfs/ltsp/cow   16G  412M   16G   3% /
```",R29vZCBhdHRlbXBsdCwgYnV0IG5vdCE,https://github.com/kubernetes/kubernetes
45,89181,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

Pods with subPath are stuck on Terminating when deleting

**What you expected to happen**:

Pod succefully removed.

**How to reproduce it (as minimally and precisely as possible)**:

crate:

```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
  - image: docker.io/library/alpine
    name: test
    command: ['sleep', 'infinity']
    volumeMounts:
    - mountPath: /blabla/sss
      name: keys
      subPath: somekey
  volumes:
  - name: keys
    secret:
      secretName: test
---
apiVersion: v1
kind: Secret
metadata:
  name: test
data:
  somekey: R29vZCBhdHRlbXBsdCwgYnV0IG5vdCE=
  someotherkey: QXJlIHlvdSBraWRkaW5nIG1lPw==
```

remove the pod:

```
kubectl delete pod test
```

check the kubelet logs:

```
Mar 17 20:40:31 m1c4 kubelet[13866]: E0317 20:40:31.230957   13866 nestedpendingoperations.go:270] Operation for ""\""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"" (\""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"")"" failed. No retries permitted until 2020-03-17 20:42:33.230929328 +0100 CET m=+1192722.291304745 (durationBeforeRetry 2m2s). Error: ""error cleaning subPath mounts for volume \""keys\"" (UniqueName: \""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"") pod \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"" (UID: \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"") : error processing /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test: error cleaning subpath mount /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0\nOutput: umount: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: not mounted.\n""
Mar 17 20:42:33 m1c4 kubelet[13866]: I0317 20:42:33.234534   13866 reconciler.go:183] operationExecutor.UnmountVolume started for volume ""keys"" (UniqueName: ""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys"") pod ""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238"" (UID: ""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238"")
Mar 17 20:42:33 m1c4 kubelet[13866]: E0317 20:42:33.257819   13866 nestedpendingoperations.go:270] Operation for ""\""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"" (\""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"")"" failed. No retries permitted until 2020-03-17 20:44:35.257684981 +0100 CET m=+1192844.318060559 (durationBeforeRetry 2m2s). Error: ""error cleaning subPath mounts for volume \""keys\"" (UniqueName: \""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"") pod \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"" (UID: \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"") : error processing /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test: error cleaning subpath mount /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0\nOutput: umount: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: not mounted.\n""
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.0"", GitCommit:""70132b0f130acc0bed193d9ba59dd186f0e634cf"", GitTreeState:""clean"", BuildDate:""2019-12-07T21:20:10Z"", GoVersion:""go1.13.4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.0"", GitCommit:""70132b0f130acc0bed193d9ba59dd186f0e634cf"", GitTreeState:""clean"", BuildDate:""2019-12-07T21:12:17Z"", GoVersion:""go1.13.4"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration: Bare Metal
- OS (e.g: `cat /etc/os-release`):
```
NAME=""Ubuntu""
VERSION=""18.04.3 LTS (Bionic Beaver)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 18.04.3 LTS""
VERSION_ID=""18.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic
```
- Kernel (e.g. `uname -a`):
```
Linux m1c4 4.15.18-24-pve #1 SMP PVE 4.15.18-52 (Thu, 05 Dec 2019 10:14:17 +0100) x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools: kubeadm
- Network plugin and version (if this is a network-related bug):
- Others:
docker version: 19.3.5

docker is running with the next config:
```json
{
  ""exec-opts"": [""native.cgroupdriver=systemd""],
  ""iptables"": false,
  ""ip-forward"": false,
  ""bridge"": ""none"",
  ""log-driver"": ""journald"",
  ""storage-driver"": ""overlay2""
}
```

kubelet versions:
```
Kubernetes v1.17.1
Kubernetes v1.17.4
```

created file is empty and not mountpoint, path is not symlink:
```bash
# mountpoint /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 
/var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 is not a mountpoint

# ls -lah
-rw-r----- 1 root root 0 Mar 17 20:37 /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0

# readlink -f /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 
/var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0
```

Not sure, but maybe problem caused by overlayfs root:

```bash
# df -h /var/lib/kubelet/pods/
Filesystem               Size  Used Avail Use% Mounted on
/run/initramfs/ltsp/cow   16G  412M   16G   3% /
```",89a0-2fd9a346e238,https://github.com/kubernetes/kubernetes
46,88410,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:
I installed 4 new nodes on my cluster in same time I upgrade it to 1.16.7.
No problem for logs on old existing nodes.
And on this 4 new node I can't have logs with kubectl: 
```sh
 $ kubectl logs piddashboard-57b9d6bfbd-cbn9c
Error from server: Get https://10.194.26.119:10250/containerLogs/pid-dashboard/piddashboard-57b9d6bfbd-cbn9c/piddashboard: Service Unavailable
```

**What you expected to happen**:
Getting logs and no errors, from new nodes

**How to reproduce it (as minimally and precisely as possible)**:
I don't know if it's upgrade of cluster version 1.16.5 to 1.16.7

Or usage of proxy for the docker's daemon (I setup exactly like other working nodes)

**Anything else we need to know?**:
API logs:
```sh
...
E0221 14:48:32.630188       1 status.go:71] apiserver received an error that is not an metav1.Status: &url.Error{Op:""Get"", URL:""https://10.194.26.119:10250/containerLogs/pid-dashboard/piddashboard-57b9d6bfbd-cbn9c/piddashboard"", Err:(*errors.errorString)(0xc00b18a900)}
...
```

No problem from my PC:
```sh
$ export TOKEN=eyJhbGciOiJSUzI1NiIs...

$ curl --insecure --header ""Authorization: Bearer ${TOKEN}"" https://10.194.26.119:10250/containerLogs/pid-dashboard/piddashboard-57b9d6bfbd-cbn9c/piddashboard


Start command: java -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -Djava.security.egd=file:/dev/./urandom -jar /app.jar

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::        (v2.1.0.RELEASE)

2020-02-21 14:52:51.180  INFO 1 --- [           main] f.p.dashboard.PidDashboardApplicationKt  : Starting PidDashboardApplicationKt v0.0.1-SNAPSHOT on piddashboard-57b9d6bfbd-cbn9c with PID 1 (/app.jar started by root in /)
2020-02-21 14:52:51.187  INFO 1 --- [           main] f.p.dashboard.PidDashboardApplicationKt  : No active profile set, falling back to default profiles: default
2020-02-21 14:52:52.613  INFO 1 --- [           main] o.s.c.a.ConfigurationClassPostProcessor  : Cannot enhance @Configuration bean definition 'org.springframework.guice.annotation.ModuleRegistryConfiguration' since its singleton instanc
e has been created too early. The typical cause is a non-static @Bean method with a BeanDefinitionRegistryPostProcessor return type: Consider declaring such methods as 'static'.
2020-02-21 14:52:53.570  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
2020-02-21 14:52:53.594  INFO 1 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
...
```

**Environment**:
- Kubernetes version (use `kubectl version`):
```sh
Client Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.2"", GitCommit:""59603c6e503c87169aea6106f57b9f242f64df89"", GitTreeState:""clean"", BuildDate:""2020-01-18T23:30:10Z"", GoVersion:""go1.13.5"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""16"", GitVersion:""v1.16.3"", GitCommit:""b3cbbae08ec52a7fc73d334838e18d17e8512749"", GitTreeState:""clean"", BuildDate:""2019-11-13T11:13:49Z"", GoVersion:""go1.12.12"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration:
`On premise with kubespray`
- OS (e.g: `cat /etc/os-release`):
For all nodes:
```sh
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
```
- Kernel (e.g. `uname -a`):
For all nodes:
```sh
Linux forge-XX 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools:
`n/a`
- Network plugin and version (if this is a network-related bug):
`calico`
- Others:
Usage of proxy

Actual Cluster:
```sh
$ kubectl get nodes -o wide
NAME       STATUS   ROLES    AGE    VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME
forge-04   Ready    <none>   340d   v1.16.7   10.194.26.60    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-05   Ready    <none>   340d   v1.16.7   10.194.26.61    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-06   Ready    master   344d   v1.16.7   10.194.26.96    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-07   Ready    master   344d   v1.16.7   10.194.27.39    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-08   Ready    master   340d   v1.16.7   10.194.27.63    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-10   Ready    <none>   344d   v1.16.7   10.194.26.49    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-11   Ready    <none>   344d   v1.16.7   10.194.26.37    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-12   Ready    <none>   344d   v1.16.7   10.194.26.39    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
### LOGS KO since here
forge-13   Ready    <none>   26h    v1.16.7   10.194.26.59    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6 
forge-14   Ready    <none>   26h    v1.16.7   10.194.27.120   <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-15   Ready    <none>   26h    v1.16.7   10.194.26.213   <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-16   Ready    <none>   26h    v1.16.7   10.194.26.119   <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
```

There is no other visible bug, pod are scheduled on it, etc, etc.. just logs.

I thinks it's a config problem on masters but I don't know what...

",UnlockExperimentalVMOptions,https://github.com/kubernetes/kubernetes
47,88410,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:
I installed 4 new nodes on my cluster in same time I upgrade it to 1.16.7.
No problem for logs on old existing nodes.
And on this 4 new node I can't have logs with kubectl: 
```sh
 $ kubectl logs piddashboard-57b9d6bfbd-cbn9c
Error from server: Get https://10.194.26.119:10250/containerLogs/pid-dashboard/piddashboard-57b9d6bfbd-cbn9c/piddashboard: Service Unavailable
```

**What you expected to happen**:
Getting logs and no errors, from new nodes

**How to reproduce it (as minimally and precisely as possible)**:
I don't know if it's upgrade of cluster version 1.16.5 to 1.16.7

Or usage of proxy for the docker's daemon (I setup exactly like other working nodes)

**Anything else we need to know?**:
API logs:
```sh
...
E0221 14:48:32.630188       1 status.go:71] apiserver received an error that is not an metav1.Status: &url.Error{Op:""Get"", URL:""https://10.194.26.119:10250/containerLogs/pid-dashboard/piddashboard-57b9d6bfbd-cbn9c/piddashboard"", Err:(*errors.errorString)(0xc00b18a900)}
...
```

No problem from my PC:
```sh
$ export TOKEN=eyJhbGciOiJSUzI1NiIs...

$ curl --insecure --header ""Authorization: Bearer ${TOKEN}"" https://10.194.26.119:10250/containerLogs/pid-dashboard/piddashboard-57b9d6bfbd-cbn9c/piddashboard


Start command: java -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -Djava.security.egd=file:/dev/./urandom -jar /app.jar

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::        (v2.1.0.RELEASE)

2020-02-21 14:52:51.180  INFO 1 --- [           main] f.p.dashboard.PidDashboardApplicationKt  : Starting PidDashboardApplicationKt v0.0.1-SNAPSHOT on piddashboard-57b9d6bfbd-cbn9c with PID 1 (/app.jar started by root in /)
2020-02-21 14:52:51.187  INFO 1 --- [           main] f.p.dashboard.PidDashboardApplicationKt  : No active profile set, falling back to default profiles: default
2020-02-21 14:52:52.613  INFO 1 --- [           main] o.s.c.a.ConfigurationClassPostProcessor  : Cannot enhance @Configuration bean definition 'org.springframework.guice.annotation.ModuleRegistryConfiguration' since its singleton instanc
e has been created too early. The typical cause is a non-static @Bean method with a BeanDefinitionRegistryPostProcessor return type: Consider declaring such methods as 'static'.
2020-02-21 14:52:53.570  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
2020-02-21 14:52:53.594  INFO 1 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
...
```

**Environment**:
- Kubernetes version (use `kubectl version`):
```sh
Client Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.2"", GitCommit:""59603c6e503c87169aea6106f57b9f242f64df89"", GitTreeState:""clean"", BuildDate:""2020-01-18T23:30:10Z"", GoVersion:""go1.13.5"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""16"", GitVersion:""v1.16.3"", GitCommit:""b3cbbae08ec52a7fc73d334838e18d17e8512749"", GitTreeState:""clean"", BuildDate:""2019-11-13T11:13:49Z"", GoVersion:""go1.12.12"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration:
`On premise with kubespray`
- OS (e.g: `cat /etc/os-release`):
For all nodes:
```sh
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
```
- Kernel (e.g. `uname -a`):
For all nodes:
```sh
Linux forge-XX 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools:
`n/a`
- Network plugin and version (if this is a network-related bug):
`calico`
- Others:
Usage of proxy

Actual Cluster:
```sh
$ kubectl get nodes -o wide
NAME       STATUS   ROLES    AGE    VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME
forge-04   Ready    <none>   340d   v1.16.7   10.194.26.60    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-05   Ready    <none>   340d   v1.16.7   10.194.26.61    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-06   Ready    master   344d   v1.16.7   10.194.26.96    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-07   Ready    master   344d   v1.16.7   10.194.27.39    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-08   Ready    master   340d   v1.16.7   10.194.27.63    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-10   Ready    <none>   344d   v1.16.7   10.194.26.49    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-11   Ready    <none>   344d   v1.16.7   10.194.26.37    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-12   Ready    <none>   344d   v1.16.7   10.194.26.39    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
### LOGS KO since here
forge-13   Ready    <none>   26h    v1.16.7   10.194.26.59    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6 
forge-14   Ready    <none>   26h    v1.16.7   10.194.27.120   <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-15   Ready    <none>   26h    v1.16.7   10.194.26.213   <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-16   Ready    <none>   26h    v1.16.7   10.194.26.119   <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
```

There is no other visible bug, pod are scheduled on it, etc, etc.. just logs.

I thinks it's a config problem on masters but I don't know what...

",eyJhbGciOiJSUzI1NiIs,https://github.com/kubernetes/kubernetes
48,87229,"**What happened**:
```
[root@master01 kubernetes]# kubeadm init --config kubeadm.conf --upload-certs --certificate-key 5cf38ca270d75eb11fac6b3ef77b0e1ef7318d56a1edf3ad0687c72281c89aca
can not mix '--config' with arguments [certificate-key]
To see the stack trace of this error execute with --v=5 or higher
```
**What you expected to happen**:
i hope init success
**How to reproduce it (as minimally and precisely as possible)**:
```
kubeadm init --config kubeadm.conf --upload-certs --certificate-key 5cf38ca270d75eb11fac6b3ef77b0e1ef7318d56a1edf3ad0687c72281c89aca
```
**Anything else we need to know?**:

**Environment**:
[root@master01 kubernetes]# kubectl version --short
Client Version: v1.17.0
Unable to connect to the server: EOF
",5cf38ca270d75eb11fac6b3ef77b0e1ef7318d56a1edf3ad0687c72281c89aca,https://github.com/kubernetes/kubernetes
49,87229,"**What happened**:
```
[root@master01 kubernetes]# kubeadm init --config kubeadm.conf --upload-certs --certificate-key 5cf38ca270d75eb11fac6b3ef77b0e1ef7318d56a1edf3ad0687c72281c89aca
can not mix '--config' with arguments [certificate-key]
To see the stack trace of this error execute with --v=5 or higher
```
**What you expected to happen**:
i hope init success
**How to reproduce it (as minimally and precisely as possible)**:
```
kubeadm init --config kubeadm.conf --upload-certs --certificate-key 5cf38ca270d75eb11fac6b3ef77b0e1ef7318d56a1edf3ad0687c72281c89aca
```
**Anything else we need to know?**:

**Environment**:
[root@master01 kubernetes]# kubectl version --short
Client Version: v1.17.0
Unable to connect to the server: EOF
",5cf38ca270d75eb11fac6b3ef77b0e1ef7318d56a1edf3ad06,https://github.com/kubernetes/kubernetes
50,85578,"I am trying to create a new CRD with a password field as shown below

```
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: tests.abc.com
spec:
  group: abc.com
  versions:
    - name: v1
      served: true
      storage: true
  scope: Cluster
  names:
    plural: tests
    singular: test
    kind: Test
  validation:
    openAPIV3Schema:
      type: object
      properties:
        spec:
          type: object
          properties:
            user:
              type: string
            password:
              type: password
  additionalPrinterColumns:
    - name: User
      type: string
      description: The user who created this object
      JSONPath: .spec.user
    - name: Age
      type: date
      JSONPath: .metadata.creationTimestamp
```
When I try to create a new object

```
apiVersion: abc.com/v1
kind: Test
metadata:
  name: test
spec:
  user: john
  password: aGVsbG93b3JsZA==

```

**What happened**:

When I try to do `kubectl apply -f test.yaml`

I get the below error

```
The Test ""test"" is invalid: []: Invalid value: map[string]interface {}{""apiVersion"":""abc.com/v1"", ""kind"":""Test"", ""metadata"":map[string]interface {}{""annotations"":map[string]interface {}{""kubectl.kubernetes.io/last-applied-configuration"":""{\""apiVersion\"":\""abc.com/v1\"",\""kind\"":\""Test\"",\""metadata\"":{\""annotations\"":{},\""name\"":\""test\""},\""spec\"":{\""password\"":\""MWYyZDFlMmU2N2Rm\"",\""user\"":\""john\""}}\n""}, ""creationTimestamp"":""2019-11-24T01:24:24Z"", ""generation"":1, ""name"":""test"", ""uid"":""267b100d-0e59-11ea-8341-42010a80018e""}, ""spec"":map[string]interface {}{""password"":""aGVsbG93b3JsZA=="", ""user"":""john""}}: validation failure list:
spec.password in body must be of type password: ""string""
```

Am I missing something? My understanding was any string I provide should now be treated as a password and not exposed in kubectl get and describe commands but it returns me an error saying it should be on type password. Can someone pls clarify?

**Environment**:
- Kubernetes version (use `kubectl version`):
`Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.8"", GitCommit:""211047e9a1922595eaa3a1127ed365e9299a6c23"", GitTreeState:""clean"", BuildDate:""2019-10-15T12:11:03Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""14+"", GitVersion:""v1.14.8-gke.17"", GitCommit:""188432a69210ca32cafded81b4dd1c063720cac0"", GitTreeState:""clean"", BuildDate:""2019-11-13T20:47:11Z"", GoVersion:""go1.12.11b4"", Compiler:""gc"", Platform:""linux/amd64""}`
- Cloud provider or hardware configuration: GKE
- OS (e.g: `cat /etc/os-release`):

",additionalPrinterColumns,https://github.com/kubernetes/kubernetes
51,85578,"I am trying to create a new CRD with a password field as shown below

```
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: tests.abc.com
spec:
  group: abc.com
  versions:
    - name: v1
      served: true
      storage: true
  scope: Cluster
  names:
    plural: tests
    singular: test
    kind: Test
  validation:
    openAPIV3Schema:
      type: object
      properties:
        spec:
          type: object
          properties:
            user:
              type: string
            password:
              type: password
  additionalPrinterColumns:
    - name: User
      type: string
      description: The user who created this object
      JSONPath: .spec.user
    - name: Age
      type: date
      JSONPath: .metadata.creationTimestamp
```
When I try to create a new object

```
apiVersion: abc.com/v1
kind: Test
metadata:
  name: test
spec:
  user: john
  password: aGVsbG93b3JsZA==

```

**What happened**:

When I try to do `kubectl apply -f test.yaml`

I get the below error

```
The Test ""test"" is invalid: []: Invalid value: map[string]interface {}{""apiVersion"":""abc.com/v1"", ""kind"":""Test"", ""metadata"":map[string]interface {}{""annotations"":map[string]interface {}{""kubectl.kubernetes.io/last-applied-configuration"":""{\""apiVersion\"":\""abc.com/v1\"",\""kind\"":\""Test\"",\""metadata\"":{\""annotations\"":{},\""name\"":\""test\""},\""spec\"":{\""password\"":\""MWYyZDFlMmU2N2Rm\"",\""user\"":\""john\""}}\n""}, ""creationTimestamp"":""2019-11-24T01:24:24Z"", ""generation"":1, ""name"":""test"", ""uid"":""267b100d-0e59-11ea-8341-42010a80018e""}, ""spec"":map[string]interface {}{""password"":""aGVsbG93b3JsZA=="", ""user"":""john""}}: validation failure list:
spec.password in body must be of type password: ""string""
```

Am I missing something? My understanding was any string I provide should now be treated as a password and not exposed in kubectl get and describe commands but it returns me an error saying it should be on type password. Can someone pls clarify?

**Environment**:
- Kubernetes version (use `kubectl version`):
`Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.8"", GitCommit:""211047e9a1922595eaa3a1127ed365e9299a6c23"", GitTreeState:""clean"", BuildDate:""2019-10-15T12:11:03Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""14+"", GitVersion:""v1.14.8-gke.17"", GitCommit:""188432a69210ca32cafded81b4dd1c063720cac0"", GitTreeState:""clean"", BuildDate:""2019-11-13T20:47:11Z"", GoVersion:""go1.12.11b4"", Compiler:""gc"", Platform:""linux/amd64""}`
- Cloud provider or hardware configuration: GKE
- OS (e.g: `cat /etc/os-release`):

",user\:\john\}}\n,https://github.com/kubernetes/kubernetes
52,85578,"I am trying to create a new CRD with a password field as shown below

```
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: tests.abc.com
spec:
  group: abc.com
  versions:
    - name: v1
      served: true
      storage: true
  scope: Cluster
  names:
    plural: tests
    singular: test
    kind: Test
  validation:
    openAPIV3Schema:
      type: object
      properties:
        spec:
          type: object
          properties:
            user:
              type: string
            password:
              type: password
  additionalPrinterColumns:
    - name: User
      type: string
      description: The user who created this object
      JSONPath: .spec.user
    - name: Age
      type: date
      JSONPath: .metadata.creationTimestamp
```
When I try to create a new object

```
apiVersion: abc.com/v1
kind: Test
metadata:
  name: test
spec:
  user: john
  password: aGVsbG93b3JsZA==

```

**What happened**:

When I try to do `kubectl apply -f test.yaml`

I get the below error

```
The Test ""test"" is invalid: []: Invalid value: map[string]interface {}{""apiVersion"":""abc.com/v1"", ""kind"":""Test"", ""metadata"":map[string]interface {}{""annotations"":map[string]interface {}{""kubectl.kubernetes.io/last-applied-configuration"":""{\""apiVersion\"":\""abc.com/v1\"",\""kind\"":\""Test\"",\""metadata\"":{\""annotations\"":{},\""name\"":\""test\""},\""spec\"":{\""password\"":\""MWYyZDFlMmU2N2Rm\"",\""user\"":\""john\""}}\n""}, ""creationTimestamp"":""2019-11-24T01:24:24Z"", ""generation"":1, ""name"":""test"", ""uid"":""267b100d-0e59-11ea-8341-42010a80018e""}, ""spec"":map[string]interface {}{""password"":""aGVsbG93b3JsZA=="", ""user"":""john""}}: validation failure list:
spec.password in body must be of type password: ""string""
```

Am I missing something? My understanding was any string I provide should now be treated as a password and not exposed in kubectl get and describe commands but it returns me an error saying it should be on type password. Can someone pls clarify?

**Environment**:
- Kubernetes version (use `kubectl version`):
`Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.8"", GitCommit:""211047e9a1922595eaa3a1127ed365e9299a6c23"", GitTreeState:""clean"", BuildDate:""2019-10-15T12:11:03Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""14+"", GitVersion:""v1.14.8-gke.17"", GitCommit:""188432a69210ca32cafded81b4dd1c063720cac0"", GitTreeState:""clean"", BuildDate:""2019-11-13T20:47:11Z"", GoVersion:""go1.12.11b4"", Compiler:""gc"", Platform:""linux/amd64""}`
- Cloud provider or hardware configuration: GKE
- OS (e.g: `cat /etc/os-release`):

",MWYyZDFlMmU2N2Rm,https://github.com/kubernetes/kubernetes
53,85578,"I am trying to create a new CRD with a password field as shown below

```
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: tests.abc.com
spec:
  group: abc.com
  versions:
    - name: v1
      served: true
      storage: true
  scope: Cluster
  names:
    plural: tests
    singular: test
    kind: Test
  validation:
    openAPIV3Schema:
      type: object
      properties:
        spec:
          type: object
          properties:
            user:
              type: string
            password:
              type: password
  additionalPrinterColumns:
    - name: User
      type: string
      description: The user who created this object
      JSONPath: .spec.user
    - name: Age
      type: date
      JSONPath: .metadata.creationTimestamp
```
When I try to create a new object

```
apiVersion: abc.com/v1
kind: Test
metadata:
  name: test
spec:
  user: john
  password: aGVsbG93b3JsZA==

```

**What happened**:

When I try to do `kubectl apply -f test.yaml`

I get the below error

```
The Test ""test"" is invalid: []: Invalid value: map[string]interface {}{""apiVersion"":""abc.com/v1"", ""kind"":""Test"", ""metadata"":map[string]interface {}{""annotations"":map[string]interface {}{""kubectl.kubernetes.io/last-applied-configuration"":""{\""apiVersion\"":\""abc.com/v1\"",\""kind\"":\""Test\"",\""metadata\"":{\""annotations\"":{},\""name\"":\""test\""},\""spec\"":{\""password\"":\""MWYyZDFlMmU2N2Rm\"",\""user\"":\""john\""}}\n""}, ""creationTimestamp"":""2019-11-24T01:24:24Z"", ""generation"":1, ""name"":""test"", ""uid"":""267b100d-0e59-11ea-8341-42010a80018e""}, ""spec"":map[string]interface {}{""password"":""aGVsbG93b3JsZA=="", ""user"":""john""}}: validation failure list:
spec.password in body must be of type password: ""string""
```

Am I missing something? My understanding was any string I provide should now be treated as a password and not exposed in kubectl get and describe commands but it returns me an error saying it should be on type password. Can someone pls clarify?

**Environment**:
- Kubernetes version (use `kubectl version`):
`Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.8"", GitCommit:""211047e9a1922595eaa3a1127ed365e9299a6c23"", GitTreeState:""clean"", BuildDate:""2019-10-15T12:11:03Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""14+"", GitVersion:""v1.14.8-gke.17"", GitCommit:""188432a69210ca32cafded81b4dd1c063720cac0"", GitTreeState:""clean"", BuildDate:""2019-11-13T20:47:11Z"", GoVersion:""go1.12.11b4"", Compiler:""gc"", Platform:""linux/amd64""}`
- Cloud provider or hardware configuration: GKE
- OS (e.g: `cat /etc/os-release`):

",describe,https://github.com/kubernetes/kubernetes
54,85578,"I am trying to create a new CRD with a password field as shown below

```
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: tests.abc.com
spec:
  group: abc.com
  versions:
    - name: v1
      served: true
      storage: true
  scope: Cluster
  names:
    plural: tests
    singular: test
    kind: Test
  validation:
    openAPIV3Schema:
      type: object
      properties:
        spec:
          type: object
          properties:
            user:
              type: string
            password:
              type: password
  additionalPrinterColumns:
    - name: User
      type: string
      description: The user who created this object
      JSONPath: .spec.user
    - name: Age
      type: date
      JSONPath: .metadata.creationTimestamp
```
When I try to create a new object

```
apiVersion: abc.com/v1
kind: Test
metadata:
  name: test
spec:
  user: john
  password: aGVsbG93b3JsZA==

```

**What happened**:

When I try to do `kubectl apply -f test.yaml`

I get the below error

```
The Test ""test"" is invalid: []: Invalid value: map[string]interface {}{""apiVersion"":""abc.com/v1"", ""kind"":""Test"", ""metadata"":map[string]interface {}{""annotations"":map[string]interface {}{""kubectl.kubernetes.io/last-applied-configuration"":""{\""apiVersion\"":\""abc.com/v1\"",\""kind\"":\""Test\"",\""metadata\"":{\""annotations\"":{},\""name\"":\""test\""},\""spec\"":{\""password\"":\""MWYyZDFlMmU2N2Rm\"",\""user\"":\""john\""}}\n""}, ""creationTimestamp"":""2019-11-24T01:24:24Z"", ""generation"":1, ""name"":""test"", ""uid"":""267b100d-0e59-11ea-8341-42010a80018e""}, ""spec"":map[string]interface {}{""password"":""aGVsbG93b3JsZA=="", ""user"":""john""}}: validation failure list:
spec.password in body must be of type password: ""string""
```

Am I missing something? My understanding was any string I provide should now be treated as a password and not exposed in kubectl get and describe commands but it returns me an error saying it should be on type password. Can someone pls clarify?

**Environment**:
- Kubernetes version (use `kubectl version`):
`Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.8"", GitCommit:""211047e9a1922595eaa3a1127ed365e9299a6c23"", GitTreeState:""clean"", BuildDate:""2019-10-15T12:11:03Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""14+"", GitVersion:""v1.14.8-gke.17"", GitCommit:""188432a69210ca32cafded81b4dd1c063720cac0"", GitTreeState:""clean"", BuildDate:""2019-11-13T20:47:11Z"", GoVersion:""go1.12.11b4"", Compiler:""gc"", Platform:""linux/amd64""}`
- Cloud provider or hardware configuration: GKE
- OS (e.g: `cat /etc/os-release`):

",validation,https://github.com/kubernetes/kubernetes
55,85578,"I am trying to create a new CRD with a password field as shown below

```
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: tests.abc.com
spec:
  group: abc.com
  versions:
    - name: v1
      served: true
      storage: true
  scope: Cluster
  names:
    plural: tests
    singular: test
    kind: Test
  validation:
    openAPIV3Schema:
      type: object
      properties:
        spec:
          type: object
          properties:
            user:
              type: string
            password:
              type: password
  additionalPrinterColumns:
    - name: User
      type: string
      description: The user who created this object
      JSONPath: .spec.user
    - name: Age
      type: date
      JSONPath: .metadata.creationTimestamp
```
When I try to create a new object

```
apiVersion: abc.com/v1
kind: Test
metadata:
  name: test
spec:
  user: john
  password: aGVsbG93b3JsZA==

```

**What happened**:

When I try to do `kubectl apply -f test.yaml`

I get the below error

```
The Test ""test"" is invalid: []: Invalid value: map[string]interface {}{""apiVersion"":""abc.com/v1"", ""kind"":""Test"", ""metadata"":map[string]interface {}{""annotations"":map[string]interface {}{""kubectl.kubernetes.io/last-applied-configuration"":""{\""apiVersion\"":\""abc.com/v1\"",\""kind\"":\""Test\"",\""metadata\"":{\""annotations\"":{},\""name\"":\""test\""},\""spec\"":{\""password\"":\""MWYyZDFlMmU2N2Rm\"",\""user\"":\""john\""}}\n""}, ""creationTimestamp"":""2019-11-24T01:24:24Z"", ""generation"":1, ""name"":""test"", ""uid"":""267b100d-0e59-11ea-8341-42010a80018e""}, ""spec"":map[string]interface {}{""password"":""aGVsbG93b3JsZA=="", ""user"":""john""}}: validation failure list:
spec.password in body must be of type password: ""string""
```

Am I missing something? My understanding was any string I provide should now be treated as a password and not exposed in kubectl get and describe commands but it returns me an error saying it should be on type password. Can someone pls clarify?

**Environment**:
- Kubernetes version (use `kubectl version`):
`Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.8"", GitCommit:""211047e9a1922595eaa3a1127ed365e9299a6c23"", GitTreeState:""clean"", BuildDate:""2019-10-15T12:11:03Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""14+"", GitVersion:""v1.14.8-gke.17"", GitCommit:""188432a69210ca32cafded81b4dd1c063720cac0"", GitTreeState:""clean"", BuildDate:""2019-11-13T20:47:11Z"", GoVersion:""go1.12.11b4"", Compiler:""gc"", Platform:""linux/amd64""}`
- Cloud provider or hardware configuration: GKE
- OS (e.g: `cat /etc/os-release`):

",something,https://github.com/kubernetes/kubernetes
56,85578,"I am trying to create a new CRD with a password field as shown below

```
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: tests.abc.com
spec:
  group: abc.com
  versions:
    - name: v1
      served: true
      storage: true
  scope: Cluster
  names:
    plural: tests
    singular: test
    kind: Test
  validation:
    openAPIV3Schema:
      type: object
      properties:
        spec:
          type: object
          properties:
            user:
              type: string
            password:
              type: password
  additionalPrinterColumns:
    - name: User
      type: string
      description: The user who created this object
      JSONPath: .spec.user
    - name: Age
      type: date
      JSONPath: .metadata.creationTimestamp
```
When I try to create a new object

```
apiVersion: abc.com/v1
kind: Test
metadata:
  name: test
spec:
  user: john
  password: aGVsbG93b3JsZA==

```

**What happened**:

When I try to do `kubectl apply -f test.yaml`

I get the below error

```
The Test ""test"" is invalid: []: Invalid value: map[string]interface {}{""apiVersion"":""abc.com/v1"", ""kind"":""Test"", ""metadata"":map[string]interface {}{""annotations"":map[string]interface {}{""kubectl.kubernetes.io/last-applied-configuration"":""{\""apiVersion\"":\""abc.com/v1\"",\""kind\"":\""Test\"",\""metadata\"":{\""annotations\"":{},\""name\"":\""test\""},\""spec\"":{\""password\"":\""MWYyZDFlMmU2N2Rm\"",\""user\"":\""john\""}}\n""}, ""creationTimestamp"":""2019-11-24T01:24:24Z"", ""generation"":1, ""name"":""test"", ""uid"":""267b100d-0e59-11ea-8341-42010a80018e""}, ""spec"":map[string]interface {}{""password"":""aGVsbG93b3JsZA=="", ""user"":""john""}}: validation failure list:
spec.password in body must be of type password: ""string""
```

Am I missing something? My understanding was any string I provide should now be treated as a password and not exposed in kubectl get and describe commands but it returns me an error saying it should be on type password. Can someone pls clarify?

**Environment**:
- Kubernetes version (use `kubectl version`):
`Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.8"", GitCommit:""211047e9a1922595eaa3a1127ed365e9299a6c23"", GitTreeState:""clean"", BuildDate:""2019-10-15T12:11:03Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""14+"", GitVersion:""v1.14.8-gke.17"", GitCommit:""188432a69210ca32cafded81b4dd1c063720cac0"", GitTreeState:""clean"", BuildDate:""2019-11-13T20:47:11Z"", GoVersion:""go1.12.11b4"", Compiler:""gc"", Platform:""linux/amd64""}`
- Cloud provider or hardware configuration: GKE
- OS (e.g: `cat /etc/os-release`):

",apiVersion,https://github.com/kubernetes/kubernetes
57,85578,"I am trying to create a new CRD with a password field as shown below

```
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: tests.abc.com
spec:
  group: abc.com
  versions:
    - name: v1
      served: true
      storage: true
  scope: Cluster
  names:
    plural: tests
    singular: test
    kind: Test
  validation:
    openAPIV3Schema:
      type: object
      properties:
        spec:
          type: object
          properties:
            user:
              type: string
            password:
              type: password
  additionalPrinterColumns:
    - name: User
      type: string
      description: The user who created this object
      JSONPath: .spec.user
    - name: Age
      type: date
      JSONPath: .metadata.creationTimestamp
```
When I try to create a new object

```
apiVersion: abc.com/v1
kind: Test
metadata:
  name: test
spec:
  user: john
  password: aGVsbG93b3JsZA==

```

**What happened**:

When I try to do `kubectl apply -f test.yaml`

I get the below error

```
The Test ""test"" is invalid: []: Invalid value: map[string]interface {}{""apiVersion"":""abc.com/v1"", ""kind"":""Test"", ""metadata"":map[string]interface {}{""annotations"":map[string]interface {}{""kubectl.kubernetes.io/last-applied-configuration"":""{\""apiVersion\"":\""abc.com/v1\"",\""kind\"":\""Test\"",\""metadata\"":{\""annotations\"":{},\""name\"":\""test\""},\""spec\"":{\""password\"":\""MWYyZDFlMmU2N2Rm\"",\""user\"":\""john\""}}\n""}, ""creationTimestamp"":""2019-11-24T01:24:24Z"", ""generation"":1, ""name"":""test"", ""uid"":""267b100d-0e59-11ea-8341-42010a80018e""}, ""spec"":map[string]interface {}{""password"":""aGVsbG93b3JsZA=="", ""user"":""john""}}: validation failure list:
spec.password in body must be of type password: ""string""
```

Am I missing something? My understanding was any string I provide should now be treated as a password and not exposed in kubectl get and describe commands but it returns me an error saying it should be on type password. Can someone pls clarify?

**Environment**:
- Kubernetes version (use `kubectl version`):
`Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.8"", GitCommit:""211047e9a1922595eaa3a1127ed365e9299a6c23"", GitTreeState:""clean"", BuildDate:""2019-10-15T12:11:03Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""14+"", GitVersion:""v1.14.8-gke.17"", GitCommit:""188432a69210ca32cafded81b4dd1c063720cac0"", GitTreeState:""clean"", BuildDate:""2019-11-13T20:47:11Z"", GoVersion:""go1.12.11b4"", Compiler:""gc"", Platform:""linux/amd64""}`
- Cloud provider or hardware configuration: GKE
- OS (e.g: `cat /etc/os-release`):

",Environment,https://github.com/kubernetes/kubernetes
58,85578,"I am trying to create a new CRD with a password field as shown below

```
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: tests.abc.com
spec:
  group: abc.com
  versions:
    - name: v1
      served: true
      storage: true
  scope: Cluster
  names:
    plural: tests
    singular: test
    kind: Test
  validation:
    openAPIV3Schema:
      type: object
      properties:
        spec:
          type: object
          properties:
            user:
              type: string
            password:
              type: password
  additionalPrinterColumns:
    - name: User
      type: string
      description: The user who created this object
      JSONPath: .spec.user
    - name: Age
      type: date
      JSONPath: .metadata.creationTimestamp
```
When I try to create a new object

```
apiVersion: abc.com/v1
kind: Test
metadata:
  name: test
spec:
  user: john
  password: aGVsbG93b3JsZA==

```

**What happened**:

When I try to do `kubectl apply -f test.yaml`

I get the below error

```
The Test ""test"" is invalid: []: Invalid value: map[string]interface {}{""apiVersion"":""abc.com/v1"", ""kind"":""Test"", ""metadata"":map[string]interface {}{""annotations"":map[string]interface {}{""kubectl.kubernetes.io/last-applied-configuration"":""{\""apiVersion\"":\""abc.com/v1\"",\""kind\"":\""Test\"",\""metadata\"":{\""annotations\"":{},\""name\"":\""test\""},\""spec\"":{\""password\"":\""MWYyZDFlMmU2N2Rm\"",\""user\"":\""john\""}}\n""}, ""creationTimestamp"":""2019-11-24T01:24:24Z"", ""generation"":1, ""name"":""test"", ""uid"":""267b100d-0e59-11ea-8341-42010a80018e""}, ""spec"":map[string]interface {}{""password"":""aGVsbG93b3JsZA=="", ""user"":""john""}}: validation failure list:
spec.password in body must be of type password: ""string""
```

Am I missing something? My understanding was any string I provide should now be treated as a password and not exposed in kubectl get and describe commands but it returns me an error saying it should be on type password. Can someone pls clarify?

**Environment**:
- Kubernetes version (use `kubectl version`):
`Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.8"", GitCommit:""211047e9a1922595eaa3a1127ed365e9299a6c23"", GitTreeState:""clean"", BuildDate:""2019-10-15T12:11:03Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""14+"", GitVersion:""v1.14.8-gke.17"", GitCommit:""188432a69210ca32cafded81b4dd1c063720cac0"", GitTreeState:""clean"", BuildDate:""2019-11-13T20:47:11Z"", GoVersion:""go1.12.11b4"", Compiler:""gc"", Platform:""linux/amd64""}`
- Cloud provider or hardware configuration: GKE
- OS (e.g: `cat /etc/os-release`):

",happened,https://github.com/kubernetes/kubernetes
59,80582,"I can't create (join) a new control plane node to a cluster using a JoinConfiguration file (with ControlPlane informations in it).

**What happened**:
I'm using this JoinConfiguration:
```yaml
apiVersion: kubeadm.k8s.io/v1beta2
kind: JoinConfiguration
discovery:
  bootstrapToken:
    apiServerEndpoint: ""10.0.0.1:6443""
    token: ""3a08jv.c0izixklcxtmnze7""
    unsafeSkipCAVerification: true
nodeRegistration:
  kubeletExtraArgs:
    cloud-provider: ""external""
ControlPlane:
  localAPIEndpoint:
    advertiseAddress: ""10.0.0.2""
    bindPort: 6443
  certificateKey: ""e6a2eb8381237ab72a4fa94f30285ec12a9694d750b9785706a83bfcbbbd2204""
```
and try to join the cluster with the command
```shell
kubeadm join --config=join-config.yaml

[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Downloading configuration for the kubelet from the ""kubelet-config-1.15"" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file ""/var/lib/kubelet/config.yaml""
[kubelet-start] Writing kubelet environment file with flags to file ""/var/lib/kubelet/kubeadm-flags.env""
[kubelet-start] Activating the kubelet service
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

```
**What you expected to happen**:
I expected to have a new **control plane** node configured but kubeadm only configure a **worker** node (not a control plane node)

**Environment**:
- Kubernetes version (use `kubectl version`): v1.15.0
- Cloud provider or hardware configuration: external (openstack)
- OS (e.g: `cat /etc/os-release`): Container Linux by CoreOS 2023.4.0 (Rhyolite)
- Kernel (e.g. `uname -a`): 4.19.23-coreos-r1
- Install tools: kubeadm 1.15.0

@kubernetes/sig/cluster-lifecycle-bugs",e6a2eb8381237ab72a4fa94f30285ec12a9694d750b9785706a83bfcbbbd2204,https://github.com/kubernetes/kubernetes
60,80582,"I can't create (join) a new control plane node to a cluster using a JoinConfiguration file (with ControlPlane informations in it).

**What happened**:
I'm using this JoinConfiguration:
```yaml
apiVersion: kubeadm.k8s.io/v1beta2
kind: JoinConfiguration
discovery:
  bootstrapToken:
    apiServerEndpoint: ""10.0.0.1:6443""
    token: ""3a08jv.c0izixklcxtmnze7""
    unsafeSkipCAVerification: true
nodeRegistration:
  kubeletExtraArgs:
    cloud-provider: ""external""
ControlPlane:
  localAPIEndpoint:
    advertiseAddress: ""10.0.0.2""
    bindPort: 6443
  certificateKey: ""e6a2eb8381237ab72a4fa94f30285ec12a9694d750b9785706a83bfcbbbd2204""
```
and try to join the cluster with the command
```shell
kubeadm join --config=join-config.yaml

[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Downloading configuration for the kubelet from the ""kubelet-config-1.15"" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file ""/var/lib/kubelet/config.yaml""
[kubelet-start] Writing kubelet environment file with flags to file ""/var/lib/kubelet/kubeadm-flags.env""
[kubelet-start] Activating the kubelet service
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

```
**What you expected to happen**:
I expected to have a new **control plane** node configured but kubeadm only configure a **worker** node (not a control plane node)

**Environment**:
- Kubernetes version (use `kubectl version`): v1.15.0
- Cloud provider or hardware configuration: external (openstack)
- OS (e.g: `cat /etc/os-release`): Container Linux by CoreOS 2023.4.0 (Rhyolite)
- Kernel (e.g. `uname -a`): 4.19.23-coreos-r1
- Install tools: kubeadm 1.15.0

@kubernetes/sig/cluster-lifecycle-bugs",nodeRegistration,https://github.com/kubernetes/kubernetes
61,80582,"I can't create (join) a new control plane node to a cluster using a JoinConfiguration file (with ControlPlane informations in it).

**What happened**:
I'm using this JoinConfiguration:
```yaml
apiVersion: kubeadm.k8s.io/v1beta2
kind: JoinConfiguration
discovery:
  bootstrapToken:
    apiServerEndpoint: ""10.0.0.1:6443""
    token: ""3a08jv.c0izixklcxtmnze7""
    unsafeSkipCAVerification: true
nodeRegistration:
  kubeletExtraArgs:
    cloud-provider: ""external""
ControlPlane:
  localAPIEndpoint:
    advertiseAddress: ""10.0.0.2""
    bindPort: 6443
  certificateKey: ""e6a2eb8381237ab72a4fa94f30285ec12a9694d750b9785706a83bfcbbbd2204""
```
and try to join the cluster with the command
```shell
kubeadm join --config=join-config.yaml

[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Downloading configuration for the kubelet from the ""kubelet-config-1.15"" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file ""/var/lib/kubelet/config.yaml""
[kubelet-start] Writing kubelet environment file with flags to file ""/var/lib/kubelet/kubeadm-flags.env""
[kubelet-start] Activating the kubelet service
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

```
**What you expected to happen**:
I expected to have a new **control plane** node configured but kubeadm only configure a **worker** node (not a control plane node)

**Environment**:
- Kubernetes version (use `kubectl version`): v1.15.0
- Cloud provider or hardware configuration: external (openstack)
- OS (e.g: `cat /etc/os-release`): Container Linux by CoreOS 2023.4.0 (Rhyolite)
- Kernel (e.g. `uname -a`): 4.19.23-coreos-r1
- Install tools: kubeadm 1.15.0

@kubernetes/sig/cluster-lifecycle-bugs",apiServerEndpoint,https://github.com/kubernetes/kubernetes
62,76667,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

```
kubectl get secrets personal-secret  -o yaml
Error from server: illegal base64 data at input byte 3
```

this is the output most of the times. It sometimes gives the correct output also.

**What you expected to happen**:
```
kubectl get secrets personal-secret  -o yaml
apiVersion: v1
data:
  password: c3Vwc2VyLXN0cm9uZy1wYXNz
  username: bXVuYWk=
kind: Secret
metadata:
  creationTimestamp: 2019-04-16T15:07:24Z
  name: personal-secret
  namespace: default
  resourceVersion: ""74250""
  selfLink: /api/v1/namespaces/default/secrets/personal-secret
  uid: 57288509-6059-11e9-affd-021b36e1a948
type: Opaque
```

**How to reproduce it (as minimally and precisely as possible)**:

For encryption of etcd I followed [docs](https://v1-12.docs.kubernetes.io/docs/tasks/administer-cluster/encrypt-data/)

Encryption Config with aescbc
```
EtcdEncryptionKey=$(head -c 32 /dev/urandom | base64)
```
```cat >> /etc/kubernetes/etcdEncryption/etcdEncryptionConfig.conf << EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
    - secrets
    providers:
    - aescbc:
        keys:
        - name: key1
          secret: ${EtcdEncryptionKey}
    - identity: {}
```

Create a secret
```
kubectl create secret generic personal-secret --from-literal=username=munai --from-literal=password=supser-strong-pass
```
Retrive the secret
```
kubectl get secrets personal-secret  -o yaml
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```./kubectl version
Client Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:24:45Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:14:49Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration:
AWS - EC2
- OS (e.g: `cat /etc/os-release`):
```
cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
```
- Kernel (e.g. `uname -a`):
```uname -a
Linux ip-10-13-0-207.us-west-2.compute.internal 3.10.0-957.10.1.el7.x86_64 #1 SMP Mon Mar 18 15:06:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools:
kubeadm
- Others:
",EtcdEncryptionKey,https://github.com/kubernetes/kubernetes
63,76667,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

```
kubectl get secrets personal-secret  -o yaml
Error from server: illegal base64 data at input byte 3
```

this is the output most of the times. It sometimes gives the correct output also.

**What you expected to happen**:
```
kubectl get secrets personal-secret  -o yaml
apiVersion: v1
data:
  password: c3Vwc2VyLXN0cm9uZy1wYXNz
  username: bXVuYWk=
kind: Secret
metadata:
  creationTimestamp: 2019-04-16T15:07:24Z
  name: personal-secret
  namespace: default
  resourceVersion: ""74250""
  selfLink: /api/v1/namespaces/default/secrets/personal-secret
  uid: 57288509-6059-11e9-affd-021b36e1a948
type: Opaque
```

**How to reproduce it (as minimally and precisely as possible)**:

For encryption of etcd I followed [docs](https://v1-12.docs.kubernetes.io/docs/tasks/administer-cluster/encrypt-data/)

Encryption Config with aescbc
```
EtcdEncryptionKey=$(head -c 32 /dev/urandom | base64)
```
```cat >> /etc/kubernetes/etcdEncryption/etcdEncryptionConfig.conf << EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
    - secrets
    providers:
    - aescbc:
        keys:
        - name: key1
          secret: ${EtcdEncryptionKey}
    - identity: {}
```

Create a secret
```
kubectl create secret generic personal-secret --from-literal=username=munai --from-literal=password=supser-strong-pass
```
Retrive the secret
```
kubectl get secrets personal-secret  -o yaml
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```./kubectl version
Client Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:24:45Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:14:49Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration:
AWS - EC2
- OS (e.g: `cat /etc/os-release`):
```
cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
```
- Kernel (e.g. `uname -a`):
```uname -a
Linux ip-10-13-0-207.us-west-2.compute.internal 3.10.0-957.10.1.el7.x86_64 #1 SMP Mon Mar 18 15:06:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools:
kubeadm
- Others:
",c3Vwc2VyLXN0cm9uZy1wYXNz,https://github.com/kubernetes/kubernetes
64,76667,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

```
kubectl get secrets personal-secret  -o yaml
Error from server: illegal base64 data at input byte 3
```

this is the output most of the times. It sometimes gives the correct output also.

**What you expected to happen**:
```
kubectl get secrets personal-secret  -o yaml
apiVersion: v1
data:
  password: c3Vwc2VyLXN0cm9uZy1wYXNz
  username: bXVuYWk=
kind: Secret
metadata:
  creationTimestamp: 2019-04-16T15:07:24Z
  name: personal-secret
  namespace: default
  resourceVersion: ""74250""
  selfLink: /api/v1/namespaces/default/secrets/personal-secret
  uid: 57288509-6059-11e9-affd-021b36e1a948
type: Opaque
```

**How to reproduce it (as minimally and precisely as possible)**:

For encryption of etcd I followed [docs](https://v1-12.docs.kubernetes.io/docs/tasks/administer-cluster/encrypt-data/)

Encryption Config with aescbc
```
EtcdEncryptionKey=$(head -c 32 /dev/urandom | base64)
```
```cat >> /etc/kubernetes/etcdEncryption/etcdEncryptionConfig.conf << EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
    - secrets
    providers:
    - aescbc:
        keys:
        - name: key1
          secret: ${EtcdEncryptionKey}
    - identity: {}
```

Create a secret
```
kubectl create secret generic personal-secret --from-literal=username=munai --from-literal=password=supser-strong-pass
```
Retrive the secret
```
kubectl get secrets personal-secret  -o yaml
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```./kubectl version
Client Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:24:45Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:14:49Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration:
AWS - EC2
- OS (e.g: `cat /etc/os-release`):
```
cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
```
- Kernel (e.g. `uname -a`):
```uname -a
Linux ip-10-13-0-207.us-west-2.compute.internal 3.10.0-957.10.1.el7.x86_64 #1 SMP Mon Mar 18 15:06:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools:
kubeadm
- Others:
",-04-16T15:07:24Z,https://github.com/kubernetes/kubernetes
65,76667,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

```
kubectl get secrets personal-secret  -o yaml
Error from server: illegal base64 data at input byte 3
```

this is the output most of the times. It sometimes gives the correct output also.

**What you expected to happen**:
```
kubectl get secrets personal-secret  -o yaml
apiVersion: v1
data:
  password: c3Vwc2VyLXN0cm9uZy1wYXNz
  username: bXVuYWk=
kind: Secret
metadata:
  creationTimestamp: 2019-04-16T15:07:24Z
  name: personal-secret
  namespace: default
  resourceVersion: ""74250""
  selfLink: /api/v1/namespaces/default/secrets/personal-secret
  uid: 57288509-6059-11e9-affd-021b36e1a948
type: Opaque
```

**How to reproduce it (as minimally and precisely as possible)**:

For encryption of etcd I followed [docs](https://v1-12.docs.kubernetes.io/docs/tasks/administer-cluster/encrypt-data/)

Encryption Config with aescbc
```
EtcdEncryptionKey=$(head -c 32 /dev/urandom | base64)
```
```cat >> /etc/kubernetes/etcdEncryption/etcdEncryptionConfig.conf << EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
    - secrets
    providers:
    - aescbc:
        keys:
        - name: key1
          secret: ${EtcdEncryptionKey}
    - identity: {}
```

Create a secret
```
kubectl create secret generic personal-secret --from-literal=username=munai --from-literal=password=supser-strong-pass
```
Retrive the secret
```
kubectl get secrets personal-secret  -o yaml
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```./kubectl version
Client Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:24:45Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:14:49Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration:
AWS - EC2
- OS (e.g: `cat /etc/os-release`):
```
cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
```
- Kernel (e.g. `uname -a`):
```uname -a
Linux ip-10-13-0-207.us-west-2.compute.internal 3.10.0-957.10.1.el7.x86_64 #1 SMP Mon Mar 18 15:06:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools:
kubeadm
- Others:
",supser-strong-pass,https://github.com/kubernetes/kubernetes
66,76667,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

```
kubectl get secrets personal-secret  -o yaml
Error from server: illegal base64 data at input byte 3
```

this is the output most of the times. It sometimes gives the correct output also.

**What you expected to happen**:
```
kubectl get secrets personal-secret  -o yaml
apiVersion: v1
data:
  password: c3Vwc2VyLXN0cm9uZy1wYXNz
  username: bXVuYWk=
kind: Secret
metadata:
  creationTimestamp: 2019-04-16T15:07:24Z
  name: personal-secret
  namespace: default
  resourceVersion: ""74250""
  selfLink: /api/v1/namespaces/default/secrets/personal-secret
  uid: 57288509-6059-11e9-affd-021b36e1a948
type: Opaque
```

**How to reproduce it (as minimally and precisely as possible)**:

For encryption of etcd I followed [docs](https://v1-12.docs.kubernetes.io/docs/tasks/administer-cluster/encrypt-data/)

Encryption Config with aescbc
```
EtcdEncryptionKey=$(head -c 32 /dev/urandom | base64)
```
```cat >> /etc/kubernetes/etcdEncryption/etcdEncryptionConfig.conf << EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
    - secrets
    providers:
    - aescbc:
        keys:
        - name: key1
          secret: ${EtcdEncryptionKey}
    - identity: {}
```

Create a secret
```
kubectl create secret generic personal-secret --from-literal=username=munai --from-literal=password=supser-strong-pass
```
Retrive the secret
```
kubectl get secrets personal-secret  -o yaml
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```./kubectl version
Client Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:24:45Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:14:49Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration:
AWS - EC2
- OS (e.g: `cat /etc/os-release`):
```
cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
```
- Kernel (e.g. `uname -a`):
```uname -a
Linux ip-10-13-0-207.us-west-2.compute.internal 3.10.0-957.10.1.el7.x86_64 #1 SMP Mon Mar 18 15:06:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools:
kubeadm
- Others:
",literal=username=munai,https://github.com/kubernetes/kubernetes
67,76667,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

```
kubectl get secrets personal-secret  -o yaml
Error from server: illegal base64 data at input byte 3
```

this is the output most of the times. It sometimes gives the correct output also.

**What you expected to happen**:
```
kubectl get secrets personal-secret  -o yaml
apiVersion: v1
data:
  password: c3Vwc2VyLXN0cm9uZy1wYXNz
  username: bXVuYWk=
kind: Secret
metadata:
  creationTimestamp: 2019-04-16T15:07:24Z
  name: personal-secret
  namespace: default
  resourceVersion: ""74250""
  selfLink: /api/v1/namespaces/default/secrets/personal-secret
  uid: 57288509-6059-11e9-affd-021b36e1a948
type: Opaque
```

**How to reproduce it (as minimally and precisely as possible)**:

For encryption of etcd I followed [docs](https://v1-12.docs.kubernetes.io/docs/tasks/administer-cluster/encrypt-data/)

Encryption Config with aescbc
```
EtcdEncryptionKey=$(head -c 32 /dev/urandom | base64)
```
```cat >> /etc/kubernetes/etcdEncryption/etcdEncryptionConfig.conf << EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
    - secrets
    providers:
    - aescbc:
        keys:
        - name: key1
          secret: ${EtcdEncryptionKey}
    - identity: {}
```

Create a secret
```
kubectl create secret generic personal-secret --from-literal=username=munai --from-literal=password=supser-strong-pass
```
Retrive the secret
```
kubectl get secrets personal-secret  -o yaml
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```./kubectl version
Client Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:24:45Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:14:49Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration:
AWS - EC2
- OS (e.g: `cat /etc/os-release`):
```
cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
```
- Kernel (e.g. `uname -a`):
```uname -a
Linux ip-10-13-0-207.us-west-2.compute.internal 3.10.0-957.10.1.el7.x86_64 #1 SMP Mon Mar 18 15:06:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools:
kubeadm
- Others:
",bXVuYWk=,https://github.com/kubernetes/kubernetes
68,76010,"I set kubernetes cluster by kubeadm on my local ubuntu server. I tried to encrypt etcd by the same way as https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ , then it breaks `/registry/secrets/default/default-token-xxxxx`

here is my encryption configuration file.

```yaml
    kind: EncryptionConfiguration
    apiVersion: apiserver.config.k8s.io/v1
    resources:
      - resources:
        - secrets
        providers:
        - aescbc:
            keys:
            - name: key1
              secret: F3rax7e5B/gGbAR8LLLLbPAp+R4W8KGXKjXfhTp8bb0=
            - name: key2
              secret: yuat0/ntgNh8d4NJwkWWSCdi7/WCYJ7AGfjTSU1XKHE=
```

and after created it, I configured my kube-apiserver.yaml like below.

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=xx.x.xxx.xxx
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/path/to/my/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/path/to/my/etcd/ca.crt
    - --etcd-certfile=/path/to/my/apiserver-etcd-client.crt
    - --etcd-keyfile=/path/to/my/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --encryption-provider-config=/path/to/enc/encryption_conf.yaml
    - --insecure-port=0
    - --kubelet-client-certificate=/path/to/myapiserver-kubelet-client.crt
    - --kubelet-client-key=/path/to/my/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/path/to/my/front-proxy-client.crt
    - --proxy-client-key-file=/path/to/my/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/path/to/my/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-key-file=/path/to/my/sa.pub
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/path/to/my/apiserver.crt
    - --tls-private-key-file=/path/to/my/apiserver.key
    image: k8s.gcr.io/kube-apiserver:v1.14.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: xx.x.xxx.xxx
        path: /healthz
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 15
      timeoutSeconds: 15
    name: kube-apiserver
    resources:
      requests:
        cpu: 250m
    volumeMounts:
    - mountPath: /path/to/enc
      name: encryption-conf
      readOnly: true
  ...
  hostNetwork: true
  priorityClassName: system-cluster-critical
  volumes:
  - hostPath:
      path: /path/to/enc
      type: DirectoryOrCreate
    name: encryption-conf
 ...
status: {}
```

after restarting apiserver, `unable to transform key ""/registry/secrets/default/default-token-xxx"": no matching prefix found` appeared.

**environment**

- Ubuntu Server 18.04
- build cluster by using kubeadm(v1.14.0)
- `kubectl version`: v1.14.0
- cni:  Calico(v3.5)

/kind bug",R4W8KGXKjXfhTp8bb0,https://github.com/kubernetes/kubernetes
69,76010,"I set kubernetes cluster by kubeadm on my local ubuntu server. I tried to encrypt etcd by the same way as https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ , then it breaks `/registry/secrets/default/default-token-xxxxx`

here is my encryption configuration file.

```yaml
    kind: EncryptionConfiguration
    apiVersion: apiserver.config.k8s.io/v1
    resources:
      - resources:
        - secrets
        providers:
        - aescbc:
            keys:
            - name: key1
              secret: F3rax7e5B/gGbAR8LLLLbPAp+R4W8KGXKjXfhTp8bb0=
            - name: key2
              secret: yuat0/ntgNh8d4NJwkWWSCdi7/WCYJ7AGfjTSU1XKHE=
```

and after created it, I configured my kube-apiserver.yaml like below.

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=xx.x.xxx.xxx
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/path/to/my/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/path/to/my/etcd/ca.crt
    - --etcd-certfile=/path/to/my/apiserver-etcd-client.crt
    - --etcd-keyfile=/path/to/my/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --encryption-provider-config=/path/to/enc/encryption_conf.yaml
    - --insecure-port=0
    - --kubelet-client-certificate=/path/to/myapiserver-kubelet-client.crt
    - --kubelet-client-key=/path/to/my/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/path/to/my/front-proxy-client.crt
    - --proxy-client-key-file=/path/to/my/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/path/to/my/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-key-file=/path/to/my/sa.pub
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/path/to/my/apiserver.crt
    - --tls-private-key-file=/path/to/my/apiserver.key
    image: k8s.gcr.io/kube-apiserver:v1.14.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: xx.x.xxx.xxx
        path: /healthz
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 15
      timeoutSeconds: 15
    name: kube-apiserver
    resources:
      requests:
        cpu: 250m
    volumeMounts:
    - mountPath: /path/to/enc
      name: encryption-conf
      readOnly: true
  ...
  hostNetwork: true
  priorityClassName: system-cluster-critical
  volumes:
  - hostPath:
      path: /path/to/enc
      type: DirectoryOrCreate
    name: encryption-conf
 ...
status: {}
```

after restarting apiserver, `unable to transform key ""/registry/secrets/default/default-token-xxx"": no matching prefix found` appeared.

**environment**

- Ubuntu Server 18.04
- build cluster by using kubeadm(v1.14.0)
- `kubectl version`: v1.14.0
- cni:  Calico(v3.5)

/kind bug","types=InternalIP,ExternalIP,Hostname",https://github.com/kubernetes/kubernetes
70,76010,"I set kubernetes cluster by kubeadm on my local ubuntu server. I tried to encrypt etcd by the same way as https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ , then it breaks `/registry/secrets/default/default-token-xxxxx`

here is my encryption configuration file.

```yaml
    kind: EncryptionConfiguration
    apiVersion: apiserver.config.k8s.io/v1
    resources:
      - resources:
        - secrets
        providers:
        - aescbc:
            keys:
            - name: key1
              secret: F3rax7e5B/gGbAR8LLLLbPAp+R4W8KGXKjXfhTp8bb0=
            - name: key2
              secret: yuat0/ntgNh8d4NJwkWWSCdi7/WCYJ7AGfjTSU1XKHE=
```

and after created it, I configured my kube-apiserver.yaml like below.

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=xx.x.xxx.xxx
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/path/to/my/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/path/to/my/etcd/ca.crt
    - --etcd-certfile=/path/to/my/apiserver-etcd-client.crt
    - --etcd-keyfile=/path/to/my/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --encryption-provider-config=/path/to/enc/encryption_conf.yaml
    - --insecure-port=0
    - --kubelet-client-certificate=/path/to/myapiserver-kubelet-client.crt
    - --kubelet-client-key=/path/to/my/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/path/to/my/front-proxy-client.crt
    - --proxy-client-key-file=/path/to/my/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/path/to/my/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-key-file=/path/to/my/sa.pub
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/path/to/my/apiserver.crt
    - --tls-private-key-file=/path/to/my/apiserver.key
    image: k8s.gcr.io/kube-apiserver:v1.14.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: xx.x.xxx.xxx
        path: /healthz
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 15
      timeoutSeconds: 15
    name: kube-apiserver
    resources:
      requests:
        cpu: 250m
    volumeMounts:
    - mountPath: /path/to/enc
      name: encryption-conf
      readOnly: true
  ...
  hostNetwork: true
  priorityClassName: system-cluster-critical
  volumes:
  - hostPath:
      path: /path/to/enc
      type: DirectoryOrCreate
    name: encryption-conf
 ...
status: {}
```

after restarting apiserver, `unable to transform key ""/registry/secrets/default/default-token-xxx"": no matching prefix found` appeared.

**environment**

- Ubuntu Server 18.04
- build cluster by using kubeadm(v1.14.0)
- `kubectl version`: v1.14.0
- cni:  Calico(v3.5)

/kind bug",cluster-ip-range,https://github.com/kubernetes/kubernetes
71,76010,"I set kubernetes cluster by kubeadm on my local ubuntu server. I tried to encrypt etcd by the same way as https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ , then it breaks `/registry/secrets/default/default-token-xxxxx`

here is my encryption configuration file.

```yaml
    kind: EncryptionConfiguration
    apiVersion: apiserver.config.k8s.io/v1
    resources:
      - resources:
        - secrets
        providers:
        - aescbc:
            keys:
            - name: key1
              secret: F3rax7e5B/gGbAR8LLLLbPAp+R4W8KGXKjXfhTp8bb0=
            - name: key2
              secret: yuat0/ntgNh8d4NJwkWWSCdi7/WCYJ7AGfjTSU1XKHE=
```

and after created it, I configured my kube-apiserver.yaml like below.

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=xx.x.xxx.xxx
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/path/to/my/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/path/to/my/etcd/ca.crt
    - --etcd-certfile=/path/to/my/apiserver-etcd-client.crt
    - --etcd-keyfile=/path/to/my/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --encryption-provider-config=/path/to/enc/encryption_conf.yaml
    - --insecure-port=0
    - --kubelet-client-certificate=/path/to/myapiserver-kubelet-client.crt
    - --kubelet-client-key=/path/to/my/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/path/to/my/front-proxy-client.crt
    - --proxy-client-key-file=/path/to/my/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/path/to/my/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-key-file=/path/to/my/sa.pub
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/path/to/my/apiserver.crt
    - --tls-private-key-file=/path/to/my/apiserver.key
    image: k8s.gcr.io/kube-apiserver:v1.14.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: xx.x.xxx.xxx
        path: /healthz
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 15
      timeoutSeconds: 15
    name: kube-apiserver
    resources:
      requests:
        cpu: 250m
    volumeMounts:
    - mountPath: /path/to/enc
      name: encryption-conf
      readOnly: true
  ...
  hostNetwork: true
  priorityClassName: system-cluster-critical
  volumes:
  - hostPath:
      path: /path/to/enc
      type: DirectoryOrCreate
    name: encryption-conf
 ...
status: {}
```

after restarting apiserver, `unable to transform key ""/registry/secrets/default/default-token-xxx"": no matching prefix found` appeared.

**environment**

- Ubuntu Server 18.04
- build cluster by using kubeadm(v1.14.0)
- `kubectl version`: v1.14.0
- cni:  Calico(v3.5)

/kind bug",encryption-provider-config,https://github.com/kubernetes/kubernetes
72,76010,"I set kubernetes cluster by kubeadm on my local ubuntu server. I tried to encrypt etcd by the same way as https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ , then it breaks `/registry/secrets/default/default-token-xxxxx`

here is my encryption configuration file.

```yaml
    kind: EncryptionConfiguration
    apiVersion: apiserver.config.k8s.io/v1
    resources:
      - resources:
        - secrets
        providers:
        - aescbc:
            keys:
            - name: key1
              secret: F3rax7e5B/gGbAR8LLLLbPAp+R4W8KGXKjXfhTp8bb0=
            - name: key2
              secret: yuat0/ntgNh8d4NJwkWWSCdi7/WCYJ7AGfjTSU1XKHE=
```

and after created it, I configured my kube-apiserver.yaml like below.

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=xx.x.xxx.xxx
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/path/to/my/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/path/to/my/etcd/ca.crt
    - --etcd-certfile=/path/to/my/apiserver-etcd-client.crt
    - --etcd-keyfile=/path/to/my/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --encryption-provider-config=/path/to/enc/encryption_conf.yaml
    - --insecure-port=0
    - --kubelet-client-certificate=/path/to/myapiserver-kubelet-client.crt
    - --kubelet-client-key=/path/to/my/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/path/to/my/front-proxy-client.crt
    - --proxy-client-key-file=/path/to/my/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/path/to/my/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-key-file=/path/to/my/sa.pub
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/path/to/my/apiserver.crt
    - --tls-private-key-file=/path/to/my/apiserver.key
    image: k8s.gcr.io/kube-apiserver:v1.14.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: xx.x.xxx.xxx
        path: /healthz
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 15
      timeoutSeconds: 15
    name: kube-apiserver
    resources:
      requests:
        cpu: 250m
    volumeMounts:
    - mountPath: /path/to/enc
      name: encryption-conf
      readOnly: true
  ...
  hostNetwork: true
  priorityClassName: system-cluster-critical
  volumes:
  - hostPath:
      path: /path/to/enc
      type: DirectoryOrCreate
    name: encryption-conf
 ...
status: {}
```

after restarting apiserver, `unable to transform key ""/registry/secrets/default/default-token-xxx"": no matching prefix found` appeared.

**environment**

- Ubuntu Server 18.04
- build cluster by using kubeadm(v1.14.0)
- `kubectl version`: v1.14.0
- cni:  Calico(v3.5)

/kind bug",names=front-proxy-client,https://github.com/kubernetes/kubernetes
73,75633,"**What type of PR is this?**

/kind feature

**What this PR does / why we need it**:

Implemented --output option for 'kubeadm token list' to
show token info in parseable form.

The output includes all information needed for 'kubeadm join'
command. This should help other tools to better integrate with
kubeadm.

Example output:

```
root@kind-control-plane:/# kubeadm token list --output json
{
  ""publicKeyPins"": [
    ""sha256:5f50bc74bd59d2ad0063e7aa639a0c1161dbde0101cb499b33e9881f37b60d77""
  ],
  ""controlPlaneHostPort"": ""172.17.0.2:6443"",
  ""tokens"": [
    {
      ""token"": ""uvxdac.fq35fuyue3kd4gda"",
      ""expires"": ""2019-03-30T09:54:53Z"",
      ""usages"": [
        ""authentication"",
        ""signing""
      ],
      ""groups"": [
        ""system:bootstrappers:kubeadm:default-node-token""
      ]
    },
    {
      ""token"": ""ywmdha.ruymv2h6n9i9mdlj"",
      ""expires"": ""2019-03-30T09:54:51Z"",
      ""usages"": [
        ""authentication"",
        ""signing""
      ],
      ""groups"": [
        ""system:bootstrappers:kubeadm:default-node-token""
      ]
    }
  ]
}

root@kind-control-plane:/# kubeadm token list --output yaml
controlPlaneHostPort: 172.17.0.2:6443
publicKeyPins:
- sha256:5f50bc74bd59d2ad0063e7aa639a0c1161dbde0101cb499b33e9881f37b60d77
tokens:
- expires: ""2019-03-30T09:54:53Z""
  groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: uvxdac.fq35fuyue3kd4gda
  usages:
  - authentication
  - signing
- expires: ""2019-03-30T09:54:51Z""
  groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: ywmdha.ruymv2h6n9i9mdlj
  usages:
  - authentication
  - signing

```

**Does this PR introduce a user-facing change?**:
<!--
If no, just write ""NONE"" in the release-note block below.
If yes, a release note is required:
Enter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string ""action required"".
-->
```release-note
kubeadm: support optional YAML/JSON output when listing bootstrap tokens
```",-03-30T09:54:53Z,https://github.com/kubernetes/kubernetes
74,75633,"**What type of PR is this?**

/kind feature

**What this PR does / why we need it**:

Implemented --output option for 'kubeadm token list' to
show token info in parseable form.

The output includes all information needed for 'kubeadm join'
command. This should help other tools to better integrate with
kubeadm.

Example output:

```
root@kind-control-plane:/# kubeadm token list --output json
{
  ""publicKeyPins"": [
    ""sha256:5f50bc74bd59d2ad0063e7aa639a0c1161dbde0101cb499b33e9881f37b60d77""
  ],
  ""controlPlaneHostPort"": ""172.17.0.2:6443"",
  ""tokens"": [
    {
      ""token"": ""uvxdac.fq35fuyue3kd4gda"",
      ""expires"": ""2019-03-30T09:54:53Z"",
      ""usages"": [
        ""authentication"",
        ""signing""
      ],
      ""groups"": [
        ""system:bootstrappers:kubeadm:default-node-token""
      ]
    },
    {
      ""token"": ""ywmdha.ruymv2h6n9i9mdlj"",
      ""expires"": ""2019-03-30T09:54:51Z"",
      ""usages"": [
        ""authentication"",
        ""signing""
      ],
      ""groups"": [
        ""system:bootstrappers:kubeadm:default-node-token""
      ]
    }
  ]
}

root@kind-control-plane:/# kubeadm token list --output yaml
controlPlaneHostPort: 172.17.0.2:6443
publicKeyPins:
- sha256:5f50bc74bd59d2ad0063e7aa639a0c1161dbde0101cb499b33e9881f37b60d77
tokens:
- expires: ""2019-03-30T09:54:53Z""
  groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: uvxdac.fq35fuyue3kd4gda
  usages:
  - authentication
  - signing
- expires: ""2019-03-30T09:54:51Z""
  groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: ywmdha.ruymv2h6n9i9mdlj
  usages:
  - authentication
  - signing

```

**Does this PR introduce a user-facing change?**:
<!--
If no, just write ""NONE"" in the release-note block below.
If yes, a release note is required:
Enter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string ""action required"".
-->
```release-note
kubeadm: support optional YAML/JSON output when listing bootstrap tokens
```",controlPlaneHostPort,https://github.com/kubernetes/kubernetes
75,75633,"**What type of PR is this?**

/kind feature

**What this PR does / why we need it**:

Implemented --output option for 'kubeadm token list' to
show token info in parseable form.

The output includes all information needed for 'kubeadm join'
command. This should help other tools to better integrate with
kubeadm.

Example output:

```
root@kind-control-plane:/# kubeadm token list --output json
{
  ""publicKeyPins"": [
    ""sha256:5f50bc74bd59d2ad0063e7aa639a0c1161dbde0101cb499b33e9881f37b60d77""
  ],
  ""controlPlaneHostPort"": ""172.17.0.2:6443"",
  ""tokens"": [
    {
      ""token"": ""uvxdac.fq35fuyue3kd4gda"",
      ""expires"": ""2019-03-30T09:54:53Z"",
      ""usages"": [
        ""authentication"",
        ""signing""
      ],
      ""groups"": [
        ""system:bootstrappers:kubeadm:default-node-token""
      ]
    },
    {
      ""token"": ""ywmdha.ruymv2h6n9i9mdlj"",
      ""expires"": ""2019-03-30T09:54:51Z"",
      ""usages"": [
        ""authentication"",
        ""signing""
      ],
      ""groups"": [
        ""system:bootstrappers:kubeadm:default-node-token""
      ]
    }
  ]
}

root@kind-control-plane:/# kubeadm token list --output yaml
controlPlaneHostPort: 172.17.0.2:6443
publicKeyPins:
- sha256:5f50bc74bd59d2ad0063e7aa639a0c1161dbde0101cb499b33e9881f37b60d77
tokens:
- expires: ""2019-03-30T09:54:53Z""
  groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: uvxdac.fq35fuyue3kd4gda
  usages:
  - authentication
  - signing
- expires: ""2019-03-30T09:54:51Z""
  groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: ywmdha.ruymv2h6n9i9mdlj
  usages:
  - authentication
  - signing

```

**Does this PR introduce a user-facing change?**:
<!--
If no, just write ""NONE"" in the release-note block below.
If yes, a release note is required:
Enter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string ""action required"".
-->
```release-note
kubeadm: support optional YAML/JSON output when listing bootstrap tokens
```",-03-30T09:54:51Z,https://github.com/kubernetes/kubernetes
76,75633,"**What type of PR is this?**

/kind feature

**What this PR does / why we need it**:

Implemented --output option for 'kubeadm token list' to
show token info in parseable form.

The output includes all information needed for 'kubeadm join'
command. This should help other tools to better integrate with
kubeadm.

Example output:

```
root@kind-control-plane:/# kubeadm token list --output json
{
  ""publicKeyPins"": [
    ""sha256:5f50bc74bd59d2ad0063e7aa639a0c1161dbde0101cb499b33e9881f37b60d77""
  ],
  ""controlPlaneHostPort"": ""172.17.0.2:6443"",
  ""tokens"": [
    {
      ""token"": ""uvxdac.fq35fuyue3kd4gda"",
      ""expires"": ""2019-03-30T09:54:53Z"",
      ""usages"": [
        ""authentication"",
        ""signing""
      ],
      ""groups"": [
        ""system:bootstrappers:kubeadm:default-node-token""
      ]
    },
    {
      ""token"": ""ywmdha.ruymv2h6n9i9mdlj"",
      ""expires"": ""2019-03-30T09:54:51Z"",
      ""usages"": [
        ""authentication"",
        ""signing""
      ],
      ""groups"": [
        ""system:bootstrappers:kubeadm:default-node-token""
      ]
    }
  ]
}

root@kind-control-plane:/# kubeadm token list --output yaml
controlPlaneHostPort: 172.17.0.2:6443
publicKeyPins:
- sha256:5f50bc74bd59d2ad0063e7aa639a0c1161dbde0101cb499b33e9881f37b60d77
tokens:
- expires: ""2019-03-30T09:54:53Z""
  groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: uvxdac.fq35fuyue3kd4gda
  usages:
  - authentication
  - signing
- expires: ""2019-03-30T09:54:51Z""
  groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: ywmdha.ruymv2h6n9i9mdlj
  usages:
  - authentication
  - signing

```

**Does this PR introduce a user-facing change?**:
<!--
If no, just write ""NONE"" in the release-note block below.
If yes, a release note is required:
Enter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string ""action required"".
-->
```release-note
kubeadm: support optional YAML/JSON output when listing bootstrap tokens
```",kind-control-plane,https://github.com/kubernetes/kubernetes
77,75626,"Didn't find the right answer after trying Google

After running the cluster with kubeadm for a while, the node node often fails to log in remotely due to the system hang.

Later, the default resource reservation for kubelet was found to be none. Now I want to make a kubelet configuration update for an already running cluster.

The following is the configuration of my initial cluster.
```yaml
apiVersion: kubeadm.k8s.io/v1beta1
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.1.166
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s-master-1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: 1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: ""10.244.0.0/16""
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
pod-infra-container-image: registry.aliyuncs.com/google_containers/pause:3.1
maxPods: 90
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
featureGates:
  SupportIPVSProxyMode: true
mode: ipvs
```

After I updated the configuration.Got the following configuration
```yaml
apiVersion: kubeadm.k8s.io/v1beta1
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.1.166
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s-master-1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: 1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: ""10.244.0.0/16""
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
#pod-infra-container-image: registry.aliyuncs.com/google_containers/pause:3.1
#podInfraContainerImage: registry.aliyuncs.com/google_containers/pause:3.1
maxPods: 80
evictionHard:
  memory.available: ""1024Mi""
  nodefs.available: ""10%""
kubeReserved:
  cpu: ""500m""
  memory: ""1Gi""
systemReserved:
  cpu: ""1""
  memory: ""1Gi""
  ephemeral-storage: ""10Gi""
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
featureGates:
  SupportIPVSProxyMode: true
mode: ipvs
```

Then executed the cluster configuration update command
```bash
kubeadm config upload from-file --config kubeadm-kubelet-update.yaml
```


Viewing the cluster configuration has not changed
```bash
$ kubeadm config view
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: 10.244.0.0/16
  serviceSubnet: 10.96.0.0/12
scheduler: {}
```

```bash
$ kubectl get -n kube-system  configmaps kubelet-config-1.13 -o yaml
apiVersion: v1
data:
  kubelet: |
    address: 0.0.0.0
    apiVersion: kubelet.config.k8s.io/v1beta1
    authentication:
      anonymous:
        enabled: false
      webhook:
        cacheTTL: 2m0s
        enabled: true
      x509:
        clientCAFile: /etc/kubernetes/pki/ca.crt
    authorization:
      mode: Webhook
      webhook:
        cacheAuthorizedTTL: 5m0s
        cacheUnauthorizedTTL: 30s
    cgroupDriver: cgroupfs
    cgroupsPerQOS: true
    clusterDNS:
    - 10.96.0.10
    clusterDomain: cluster.local
    configMapAndSecretChangeDetectionStrategy: Watch
    containerLogMaxFiles: 5
    containerLogMaxSize: 10Mi
    contentType: application/vnd.kubernetes.protobuf
    cpuCFSQuota: true
    cpuCFSQuotaPeriod: 100ms
    cpuManagerPolicy: none
    cpuManagerReconcilePeriod: 10s
    enableControllerAttachDetach: true
    enableDebuggingHandlers: true
    enforceNodeAllocatable:
    - pods
    eventBurst: 10
    eventRecordQPS: 5
    evictionHard:
      imagefs.available: 15%
      memory.available: 100Mi
      nodefs.available: 10%
      nodefs.inodesFree: 5%
    evictionPressureTransitionPeriod: 5m0s
    failSwapOn: true
    fileCheckFrequency: 20s
    hairpinMode: promiscuous-bridge
    healthzBindAddress: 127.0.0.1
    healthzPort: 10248
    httpCheckFrequency: 20s
    imageGCHighThresholdPercent: 85
    imageGCLowThresholdPercent: 80
    imageMinimumGCAge: 2m0s
    iptablesDropBit: 15
    iptablesMasqueradeBit: 14
    kind: KubeletConfiguration
    kubeAPIBurst: 10
    kubeAPIQPS: 5
    makeIPTablesUtilChains: true
    maxOpenFiles: 1000000
    maxPods: 90
    nodeLeaseDurationSeconds: 40
    nodeStatusReportFrequency: 1m0s
    nodeStatusUpdateFrequency: 10s
    oomScoreAdj: -999
    podPidsLimit: -1
    port: 10250
    registryBurst: 10
    registryPullQPS: 5
    resolvConf: /etc/resolv.conf
    rotateCertificates: true
    runtimeRequestTimeout: 2m0s
    serializeImagePulls: true
    staticPodPath: /etc/kubernetes/manifests
    streamingConnectionIdleTimeout: 4h0m0s
    syncFrequency: 1m0s
    volumeStatsAggPeriod: 1m0s
kind: ConfigMap
metadata:
  creationTimestamp: ""2019-02-18T09:45:25Z""
  name: kubelet-config-1.13
  namespace: kube-system
  resourceVersion: ""153""
  selfLink: /api/v1/namespaces/kube-system/configmaps/kubelet-config-1.13
  uid: eb08bbd9-3361-11e9-8888-000c299021ee
```

How can I use kubeadm to update the configuration of a kubernetes cluster that is already running?


",streamingConnectionIdleTimeout,https://github.com/kubernetes/kubernetes
78,75626,"Didn't find the right answer after trying Google

After running the cluster with kubeadm for a while, the node node often fails to log in remotely due to the system hang.

Later, the default resource reservation for kubelet was found to be none. Now I want to make a kubelet configuration update for an already running cluster.

The following is the configuration of my initial cluster.
```yaml
apiVersion: kubeadm.k8s.io/v1beta1
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.1.166
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s-master-1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: 1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: ""10.244.0.0/16""
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
pod-infra-container-image: registry.aliyuncs.com/google_containers/pause:3.1
maxPods: 90
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
featureGates:
  SupportIPVSProxyMode: true
mode: ipvs
```

After I updated the configuration.Got the following configuration
```yaml
apiVersion: kubeadm.k8s.io/v1beta1
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.1.166
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s-master-1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: 1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: ""10.244.0.0/16""
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
#pod-infra-container-image: registry.aliyuncs.com/google_containers/pause:3.1
#podInfraContainerImage: registry.aliyuncs.com/google_containers/pause:3.1
maxPods: 80
evictionHard:
  memory.available: ""1024Mi""
  nodefs.available: ""10%""
kubeReserved:
  cpu: ""500m""
  memory: ""1Gi""
systemReserved:
  cpu: ""1""
  memory: ""1Gi""
  ephemeral-storage: ""10Gi""
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
featureGates:
  SupportIPVSProxyMode: true
mode: ipvs
```

Then executed the cluster configuration update command
```bash
kubeadm config upload from-file --config kubeadm-kubelet-update.yaml
```


Viewing the cluster configuration has not changed
```bash
$ kubeadm config view
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: 10.244.0.0/16
  serviceSubnet: 10.96.0.0/12
scheduler: {}
```

```bash
$ kubectl get -n kube-system  configmaps kubelet-config-1.13 -o yaml
apiVersion: v1
data:
  kubelet: |
    address: 0.0.0.0
    apiVersion: kubelet.config.k8s.io/v1beta1
    authentication:
      anonymous:
        enabled: false
      webhook:
        cacheTTL: 2m0s
        enabled: true
      x509:
        clientCAFile: /etc/kubernetes/pki/ca.crt
    authorization:
      mode: Webhook
      webhook:
        cacheAuthorizedTTL: 5m0s
        cacheUnauthorizedTTL: 30s
    cgroupDriver: cgroupfs
    cgroupsPerQOS: true
    clusterDNS:
    - 10.96.0.10
    clusterDomain: cluster.local
    configMapAndSecretChangeDetectionStrategy: Watch
    containerLogMaxFiles: 5
    containerLogMaxSize: 10Mi
    contentType: application/vnd.kubernetes.protobuf
    cpuCFSQuota: true
    cpuCFSQuotaPeriod: 100ms
    cpuManagerPolicy: none
    cpuManagerReconcilePeriod: 10s
    enableControllerAttachDetach: true
    enableDebuggingHandlers: true
    enforceNodeAllocatable:
    - pods
    eventBurst: 10
    eventRecordQPS: 5
    evictionHard:
      imagefs.available: 15%
      memory.available: 100Mi
      nodefs.available: 10%
      nodefs.inodesFree: 5%
    evictionPressureTransitionPeriod: 5m0s
    failSwapOn: true
    fileCheckFrequency: 20s
    hairpinMode: promiscuous-bridge
    healthzBindAddress: 127.0.0.1
    healthzPort: 10248
    httpCheckFrequency: 20s
    imageGCHighThresholdPercent: 85
    imageGCLowThresholdPercent: 80
    imageMinimumGCAge: 2m0s
    iptablesDropBit: 15
    iptablesMasqueradeBit: 14
    kind: KubeletConfiguration
    kubeAPIBurst: 10
    kubeAPIQPS: 5
    makeIPTablesUtilChains: true
    maxOpenFiles: 1000000
    maxPods: 90
    nodeLeaseDurationSeconds: 40
    nodeStatusReportFrequency: 1m0s
    nodeStatusUpdateFrequency: 10s
    oomScoreAdj: -999
    podPidsLimit: -1
    port: 10250
    registryBurst: 10
    registryPullQPS: 5
    resolvConf: /etc/resolv.conf
    rotateCertificates: true
    runtimeRequestTimeout: 2m0s
    serializeImagePulls: true
    staticPodPath: /etc/kubernetes/manifests
    streamingConnectionIdleTimeout: 4h0m0s
    syncFrequency: 1m0s
    volumeStatsAggPeriod: 1m0s
kind: ConfigMap
metadata:
  creationTimestamp: ""2019-02-18T09:45:25Z""
  name: kubelet-config-1.13
  namespace: kube-system
  resourceVersion: ""153""
  selfLink: /api/v1/namespaces/kube-system/configmaps/kubelet-config-1.13
  uid: eb08bbd9-3361-11e9-8888-000c299021ee
```

How can I use kubeadm to update the configuration of a kubernetes cluster that is already running?


",containerLogMaxFiles,https://github.com/kubernetes/kubernetes
79,75626,"Didn't find the right answer after trying Google

After running the cluster with kubeadm for a while, the node node often fails to log in remotely due to the system hang.

Later, the default resource reservation for kubelet was found to be none. Now I want to make a kubelet configuration update for an already running cluster.

The following is the configuration of my initial cluster.
```yaml
apiVersion: kubeadm.k8s.io/v1beta1
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.1.166
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s-master-1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: 1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: ""10.244.0.0/16""
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
pod-infra-container-image: registry.aliyuncs.com/google_containers/pause:3.1
maxPods: 90
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
featureGates:
  SupportIPVSProxyMode: true
mode: ipvs
```

After I updated the configuration.Got the following configuration
```yaml
apiVersion: kubeadm.k8s.io/v1beta1
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.1.166
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s-master-1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: 1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: ""10.244.0.0/16""
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
#pod-infra-container-image: registry.aliyuncs.com/google_containers/pause:3.1
#podInfraContainerImage: registry.aliyuncs.com/google_containers/pause:3.1
maxPods: 80
evictionHard:
  memory.available: ""1024Mi""
  nodefs.available: ""10%""
kubeReserved:
  cpu: ""500m""
  memory: ""1Gi""
systemReserved:
  cpu: ""1""
  memory: ""1Gi""
  ephemeral-storage: ""10Gi""
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
featureGates:
  SupportIPVSProxyMode: true
mode: ipvs
```

Then executed the cluster configuration update command
```bash
kubeadm config upload from-file --config kubeadm-kubelet-update.yaml
```


Viewing the cluster configuration has not changed
```bash
$ kubeadm config view
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: 10.244.0.0/16
  serviceSubnet: 10.96.0.0/12
scheduler: {}
```

```bash
$ kubectl get -n kube-system  configmaps kubelet-config-1.13 -o yaml
apiVersion: v1
data:
  kubelet: |
    address: 0.0.0.0
    apiVersion: kubelet.config.k8s.io/v1beta1
    authentication:
      anonymous:
        enabled: false
      webhook:
        cacheTTL: 2m0s
        enabled: true
      x509:
        clientCAFile: /etc/kubernetes/pki/ca.crt
    authorization:
      mode: Webhook
      webhook:
        cacheAuthorizedTTL: 5m0s
        cacheUnauthorizedTTL: 30s
    cgroupDriver: cgroupfs
    cgroupsPerQOS: true
    clusterDNS:
    - 10.96.0.10
    clusterDomain: cluster.local
    configMapAndSecretChangeDetectionStrategy: Watch
    containerLogMaxFiles: 5
    containerLogMaxSize: 10Mi
    contentType: application/vnd.kubernetes.protobuf
    cpuCFSQuota: true
    cpuCFSQuotaPeriod: 100ms
    cpuManagerPolicy: none
    cpuManagerReconcilePeriod: 10s
    enableControllerAttachDetach: true
    enableDebuggingHandlers: true
    enforceNodeAllocatable:
    - pods
    eventBurst: 10
    eventRecordQPS: 5
    evictionHard:
      imagefs.available: 15%
      memory.available: 100Mi
      nodefs.available: 10%
      nodefs.inodesFree: 5%
    evictionPressureTransitionPeriod: 5m0s
    failSwapOn: true
    fileCheckFrequency: 20s
    hairpinMode: promiscuous-bridge
    healthzBindAddress: 127.0.0.1
    healthzPort: 10248
    httpCheckFrequency: 20s
    imageGCHighThresholdPercent: 85
    imageGCLowThresholdPercent: 80
    imageMinimumGCAge: 2m0s
    iptablesDropBit: 15
    iptablesMasqueradeBit: 14
    kind: KubeletConfiguration
    kubeAPIBurst: 10
    kubeAPIQPS: 5
    makeIPTablesUtilChains: true
    maxOpenFiles: 1000000
    maxPods: 90
    nodeLeaseDurationSeconds: 40
    nodeStatusReportFrequency: 1m0s
    nodeStatusUpdateFrequency: 10s
    oomScoreAdj: -999
    podPidsLimit: -1
    port: 10250
    registryBurst: 10
    registryPullQPS: 5
    resolvConf: /etc/resolv.conf
    rotateCertificates: true
    runtimeRequestTimeout: 2m0s
    serializeImagePulls: true
    staticPodPath: /etc/kubernetes/manifests
    streamingConnectionIdleTimeout: 4h0m0s
    syncFrequency: 1m0s
    volumeStatsAggPeriod: 1m0s
kind: ConfigMap
metadata:
  creationTimestamp: ""2019-02-18T09:45:25Z""
  name: kubelet-config-1.13
  namespace: kube-system
  resourceVersion: ""153""
  selfLink: /api/v1/namespaces/kube-system/configmaps/kubelet-config-1.13
  uid: eb08bbd9-3361-11e9-8888-000c299021ee
```

How can I use kubeadm to update the configuration of a kubernetes cluster that is already running?


",kubeadm:default-node-token,https://github.com/kubernetes/kubernetes
80,75626,"Didn't find the right answer after trying Google

After running the cluster with kubeadm for a while, the node node often fails to log in remotely due to the system hang.

Later, the default resource reservation for kubelet was found to be none. Now I want to make a kubelet configuration update for an already running cluster.

The following is the configuration of my initial cluster.
```yaml
apiVersion: kubeadm.k8s.io/v1beta1
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.1.166
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s-master-1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: 1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: ""10.244.0.0/16""
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
pod-infra-container-image: registry.aliyuncs.com/google_containers/pause:3.1
maxPods: 90
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
featureGates:
  SupportIPVSProxyMode: true
mode: ipvs
```

After I updated the configuration.Got the following configuration
```yaml
apiVersion: kubeadm.k8s.io/v1beta1
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.1.166
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s-master-1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: 1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: ""10.244.0.0/16""
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
#pod-infra-container-image: registry.aliyuncs.com/google_containers/pause:3.1
#podInfraContainerImage: registry.aliyuncs.com/google_containers/pause:3.1
maxPods: 80
evictionHard:
  memory.available: ""1024Mi""
  nodefs.available: ""10%""
kubeReserved:
  cpu: ""500m""
  memory: ""1Gi""
systemReserved:
  cpu: ""1""
  memory: ""1Gi""
  ephemeral-storage: ""10Gi""
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
featureGates:
  SupportIPVSProxyMode: true
mode: ipvs
```

Then executed the cluster configuration update command
```bash
kubeadm config upload from-file --config kubeadm-kubelet-update.yaml
```


Viewing the cluster configuration has not changed
```bash
$ kubeadm config view
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: 10.244.0.0/16
  serviceSubnet: 10.96.0.0/12
scheduler: {}
```

```bash
$ kubectl get -n kube-system  configmaps kubelet-config-1.13 -o yaml
apiVersion: v1
data:
  kubelet: |
    address: 0.0.0.0
    apiVersion: kubelet.config.k8s.io/v1beta1
    authentication:
      anonymous:
        enabled: false
      webhook:
        cacheTTL: 2m0s
        enabled: true
      x509:
        clientCAFile: /etc/kubernetes/pki/ca.crt
    authorization:
      mode: Webhook
      webhook:
        cacheAuthorizedTTL: 5m0s
        cacheUnauthorizedTTL: 30s
    cgroupDriver: cgroupfs
    cgroupsPerQOS: true
    clusterDNS:
    - 10.96.0.10
    clusterDomain: cluster.local
    configMapAndSecretChangeDetectionStrategy: Watch
    containerLogMaxFiles: 5
    containerLogMaxSize: 10Mi
    contentType: application/vnd.kubernetes.protobuf
    cpuCFSQuota: true
    cpuCFSQuotaPeriod: 100ms
    cpuManagerPolicy: none
    cpuManagerReconcilePeriod: 10s
    enableControllerAttachDetach: true
    enableDebuggingHandlers: true
    enforceNodeAllocatable:
    - pods
    eventBurst: 10
    eventRecordQPS: 5
    evictionHard:
      imagefs.available: 15%
      memory.available: 100Mi
      nodefs.available: 10%
      nodefs.inodesFree: 5%
    evictionPressureTransitionPeriod: 5m0s
    failSwapOn: true
    fileCheckFrequency: 20s
    hairpinMode: promiscuous-bridge
    healthzBindAddress: 127.0.0.1
    healthzPort: 10248
    httpCheckFrequency: 20s
    imageGCHighThresholdPercent: 85
    imageGCLowThresholdPercent: 80
    imageMinimumGCAge: 2m0s
    iptablesDropBit: 15
    iptablesMasqueradeBit: 14
    kind: KubeletConfiguration
    kubeAPIBurst: 10
    kubeAPIQPS: 5
    makeIPTablesUtilChains: true
    maxOpenFiles: 1000000
    maxPods: 90
    nodeLeaseDurationSeconds: 40
    nodeStatusReportFrequency: 1m0s
    nodeStatusUpdateFrequency: 10s
    oomScoreAdj: -999
    podPidsLimit: -1
    port: 10250
    registryBurst: 10
    registryPullQPS: 5
    resolvConf: /etc/resolv.conf
    rotateCertificates: true
    runtimeRequestTimeout: 2m0s
    serializeImagePulls: true
    staticPodPath: /etc/kubernetes/manifests
    streamingConnectionIdleTimeout: 4h0m0s
    syncFrequency: 1m0s
    volumeStatsAggPeriod: 1m0s
kind: ConfigMap
metadata:
  creationTimestamp: ""2019-02-18T09:45:25Z""
  name: kubelet-config-1.13
  namespace: kube-system
  resourceVersion: ""153""
  selfLink: /api/v1/namespaces/kube-system/configmaps/kubelet-config-1.13
  uid: eb08bbd9-3361-11e9-8888-000c299021ee
```

How can I use kubeadm to update the configuration of a kubernetes cluster that is already running?


",timeoutForControlPlane,https://github.com/kubernetes/kubernetes
81,69850,"Microsoft.AspNetCore.Session.SessionMiddleware[7]
      Error unprotecting the session cookie.
System.Security.Cryptography.CryptographicException: The key {d13b9581-1fee-45c2-ad1b-e89680402540} was not found in the key ring.
   at Microsoft.AspNetCore.DataProtection.KeyManagement.KeyRingBasedDataProtector.UnprotectCore(Byte[] protectedData, Boolean allowOperationsOnRevokedKeys, UnprotectStatus& status)
   at Microsoft.AspNetCore.DataProtection.KeyManagement.KeyRingBasedDataProtector.DangerousUnprotect(Byte[] protectedData, Boolean ignoreRevocationErrors, Boolean& requiresMigration, Boolean& wasRevoked)
   at Microsoft.AspNetCore.DataProtection.KeyManagement.KeyRingBasedDataProtector.Unprotect(Byte[] protectedData)
   at Microsoft.AspNetCore.Session.CookieProtection.Unprotect(IDataProtector protector, String protectedText, ILogger logger)",d13b9581-1fee-45c2-ad1b-e89680402540,https://github.com/kubernetes/kubernetes
82,69850,"Microsoft.AspNetCore.Session.SessionMiddleware[7]
      Error unprotecting the session cookie.
System.Security.Cryptography.CryptographicException: The key {d13b9581-1fee-45c2-ad1b-e89680402540} was not found in the key ring.
   at Microsoft.AspNetCore.DataProtection.KeyManagement.KeyRingBasedDataProtector.UnprotectCore(Byte[] protectedData, Boolean allowOperationsOnRevokedKeys, UnprotectStatus& status)
   at Microsoft.AspNetCore.DataProtection.KeyManagement.KeyRingBasedDataProtector.DangerousUnprotect(Byte[] protectedData, Boolean ignoreRevocationErrors, Boolean& requiresMigration, Boolean& wasRevoked)
   at Microsoft.AspNetCore.DataProtection.KeyManagement.KeyRingBasedDataProtector.Unprotect(Byte[] protectedData)
   at Microsoft.AspNetCore.Session.CookieProtection.Unprotect(IDataProtector protector, String protectedText, ILogger logger)",ad1b-e89680402540,https://github.com/kubernetes/kubernetes
83,69221,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If the matter is security related, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>
> /kind bug
> /kind feature


**What happened**:
Tried to join node to kube.

**What you expected to happen**:
Join cluster.

**How to reproduce it (as minimally and precisely as possible)**:
kubeadm join 172.17.15.14:6443 --token n31b8f.yquu66k6az5ktlo7 --discovery-token-ca-cert-hash sha256:9ee1d6eed7eba52cf3f425b2d0757fcf10148b5261...........

**Anything else we need to know?**:
[discovery] Trying to connect to API Server ""172.17.15.14:6443""
[discovery] Created cluster-info discovery client, requesting info from ""https://172.17.15.14:6443""
[discovery] Requesting info from ""https://172.17.15.14:6443"" again to validate TLS against the pinned public key
[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server ""172.17.15.14:6443""
[discovery] Successfully established connection with API Server ""172.17.15.14:6443""
[kubelet] Downloading configuration for the kubelet from the ""kubelet-config-1.12"" ConfigMap in the kube-system namespace
configmaps ""kubelet-config-1.12"" is forbidden: User ""system:bootstrap:n31b8f"" cannot get configmaps in the namespace ""kube-system""


**Environment**:
Bare metal

- Kubernetes version (use `kubectl version`):
1.12

- Cloud provider or hardware configuration:
NA

- OS (e.g. from /etc/os-release):
RHEL 7.5

- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",9ee1d6eed7eba52cf3f425b2d0757fcf10148b5261,https://github.com/kubernetes/kubernetes
84,68334,"**What this PR does / why we need it**:
fix kubeadm bug
**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #68333 
```
Is this a BUG REPORT or FEATURE REQUEST?:
/kind bug

What happened:
When I use kubeadm to create a cluster, the specified parameter ""listen-client-urls"" does not work.
kubeadm init --config=kube-config.yaml fail with connection refuse for get http://127.0.0.1:2379
I think the element 'listen-client-urls' is unrecognized if the customized value is not equal '127.0.0.1'
Here is my kube-config.yaml

apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
apiServerCertSANs:    
- 192.168.192.128
- kube-dev
api:
  advertiseAddress: 192.168.192.128
  bindPort: 6440
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 48h0m0s
  usages:
  - signing
  - authentication
etcd:
  local:
    extraArgs:
      listen-client-urls: https://192.168.192.128:2379
      advertise-client-urls: https://192.168.192.128:2379
      listen-peer-urls: https://192.168.192.128:2380
      initial-advertise-peer-urls: https://192.168.192.128:2380
      initial-cluster: kube-dev=https://192.168.192.128:2380
      initial-cluster-state: new
    serverCertSANs:
      - kube-dev
      - 192.168.192.128
    peerCertSANs:
      - kube-dev
      - 192.168.192.128
ServerExtraArgs:
  endpoint-reconciler-type: lease

networking:
  podSubnet: 192.168.0.0/16  
kubernetesVersion: v1.11.2 
featureGates:  
   CoreDNS: true

No matter how I set the ""listen-client-urls"" parameter in the kube-config.yaml ,
it is awlays be ""https://127.0.0.1:2379""

What you expected to happen:
The parameter ""listen-client-urls"" can work!
How to reproduce it (as minimally and precisely as possible):

Anything else we need to know?:

Environment:

Kubernetes version (use kubectl version): v1.11.2
Cloud provider or hardware configuration:
OS (e.g. from /etc/os-release): CentOS Linux release 7.5.1804 (Core)
Kernel (e.g. uname -a): Linux kube-dev 3.10.0-862.el7.x86_64 #1 SMP Fri Apr 20 16:44:24 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
Install tools:
Others:
```
**Special notes for your reviewer**:

**Release note**:
```
kubeadm: if defined, pass the local etcd value of `listen-client-urls` as `etcd-servers` argument to kube-apiserver.
```
",listen-client-urls,https://github.com/kubernetes/kubernetes
85,68334,"**What this PR does / why we need it**:
fix kubeadm bug
**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #68333 
```
Is this a BUG REPORT or FEATURE REQUEST?:
/kind bug

What happened:
When I use kubeadm to create a cluster, the specified parameter ""listen-client-urls"" does not work.
kubeadm init --config=kube-config.yaml fail with connection refuse for get http://127.0.0.1:2379
I think the element 'listen-client-urls' is unrecognized if the customized value is not equal '127.0.0.1'
Here is my kube-config.yaml

apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
apiServerCertSANs:    
- 192.168.192.128
- kube-dev
api:
  advertiseAddress: 192.168.192.128
  bindPort: 6440
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 48h0m0s
  usages:
  - signing
  - authentication
etcd:
  local:
    extraArgs:
      listen-client-urls: https://192.168.192.128:2379
      advertise-client-urls: https://192.168.192.128:2379
      listen-peer-urls: https://192.168.192.128:2380
      initial-advertise-peer-urls: https://192.168.192.128:2380
      initial-cluster: kube-dev=https://192.168.192.128:2380
      initial-cluster-state: new
    serverCertSANs:
      - kube-dev
      - 192.168.192.128
    peerCertSANs:
      - kube-dev
      - 192.168.192.128
ServerExtraArgs:
  endpoint-reconciler-type: lease

networking:
  podSubnet: 192.168.0.0/16  
kubernetesVersion: v1.11.2 
featureGates:  
   CoreDNS: true

No matter how I set the ""listen-client-urls"" parameter in the kube-config.yaml ,
it is awlays be ""https://127.0.0.1:2379""

What you expected to happen:
The parameter ""listen-client-urls"" can work!
How to reproduce it (as minimally and precisely as possible):

Anything else we need to know?:

Environment:

Kubernetes version (use kubectl version): v1.11.2
Cloud provider or hardware configuration:
OS (e.g. from /etc/os-release): CentOS Linux release 7.5.1804 (Core)
Kernel (e.g. uname -a): Linux kube-dev 3.10.0-862.el7.x86_64 #1 SMP Fri Apr 20 16:44:24 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
Install tools:
Others:
```
**Special notes for your reviewer**:

**Release note**:
```
kubeadm: if defined, pass the local etcd value of `listen-client-urls` as `etcd-servers` argument to kube-apiserver.
```
",kubeadm:default-node-token,https://github.com/kubernetes/kubernetes
86,65695,"# Pod restart multiple times

I have a question...

- Kubernetes version: 1.10.4

### Question 1:

- Environment variables: 
- key: POD_XXX_KEY value: helloworld

my application start code:

```go
if key := os.Getenv(""POD_XXX_KEY""); key == """" {
	panic(""POD_XXX_KEY not found."")
}
fmt.println(""biubiuibu..."")
```

so.. Then pod at start
0 start: POD_XXX_KEY is null, panic POD_XXX_KEY not found.
1 start: POD_XXX_KEY is null, panic POD_XXX_KEY not found.
2 start: POD_XXX_KEY is 282ecd65-222f-e22c1aa8986b

deployment yaml:

```
env:
- name: POD_XXX_KEY
  valueFrom:
    configMapKeyRef:
      name: myservice
      key: hellokey
```

configmap yaml:

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: myservice
data:
  hellokey: 282ecd65-222f-e22c1aa8986b
```

No problem at 1.10.0 But 1.10.4 has the question.

What is the reason?

![](https://ofbudvg4c.qnssl.com/images/k8s/WechatIMG2274.jpeg)",282ecd65-222f-e22c1aa8986b,https://github.com/kubernetes/kubernetes
87,65695,"# Pod restart multiple times

I have a question...

- Kubernetes version: 1.10.4

### Question 1:

- Environment variables: 
- key: POD_XXX_KEY value: helloworld

my application start code:

```go
if key := os.Getenv(""POD_XXX_KEY""); key == """" {
	panic(""POD_XXX_KEY not found."")
}
fmt.println(""biubiuibu..."")
```

so.. Then pod at start
0 start: POD_XXX_KEY is null, panic POD_XXX_KEY not found.
1 start: POD_XXX_KEY is null, panic POD_XXX_KEY not found.
2 start: POD_XXX_KEY is 282ecd65-222f-e22c1aa8986b

deployment yaml:

```
env:
- name: POD_XXX_KEY
  valueFrom:
    configMapKeyRef:
      name: myservice
      key: hellokey
```

configmap yaml:

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: myservice
data:
  hellokey: 282ecd65-222f-e22c1aa8986b
```

No problem at 1.10.0 But 1.10.4 has the question.

What is the reason?

![](https://ofbudvg4c.qnssl.com/images/k8s/WechatIMG2274.jpeg)",panic(POD_XXX_KEY,https://github.com/kubernetes/kubernetes
88,65695,"# Pod restart multiple times

I have a question...

- Kubernetes version: 1.10.4

### Question 1:

- Environment variables: 
- key: POD_XXX_KEY value: helloworld

my application start code:

```go
if key := os.Getenv(""POD_XXX_KEY""); key == """" {
	panic(""POD_XXX_KEY not found."")
}
fmt.println(""biubiuibu..."")
```

so.. Then pod at start
0 start: POD_XXX_KEY is null, panic POD_XXX_KEY not found.
1 start: POD_XXX_KEY is null, panic POD_XXX_KEY not found.
2 start: POD_XXX_KEY is 282ecd65-222f-e22c1aa8986b

deployment yaml:

```
env:
- name: POD_XXX_KEY
  valueFrom:
    configMapKeyRef:
      name: myservice
      key: hellokey
```

configmap yaml:

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: myservice
data:
  hellokey: 282ecd65-222f-e22c1aa8986b
```

No problem at 1.10.0 But 1.10.4 has the question.

What is the reason?

![](https://ofbudvg4c.qnssl.com/images/k8s/WechatIMG2274.jpeg)",222f-e22c1aa8986b,https://github.com/kubernetes/kubernetes
89,64802,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If the matter is security related, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>
/kind bug
> /kind feature


**What happened**:
In low memory, crucial pods are candidates for eviction.

**What you expected to happen**:
Evict only non-critical pods

**How to reproduce it (as minimally and precisely as possible)**:
1. minikube start  --extra-config=kubelet.eviction-hard=""memory.available<500Mi""
2. create a pod (I used [memory-demo](https://github.com/kubernetes/website/blob/master/content/en/docs/tasks/configure-pod-container/memory-request-limit.yaml))
3. see a critical pod will be evicted (in my case, kube-apiserver-minikube_kube-system)

**Anything else we need to know?**:

`minikube logs` snippet:
```
Jun 06 00:34:08 minikube kubelet[3594]: I0606 00:34:08.130867    3594 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume ""default-token-bvkxc"" (UniqueName: ""kubernetes.io/secret/52e5e2af-6921-11e8-8ba7-080027da9d03-default-token-bvkxc"") pod ""memory-demo-787bcd6d76-nw7rz"" (UID: ""52e5e2af-6921-11e8-8ba7-080027da9d03"")
Jun 06 00:34:19 minikube kubelet[3594]: W0606 00:34:19.515478    3594 eviction_manager.go:343] eviction manager: attempting to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515701    3594 eviction_manager.go:357] eviction manager: must evict pod(s) to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515869    3594 eviction_manager.go:375] eviction manager: pods ranked for eviction: kube-apiserver-minikube_kube-system(9e218749db575502dc2153f1dcfcd8f5), kube-controller-manager-minikube_kube-system(af5fba67ac3471c82c00451a6f0db9a5), storage-provisioner_kube-system(1a69c6d7-6921-11e8-8ba7-080027da9d03), etcd-minikube_kube-system(8904a21094f73743ab3c7734da699963), kube-proxy-988jf_kube-system(18edc583-6921-11e8-8ba7-080027da9d03), kube-scheduler-minikube_kube-system(31cf0ccbee286239d451edb6fb511513), kubernetes-dashboard-5498ccf677-vgcw2_kube-system(19af187b-6921-11e8-8ba7-080027da9d03), influxdb-grafana-9sxtq_kube-system(19c5427a-6921-11e8-8ba7-080027da9d03), heapster-z8wfw_kube-system(19ba41de-6921-11e8-8ba7-080027da9d03), kube-addon-manager-minikube_kube-system(3afaf06535cc3b85be93c31632b765da), memory-demo-787bcd6d76-nw7rz_default(52e5e2af-6921-11e8-8ba7-080027da9d03), kube-dns-86f4d74b45-bczhr_kube-system(18e7ed0d-6921-11e8-8ba7-080027da9d03)
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550634    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to watch *v1.Service: Get https://192.168.99.100:8443/api/v1/services?resourceVersion=367&timeoutSeconds=385&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550700    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:460: Failed to watch *v1.Node: Get https://192.168.99.100:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=606&timeoutSeconds=362&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
```

May relate to other similar issues (ex. https://github.com/kubernetes/kubernetes/issues/53659 https://github.com/kubernetes/kubernetes/issues/63005)

**Environment**:
- Kubernetes version (use `kubectl version`): v1.10.0
- Cloud provider or hardware configuration: minikube v0.27.0
",19af187b-6921-11e8-8ba7-080027da9d03,https://github.com/kubernetes/kubernetes
90,64802,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If the matter is security related, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>
/kind bug
> /kind feature


**What happened**:
In low memory, crucial pods are candidates for eviction.

**What you expected to happen**:
Evict only non-critical pods

**How to reproduce it (as minimally and precisely as possible)**:
1. minikube start  --extra-config=kubelet.eviction-hard=""memory.available<500Mi""
2. create a pod (I used [memory-demo](https://github.com/kubernetes/website/blob/master/content/en/docs/tasks/configure-pod-container/memory-request-limit.yaml))
3. see a critical pod will be evicted (in my case, kube-apiserver-minikube_kube-system)

**Anything else we need to know?**:

`minikube logs` snippet:
```
Jun 06 00:34:08 minikube kubelet[3594]: I0606 00:34:08.130867    3594 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume ""default-token-bvkxc"" (UniqueName: ""kubernetes.io/secret/52e5e2af-6921-11e8-8ba7-080027da9d03-default-token-bvkxc"") pod ""memory-demo-787bcd6d76-nw7rz"" (UID: ""52e5e2af-6921-11e8-8ba7-080027da9d03"")
Jun 06 00:34:19 minikube kubelet[3594]: W0606 00:34:19.515478    3594 eviction_manager.go:343] eviction manager: attempting to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515701    3594 eviction_manager.go:357] eviction manager: must evict pod(s) to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515869    3594 eviction_manager.go:375] eviction manager: pods ranked for eviction: kube-apiserver-minikube_kube-system(9e218749db575502dc2153f1dcfcd8f5), kube-controller-manager-minikube_kube-system(af5fba67ac3471c82c00451a6f0db9a5), storage-provisioner_kube-system(1a69c6d7-6921-11e8-8ba7-080027da9d03), etcd-minikube_kube-system(8904a21094f73743ab3c7734da699963), kube-proxy-988jf_kube-system(18edc583-6921-11e8-8ba7-080027da9d03), kube-scheduler-minikube_kube-system(31cf0ccbee286239d451edb6fb511513), kubernetes-dashboard-5498ccf677-vgcw2_kube-system(19af187b-6921-11e8-8ba7-080027da9d03), influxdb-grafana-9sxtq_kube-system(19c5427a-6921-11e8-8ba7-080027da9d03), heapster-z8wfw_kube-system(19ba41de-6921-11e8-8ba7-080027da9d03), kube-addon-manager-minikube_kube-system(3afaf06535cc3b85be93c31632b765da), memory-demo-787bcd6d76-nw7rz_default(52e5e2af-6921-11e8-8ba7-080027da9d03), kube-dns-86f4d74b45-bczhr_kube-system(18e7ed0d-6921-11e8-8ba7-080027da9d03)
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550634    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to watch *v1.Service: Get https://192.168.99.100:8443/api/v1/services?resourceVersion=367&timeoutSeconds=385&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550700    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:460: Failed to watch *v1.Node: Get https://192.168.99.100:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=606&timeoutSeconds=362&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
```

May relate to other similar issues (ex. https://github.com/kubernetes/kubernetes/issues/53659 https://github.com/kubernetes/kubernetes/issues/63005)

**Environment**:
- Kubernetes version (use `kubectl version`): v1.10.0
- Cloud provider or hardware configuration: minikube v0.27.0
",18e7ed0d-6921-11e8-8ba7-080027da9d03,https://github.com/kubernetes/kubernetes
91,64802,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If the matter is security related, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>
/kind bug
> /kind feature


**What happened**:
In low memory, crucial pods are candidates for eviction.

**What you expected to happen**:
Evict only non-critical pods

**How to reproduce it (as minimally and precisely as possible)**:
1. minikube start  --extra-config=kubelet.eviction-hard=""memory.available<500Mi""
2. create a pod (I used [memory-demo](https://github.com/kubernetes/website/blob/master/content/en/docs/tasks/configure-pod-container/memory-request-limit.yaml))
3. see a critical pod will be evicted (in my case, kube-apiserver-minikube_kube-system)

**Anything else we need to know?**:

`minikube logs` snippet:
```
Jun 06 00:34:08 minikube kubelet[3594]: I0606 00:34:08.130867    3594 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume ""default-token-bvkxc"" (UniqueName: ""kubernetes.io/secret/52e5e2af-6921-11e8-8ba7-080027da9d03-default-token-bvkxc"") pod ""memory-demo-787bcd6d76-nw7rz"" (UID: ""52e5e2af-6921-11e8-8ba7-080027da9d03"")
Jun 06 00:34:19 minikube kubelet[3594]: W0606 00:34:19.515478    3594 eviction_manager.go:343] eviction manager: attempting to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515701    3594 eviction_manager.go:357] eviction manager: must evict pod(s) to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515869    3594 eviction_manager.go:375] eviction manager: pods ranked for eviction: kube-apiserver-minikube_kube-system(9e218749db575502dc2153f1dcfcd8f5), kube-controller-manager-minikube_kube-system(af5fba67ac3471c82c00451a6f0db9a5), storage-provisioner_kube-system(1a69c6d7-6921-11e8-8ba7-080027da9d03), etcd-minikube_kube-system(8904a21094f73743ab3c7734da699963), kube-proxy-988jf_kube-system(18edc583-6921-11e8-8ba7-080027da9d03), kube-scheduler-minikube_kube-system(31cf0ccbee286239d451edb6fb511513), kubernetes-dashboard-5498ccf677-vgcw2_kube-system(19af187b-6921-11e8-8ba7-080027da9d03), influxdb-grafana-9sxtq_kube-system(19c5427a-6921-11e8-8ba7-080027da9d03), heapster-z8wfw_kube-system(19ba41de-6921-11e8-8ba7-080027da9d03), kube-addon-manager-minikube_kube-system(3afaf06535cc3b85be93c31632b765da), memory-demo-787bcd6d76-nw7rz_default(52e5e2af-6921-11e8-8ba7-080027da9d03), kube-dns-86f4d74b45-bczhr_kube-system(18e7ed0d-6921-11e8-8ba7-080027da9d03)
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550634    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to watch *v1.Service: Get https://192.168.99.100:8443/api/v1/services?resourceVersion=367&timeoutSeconds=385&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550700    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:460: Failed to watch *v1.Node: Get https://192.168.99.100:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=606&timeoutSeconds=362&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
```

May relate to other similar issues (ex. https://github.com/kubernetes/kubernetes/issues/53659 https://github.com/kubernetes/kubernetes/issues/63005)

**Environment**:
- Kubernetes version (use `kubectl version`): v1.10.0
- Cloud provider or hardware configuration: minikube v0.27.0
",19ba41de-6921-11e8-8ba7-080027da9d03,https://github.com/kubernetes/kubernetes
92,64802,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If the matter is security related, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>
/kind bug
> /kind feature


**What happened**:
In low memory, crucial pods are candidates for eviction.

**What you expected to happen**:
Evict only non-critical pods

**How to reproduce it (as minimally and precisely as possible)**:
1. minikube start  --extra-config=kubelet.eviction-hard=""memory.available<500Mi""
2. create a pod (I used [memory-demo](https://github.com/kubernetes/website/blob/master/content/en/docs/tasks/configure-pod-container/memory-request-limit.yaml))
3. see a critical pod will be evicted (in my case, kube-apiserver-minikube_kube-system)

**Anything else we need to know?**:

`minikube logs` snippet:
```
Jun 06 00:34:08 minikube kubelet[3594]: I0606 00:34:08.130867    3594 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume ""default-token-bvkxc"" (UniqueName: ""kubernetes.io/secret/52e5e2af-6921-11e8-8ba7-080027da9d03-default-token-bvkxc"") pod ""memory-demo-787bcd6d76-nw7rz"" (UID: ""52e5e2af-6921-11e8-8ba7-080027da9d03"")
Jun 06 00:34:19 minikube kubelet[3594]: W0606 00:34:19.515478    3594 eviction_manager.go:343] eviction manager: attempting to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515701    3594 eviction_manager.go:357] eviction manager: must evict pod(s) to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515869    3594 eviction_manager.go:375] eviction manager: pods ranked for eviction: kube-apiserver-minikube_kube-system(9e218749db575502dc2153f1dcfcd8f5), kube-controller-manager-minikube_kube-system(af5fba67ac3471c82c00451a6f0db9a5), storage-provisioner_kube-system(1a69c6d7-6921-11e8-8ba7-080027da9d03), etcd-minikube_kube-system(8904a21094f73743ab3c7734da699963), kube-proxy-988jf_kube-system(18edc583-6921-11e8-8ba7-080027da9d03), kube-scheduler-minikube_kube-system(31cf0ccbee286239d451edb6fb511513), kubernetes-dashboard-5498ccf677-vgcw2_kube-system(19af187b-6921-11e8-8ba7-080027da9d03), influxdb-grafana-9sxtq_kube-system(19c5427a-6921-11e8-8ba7-080027da9d03), heapster-z8wfw_kube-system(19ba41de-6921-11e8-8ba7-080027da9d03), kube-addon-manager-minikube_kube-system(3afaf06535cc3b85be93c31632b765da), memory-demo-787bcd6d76-nw7rz_default(52e5e2af-6921-11e8-8ba7-080027da9d03), kube-dns-86f4d74b45-bczhr_kube-system(18e7ed0d-6921-11e8-8ba7-080027da9d03)
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550634    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to watch *v1.Service: Get https://192.168.99.100:8443/api/v1/services?resourceVersion=367&timeoutSeconds=385&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550700    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:460: Failed to watch *v1.Node: Get https://192.168.99.100:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=606&timeoutSeconds=362&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
```

May relate to other similar issues (ex. https://github.com/kubernetes/kubernetes/issues/53659 https://github.com/kubernetes/kubernetes/issues/63005)

**Environment**:
- Kubernetes version (use `kubectl version`): v1.10.0
- Cloud provider or hardware configuration: minikube v0.27.0
",52e5e2af-6921-11e8-8ba7-080027da9d03,https://github.com/kubernetes/kubernetes
93,64802,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If the matter is security related, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>
/kind bug
> /kind feature


**What happened**:
In low memory, crucial pods are candidates for eviction.

**What you expected to happen**:
Evict only non-critical pods

**How to reproduce it (as minimally and precisely as possible)**:
1. minikube start  --extra-config=kubelet.eviction-hard=""memory.available<500Mi""
2. create a pod (I used [memory-demo](https://github.com/kubernetes/website/blob/master/content/en/docs/tasks/configure-pod-container/memory-request-limit.yaml))
3. see a critical pod will be evicted (in my case, kube-apiserver-minikube_kube-system)

**Anything else we need to know?**:

`minikube logs` snippet:
```
Jun 06 00:34:08 minikube kubelet[3594]: I0606 00:34:08.130867    3594 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume ""default-token-bvkxc"" (UniqueName: ""kubernetes.io/secret/52e5e2af-6921-11e8-8ba7-080027da9d03-default-token-bvkxc"") pod ""memory-demo-787bcd6d76-nw7rz"" (UID: ""52e5e2af-6921-11e8-8ba7-080027da9d03"")
Jun 06 00:34:19 minikube kubelet[3594]: W0606 00:34:19.515478    3594 eviction_manager.go:343] eviction manager: attempting to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515701    3594 eviction_manager.go:357] eviction manager: must evict pod(s) to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515869    3594 eviction_manager.go:375] eviction manager: pods ranked for eviction: kube-apiserver-minikube_kube-system(9e218749db575502dc2153f1dcfcd8f5), kube-controller-manager-minikube_kube-system(af5fba67ac3471c82c00451a6f0db9a5), storage-provisioner_kube-system(1a69c6d7-6921-11e8-8ba7-080027da9d03), etcd-minikube_kube-system(8904a21094f73743ab3c7734da699963), kube-proxy-988jf_kube-system(18edc583-6921-11e8-8ba7-080027da9d03), kube-scheduler-minikube_kube-system(31cf0ccbee286239d451edb6fb511513), kubernetes-dashboard-5498ccf677-vgcw2_kube-system(19af187b-6921-11e8-8ba7-080027da9d03), influxdb-grafana-9sxtq_kube-system(19c5427a-6921-11e8-8ba7-080027da9d03), heapster-z8wfw_kube-system(19ba41de-6921-11e8-8ba7-080027da9d03), kube-addon-manager-minikube_kube-system(3afaf06535cc3b85be93c31632b765da), memory-demo-787bcd6d76-nw7rz_default(52e5e2af-6921-11e8-8ba7-080027da9d03), kube-dns-86f4d74b45-bczhr_kube-system(18e7ed0d-6921-11e8-8ba7-080027da9d03)
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550634    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to watch *v1.Service: Get https://192.168.99.100:8443/api/v1/services?resourceVersion=367&timeoutSeconds=385&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550700    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:460: Failed to watch *v1.Node: Get https://192.168.99.100:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=606&timeoutSeconds=362&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
```

May relate to other similar issues (ex. https://github.com/kubernetes/kubernetes/issues/53659 https://github.com/kubernetes/kubernetes/issues/63005)

**Environment**:
- Kubernetes version (use `kubectl version`): v1.10.0
- Cloud provider or hardware configuration: minikube v0.27.0
",18edc583-6921-11e8-8ba7-080027da9d03,https://github.com/kubernetes/kubernetes
94,64802,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If the matter is security related, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>
/kind bug
> /kind feature


**What happened**:
In low memory, crucial pods are candidates for eviction.

**What you expected to happen**:
Evict only non-critical pods

**How to reproduce it (as minimally and precisely as possible)**:
1. minikube start  --extra-config=kubelet.eviction-hard=""memory.available<500Mi""
2. create a pod (I used [memory-demo](https://github.com/kubernetes/website/blob/master/content/en/docs/tasks/configure-pod-container/memory-request-limit.yaml))
3. see a critical pod will be evicted (in my case, kube-apiserver-minikube_kube-system)

**Anything else we need to know?**:

`minikube logs` snippet:
```
Jun 06 00:34:08 minikube kubelet[3594]: I0606 00:34:08.130867    3594 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume ""default-token-bvkxc"" (UniqueName: ""kubernetes.io/secret/52e5e2af-6921-11e8-8ba7-080027da9d03-default-token-bvkxc"") pod ""memory-demo-787bcd6d76-nw7rz"" (UID: ""52e5e2af-6921-11e8-8ba7-080027da9d03"")
Jun 06 00:34:19 minikube kubelet[3594]: W0606 00:34:19.515478    3594 eviction_manager.go:343] eviction manager: attempting to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515701    3594 eviction_manager.go:357] eviction manager: must evict pod(s) to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515869    3594 eviction_manager.go:375] eviction manager: pods ranked for eviction: kube-apiserver-minikube_kube-system(9e218749db575502dc2153f1dcfcd8f5), kube-controller-manager-minikube_kube-system(af5fba67ac3471c82c00451a6f0db9a5), storage-provisioner_kube-system(1a69c6d7-6921-11e8-8ba7-080027da9d03), etcd-minikube_kube-system(8904a21094f73743ab3c7734da699963), kube-proxy-988jf_kube-system(18edc583-6921-11e8-8ba7-080027da9d03), kube-scheduler-minikube_kube-system(31cf0ccbee286239d451edb6fb511513), kubernetes-dashboard-5498ccf677-vgcw2_kube-system(19af187b-6921-11e8-8ba7-080027da9d03), influxdb-grafana-9sxtq_kube-system(19c5427a-6921-11e8-8ba7-080027da9d03), heapster-z8wfw_kube-system(19ba41de-6921-11e8-8ba7-080027da9d03), kube-addon-manager-minikube_kube-system(3afaf06535cc3b85be93c31632b765da), memory-demo-787bcd6d76-nw7rz_default(52e5e2af-6921-11e8-8ba7-080027da9d03), kube-dns-86f4d74b45-bczhr_kube-system(18e7ed0d-6921-11e8-8ba7-080027da9d03)
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550634    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to watch *v1.Service: Get https://192.168.99.100:8443/api/v1/services?resourceVersion=367&timeoutSeconds=385&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550700    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:460: Failed to watch *v1.Node: Get https://192.168.99.100:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=606&timeoutSeconds=362&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
```

May relate to other similar issues (ex. https://github.com/kubernetes/kubernetes/issues/53659 https://github.com/kubernetes/kubernetes/issues/63005)

**Environment**:
- Kubernetes version (use `kubectl version`): v1.10.0
- Cloud provider or hardware configuration: minikube v0.27.0
",1a69c6d7-6921-11e8-8ba7-080027da9d03,https://github.com/kubernetes/kubernetes
95,64802,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If the matter is security related, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>
/kind bug
> /kind feature


**What happened**:
In low memory, crucial pods are candidates for eviction.

**What you expected to happen**:
Evict only non-critical pods

**How to reproduce it (as minimally and precisely as possible)**:
1. minikube start  --extra-config=kubelet.eviction-hard=""memory.available<500Mi""
2. create a pod (I used [memory-demo](https://github.com/kubernetes/website/blob/master/content/en/docs/tasks/configure-pod-container/memory-request-limit.yaml))
3. see a critical pod will be evicted (in my case, kube-apiserver-minikube_kube-system)

**Anything else we need to know?**:

`minikube logs` snippet:
```
Jun 06 00:34:08 minikube kubelet[3594]: I0606 00:34:08.130867    3594 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume ""default-token-bvkxc"" (UniqueName: ""kubernetes.io/secret/52e5e2af-6921-11e8-8ba7-080027da9d03-default-token-bvkxc"") pod ""memory-demo-787bcd6d76-nw7rz"" (UID: ""52e5e2af-6921-11e8-8ba7-080027da9d03"")
Jun 06 00:34:19 minikube kubelet[3594]: W0606 00:34:19.515478    3594 eviction_manager.go:343] eviction manager: attempting to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515701    3594 eviction_manager.go:357] eviction manager: must evict pod(s) to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515869    3594 eviction_manager.go:375] eviction manager: pods ranked for eviction: kube-apiserver-minikube_kube-system(9e218749db575502dc2153f1dcfcd8f5), kube-controller-manager-minikube_kube-system(af5fba67ac3471c82c00451a6f0db9a5), storage-provisioner_kube-system(1a69c6d7-6921-11e8-8ba7-080027da9d03), etcd-minikube_kube-system(8904a21094f73743ab3c7734da699963), kube-proxy-988jf_kube-system(18edc583-6921-11e8-8ba7-080027da9d03), kube-scheduler-minikube_kube-system(31cf0ccbee286239d451edb6fb511513), kubernetes-dashboard-5498ccf677-vgcw2_kube-system(19af187b-6921-11e8-8ba7-080027da9d03), influxdb-grafana-9sxtq_kube-system(19c5427a-6921-11e8-8ba7-080027da9d03), heapster-z8wfw_kube-system(19ba41de-6921-11e8-8ba7-080027da9d03), kube-addon-manager-minikube_kube-system(3afaf06535cc3b85be93c31632b765da), memory-demo-787bcd6d76-nw7rz_default(52e5e2af-6921-11e8-8ba7-080027da9d03), kube-dns-86f4d74b45-bczhr_kube-system(18e7ed0d-6921-11e8-8ba7-080027da9d03)
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550634    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to watch *v1.Service: Get https://192.168.99.100:8443/api/v1/services?resourceVersion=367&timeoutSeconds=385&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550700    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:460: Failed to watch *v1.Node: Get https://192.168.99.100:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=606&timeoutSeconds=362&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
```

May relate to other similar issues (ex. https://github.com/kubernetes/kubernetes/issues/53659 https://github.com/kubernetes/kubernetes/issues/63005)

**Environment**:
- Kubernetes version (use `kubectl version`): v1.10.0
- Cloud provider or hardware configuration: minikube v0.27.0
",19c5427a-6921-11e8-8ba7-080027da9d03,https://github.com/kubernetes/kubernetes
96,64802,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If the matter is security related, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>
/kind bug
> /kind feature


**What happened**:
In low memory, crucial pods are candidates for eviction.

**What you expected to happen**:
Evict only non-critical pods

**How to reproduce it (as minimally and precisely as possible)**:
1. minikube start  --extra-config=kubelet.eviction-hard=""memory.available<500Mi""
2. create a pod (I used [memory-demo](https://github.com/kubernetes/website/blob/master/content/en/docs/tasks/configure-pod-container/memory-request-limit.yaml))
3. see a critical pod will be evicted (in my case, kube-apiserver-minikube_kube-system)

**Anything else we need to know?**:

`minikube logs` snippet:
```
Jun 06 00:34:08 minikube kubelet[3594]: I0606 00:34:08.130867    3594 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume ""default-token-bvkxc"" (UniqueName: ""kubernetes.io/secret/52e5e2af-6921-11e8-8ba7-080027da9d03-default-token-bvkxc"") pod ""memory-demo-787bcd6d76-nw7rz"" (UID: ""52e5e2af-6921-11e8-8ba7-080027da9d03"")
Jun 06 00:34:19 minikube kubelet[3594]: W0606 00:34:19.515478    3594 eviction_manager.go:343] eviction manager: attempting to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515701    3594 eviction_manager.go:357] eviction manager: must evict pod(s) to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515869    3594 eviction_manager.go:375] eviction manager: pods ranked for eviction: kube-apiserver-minikube_kube-system(9e218749db575502dc2153f1dcfcd8f5), kube-controller-manager-minikube_kube-system(af5fba67ac3471c82c00451a6f0db9a5), storage-provisioner_kube-system(1a69c6d7-6921-11e8-8ba7-080027da9d03), etcd-minikube_kube-system(8904a21094f73743ab3c7734da699963), kube-proxy-988jf_kube-system(18edc583-6921-11e8-8ba7-080027da9d03), kube-scheduler-minikube_kube-system(31cf0ccbee286239d451edb6fb511513), kubernetes-dashboard-5498ccf677-vgcw2_kube-system(19af187b-6921-11e8-8ba7-080027da9d03), influxdb-grafana-9sxtq_kube-system(19c5427a-6921-11e8-8ba7-080027da9d03), heapster-z8wfw_kube-system(19ba41de-6921-11e8-8ba7-080027da9d03), kube-addon-manager-minikube_kube-system(3afaf06535cc3b85be93c31632b765da), memory-demo-787bcd6d76-nw7rz_default(52e5e2af-6921-11e8-8ba7-080027da9d03), kube-dns-86f4d74b45-bczhr_kube-system(18e7ed0d-6921-11e8-8ba7-080027da9d03)
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550634    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to watch *v1.Service: Get https://192.168.99.100:8443/api/v1/services?resourceVersion=367&timeoutSeconds=385&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550700    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:460: Failed to watch *v1.Node: Get https://192.168.99.100:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=606&timeoutSeconds=362&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
```

May relate to other similar issues (ex. https://github.com/kubernetes/kubernetes/issues/53659 https://github.com/kubernetes/kubernetes/issues/63005)

**Environment**:
- Kubernetes version (use `kubectl version`): v1.10.0
- Cloud provider or hardware configuration: minikube v0.27.0
",787bcd6d76-nw7rz,https://github.com/kubernetes/kubernetes
97,64492,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>
/kind bug
> /kind feature


**What happened**:
When get resource with invalid token, the error info kubectl outputs is wrong.
For example:
`kubectl get pod --token=eyJhbGciOiJSUzI1NiIsIm`

Apparently, the token here is invalid. And the kubectl will output error info like
`error: the server doesn't have a resource type ""pod""`
It would mislead user.

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
- Cloud provider or hardware configuration:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",eyJhbGciOiJSUzI1NiIsIm,https://github.com/kubernetes/kubernetes
98,63435,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",#NAME?,https://github.com/kubernetes/kubernetes
99,63435,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",token=bearer_token,https://github.com/kubernetes/kubernetes
100,63435,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",certificate=~=true,https://github.com/kubernetes/kubernetes
101,63435,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",auth-provider=oidc,https://github.com/kubernetes/kubernetes
102,63435,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",username=basic_user,https://github.com/kubernetes/kubernetes
103,63435,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",certificate=path,https://github.com/kubernetes/kubernetes
104,63435,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",client-certificate,https://github.com/kubernetes/kubernetes
105,63435,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",provider=provider_name,https://github.com/kubernetes/kubernetes
106,63435,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",auth-provider=gcp,https://github.com/kubernetes/kubernetes
107,63435,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",mutually,https://github.com/kubernetes/kubernetes
108,63435,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",certificate,https://github.com/kubernetes/kubernetes
109,63435,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",kubeconfig,https://github.com/kubernetes/kubernetes
110,63435,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",provider,https://github.com/kubernetes/kubernetes
111,62950,"I'm trying to add a clusters to federation using kubefed join : 
this is the federation kube config file : 
```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://k8s-apiserver.cluster.local:8443
  name: kubernetes
- cluster:
    certificate-authority-data: REDACTED
    server: https://172.16.1.4:32471
  name: federation
- cluster:
    insecure-skip-tls-verify: true
    server: https://139.54.130.49:32046
  name: kubernetes-s1
contexts:
- context:
    cluster: kubernetes
    namespace: default
    user: kubectl
  name: default-context
- context:
    cluster: federation
    user: federation
  name: federation
- context:
    cluster: kubernetes
    namespace: kube-system
    user: kubectl
  name: kube-system-context
- context:
    cluster: kubernetes-s1
    namespace: default
    user: kubernetes-admins1
  name: kubernetes-admin-s1
current-context: federation
kind: Config
preferences: {}
users:
- name: federation
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
    token: e7506989-42eb-11e8-bf70-fa163eb593a3
- name: federation-basic-auth
  user:
    password: e7506937-42eb-11e8-bf70-fa163eb593a3
    username: admin
- name: kubectl
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
- name: kubernetes-admins1
  user:
    token: eyJhbGciOiJSUz............
```

i run this command : kubefed join site-1 --host-cluster-context=default-context --cluster-context=kubernetes-admin-s1 --insecure-skip-tls-verify=true, **the cluster is created but with offline status , is not reacheable ;** 
I'm using token bearer to reach the api server of the target cluster; 
where am I going wrong ? 


/sig multicluster
/sig federation ",e7506989-42eb-11e8-bf70-fa163eb593a3,https://github.com/kubernetes/kubernetes
112,62950,"I'm trying to add a clusters to federation using kubefed join : 
this is the federation kube config file : 
```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://k8s-apiserver.cluster.local:8443
  name: kubernetes
- cluster:
    certificate-authority-data: REDACTED
    server: https://172.16.1.4:32471
  name: federation
- cluster:
    insecure-skip-tls-verify: true
    server: https://139.54.130.49:32046
  name: kubernetes-s1
contexts:
- context:
    cluster: kubernetes
    namespace: default
    user: kubectl
  name: default-context
- context:
    cluster: federation
    user: federation
  name: federation
- context:
    cluster: kubernetes
    namespace: kube-system
    user: kubectl
  name: kube-system-context
- context:
    cluster: kubernetes-s1
    namespace: default
    user: kubernetes-admins1
  name: kubernetes-admin-s1
current-context: federation
kind: Config
preferences: {}
users:
- name: federation
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
    token: e7506989-42eb-11e8-bf70-fa163eb593a3
- name: federation-basic-auth
  user:
    password: e7506937-42eb-11e8-bf70-fa163eb593a3
    username: admin
- name: kubectl
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
- name: kubernetes-admins1
  user:
    token: eyJhbGciOiJSUz............
```

i run this command : kubefed join site-1 --host-cluster-context=default-context --cluster-context=kubernetes-admin-s1 --insecure-skip-tls-verify=true, **the cluster is created but with offline status , is not reacheable ;** 
I'm using token bearer to reach the api server of the target cluster; 
where am I going wrong ? 


/sig multicluster
/sig federation ",e7506937-42eb-11e8-bf70-fa163eb593a3,https://github.com/kubernetes/kubernetes
113,62950,"I'm trying to add a clusters to federation using kubefed join : 
this is the federation kube config file : 
```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://k8s-apiserver.cluster.local:8443
  name: kubernetes
- cluster:
    certificate-authority-data: REDACTED
    server: https://172.16.1.4:32471
  name: federation
- cluster:
    insecure-skip-tls-verify: true
    server: https://139.54.130.49:32046
  name: kubernetes-s1
contexts:
- context:
    cluster: kubernetes
    namespace: default
    user: kubectl
  name: default-context
- context:
    cluster: federation
    user: federation
  name: federation
- context:
    cluster: kubernetes
    namespace: kube-system
    user: kubectl
  name: kube-system-context
- context:
    cluster: kubernetes-s1
    namespace: default
    user: kubernetes-admins1
  name: kubernetes-admin-s1
current-context: federation
kind: Config
preferences: {}
users:
- name: federation
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
    token: e7506989-42eb-11e8-bf70-fa163eb593a3
- name: federation-basic-auth
  user:
    password: e7506937-42eb-11e8-bf70-fa163eb593a3
    username: admin
- name: kubectl
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
- name: kubernetes-admins1
  user:
    token: eyJhbGciOiJSUz............
```

i run this command : kubefed join site-1 --host-cluster-context=default-context --cluster-context=kubernetes-admin-s1 --insecure-skip-tls-verify=true, **the cluster is created but with offline status , is not reacheable ;** 
I'm using token bearer to reach the api server of the target cluster; 
where am I going wrong ? 


/sig multicluster
/sig federation ",42eb-11e8-bf70-fa163eb593a3,https://github.com/kubernetes/kubernetes
114,62950,"I'm trying to add a clusters to federation using kubefed join : 
this is the federation kube config file : 
```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://k8s-apiserver.cluster.local:8443
  name: kubernetes
- cluster:
    certificate-authority-data: REDACTED
    server: https://172.16.1.4:32471
  name: federation
- cluster:
    insecure-skip-tls-verify: true
    server: https://139.54.130.49:32046
  name: kubernetes-s1
contexts:
- context:
    cluster: kubernetes
    namespace: default
    user: kubectl
  name: default-context
- context:
    cluster: federation
    user: federation
  name: federation
- context:
    cluster: kubernetes
    namespace: kube-system
    user: kubectl
  name: kube-system-context
- context:
    cluster: kubernetes-s1
    namespace: default
    user: kubernetes-admins1
  name: kubernetes-admin-s1
current-context: federation
kind: Config
preferences: {}
users:
- name: federation
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
    token: e7506989-42eb-11e8-bf70-fa163eb593a3
- name: federation-basic-auth
  user:
    password: e7506937-42eb-11e8-bf70-fa163eb593a3
    username: admin
- name: kubectl
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
- name: kubernetes-admins1
  user:
    token: eyJhbGciOiJSUz............
```

i run this command : kubefed join site-1 --host-cluster-context=default-context --cluster-context=kubernetes-admin-s1 --insecure-skip-tls-verify=true, **the cluster is created but with offline status , is not reacheable ;** 
I'm using token bearer to reach the api server of the target cluster; 
where am I going wrong ? 


/sig multicluster
/sig federation ",bf70-fa163eb593a3,https://github.com/kubernetes/kubernetes
115,62950,"I'm trying to add a clusters to federation using kubefed join : 
this is the federation kube config file : 
```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://k8s-apiserver.cluster.local:8443
  name: kubernetes
- cluster:
    certificate-authority-data: REDACTED
    server: https://172.16.1.4:32471
  name: federation
- cluster:
    insecure-skip-tls-verify: true
    server: https://139.54.130.49:32046
  name: kubernetes-s1
contexts:
- context:
    cluster: kubernetes
    namespace: default
    user: kubectl
  name: default-context
- context:
    cluster: federation
    user: federation
  name: federation
- context:
    cluster: kubernetes
    namespace: kube-system
    user: kubectl
  name: kube-system-context
- context:
    cluster: kubernetes-s1
    namespace: default
    user: kubernetes-admins1
  name: kubernetes-admin-s1
current-context: federation
kind: Config
preferences: {}
users:
- name: federation
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
    token: e7506989-42eb-11e8-bf70-fa163eb593a3
- name: federation-basic-auth
  user:
    password: e7506937-42eb-11e8-bf70-fa163eb593a3
    username: admin
- name: kubectl
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
- name: kubernetes-admins1
  user:
    token: eyJhbGciOiJSUz............
```

i run this command : kubefed join site-1 --host-cluster-context=default-context --cluster-context=kubernetes-admin-s1 --insecure-skip-tls-verify=true, **the cluster is created but with offline status , is not reacheable ;** 
I'm using token bearer to reach the api server of the target cluster; 
where am I going wrong ? 


/sig multicluster
/sig federation ",kubernetes-admins1,https://github.com/kubernetes/kubernetes
116,62950,"I'm trying to add a clusters to federation using kubefed join : 
this is the federation kube config file : 
```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://k8s-apiserver.cluster.local:8443
  name: kubernetes
- cluster:
    certificate-authority-data: REDACTED
    server: https://172.16.1.4:32471
  name: federation
- cluster:
    insecure-skip-tls-verify: true
    server: https://139.54.130.49:32046
  name: kubernetes-s1
contexts:
- context:
    cluster: kubernetes
    namespace: default
    user: kubectl
  name: default-context
- context:
    cluster: federation
    user: federation
  name: federation
- context:
    cluster: kubernetes
    namespace: kube-system
    user: kubectl
  name: kube-system-context
- context:
    cluster: kubernetes-s1
    namespace: default
    user: kubernetes-admins1
  name: kubernetes-admin-s1
current-context: federation
kind: Config
preferences: {}
users:
- name: federation
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
    token: e7506989-42eb-11e8-bf70-fa163eb593a3
- name: federation-basic-auth
  user:
    password: e7506937-42eb-11e8-bf70-fa163eb593a3
    username: admin
- name: kubectl
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
- name: kubernetes-admins1
  user:
    token: eyJhbGciOiJSUz............
```

i run this command : kubefed join site-1 --host-cluster-context=default-context --cluster-context=kubernetes-admin-s1 --insecure-skip-tls-verify=true, **the cluster is created but with offline status , is not reacheable ;** 
I'm using token bearer to reach the api server of the target cluster; 
where am I going wrong ? 


/sig multicluster
/sig federation ",fa163eb593a3,https://github.com/kubernetes/kubernetes
117,61532,"I install ceph luminous, and create rbd pool. When I configure kubernetes to use ceph by storageclass, the pvc is pending. I have install ceph-common and modprobe rbd on kubernetes node（version:v1.8.3）.
The config of ceph-secret and ceph-storageclass is like this:
apiVersion: v1
kind: Secret
metadata:
name: ceph-secret
namespace: default
type: ""kubernetes.io/rbd""
data:
key: QVFBckViSmFuVDdvSXhBQXgwNXFNZGVNMzhSM012dG1ISzFqMWc9PQ==

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
name: ceph-iop
provisioner: kubernetes.io/rbd
parameters:
monitors: 10.110.25.71:6789,10.110.25.72:6789,10.110.25.73:6789
adminId: admin
adminSecretName: ceph-secret
adminSecretNamespace: default
pool: rbd
userId: admin
userSecretName: ceph-secret
imageFormat: ""1""

I found some error in kube-control-manager log is like this:
2018-03-22 07:30:32.039740 7f6e50267700 0 -- 10.110.22.81:0/1000234 >> 10.110.25.73:6789/0 pipe(0x7f6e3c001c30 sd=3 :39474 s=1 pgs=0 cs=0 l=1 c=0x7f6e3c005e10).connect protocol feature mismatch, my fffffffffff < peer 4010ff8eea4fffb missing 401000000000000
2018-03-22 07:30:33.532998 7f6e568b8780 0 monclient(hunting): authenticate timed out after 300
2018-03-22 07:30:33.554389 7f6e568b8780 0 librados: client.admin authentication error (110) Connection timed out
rbd: couldn't connect to the cluster!
I0322 07:30:33.818456 1 pv_controller.go:1317] failed to provision volume for claim ""default/pvc"" with StorageClass ""ceph-iop"": failed to create rbd image: exit status 1, command output: 2018-03-22 07:25:33.379161 7f6e568b8780 -1 did not load config file, using default settings.",ceph-storageclass,https://github.com/kubernetes/kubernetes
118,61532,"I install ceph luminous, and create rbd pool. When I configure kubernetes to use ceph by storageclass, the pvc is pending. I have install ceph-common and modprobe rbd on kubernetes node（version:v1.8.3）.
The config of ceph-secret and ceph-storageclass is like this:
apiVersion: v1
kind: Secret
metadata:
name: ceph-secret
namespace: default
type: ""kubernetes.io/rbd""
data:
key: QVFBckViSmFuVDdvSXhBQXgwNXFNZGVNMzhSM012dG1ISzFqMWc9PQ==

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
name: ceph-iop
provisioner: kubernetes.io/rbd
parameters:
monitors: 10.110.25.71:6789,10.110.25.72:6789,10.110.25.73:6789
adminId: admin
adminSecretName: ceph-secret
adminSecretNamespace: default
pool: rbd
userId: admin
userSecretName: ceph-secret
imageFormat: ""1""

I found some error in kube-control-manager log is like this:
2018-03-22 07:30:32.039740 7f6e50267700 0 -- 10.110.22.81:0/1000234 >> 10.110.25.73:6789/0 pipe(0x7f6e3c001c30 sd=3 :39474 s=1 pgs=0 cs=0 l=1 c=0x7f6e3c005e10).connect protocol feature mismatch, my fffffffffff < peer 4010ff8eea4fffb missing 401000000000000
2018-03-22 07:30:33.532998 7f6e568b8780 0 monclient(hunting): authenticate timed out after 300
2018-03-22 07:30:33.554389 7f6e568b8780 0 librados: client.admin authentication error (110) Connection timed out
rbd: couldn't connect to the cluster!
I0322 07:30:33.818456 1 pv_controller.go:1317] failed to provision volume for claim ""default/pvc"" with StorageClass ""ceph-iop"": failed to create rbd image: exit status 1, command output: 2018-03-22 07:25:33.379161 7f6e568b8780 -1 did not load config file, using default settings.",adminSecretNamespace,https://github.com/kubernetes/kubernetes
119,61532,"I install ceph luminous, and create rbd pool. When I configure kubernetes to use ceph by storageclass, the pvc is pending. I have install ceph-common and modprobe rbd on kubernetes node（version:v1.8.3）.
The config of ceph-secret and ceph-storageclass is like this:
apiVersion: v1
kind: Secret
metadata:
name: ceph-secret
namespace: default
type: ""kubernetes.io/rbd""
data:
key: QVFBckViSmFuVDdvSXhBQXgwNXFNZGVNMzhSM012dG1ISzFqMWc9PQ==

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
name: ceph-iop
provisioner: kubernetes.io/rbd
parameters:
monitors: 10.110.25.71:6789,10.110.25.72:6789,10.110.25.73:6789
adminId: admin
adminSecretName: ceph-secret
adminSecretNamespace: default
pool: rbd
userId: admin
userSecretName: ceph-secret
imageFormat: ""1""

I found some error in kube-control-manager log is like this:
2018-03-22 07:30:32.039740 7f6e50267700 0 -- 10.110.22.81:0/1000234 >> 10.110.25.73:6789/0 pipe(0x7f6e3c001c30 sd=3 :39474 s=1 pgs=0 cs=0 l=1 c=0x7f6e3c005e10).connect protocol feature mismatch, my fffffffffff < peer 4010ff8eea4fffb missing 401000000000000
2018-03-22 07:30:33.532998 7f6e568b8780 0 monclient(hunting): authenticate timed out after 300
2018-03-22 07:30:33.554389 7f6e568b8780 0 librados: client.admin authentication error (110) Connection timed out
rbd: couldn't connect to the cluster!
I0322 07:30:33.818456 1 pv_controller.go:1317] failed to provision volume for claim ""default/pvc"" with StorageClass ""ceph-iop"": failed to create rbd image: exit status 1, command output: 2018-03-22 07:25:33.379161 7f6e568b8780 -1 did not load config file, using default settings.",QVFBckViSmFuVDdvSXhBQXgwNXFNZGVNMzhSM012dG1ISzFqMWc9PQ,https://github.com/kubernetes/kubernetes
120,61532,"I install ceph luminous, and create rbd pool. When I configure kubernetes to use ceph by storageclass, the pvc is pending. I have install ceph-common and modprobe rbd on kubernetes node（version:v1.8.3）.
The config of ceph-secret and ceph-storageclass is like this:
apiVersion: v1
kind: Secret
metadata:
name: ceph-secret
namespace: default
type: ""kubernetes.io/rbd""
data:
key: QVFBckViSmFuVDdvSXhBQXgwNXFNZGVNMzhSM012dG1ISzFqMWc9PQ==

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
name: ceph-iop
provisioner: kubernetes.io/rbd
parameters:
monitors: 10.110.25.71:6789,10.110.25.72:6789,10.110.25.73:6789
adminId: admin
adminSecretName: ceph-secret
adminSecretNamespace: default
pool: rbd
userId: admin
userSecretName: ceph-secret
imageFormat: ""1""

I found some error in kube-control-manager log is like this:
2018-03-22 07:30:32.039740 7f6e50267700 0 -- 10.110.22.81:0/1000234 >> 10.110.25.73:6789/0 pipe(0x7f6e3c001c30 sd=3 :39474 s=1 pgs=0 cs=0 l=1 c=0x7f6e3c005e10).connect protocol feature mismatch, my fffffffffff < peer 4010ff8eea4fffb missing 401000000000000
2018-03-22 07:30:33.532998 7f6e568b8780 0 monclient(hunting): authenticate timed out after 300
2018-03-22 07:30:33.554389 7f6e568b8780 0 librados: client.admin authentication error (110) Connection timed out
rbd: couldn't connect to the cluster!
I0322 07:30:33.818456 1 pv_controller.go:1317] failed to provision volume for claim ""default/pvc"" with StorageClass ""ceph-iop"": failed to create rbd image: exit status 1, command output: 2018-03-22 07:25:33.379161 7f6e568b8780 -1 did not load config file, using default settings.",QVFBckViSmFuVDdvSXhBQXgwNXFNZGVNMzhSM012dG1ISzFqMW,https://github.com/kubernetes/kubernetes
121,61140,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG REPORT
> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
After upgrading the master to 1.9.3-gke.0 in my GKE cluster, the kubectl replare freezes when I perform the command below:

kubectl create secret -n ""test"" \
       docker-registry test-registry \
       --docker-server = ""registry.devops.teste.com"" \
       --docker-username = ""test"" \
       --docker-password = ""pKu3fzFfxW2xV2ygm-A1"" \
       --docker-email = ""test@teste.com""
       -o yaml --dry-run | kubectl replace -n ""test"" --force -f -

If the resource does not exist it creates without problems, but if the resource exists after deleting the previous one it hangs.

**What you expected to happen**:
I hope the secret is recreated

**How to reproduce it (as minimally and precisely as possible)**:

execute the command 2x

kubectl create secret -n ""teste"" \
      docker-registry teste-registry \
      --docker-server=""registry.devops.teste.com"" \
      --docker-username=""teste"" \
      --docker-password=""pKu3fzFfxW2xV2ygm-A1"" \
      --docker-email=""teste@teste.com"" \
      -o yaml --dry-run | kubectl replace -n ""teste"" --force -f -

**Anything else we need to know?**:
This started after Master's upgra for version
1.9.3-gke.0

**Environment**:
- Kubernetes version (use `kubectl version`): 
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.0"", GitCommit:""925c127ec6b946659ad0fd596fa959be43f0cc05"", GitTreeState:""clean"", BuildDate:""2017-12-15T21:07:38Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9+"", GitVersion:""v1.9.3-gke.0"", GitCommit:""a7b719f7d3463eb5431cf8a3caf5d485827b4210"", GitTreeState:""clean"", BuildDate:""2018-02-16T18:26:01Z"", GoVersion:""go1.9.2b4"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
n1-standard-4 (4 vCPUs, 15 GB memory)

- OS (e.g. from /etc/os-release):
Container-Optimized OS (cos)

- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",docker-email=teste,https://github.com/kubernetes/kubernetes
122,61140,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG REPORT
> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
After upgrading the master to 1.9.3-gke.0 in my GKE cluster, the kubectl replare freezes when I perform the command below:

kubectl create secret -n ""test"" \
       docker-registry test-registry \
       --docker-server = ""registry.devops.teste.com"" \
       --docker-username = ""test"" \
       --docker-password = ""pKu3fzFfxW2xV2ygm-A1"" \
       --docker-email = ""test@teste.com""
       -o yaml --dry-run | kubectl replace -n ""test"" --force -f -

If the resource does not exist it creates without problems, but if the resource exists after deleting the previous one it hangs.

**What you expected to happen**:
I hope the secret is recreated

**How to reproduce it (as minimally and precisely as possible)**:

execute the command 2x

kubectl create secret -n ""teste"" \
      docker-registry teste-registry \
      --docker-server=""registry.devops.teste.com"" \
      --docker-username=""teste"" \
      --docker-password=""pKu3fzFfxW2xV2ygm-A1"" \
      --docker-email=""teste@teste.com"" \
      -o yaml --dry-run | kubectl replace -n ""teste"" --force -f -

**Anything else we need to know?**:
This started after Master's upgra for version
1.9.3-gke.0

**Environment**:
- Kubernetes version (use `kubectl version`): 
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.0"", GitCommit:""925c127ec6b946659ad0fd596fa959be43f0cc05"", GitTreeState:""clean"", BuildDate:""2017-12-15T21:07:38Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9+"", GitVersion:""v1.9.3-gke.0"", GitCommit:""a7b719f7d3463eb5431cf8a3caf5d485827b4210"", GitTreeState:""clean"", BuildDate:""2018-02-16T18:26:01Z"", GoVersion:""go1.9.2b4"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
n1-standard-4 (4 vCPUs, 15 GB memory)

- OS (e.g. from /etc/os-release):
Container-Optimized OS (cos)

- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",pKu3fzFfxW2xV2ygm-A1,https://github.com/kubernetes/kubernetes
123,61140,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG REPORT
> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
After upgrading the master to 1.9.3-gke.0 in my GKE cluster, the kubectl replare freezes when I perform the command below:

kubectl create secret -n ""test"" \
       docker-registry test-registry \
       --docker-server = ""registry.devops.teste.com"" \
       --docker-username = ""test"" \
       --docker-password = ""pKu3fzFfxW2xV2ygm-A1"" \
       --docker-email = ""test@teste.com""
       -o yaml --dry-run | kubectl replace -n ""test"" --force -f -

If the resource does not exist it creates without problems, but if the resource exists after deleting the previous one it hangs.

**What you expected to happen**:
I hope the secret is recreated

**How to reproduce it (as minimally and precisely as possible)**:

execute the command 2x

kubectl create secret -n ""teste"" \
      docker-registry teste-registry \
      --docker-server=""registry.devops.teste.com"" \
      --docker-username=""teste"" \
      --docker-password=""pKu3fzFfxW2xV2ygm-A1"" \
      --docker-email=""teste@teste.com"" \
      -o yaml --dry-run | kubectl replace -n ""teste"" --force -f -

**Anything else we need to know?**:
This started after Master's upgra for version
1.9.3-gke.0

**Environment**:
- Kubernetes version (use `kubectl version`): 
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.0"", GitCommit:""925c127ec6b946659ad0fd596fa959be43f0cc05"", GitTreeState:""clean"", BuildDate:""2017-12-15T21:07:38Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9+"", GitVersion:""v1.9.3-gke.0"", GitCommit:""a7b719f7d3463eb5431cf8a3caf5d485827b4210"", GitTreeState:""clean"", BuildDate:""2018-02-16T18:26:01Z"", GoVersion:""go1.9.2b4"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
n1-standard-4 (4 vCPUs, 15 GB memory)

- OS (e.g. from /etc/os-release):
Container-Optimized OS (cos)

- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",pKu3fzFfxW2xV2ygm,https://github.com/kubernetes/kubernetes
124,61140,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG REPORT
> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
After upgrading the master to 1.9.3-gke.0 in my GKE cluster, the kubectl replare freezes when I perform the command below:

kubectl create secret -n ""test"" \
       docker-registry test-registry \
       --docker-server = ""registry.devops.teste.com"" \
       --docker-username = ""test"" \
       --docker-password = ""pKu3fzFfxW2xV2ygm-A1"" \
       --docker-email = ""test@teste.com""
       -o yaml --dry-run | kubectl replace -n ""test"" --force -f -

If the resource does not exist it creates without problems, but if the resource exists after deleting the previous one it hangs.

**What you expected to happen**:
I hope the secret is recreated

**How to reproduce it (as minimally and precisely as possible)**:

execute the command 2x

kubectl create secret -n ""teste"" \
      docker-registry teste-registry \
      --docker-server=""registry.devops.teste.com"" \
      --docker-username=""teste"" \
      --docker-password=""pKu3fzFfxW2xV2ygm-A1"" \
      --docker-email=""teste@teste.com"" \
      -o yaml --dry-run | kubectl replace -n ""teste"" --force -f -

**Anything else we need to know?**:
This started after Master's upgra for version
1.9.3-gke.0

**Environment**:
- Kubernetes version (use `kubectl version`): 
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.0"", GitCommit:""925c127ec6b946659ad0fd596fa959be43f0cc05"", GitTreeState:""clean"", BuildDate:""2017-12-15T21:07:38Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9+"", GitVersion:""v1.9.3-gke.0"", GitCommit:""a7b719f7d3463eb5431cf8a3caf5d485827b4210"", GitTreeState:""clean"", BuildDate:""2018-02-16T18:26:01Z"", GoVersion:""go1.9.2b4"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
n1-standard-4 (4 vCPUs, 15 GB memory)

- OS (e.g. from /etc/os-release):
Container-Optimized OS (cos)

- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",email=teste@,https://github.com/kubernetes/kubernetes
125,60410,"> /kind bug

> /sig cluster-lifecycle

## Problem describe
kubeadm.cfg
```
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
cloudProvider: external
imageRepository: registry.cn-hangzhou.aliyuncs.com/acs
featureGates:
  selfHosting: true
networking:
  dnsDomain: cluster.local
  serviceSubnet: 172.19.0.0/20
  podSubnet: 172.16.0.0/16
apiServerCertSANs:
  - 192.168.110.109
  - 127.0.0.1
token: cdca7c.3e8035f5959f0da6
nodeName: cn-hongkong.i-j6cal2nyrssnrcz2mju7
kubernetesVersion: v1.9.3
```

kubeadm init --config kubeadm.cfg

```
[self-hosted] Creating self-hosted control plane.
[apiclient] Found 0 Pods for label selector k8s-app=self-hosted-kube-apiserver
error creating self hosted control plane: timed out waiting for the condition
```

**error creating self hosted control plane: timed out waiting for the condition** Error occured.

```
kubectl get no -o yaml 

  spec:
    externalID: cn-hongkong.i-j6cal2nyrssnrcz2mju7
    podCIDR: 172.16.0.0/24
    providerID: cn-hongkong.i-j6cal2nyrssnrcz2mju7
    taints:
    - effect: NoSchedule
      key: node.cloudprovider.kubernetes.io/uninitialized
      value: ""true""
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
```

## Fix 

kubelet will taint node with ```node.cloudprovider.kubernetes.io/uninitialized``` on node initializing. kubeadm should take care of this taint by adding a toleration on control panel pod when bootstrap a cluster.

A PR to fix this will be submitted soon. 
",kubernetesVersion,https://github.com/kubernetes/kubernetes
126,55850,"/sig api-machinery

**What happened**:

i  want to get kubernetes  data in etcd v3 via etcd clientV3 , but however, the value of the key is unreadable ,like this 

```
key:""/registry/pods/default/tserver-79f7f9fbd6-hw4pb"" create_revision:2712131 mod_revision:2712145 version:4 value:""k8s\000\n\t\n\002v1\022\003Pod\022\361\014\n\211\004\n\030tserver-79f7f9fbd6-hw4pb\022\023tserver-79f7f9fbd6-\032\007default\""\000*$7238544b-ca7e-11e7-8e50-5820b101504c2\0008\000B\010\010\300\214\264\320\005\020\000Z\016\n\003app\022\007tserverZ\037\n\021pod-template-hash\022\n3593959682b\205\002\n\030kubernetes.io/created-by\022\350\001{\""kind\"":\""SerializedReference\"",\""apiVersion\"":\""v1\"",\""reference\"":{\""kind\"":\""ReplicaSet\"",\""namespace\"":\""default\"",\""name\"":\""tserver-79f7f9fbd6\"",\""uid\"":\""722d7d9d-ca7e-11e7-8e50-5820b101504c\"",\""apiVersion\"":\""extensions\"",\""resourceVersion\"":\""2712127\""}}\nj^\n\nReplicaSet\032\022tserver-79f7f9fbd6\""$722d7d9d-ca7e-11e7-8e50-5820b101504c*\022extensions/v1beta10\0018\001z\000\022\335\004\n$\n\013test-volume\022\025\n\023\n\017/data/hostPath/\022\000\n1\n\023default-token-cq7gh\022\0322\030\n\023default-token-cq7gh\030\244\003\022\220\002\n\007tserver\022Pregistry.cn-shanghai.aliyuncs.com/xxtest/linlintest_server:20171108_1011_582*\0002\016\n\000\020\000\030\204N\""\003TCP*\000B\000J!\n\013test-volume\020\000\032\016/data/myvolume\""\000JH\n\023default-token-cq7gh\020\001\032-/var/run/secrets/kubernetes.io/serviceaccount\""\000j\024/dev/termination-logr\014IfNotPresent\200\001\000\210\001\000\220\001\000\242\001\004File\032\006Always \0362\014ClusterFirstB\007defaultJ\007defaultR\017centos-minion-2X\000`\000h\000r\000z\020\n\016linlinregistry\202\001\000\212\001\000\232\001\021default-scheduler\262\001;\n!node.alpha.kubernetes.io/notReady\022\006Exists\032\000\""\tNoExecute(\254\002\262\001>\n$node.alpha.kubernetes.io/unreachable\022\006Exists\032\000\""\tNoExecute(\254\002\302\001\000\032\202\004\n\007Running\022#\n\013Initialized\022\004True\032\000\""\010\010\315\214\264\320\005\020\000*\0002\000\022\035\n\005Ready\022\004True\032\000\""\010\010\317\214\264\320\005\020\000*\0002\000\022$\n\014PodScheduled\022\004True\032\000\""\010\010\300\214\264\320\005\020\000*\0002\000\032\000\""\000*\017192.168.200.1472\n10.50.2.22:\010\010\315\214\264\320\005\020\000B\325\002\n\007tserver\022\014\022\n\n\010\010\316\214\264\320\005\020\000\032\000 \001(\0002Pregistry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server:20171108_1011_582:\230\001docker-pullable://registry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server@sha256:ddca47c2019bd664d9851cd0230251598f4f07d5189d57ad1c77eec2ff07da09BIdocker://6aa5baa96a48617cdd47230f08c9754ef035897d44354c2b3606842630eb660cJ\nBestEffort\032\000\""\000""
```


and  i notice that the value  been Serialized 
so i try to import k8s runtime api to decode the value , but an error occured

 ## no kind ""Pod"" is registered for version ""v1""




here are my code:

BTW:  tserver is my k8s service created by yaml file
```
package main
import (
	""context""
	""fmt""
	""github.com/coreos/etcd/clientv3""
	""k8s.io/apimachinery/pkg/runtime""
	""k8s.io/apimachinery/pkg/runtime/serializer""
	""os""
	
)

func main() {

//other code

resp, err := cli.Get(ctx, ""/registry/pods/default/tserver"", clientv3.WithPrefix(), clientv3.WithSort(clientv3.SortByKey, clientv3.SortDescend))

var Scheme = runtime.NewScheme()
var Codecs = serializer.NewCodecFactory(Scheme)
decoder := Codecs.UniversalDeserializer()

	for _, kv := range resp.Kvs {

		obj, gvk, err := decoder.Decode(kv.Value, nil, nil)
		if err != nil {

			fmt.Fprintf(os.Stderr, ""WARN: unable to decode key [%s]: %v\n"", kv.Key, err)
			fmt.Println(gvk)
			continue
		}

		fmt.Println(gvk)

	}

}
```

but when  i run this code , it make some error

```
[root@VM-201-26 etcd]# /usr/bin/go run k8sEtcdGet.go 
WARN: unable to decode key [/registry/pods/default/tserver-79f7f9fbd6-hw4pb]: no kind ""Pod"" is registered for version ""v1""
```


if there any solution ? i just what string/readable  pod/service data from etcd V3 

thanks alot 


**Environment**: Centos  7.2.X
- Kubernetes version (use `kubectl version`): v1.8.0
go version:  go1.9.1 linux/amd64
@k8s-mirror-api-machinery-api-reviews ",722d7d9d-ca7e-11e7-8e50-5820b101504c,https://github.com/kubernetes/kubernetes
127,55850,"/sig api-machinery

**What happened**:

i  want to get kubernetes  data in etcd v3 via etcd clientV3 , but however, the value of the key is unreadable ,like this 

```
key:""/registry/pods/default/tserver-79f7f9fbd6-hw4pb"" create_revision:2712131 mod_revision:2712145 version:4 value:""k8s\000\n\t\n\002v1\022\003Pod\022\361\014\n\211\004\n\030tserver-79f7f9fbd6-hw4pb\022\023tserver-79f7f9fbd6-\032\007default\""\000*$7238544b-ca7e-11e7-8e50-5820b101504c2\0008\000B\010\010\300\214\264\320\005\020\000Z\016\n\003app\022\007tserverZ\037\n\021pod-template-hash\022\n3593959682b\205\002\n\030kubernetes.io/created-by\022\350\001{\""kind\"":\""SerializedReference\"",\""apiVersion\"":\""v1\"",\""reference\"":{\""kind\"":\""ReplicaSet\"",\""namespace\"":\""default\"",\""name\"":\""tserver-79f7f9fbd6\"",\""uid\"":\""722d7d9d-ca7e-11e7-8e50-5820b101504c\"",\""apiVersion\"":\""extensions\"",\""resourceVersion\"":\""2712127\""}}\nj^\n\nReplicaSet\032\022tserver-79f7f9fbd6\""$722d7d9d-ca7e-11e7-8e50-5820b101504c*\022extensions/v1beta10\0018\001z\000\022\335\004\n$\n\013test-volume\022\025\n\023\n\017/data/hostPath/\022\000\n1\n\023default-token-cq7gh\022\0322\030\n\023default-token-cq7gh\030\244\003\022\220\002\n\007tserver\022Pregistry.cn-shanghai.aliyuncs.com/xxtest/linlintest_server:20171108_1011_582*\0002\016\n\000\020\000\030\204N\""\003TCP*\000B\000J!\n\013test-volume\020\000\032\016/data/myvolume\""\000JH\n\023default-token-cq7gh\020\001\032-/var/run/secrets/kubernetes.io/serviceaccount\""\000j\024/dev/termination-logr\014IfNotPresent\200\001\000\210\001\000\220\001\000\242\001\004File\032\006Always \0362\014ClusterFirstB\007defaultJ\007defaultR\017centos-minion-2X\000`\000h\000r\000z\020\n\016linlinregistry\202\001\000\212\001\000\232\001\021default-scheduler\262\001;\n!node.alpha.kubernetes.io/notReady\022\006Exists\032\000\""\tNoExecute(\254\002\262\001>\n$node.alpha.kubernetes.io/unreachable\022\006Exists\032\000\""\tNoExecute(\254\002\302\001\000\032\202\004\n\007Running\022#\n\013Initialized\022\004True\032\000\""\010\010\315\214\264\320\005\020\000*\0002\000\022\035\n\005Ready\022\004True\032\000\""\010\010\317\214\264\320\005\020\000*\0002\000\022$\n\014PodScheduled\022\004True\032\000\""\010\010\300\214\264\320\005\020\000*\0002\000\032\000\""\000*\017192.168.200.1472\n10.50.2.22:\010\010\315\214\264\320\005\020\000B\325\002\n\007tserver\022\014\022\n\n\010\010\316\214\264\320\005\020\000\032\000 \001(\0002Pregistry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server:20171108_1011_582:\230\001docker-pullable://registry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server@sha256:ddca47c2019bd664d9851cd0230251598f4f07d5189d57ad1c77eec2ff07da09BIdocker://6aa5baa96a48617cdd47230f08c9754ef035897d44354c2b3606842630eb660cJ\nBestEffort\032\000\""\000""
```


and  i notice that the value  been Serialized 
so i try to import k8s runtime api to decode the value , but an error occured

 ## no kind ""Pod"" is registered for version ""v1""




here are my code:

BTW:  tserver is my k8s service created by yaml file
```
package main
import (
	""context""
	""fmt""
	""github.com/coreos/etcd/clientv3""
	""k8s.io/apimachinery/pkg/runtime""
	""k8s.io/apimachinery/pkg/runtime/serializer""
	""os""
	
)

func main() {

//other code

resp, err := cli.Get(ctx, ""/registry/pods/default/tserver"", clientv3.WithPrefix(), clientv3.WithSort(clientv3.SortByKey, clientv3.SortDescend))

var Scheme = runtime.NewScheme()
var Codecs = serializer.NewCodecFactory(Scheme)
decoder := Codecs.UniversalDeserializer()

	for _, kv := range resp.Kvs {

		obj, gvk, err := decoder.Decode(kv.Value, nil, nil)
		if err != nil {

			fmt.Fprintf(os.Stderr, ""WARN: unable to decode key [%s]: %v\n"", kv.Key, err)
			fmt.Println(gvk)
			continue
		}

		fmt.Println(gvk)

	}

}
```

but when  i run this code , it make some error

```
[root@VM-201-26 etcd]# /usr/bin/go run k8sEtcdGet.go 
WARN: unable to decode key [/registry/pods/default/tserver-79f7f9fbd6-hw4pb]: no kind ""Pod"" is registered for version ""v1""
```


if there any solution ? i just what string/readable  pod/service data from etcd V3 

thanks alot 


**Environment**: Centos  7.2.X
- Kubernetes version (use `kubectl version`): v1.8.0
go version:  go1.9.1 linux/amd64
@k8s-mirror-api-machinery-api-reviews ",cq7gh\030\244\003\022\220\002\n\007tserver\-*\0002\016\n\000\020,https://github.com/kubernetes/kubernetes
128,55850,"/sig api-machinery

**What happened**:

i  want to get kubernetes  data in etcd v3 via etcd clientV3 , but however, the value of the key is unreadable ,like this 

```
key:""/registry/pods/default/tserver-79f7f9fbd6-hw4pb"" create_revision:2712131 mod_revision:2712145 version:4 value:""k8s\000\n\t\n\002v1\022\003Pod\022\361\014\n\211\004\n\030tserver-79f7f9fbd6-hw4pb\022\023tserver-79f7f9fbd6-\032\007default\""\000*$7238544b-ca7e-11e7-8e50-5820b101504c2\0008\000B\010\010\300\214\264\320\005\020\000Z\016\n\003app\022\007tserverZ\037\n\021pod-template-hash\022\n3593959682b\205\002\n\030kubernetes.io/created-by\022\350\001{\""kind\"":\""SerializedReference\"",\""apiVersion\"":\""v1\"",\""reference\"":{\""kind\"":\""ReplicaSet\"",\""namespace\"":\""default\"",\""name\"":\""tserver-79f7f9fbd6\"",\""uid\"":\""722d7d9d-ca7e-11e7-8e50-5820b101504c\"",\""apiVersion\"":\""extensions\"",\""resourceVersion\"":\""2712127\""}}\nj^\n\nReplicaSet\032\022tserver-79f7f9fbd6\""$722d7d9d-ca7e-11e7-8e50-5820b101504c*\022extensions/v1beta10\0018\001z\000\022\335\004\n$\n\013test-volume\022\025\n\023\n\017/data/hostPath/\022\000\n1\n\023default-token-cq7gh\022\0322\030\n\023default-token-cq7gh\030\244\003\022\220\002\n\007tserver\022Pregistry.cn-shanghai.aliyuncs.com/xxtest/linlintest_server:20171108_1011_582*\0002\016\n\000\020\000\030\204N\""\003TCP*\000B\000J!\n\013test-volume\020\000\032\016/data/myvolume\""\000JH\n\023default-token-cq7gh\020\001\032-/var/run/secrets/kubernetes.io/serviceaccount\""\000j\024/dev/termination-logr\014IfNotPresent\200\001\000\210\001\000\220\001\000\242\001\004File\032\006Always \0362\014ClusterFirstB\007defaultJ\007defaultR\017centos-minion-2X\000`\000h\000r\000z\020\n\016linlinregistry\202\001\000\212\001\000\232\001\021default-scheduler\262\001;\n!node.alpha.kubernetes.io/notReady\022\006Exists\032\000\""\tNoExecute(\254\002\262\001>\n$node.alpha.kubernetes.io/unreachable\022\006Exists\032\000\""\tNoExecute(\254\002\302\001\000\032\202\004\n\007Running\022#\n\013Initialized\022\004True\032\000\""\010\010\315\214\264\320\005\020\000*\0002\000\022\035\n\005Ready\022\004True\032\000\""\010\010\317\214\264\320\005\020\000*\0002\000\022$\n\014PodScheduled\022\004True\032\000\""\010\010\300\214\264\320\005\020\000*\0002\000\032\000\""\000*\017192.168.200.1472\n10.50.2.22:\010\010\315\214\264\320\005\020\000B\325\002\n\007tserver\022\014\022\n\n\010\010\316\214\264\320\005\020\000\032\000 \001(\0002Pregistry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server:20171108_1011_582:\230\001docker-pullable://registry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server@sha256:ddca47c2019bd664d9851cd0230251598f4f07d5189d57ad1c77eec2ff07da09BIdocker://6aa5baa96a48617cdd47230f08c9754ef035897d44354c2b3606842630eb660cJ\nBestEffort\032\000\""\000""
```


and  i notice that the value  been Serialized 
so i try to import k8s runtime api to decode the value , but an error occured

 ## no kind ""Pod"" is registered for version ""v1""




here are my code:

BTW:  tserver is my k8s service created by yaml file
```
package main
import (
	""context""
	""fmt""
	""github.com/coreos/etcd/clientv3""
	""k8s.io/apimachinery/pkg/runtime""
	""k8s.io/apimachinery/pkg/runtime/serializer""
	""os""
	
)

func main() {

//other code

resp, err := cli.Get(ctx, ""/registry/pods/default/tserver"", clientv3.WithPrefix(), clientv3.WithSort(clientv3.SortByKey, clientv3.SortDescend))

var Scheme = runtime.NewScheme()
var Codecs = serializer.NewCodecFactory(Scheme)
decoder := Codecs.UniversalDeserializer()

	for _, kv := range resp.Kvs {

		obj, gvk, err := decoder.Decode(kv.Value, nil, nil)
		if err != nil {

			fmt.Fprintf(os.Stderr, ""WARN: unable to decode key [%s]: %v\n"", kv.Key, err)
			fmt.Println(gvk)
			continue
		}

		fmt.Println(gvk)

	}

}
```

but when  i run this code , it make some error

```
[root@VM-201-26 etcd]# /usr/bin/go run k8sEtcdGet.go 
WARN: unable to decode key [/registry/pods/default/tserver-79f7f9fbd6-hw4pb]: no kind ""Pod"" is registered for version ""v1""
```


if there any solution ? i just what string/readable  pod/service data from etcd V3 

thanks alot 


**Environment**: Centos  7.2.X
- Kubernetes version (use `kubectl version`): v1.8.0
go version:  go1.9.1 linux/amd64
@k8s-mirror-api-machinery-api-reviews ",000\n\t\n\002v1\022\003Pod\022\361\014\n\211\004\n\030tserver-,https://github.com/kubernetes/kubernetes
129,55850,"/sig api-machinery

**What happened**:

i  want to get kubernetes  data in etcd v3 via etcd clientV3 , but however, the value of the key is unreadable ,like this 

```
key:""/registry/pods/default/tserver-79f7f9fbd6-hw4pb"" create_revision:2712131 mod_revision:2712145 version:4 value:""k8s\000\n\t\n\002v1\022\003Pod\022\361\014\n\211\004\n\030tserver-79f7f9fbd6-hw4pb\022\023tserver-79f7f9fbd6-\032\007default\""\000*$7238544b-ca7e-11e7-8e50-5820b101504c2\0008\000B\010\010\300\214\264\320\005\020\000Z\016\n\003app\022\007tserverZ\037\n\021pod-template-hash\022\n3593959682b\205\002\n\030kubernetes.io/created-by\022\350\001{\""kind\"":\""SerializedReference\"",\""apiVersion\"":\""v1\"",\""reference\"":{\""kind\"":\""ReplicaSet\"",\""namespace\"":\""default\"",\""name\"":\""tserver-79f7f9fbd6\"",\""uid\"":\""722d7d9d-ca7e-11e7-8e50-5820b101504c\"",\""apiVersion\"":\""extensions\"",\""resourceVersion\"":\""2712127\""}}\nj^\n\nReplicaSet\032\022tserver-79f7f9fbd6\""$722d7d9d-ca7e-11e7-8e50-5820b101504c*\022extensions/v1beta10\0018\001z\000\022\335\004\n$\n\013test-volume\022\025\n\023\n\017/data/hostPath/\022\000\n1\n\023default-token-cq7gh\022\0322\030\n\023default-token-cq7gh\030\244\003\022\220\002\n\007tserver\022Pregistry.cn-shanghai.aliyuncs.com/xxtest/linlintest_server:20171108_1011_582*\0002\016\n\000\020\000\030\204N\""\003TCP*\000B\000J!\n\013test-volume\020\000\032\016/data/myvolume\""\000JH\n\023default-token-cq7gh\020\001\032-/var/run/secrets/kubernetes.io/serviceaccount\""\000j\024/dev/termination-logr\014IfNotPresent\200\001\000\210\001\000\220\001\000\242\001\004File\032\006Always \0362\014ClusterFirstB\007defaultJ\007defaultR\017centos-minion-2X\000`\000h\000r\000z\020\n\016linlinregistry\202\001\000\212\001\000\232\001\021default-scheduler\262\001;\n!node.alpha.kubernetes.io/notReady\022\006Exists\032\000\""\tNoExecute(\254\002\262\001>\n$node.alpha.kubernetes.io/unreachable\022\006Exists\032\000\""\tNoExecute(\254\002\302\001\000\032\202\004\n\007Running\022#\n\013Initialized\022\004True\032\000\""\010\010\315\214\264\320\005\020\000*\0002\000\022\035\n\005Ready\022\004True\032\000\""\010\010\317\214\264\320\005\020\000*\0002\000\022$\n\014PodScheduled\022\004True\032\000\""\010\010\300\214\264\320\005\020\000*\0002\000\032\000\""\000*\017192.168.200.1472\n10.50.2.22:\010\010\315\214\264\320\005\020\000B\325\002\n\007tserver\022\014\022\n\n\010\010\316\214\264\320\005\020\000\032\000 \001(\0002Pregistry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server:20171108_1011_582:\230\001docker-pullable://registry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server@sha256:ddca47c2019bd664d9851cd0230251598f4f07d5189d57ad1c77eec2ff07da09BIdocker://6aa5baa96a48617cdd47230f08c9754ef035897d44354c2b3606842630eb660cJ\nBestEffort\032\000\""\000""
```


and  i notice that the value  been Serialized 
so i try to import k8s runtime api to decode the value , but an error occured

 ## no kind ""Pod"" is registered for version ""v1""




here are my code:

BTW:  tserver is my k8s service created by yaml file
```
package main
import (
	""context""
	""fmt""
	""github.com/coreos/etcd/clientv3""
	""k8s.io/apimachinery/pkg/runtime""
	""k8s.io/apimachinery/pkg/runtime/serializer""
	""os""
	
)

func main() {

//other code

resp, err := cli.Get(ctx, ""/registry/pods/default/tserver"", clientv3.WithPrefix(), clientv3.WithSort(clientv3.SortByKey, clientv3.SortDescend))

var Scheme = runtime.NewScheme()
var Codecs = serializer.NewCodecFactory(Scheme)
decoder := Codecs.UniversalDeserializer()

	for _, kv := range resp.Kvs {

		obj, gvk, err := decoder.Decode(kv.Value, nil, nil)
		if err != nil {

			fmt.Fprintf(os.Stderr, ""WARN: unable to decode key [%s]: %v\n"", kv.Key, err)
			fmt.Println(gvk)
			continue
		}

		fmt.Println(gvk)

	}

}
```

but when  i run this code , it make some error

```
[root@VM-201-26 etcd]# /usr/bin/go run k8sEtcdGet.go 
WARN: unable to decode key [/registry/pods/default/tserver-79f7f9fbd6-hw4pb]: no kind ""Pod"" is registered for version ""v1""
```


if there any solution ? i just what string/readable  pod/service data from etcd V3 

thanks alot 


**Environment**: Centos  7.2.X
- Kubernetes version (use `kubectl version`): v1.8.0
go version:  go1.9.1 linux/amd64
@k8s-mirror-api-machinery-api-reviews ",014IfNotPresent\200\001\000\210\001\000\220\001\000\242\001\,https://github.com/kubernetes/kubernetes
130,55850,"/sig api-machinery

**What happened**:

i  want to get kubernetes  data in etcd v3 via etcd clientV3 , but however, the value of the key is unreadable ,like this 

```
key:""/registry/pods/default/tserver-79f7f9fbd6-hw4pb"" create_revision:2712131 mod_revision:2712145 version:4 value:""k8s\000\n\t\n\002v1\022\003Pod\022\361\014\n\211\004\n\030tserver-79f7f9fbd6-hw4pb\022\023tserver-79f7f9fbd6-\032\007default\""\000*$7238544b-ca7e-11e7-8e50-5820b101504c2\0008\000B\010\010\300\214\264\320\005\020\000Z\016\n\003app\022\007tserverZ\037\n\021pod-template-hash\022\n3593959682b\205\002\n\030kubernetes.io/created-by\022\350\001{\""kind\"":\""SerializedReference\"",\""apiVersion\"":\""v1\"",\""reference\"":{\""kind\"":\""ReplicaSet\"",\""namespace\"":\""default\"",\""name\"":\""tserver-79f7f9fbd6\"",\""uid\"":\""722d7d9d-ca7e-11e7-8e50-5820b101504c\"",\""apiVersion\"":\""extensions\"",\""resourceVersion\"":\""2712127\""}}\nj^\n\nReplicaSet\032\022tserver-79f7f9fbd6\""$722d7d9d-ca7e-11e7-8e50-5820b101504c*\022extensions/v1beta10\0018\001z\000\022\335\004\n$\n\013test-volume\022\025\n\023\n\017/data/hostPath/\022\000\n1\n\023default-token-cq7gh\022\0322\030\n\023default-token-cq7gh\030\244\003\022\220\002\n\007tserver\022Pregistry.cn-shanghai.aliyuncs.com/xxtest/linlintest_server:20171108_1011_582*\0002\016\n\000\020\000\030\204N\""\003TCP*\000B\000J!\n\013test-volume\020\000\032\016/data/myvolume\""\000JH\n\023default-token-cq7gh\020\001\032-/var/run/secrets/kubernetes.io/serviceaccount\""\000j\024/dev/termination-logr\014IfNotPresent\200\001\000\210\001\000\220\001\000\242\001\004File\032\006Always \0362\014ClusterFirstB\007defaultJ\007defaultR\017centos-minion-2X\000`\000h\000r\000z\020\n\016linlinregistry\202\001\000\212\001\000\232\001\021default-scheduler\262\001;\n!node.alpha.kubernetes.io/notReady\022\006Exists\032\000\""\tNoExecute(\254\002\262\001>\n$node.alpha.kubernetes.io/unreachable\022\006Exists\032\000\""\tNoExecute(\254\002\302\001\000\032\202\004\n\007Running\022#\n\013Initialized\022\004True\032\000\""\010\010\315\214\264\320\005\020\000*\0002\000\022\035\n\005Ready\022\004True\032\000\""\010\010\317\214\264\320\005\020\000*\0002\000\022$\n\014PodScheduled\022\004True\032\000\""\010\010\300\214\264\320\005\020\000*\0002\000\032\000\""\000*\017192.168.200.1472\n10.50.2.22:\010\010\315\214\264\320\005\020\000B\325\002\n\007tserver\022\014\022\n\n\010\010\316\214\264\320\005\020\000\032\000 \001(\0002Pregistry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server:20171108_1011_582:\230\001docker-pullable://registry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server@sha256:ddca47c2019bd664d9851cd0230251598f4f07d5189d57ad1c77eec2ff07da09BIdocker://6aa5baa96a48617cdd47230f08c9754ef035897d44354c2b3606842630eb660cJ\nBestEffort\032\000\""\000""
```


and  i notice that the value  been Serialized 
so i try to import k8s runtime api to decode the value , but an error occured

 ## no kind ""Pod"" is registered for version ""v1""




here are my code:

BTW:  tserver is my k8s service created by yaml file
```
package main
import (
	""context""
	""fmt""
	""github.com/coreos/etcd/clientv3""
	""k8s.io/apimachinery/pkg/runtime""
	""k8s.io/apimachinery/pkg/runtime/serializer""
	""os""
	
)

func main() {

//other code

resp, err := cli.Get(ctx, ""/registry/pods/default/tserver"", clientv3.WithPrefix(), clientv3.WithSort(clientv3.SortByKey, clientv3.SortDescend))

var Scheme = runtime.NewScheme()
var Codecs = serializer.NewCodecFactory(Scheme)
decoder := Codecs.UniversalDeserializer()

	for _, kv := range resp.Kvs {

		obj, gvk, err := decoder.Decode(kv.Value, nil, nil)
		if err != nil {

			fmt.Fprintf(os.Stderr, ""WARN: unable to decode key [%s]: %v\n"", kv.Key, err)
			fmt.Println(gvk)
			continue
		}

		fmt.Println(gvk)

	}

}
```

but when  i run this code , it make some error

```
[root@VM-201-26 etcd]# /usr/bin/go run k8sEtcdGet.go 
WARN: unable to decode key [/registry/pods/default/tserver-79f7f9fbd6-hw4pb]: no kind ""Pod"" is registered for version ""v1""
```


if there any solution ? i just what string/readable  pod/service data from etcd V3 

thanks alot 


**Environment**: Centos  7.2.X
- Kubernetes version (use `kubectl version`): v1.8.0
go version:  go1.9.1 linux/amd64
@k8s-mirror-api-machinery-api-reviews ",\\010\010\300\214\264\320\,https://github.com/kubernetes/kubernetes
131,55850,"/sig api-machinery

**What happened**:

i  want to get kubernetes  data in etcd v3 via etcd clientV3 , but however, the value of the key is unreadable ,like this 

```
key:""/registry/pods/default/tserver-79f7f9fbd6-hw4pb"" create_revision:2712131 mod_revision:2712145 version:4 value:""k8s\000\n\t\n\002v1\022\003Pod\022\361\014\n\211\004\n\030tserver-79f7f9fbd6-hw4pb\022\023tserver-79f7f9fbd6-\032\007default\""\000*$7238544b-ca7e-11e7-8e50-5820b101504c2\0008\000B\010\010\300\214\264\320\005\020\000Z\016\n\003app\022\007tserverZ\037\n\021pod-template-hash\022\n3593959682b\205\002\n\030kubernetes.io/created-by\022\350\001{\""kind\"":\""SerializedReference\"",\""apiVersion\"":\""v1\"",\""reference\"":{\""kind\"":\""ReplicaSet\"",\""namespace\"":\""default\"",\""name\"":\""tserver-79f7f9fbd6\"",\""uid\"":\""722d7d9d-ca7e-11e7-8e50-5820b101504c\"",\""apiVersion\"":\""extensions\"",\""resourceVersion\"":\""2712127\""}}\nj^\n\nReplicaSet\032\022tserver-79f7f9fbd6\""$722d7d9d-ca7e-11e7-8e50-5820b101504c*\022extensions/v1beta10\0018\001z\000\022\335\004\n$\n\013test-volume\022\025\n\023\n\017/data/hostPath/\022\000\n1\n\023default-token-cq7gh\022\0322\030\n\023default-token-cq7gh\030\244\003\022\220\002\n\007tserver\022Pregistry.cn-shanghai.aliyuncs.com/xxtest/linlintest_server:20171108_1011_582*\0002\016\n\000\020\000\030\204N\""\003TCP*\000B\000J!\n\013test-volume\020\000\032\016/data/myvolume\""\000JH\n\023default-token-cq7gh\020\001\032-/var/run/secrets/kubernetes.io/serviceaccount\""\000j\024/dev/termination-logr\014IfNotPresent\200\001\000\210\001\000\220\001\000\242\001\004File\032\006Always \0362\014ClusterFirstB\007defaultJ\007defaultR\017centos-minion-2X\000`\000h\000r\000z\020\n\016linlinregistry\202\001\000\212\001\000\232\001\021default-scheduler\262\001;\n!node.alpha.kubernetes.io/notReady\022\006Exists\032\000\""\tNoExecute(\254\002\262\001>\n$node.alpha.kubernetes.io/unreachable\022\006Exists\032\000\""\tNoExecute(\254\002\302\001\000\032\202\004\n\007Running\022#\n\013Initialized\022\004True\032\000\""\010\010\315\214\264\320\005\020\000*\0002\000\022\035\n\005Ready\022\004True\032\000\""\010\010\317\214\264\320\005\020\000*\0002\000\022$\n\014PodScheduled\022\004True\032\000\""\010\010\300\214\264\320\005\020\000*\0002\000\032\000\""\000*\017192.168.200.1472\n10.50.2.22:\010\010\315\214\264\320\005\020\000B\325\002\n\007tserver\022\014\022\n\n\010\010\316\214\264\320\005\020\000\032\000 \001(\0002Pregistry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server:20171108_1011_582:\230\001docker-pullable://registry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server@sha256:ddca47c2019bd664d9851cd0230251598f4f07d5189d57ad1c77eec2ff07da09BIdocker://6aa5baa96a48617cdd47230f08c9754ef035897d44354c2b3606842630eb660cJ\nBestEffort\032\000\""\000""
```


and  i notice that the value  been Serialized 
so i try to import k8s runtime api to decode the value , but an error occured

 ## no kind ""Pod"" is registered for version ""v1""




here are my code:

BTW:  tserver is my k8s service created by yaml file
```
package main
import (
	""context""
	""fmt""
	""github.com/coreos/etcd/clientv3""
	""k8s.io/apimachinery/pkg/runtime""
	""k8s.io/apimachinery/pkg/runtime/serializer""
	""os""
	
)

func main() {

//other code

resp, err := cli.Get(ctx, ""/registry/pods/default/tserver"", clientv3.WithPrefix(), clientv3.WithSort(clientv3.SortByKey, clientv3.SortDescend))

var Scheme = runtime.NewScheme()
var Codecs = serializer.NewCodecFactory(Scheme)
decoder := Codecs.UniversalDeserializer()

	for _, kv := range resp.Kvs {

		obj, gvk, err := decoder.Decode(kv.Value, nil, nil)
		if err != nil {

			fmt.Fprintf(os.Stderr, ""WARN: unable to decode key [%s]: %v\n"", kv.Key, err)
			fmt.Println(gvk)
			continue
		}

		fmt.Println(gvk)

	}

}
```

but when  i run this code , it make some error

```
[root@VM-201-26 etcd]# /usr/bin/go run k8sEtcdGet.go 
WARN: unable to decode key [/registry/pods/default/tserver-79f7f9fbd6-hw4pb]: no kind ""Pod"" is registered for version ""v1""
```


if there any solution ? i just what string/readable  pod/service data from etcd V3 

thanks alot 


**Environment**: Centos  7.2.X
- Kubernetes version (use `kubectl version`): v1.8.0
go version:  go1.9.1 linux/amd64
@k8s-mirror-api-machinery-api-reviews ",\\010\010\315\214\264\320\,https://github.com/kubernetes/kubernetes
132,55850,"/sig api-machinery

**What happened**:

i  want to get kubernetes  data in etcd v3 via etcd clientV3 , but however, the value of the key is unreadable ,like this 

```
key:""/registry/pods/default/tserver-79f7f9fbd6-hw4pb"" create_revision:2712131 mod_revision:2712145 version:4 value:""k8s\000\n\t\n\002v1\022\003Pod\022\361\014\n\211\004\n\030tserver-79f7f9fbd6-hw4pb\022\023tserver-79f7f9fbd6-\032\007default\""\000*$7238544b-ca7e-11e7-8e50-5820b101504c2\0008\000B\010\010\300\214\264\320\005\020\000Z\016\n\003app\022\007tserverZ\037\n\021pod-template-hash\022\n3593959682b\205\002\n\030kubernetes.io/created-by\022\350\001{\""kind\"":\""SerializedReference\"",\""apiVersion\"":\""v1\"",\""reference\"":{\""kind\"":\""ReplicaSet\"",\""namespace\"":\""default\"",\""name\"":\""tserver-79f7f9fbd6\"",\""uid\"":\""722d7d9d-ca7e-11e7-8e50-5820b101504c\"",\""apiVersion\"":\""extensions\"",\""resourceVersion\"":\""2712127\""}}\nj^\n\nReplicaSet\032\022tserver-79f7f9fbd6\""$722d7d9d-ca7e-11e7-8e50-5820b101504c*\022extensions/v1beta10\0018\001z\000\022\335\004\n$\n\013test-volume\022\025\n\023\n\017/data/hostPath/\022\000\n1\n\023default-token-cq7gh\022\0322\030\n\023default-token-cq7gh\030\244\003\022\220\002\n\007tserver\022Pregistry.cn-shanghai.aliyuncs.com/xxtest/linlintest_server:20171108_1011_582*\0002\016\n\000\020\000\030\204N\""\003TCP*\000B\000J!\n\013test-volume\020\000\032\016/data/myvolume\""\000JH\n\023default-token-cq7gh\020\001\032-/var/run/secrets/kubernetes.io/serviceaccount\""\000j\024/dev/termination-logr\014IfNotPresent\200\001\000\210\001\000\220\001\000\242\001\004File\032\006Always \0362\014ClusterFirstB\007defaultJ\007defaultR\017centos-minion-2X\000`\000h\000r\000z\020\n\016linlinregistry\202\001\000\212\001\000\232\001\021default-scheduler\262\001;\n!node.alpha.kubernetes.io/notReady\022\006Exists\032\000\""\tNoExecute(\254\002\262\001>\n$node.alpha.kubernetes.io/unreachable\022\006Exists\032\000\""\tNoExecute(\254\002\302\001\000\032\202\004\n\007Running\022#\n\013Initialized\022\004True\032\000\""\010\010\315\214\264\320\005\020\000*\0002\000\022\035\n\005Ready\022\004True\032\000\""\010\010\317\214\264\320\005\020\000*\0002\000\022$\n\014PodScheduled\022\004True\032\000\""\010\010\300\214\264\320\005\020\000*\0002\000\032\000\""\000*\017192.168.200.1472\n10.50.2.22:\010\010\315\214\264\320\005\020\000B\325\002\n\007tserver\022\014\022\n\n\010\010\316\214\264\320\005\020\000\032\000 \001(\0002Pregistry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server:20171108_1011_582:\230\001docker-pullable://registry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server@sha256:ddca47c2019bd664d9851cd0230251598f4f07d5189d57ad1c77eec2ff07da09BIdocker://6aa5baa96a48617cdd47230f08c9754ef035897d44354c2b3606842630eb660cJ\nBestEffort\032\000\""\000""
```


and  i notice that the value  been Serialized 
so i try to import k8s runtime api to decode the value , but an error occured

 ## no kind ""Pod"" is registered for version ""v1""




here are my code:

BTW:  tserver is my k8s service created by yaml file
```
package main
import (
	""context""
	""fmt""
	""github.com/coreos/etcd/clientv3""
	""k8s.io/apimachinery/pkg/runtime""
	""k8s.io/apimachinery/pkg/runtime/serializer""
	""os""
	
)

func main() {

//other code

resp, err := cli.Get(ctx, ""/registry/pods/default/tserver"", clientv3.WithPrefix(), clientv3.WithSort(clientv3.SortByKey, clientv3.SortDescend))

var Scheme = runtime.NewScheme()
var Codecs = serializer.NewCodecFactory(Scheme)
decoder := Codecs.UniversalDeserializer()

	for _, kv := range resp.Kvs {

		obj, gvk, err := decoder.Decode(kv.Value, nil, nil)
		if err != nil {

			fmt.Fprintf(os.Stderr, ""WARN: unable to decode key [%s]: %v\n"", kv.Key, err)
			fmt.Println(gvk)
			continue
		}

		fmt.Println(gvk)

	}

}
```

but when  i run this code , it make some error

```
[root@VM-201-26 etcd]# /usr/bin/go run k8sEtcdGet.go 
WARN: unable to decode key [/registry/pods/default/tserver-79f7f9fbd6-hw4pb]: no kind ""Pod"" is registered for version ""v1""
```


if there any solution ? i just what string/readable  pod/service data from etcd V3 

thanks alot 


**Environment**: Centos  7.2.X
- Kubernetes version (use `kubectl version`): v1.8.0
go version:  go1.9.1 linux/amd64
@k8s-mirror-api-machinery-api-reviews ",\\010\010\317\214\264\320\,https://github.com/kubernetes/kubernetes
133,54941,"/kind bug
/sig storage

## **What happened**:
**logs of controller-manager**
W1101 14:05:30.134407       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134459       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134507       7 rbd_util.go:364] failed to create rbd image, output
E1101 14:05:30.134529       7 rbd.go:323] rbd: create volume failed, err: failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:
I1101 14:05:30.134553       7 pv_controller.go:1331] failed to provision volume for claim ""default/claim5"" with StorageClass ""fast"": failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:

## What you expected to happen:
rbd executed successfully to create a volume on the ceph pool 'kube'

## How to reproduce it (as minimally and precisely as possible):
**ceph config**
ceph admin key = AQAMZ/hZheYqAxAAURdTDYs4GPc378Vp0PLeqA==
ceph kube key = AQDcffhZ1i3HMxAA5oB+SuIbzSKc+9FDc5tKeQ==
ceph --cluster ceph auth get-or-create client.kube mon 'allow r' osd 'allow rwx pool=kube'

ceph-secrets for users (admin, kube) with base64 encoded key
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: QVFBTVovaFpoZVlxQXhBQVVSZFREWXM0R1BjMzc4VnAwUExlcUE9PQ==
---
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-kube
data:
  key: QVFEY2ZmaFoxaTNITXhBQTVvQitTdUlielNLYys5RkRjNXRLZVE9PQ==

**Storageclass 'fast'**
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 1:6789,2:6789,3:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: ""kube-system""
  pool: kube
  userId: kube
  userSecretName: ceph-secret-kube

**PersistentStorageClaim**
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim5
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
  storageClassName: fast

**Executing 'rbd' inside of controller-manager works:**
$ kubectl exec -ti kube-controller-manager-st14d02ls-ztda11350501 -n kube-system rbd
   
2017-11-01 14:10:26.664691 7febc9f05780 -1 did not load config file, using default settings.
rbd: you must specify a command.
**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T07:00:21Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.5+coreos.0"", GitCommit:""070d238cd2ec359928548e486a9171b498573181"", GitTreeState:""clean"", BuildDate:""2017-08-31T21:28:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
. 3 masters / api servers

- OS (e.g. from /etc/os-release):
10x centos 7.3 nodes

- Kernel (e.g. `uname -a`):
4.9.44, custom kernel build.

- Install tools:
Kubespray

- Others:
docker 1.13
hyperkube 1.7.5
ceph version 0.94.10 (b1e0532418e4631af01acbc0cedd426f1905f4af
",b1e0532418e4631af01acbc0cedd426f1905f4af,https://github.com/kubernetes/kubernetes
134,54941,"/kind bug
/sig storage

## **What happened**:
**logs of controller-manager**
W1101 14:05:30.134407       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134459       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134507       7 rbd_util.go:364] failed to create rbd image, output
E1101 14:05:30.134529       7 rbd.go:323] rbd: create volume failed, err: failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:
I1101 14:05:30.134553       7 pv_controller.go:1331] failed to provision volume for claim ""default/claim5"" with StorageClass ""fast"": failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:

## What you expected to happen:
rbd executed successfully to create a volume on the ceph pool 'kube'

## How to reproduce it (as minimally and precisely as possible):
**ceph config**
ceph admin key = AQAMZ/hZheYqAxAAURdTDYs4GPc378Vp0PLeqA==
ceph kube key = AQDcffhZ1i3HMxAA5oB+SuIbzSKc+9FDc5tKeQ==
ceph --cluster ceph auth get-or-create client.kube mon 'allow r' osd 'allow rwx pool=kube'

ceph-secrets for users (admin, kube) with base64 encoded key
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: QVFBTVovaFpoZVlxQXhBQVVSZFREWXM0R1BjMzc4VnAwUExlcUE9PQ==
---
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-kube
data:
  key: QVFEY2ZmaFoxaTNITXhBQTVvQitTdUlielNLYys5RkRjNXRLZVE9PQ==

**Storageclass 'fast'**
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 1:6789,2:6789,3:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: ""kube-system""
  pool: kube
  userId: kube
  userSecretName: ceph-secret-kube

**PersistentStorageClaim**
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim5
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
  storageClassName: fast

**Executing 'rbd' inside of controller-manager works:**
$ kubectl exec -ti kube-controller-manager-st14d02ls-ztda11350501 -n kube-system rbd
   
2017-11-01 14:10:26.664691 7febc9f05780 -1 did not load config file, using default settings.
rbd: you must specify a command.
**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T07:00:21Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.5+coreos.0"", GitCommit:""070d238cd2ec359928548e486a9171b498573181"", GitTreeState:""clean"", BuildDate:""2017-08-31T21:28:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
. 3 masters / api servers

- OS (e.g. from /etc/os-release):
10x centos 7.3 nodes

- Kernel (e.g. `uname -a`):
4.9.44, custom kernel build.

- Install tools:
Kubespray

- Others:
docker 1.13
hyperkube 1.7.5
ceph version 0.94.10 (b1e0532418e4631af01acbc0cedd426f1905f4af
",QVFEY2ZmaFoxaTNITXhBQTVvQitTdUlielNLYys5RkRjNXRLZVE9PQ,https://github.com/kubernetes/kubernetes
135,54941,"/kind bug
/sig storage

## **What happened**:
**logs of controller-manager**
W1101 14:05:30.134407       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134459       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134507       7 rbd_util.go:364] failed to create rbd image, output
E1101 14:05:30.134529       7 rbd.go:323] rbd: create volume failed, err: failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:
I1101 14:05:30.134553       7 pv_controller.go:1331] failed to provision volume for claim ""default/claim5"" with StorageClass ""fast"": failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:

## What you expected to happen:
rbd executed successfully to create a volume on the ceph pool 'kube'

## How to reproduce it (as minimally and precisely as possible):
**ceph config**
ceph admin key = AQAMZ/hZheYqAxAAURdTDYs4GPc378Vp0PLeqA==
ceph kube key = AQDcffhZ1i3HMxAA5oB+SuIbzSKc+9FDc5tKeQ==
ceph --cluster ceph auth get-or-create client.kube mon 'allow r' osd 'allow rwx pool=kube'

ceph-secrets for users (admin, kube) with base64 encoded key
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: QVFBTVovaFpoZVlxQXhBQVVSZFREWXM0R1BjMzc4VnAwUExlcUE9PQ==
---
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-kube
data:
  key: QVFEY2ZmaFoxaTNITXhBQTVvQitTdUlielNLYys5RkRjNXRLZVE9PQ==

**Storageclass 'fast'**
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 1:6789,2:6789,3:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: ""kube-system""
  pool: kube
  userId: kube
  userSecretName: ceph-secret-kube

**PersistentStorageClaim**
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim5
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
  storageClassName: fast

**Executing 'rbd' inside of controller-manager works:**
$ kubectl exec -ti kube-controller-manager-st14d02ls-ztda11350501 -n kube-system rbd
   
2017-11-01 14:10:26.664691 7febc9f05780 -1 did not load config file, using default settings.
rbd: you must specify a command.
**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T07:00:21Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.5+coreos.0"", GitCommit:""070d238cd2ec359928548e486a9171b498573181"", GitTreeState:""clean"", BuildDate:""2017-08-31T21:28:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
. 3 masters / api servers

- OS (e.g. from /etc/os-release):
10x centos 7.3 nodes

- Kernel (e.g. `uname -a`):
4.9.44, custom kernel build.

- Install tools:
Kubespray

- Others:
docker 1.13
hyperkube 1.7.5
ceph version 0.94.10 (b1e0532418e4631af01acbc0cedd426f1905f4af
",adminSecretNamespace,https://github.com/kubernetes/kubernetes
136,54941,"/kind bug
/sig storage

## **What happened**:
**logs of controller-manager**
W1101 14:05:30.134407       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134459       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134507       7 rbd_util.go:364] failed to create rbd image, output
E1101 14:05:30.134529       7 rbd.go:323] rbd: create volume failed, err: failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:
I1101 14:05:30.134553       7 pv_controller.go:1331] failed to provision volume for claim ""default/claim5"" with StorageClass ""fast"": failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:

## What you expected to happen:
rbd executed successfully to create a volume on the ceph pool 'kube'

## How to reproduce it (as minimally and precisely as possible):
**ceph config**
ceph admin key = AQAMZ/hZheYqAxAAURdTDYs4GPc378Vp0PLeqA==
ceph kube key = AQDcffhZ1i3HMxAA5oB+SuIbzSKc+9FDc5tKeQ==
ceph --cluster ceph auth get-or-create client.kube mon 'allow r' osd 'allow rwx pool=kube'

ceph-secrets for users (admin, kube) with base64 encoded key
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: QVFBTVovaFpoZVlxQXhBQVVSZFREWXM0R1BjMzc4VnAwUExlcUE9PQ==
---
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-kube
data:
  key: QVFEY2ZmaFoxaTNITXhBQTVvQitTdUlielNLYys5RkRjNXRLZVE9PQ==

**Storageclass 'fast'**
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 1:6789,2:6789,3:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: ""kube-system""
  pool: kube
  userId: kube
  userSecretName: ceph-secret-kube

**PersistentStorageClaim**
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim5
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
  storageClassName: fast

**Executing 'rbd' inside of controller-manager works:**
$ kubectl exec -ti kube-controller-manager-st14d02ls-ztda11350501 -n kube-system rbd
   
2017-11-01 14:10:26.664691 7febc9f05780 -1 did not load config file, using default settings.
rbd: you must specify a command.
**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T07:00:21Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.5+coreos.0"", GitCommit:""070d238cd2ec359928548e486a9171b498573181"", GitTreeState:""clean"", BuildDate:""2017-08-31T21:28:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
. 3 masters / api servers

- OS (e.g. from /etc/os-release):
10x centos 7.3 nodes

- Kernel (e.g. `uname -a`):
4.9.44, custom kernel build.

- Install tools:
Kubespray

- Others:
docker 1.13
hyperkube 1.7.5
ceph version 0.94.10 (b1e0532418e4631af01acbc0cedd426f1905f4af
",AQDcffhZ1i3HMxAA5oB+SuIbzSKc+9FDc5tKeQ,https://github.com/kubernetes/kubernetes
137,54941,"/kind bug
/sig storage

## **What happened**:
**logs of controller-manager**
W1101 14:05:30.134407       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134459       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134507       7 rbd_util.go:364] failed to create rbd image, output
E1101 14:05:30.134529       7 rbd.go:323] rbd: create volume failed, err: failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:
I1101 14:05:30.134553       7 pv_controller.go:1331] failed to provision volume for claim ""default/claim5"" with StorageClass ""fast"": failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:

## What you expected to happen:
rbd executed successfully to create a volume on the ceph pool 'kube'

## How to reproduce it (as minimally and precisely as possible):
**ceph config**
ceph admin key = AQAMZ/hZheYqAxAAURdTDYs4GPc378Vp0PLeqA==
ceph kube key = AQDcffhZ1i3HMxAA5oB+SuIbzSKc+9FDc5tKeQ==
ceph --cluster ceph auth get-or-create client.kube mon 'allow r' osd 'allow rwx pool=kube'

ceph-secrets for users (admin, kube) with base64 encoded key
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: QVFBTVovaFpoZVlxQXhBQVVSZFREWXM0R1BjMzc4VnAwUExlcUE9PQ==
---
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-kube
data:
  key: QVFEY2ZmaFoxaTNITXhBQTVvQitTdUlielNLYys5RkRjNXRLZVE9PQ==

**Storageclass 'fast'**
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 1:6789,2:6789,3:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: ""kube-system""
  pool: kube
  userId: kube
  userSecretName: ceph-secret-kube

**PersistentStorageClaim**
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim5
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
  storageClassName: fast

**Executing 'rbd' inside of controller-manager works:**
$ kubectl exec -ti kube-controller-manager-st14d02ls-ztda11350501 -n kube-system rbd
   
2017-11-01 14:10:26.664691 7febc9f05780 -1 did not load config file, using default settings.
rbd: you must specify a command.
**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T07:00:21Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.5+coreos.0"", GitCommit:""070d238cd2ec359928548e486a9171b498573181"", GitTreeState:""clean"", BuildDate:""2017-08-31T21:28:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
. 3 masters / api servers

- OS (e.g. from /etc/os-release):
10x centos 7.3 nodes

- Kernel (e.g. `uname -a`):
4.9.44, custom kernel build.

- Install tools:
Kubespray

- Others:
docker 1.13
hyperkube 1.7.5
ceph version 0.94.10 (b1e0532418e4631af01acbc0cedd426f1905f4af
",PersistentStorageClaim,https://github.com/kubernetes/kubernetes
138,54941,"/kind bug
/sig storage

## **What happened**:
**logs of controller-manager**
W1101 14:05:30.134407       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134459       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134507       7 rbd_util.go:364] failed to create rbd image, output
E1101 14:05:30.134529       7 rbd.go:323] rbd: create volume failed, err: failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:
I1101 14:05:30.134553       7 pv_controller.go:1331] failed to provision volume for claim ""default/claim5"" with StorageClass ""fast"": failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:

## What you expected to happen:
rbd executed successfully to create a volume on the ceph pool 'kube'

## How to reproduce it (as minimally and precisely as possible):
**ceph config**
ceph admin key = AQAMZ/hZheYqAxAAURdTDYs4GPc378Vp0PLeqA==
ceph kube key = AQDcffhZ1i3HMxAA5oB+SuIbzSKc+9FDc5tKeQ==
ceph --cluster ceph auth get-or-create client.kube mon 'allow r' osd 'allow rwx pool=kube'

ceph-secrets for users (admin, kube) with base64 encoded key
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: QVFBTVovaFpoZVlxQXhBQVVSZFREWXM0R1BjMzc4VnAwUExlcUE9PQ==
---
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-kube
data:
  key: QVFEY2ZmaFoxaTNITXhBQTVvQitTdUlielNLYys5RkRjNXRLZVE9PQ==

**Storageclass 'fast'**
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 1:6789,2:6789,3:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: ""kube-system""
  pool: kube
  userId: kube
  userSecretName: ceph-secret-kube

**PersistentStorageClaim**
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim5
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
  storageClassName: fast

**Executing 'rbd' inside of controller-manager works:**
$ kubectl exec -ti kube-controller-manager-st14d02ls-ztda11350501 -n kube-system rbd
   
2017-11-01 14:10:26.664691 7febc9f05780 -1 did not load config file, using default settings.
rbd: you must specify a command.
**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T07:00:21Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.5+coreos.0"", GitCommit:""070d238cd2ec359928548e486a9171b498573181"", GitTreeState:""clean"", BuildDate:""2017-08-31T21:28:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
. 3 masters / api servers

- OS (e.g. from /etc/os-release):
10x centos 7.3 nodes

- Kernel (e.g. `uname -a`):
4.9.44, custom kernel build.

- Install tools:
Kubespray

- Others:
docker 1.13
hyperkube 1.7.5
ceph version 0.94.10 (b1e0532418e4631af01acbc0cedd426f1905f4af
",QVFBTVovaFpoZVlxQXhBQVVSZFREWXM0R1BjMzc4VnAwUExlcUE9PQ,https://github.com/kubernetes/kubernetes
139,54941,"/kind bug
/sig storage

## **What happened**:
**logs of controller-manager**
W1101 14:05:30.134407       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134459       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134507       7 rbd_util.go:364] failed to create rbd image, output
E1101 14:05:30.134529       7 rbd.go:323] rbd: create volume failed, err: failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:
I1101 14:05:30.134553       7 pv_controller.go:1331] failed to provision volume for claim ""default/claim5"" with StorageClass ""fast"": failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:

## What you expected to happen:
rbd executed successfully to create a volume on the ceph pool 'kube'

## How to reproduce it (as minimally and precisely as possible):
**ceph config**
ceph admin key = AQAMZ/hZheYqAxAAURdTDYs4GPc378Vp0PLeqA==
ceph kube key = AQDcffhZ1i3HMxAA5oB+SuIbzSKc+9FDc5tKeQ==
ceph --cluster ceph auth get-or-create client.kube mon 'allow r' osd 'allow rwx pool=kube'

ceph-secrets for users (admin, kube) with base64 encoded key
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: QVFBTVovaFpoZVlxQXhBQVVSZFREWXM0R1BjMzc4VnAwUExlcUE9PQ==
---
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-kube
data:
  key: QVFEY2ZmaFoxaTNITXhBQTVvQitTdUlielNLYys5RkRjNXRLZVE9PQ==

**Storageclass 'fast'**
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 1:6789,2:6789,3:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: ""kube-system""
  pool: kube
  userId: kube
  userSecretName: ceph-secret-kube

**PersistentStorageClaim**
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim5
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
  storageClassName: fast

**Executing 'rbd' inside of controller-manager works:**
$ kubectl exec -ti kube-controller-manager-st14d02ls-ztda11350501 -n kube-system rbd
   
2017-11-01 14:10:26.664691 7febc9f05780 -1 did not load config file, using default settings.
rbd: you must specify a command.
**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T07:00:21Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.5+coreos.0"", GitCommit:""070d238cd2ec359928548e486a9171b498573181"", GitTreeState:""clean"", BuildDate:""2017-08-31T21:28:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
. 3 masters / api servers

- OS (e.g. from /etc/os-release):
10x centos 7.3 nodes

- Kernel (e.g. `uname -a`):
4.9.44, custom kernel build.

- Install tools:
Kubespray

- Others:
docker 1.13
hyperkube 1.7.5
ceph version 0.94.10 (b1e0532418e4631af01acbc0cedd426f1905f4af
",ceph-secret-kube,https://github.com/kubernetes/kubernetes
140,54941,"/kind bug
/sig storage

## **What happened**:
**logs of controller-manager**
W1101 14:05:30.134407       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134459       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134507       7 rbd_util.go:364] failed to create rbd image, output
E1101 14:05:30.134529       7 rbd.go:323] rbd: create volume failed, err: failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:
I1101 14:05:30.134553       7 pv_controller.go:1331] failed to provision volume for claim ""default/claim5"" with StorageClass ""fast"": failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:

## What you expected to happen:
rbd executed successfully to create a volume on the ceph pool 'kube'

## How to reproduce it (as minimally and precisely as possible):
**ceph config**
ceph admin key = AQAMZ/hZheYqAxAAURdTDYs4GPc378Vp0PLeqA==
ceph kube key = AQDcffhZ1i3HMxAA5oB+SuIbzSKc+9FDc5tKeQ==
ceph --cluster ceph auth get-or-create client.kube mon 'allow r' osd 'allow rwx pool=kube'

ceph-secrets for users (admin, kube) with base64 encoded key
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: QVFBTVovaFpoZVlxQXhBQVVSZFREWXM0R1BjMzc4VnAwUExlcUE9PQ==
---
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-kube
data:
  key: QVFEY2ZmaFoxaTNITXhBQTVvQitTdUlielNLYys5RkRjNXRLZVE9PQ==

**Storageclass 'fast'**
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 1:6789,2:6789,3:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: ""kube-system""
  pool: kube
  userId: kube
  userSecretName: ceph-secret-kube

**PersistentStorageClaim**
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim5
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
  storageClassName: fast

**Executing 'rbd' inside of controller-manager works:**
$ kubectl exec -ti kube-controller-manager-st14d02ls-ztda11350501 -n kube-system rbd
   
2017-11-01 14:10:26.664691 7febc9f05780 -1 did not load config file, using default settings.
rbd: you must specify a command.
**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T07:00:21Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.5+coreos.0"", GitCommit:""070d238cd2ec359928548e486a9171b498573181"", GitTreeState:""clean"", BuildDate:""2017-08-31T21:28:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
. 3 masters / api servers

- OS (e.g. from /etc/os-release):
10x centos 7.3 nodes

- Kernel (e.g. `uname -a`):
4.9.44, custom kernel build.

- Install tools:
Kubespray

- Others:
docker 1.13
hyperkube 1.7.5
ceph version 0.94.10 (b1e0532418e4631af01acbc0cedd426f1905f4af
",QVFEY2ZmaFoxaTNITXhBQTVvQitTdUlielNLYys5RkRjNXRLZV,https://github.com/kubernetes/kubernetes
141,54941,"/kind bug
/sig storage

## **What happened**:
**logs of controller-manager**
W1101 14:05:30.134407       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134459       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134507       7 rbd_util.go:364] failed to create rbd image, output
E1101 14:05:30.134529       7 rbd.go:323] rbd: create volume failed, err: failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:
I1101 14:05:30.134553       7 pv_controller.go:1331] failed to provision volume for claim ""default/claim5"" with StorageClass ""fast"": failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:

## What you expected to happen:
rbd executed successfully to create a volume on the ceph pool 'kube'

## How to reproduce it (as minimally and precisely as possible):
**ceph config**
ceph admin key = AQAMZ/hZheYqAxAAURdTDYs4GPc378Vp0PLeqA==
ceph kube key = AQDcffhZ1i3HMxAA5oB+SuIbzSKc+9FDc5tKeQ==
ceph --cluster ceph auth get-or-create client.kube mon 'allow r' osd 'allow rwx pool=kube'

ceph-secrets for users (admin, kube) with base64 encoded key
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: QVFBTVovaFpoZVlxQXhBQVVSZFREWXM0R1BjMzc4VnAwUExlcUE9PQ==
---
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-kube
data:
  key: QVFEY2ZmaFoxaTNITXhBQTVvQitTdUlielNLYys5RkRjNXRLZVE9PQ==

**Storageclass 'fast'**
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 1:6789,2:6789,3:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: ""kube-system""
  pool: kube
  userId: kube
  userSecretName: ceph-secret-kube

**PersistentStorageClaim**
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim5
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
  storageClassName: fast

**Executing 'rbd' inside of controller-manager works:**
$ kubectl exec -ti kube-controller-manager-st14d02ls-ztda11350501 -n kube-system rbd
   
2017-11-01 14:10:26.664691 7febc9f05780 -1 did not load config file, using default settings.
rbd: you must specify a command.
**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T07:00:21Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.5+coreos.0"", GitCommit:""070d238cd2ec359928548e486a9171b498573181"", GitTreeState:""clean"", BuildDate:""2017-08-31T21:28:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
. 3 masters / api servers

- OS (e.g. from /etc/os-release):
10x centos 7.3 nodes

- Kernel (e.g. `uname -a`):
4.9.44, custom kernel build.

- Install tools:
Kubespray

- Others:
docker 1.13
hyperkube 1.7.5
ceph version 0.94.10 (b1e0532418e4631af01acbc0cedd426f1905f4af
",QVFBTVovaFpoZVlxQXhBQVVSZFREWXM0R1BjMzc4VnAwUExlcU,https://github.com/kubernetes/kubernetes
142,53853,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug

**What happened**:
`kubeadm init` timed out before kubelet finished downloading and initializing.

**What you expected to happen**:
`kubeadm init` would wait for the kubelet to completely initialize (or fail) before exiting

**How to reproduce it (as minimally and precisely as possible)**:
(A prerequisite is probably a bad (or manually restricted) internet connection, I'm testing on ~8-10Mb/s)

You just need to run `kubeadm init` and wait for the timeout to elapse.
Eventually the `kubelet` service will download and set itself up, but not until after `kubeadm` has given up.

```
Oct 13 00:48:02 core-01 kubelet-wrapper[1268]: gpg key fingerprint is: BFF3 13CD AA56 0B16 A898  7B8F 72AB F5F6 799D 33BC
Oct 13 00:48:02 core-01 kubelet-wrapper[1268]:         Quay.io ACI Converter (ACI conversion signing key) <support@quay.io>
Oct 13 00:48:02 core-01 kubelet-wrapper[1268]: Trusting ""https://quay.io/aci-signing-key"" for prefix ""quay.io/coreos/hyperkube"" without fingerprint review.
Oct 13 00:48:02 core-01 kubelet-wrapper[1268]: Added key for prefix ""quay.io/coreos/hyperkube"" at ""/etc/rkt/trustedkeys/prefix.d/quay.io/coreos/hyperkube/bff313cdaa560b16a8987b8f72abf5f6799d33bc""
Oct 13 00:48:04 core-01 kubelet-wrapper[1268]: Downloading signature:  0 B/473 B
Oct 13 00:48:04 core-01 kubelet-wrapper[1268]: Downloading signature:  473 B/473 B
Oct 13 00:48:10 core-01 kubelet-wrapper[1268]: Downloading ACI:  0 B/158 MB
Oct 13 00:48:10 core-01 kubelet-wrapper[1268]: Downloading ACI:  8.95 KB/158 MB
Oct 13 00:48:12 core-01 kubelet-wrapper[1268]: Downloading ACI:  470 KB/158 MB
Oct 13 00:48:13 core-01 kubelet-wrapper[1268]: Downloading ACI:  1.06 MB/158 MB
...
Oct 13 00:52:05 core-01 kubelet-wrapper[1268]: Downloading ACI:  156 MB/158 MB
Oct 13 00:52:06 core-01 kubelet-wrapper[1268]: Downloading ACI:  157 MB/158 MB
Oct 13 00:52:07 core-01 kubelet-wrapper[1268]: Downloading ACI:  158 MB/158 MB
Oct 13 00:52:08 core-01 kubelet-wrapper[1268]: Downloading ACI:  158 MB/158 MB
Oct 13 00:52:09 core-01 kubelet-wrapper[1268]: image: signature verified:
Oct 13 00:52:09 core-01 kubelet-wrapper[1268]:   Quay.io ACI Converter (ACI conversion signing key) <support@quay.io>
Oct 13 00:52:32 core-01 kubelet-wrapper[1268]: Flag --require-kubeconfig has been deprecated, You no longer need to use --require-kubeconfig. This will be removed in a future version. Providing --kubeconfig enables API server mode, omitting --kubeconfig enables standalone mode unless --require-kubeconfig=true is also set. In the latter case, the legacy default kubeconfig path will be used until --require-kubeconfig is removed.
```

```
core@core-01 ~ $ systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled)
   Active: active (running) since Fri 2017-10-13 00:47:56 UTC; 23min ago
     Docs: http://kubernetes.io/docs/
 Main PID: 1268 (kubelet)
    Tasks: 16 (limit: 32768)
   Memory: 253.5M
      CPU: 52.096s
   CGroup: /system.slice/kubelet.service
           └─1268 /kubelet --kubeconfig=/etc/kubernetes/kubelet.conf --require-kubeconfig=true --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cn

core@core-01 ~ $ kubectl get all --all-namespaces
NAMESPACE     NAME                                 READY     STATUS             RESTARTS   AGE
kube-system   po/etcd-core-01                      1/1       Running            0          17m
kube-system   po/kube-apiserver-core-01            1/1       Running            0          17m
kube-system   po/kube-controller-manager-core-01   1/1       Running   0          18m
kube-system   po/kube-scheduler-core-01            1/1       Running            0          18m

NAMESPACE   NAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
default     svc/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   18m
```

```
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[init] Using Kubernetes version: v1.8.1
[init] Using Authorization modes: [Node RBAC]
[preflight] Running pre-flight checks
[preflight] WARNING: docker version is greater than the most recently validated version. Docker version: 17.06.2-ce. Max validated version: 17.03
[preflight] WARNING: hostname ""core-01"" could not be reached
[preflight] WARNING: hostname ""core-01"" lookup core-01 on 10.0.2.3:53: no such host
[preflight] WARNING: socat not found in system path
[kubeadm] WARNING: starting in 1.8, tokens expire after 24 hours by default (if you require a non-expiring token use --token-ttl 0)
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [core-01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.17.8.101]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Valid certificates and keys now exist in ""/etc/kubernetes/pki""
[kubeconfig] Wrote KubeConfig file to disk: ""admin.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""kubelet.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""controller-manager.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""scheduler.conf""
[controlplane] Wrote Static Pod manifest for component kube-apiserver to ""/etc/kubernetes/manifests/kube-apiserver.yaml""
[controlplane] Wrote Static Pod manifest for component kube-controller-manager to ""/etc/kubernetes/manifests/kube-controller-manager.yaml""
[controlplane] Wrote Static Pod manifest for component kube-scheduler to ""/etc/kubernetes/manifests/kube-scheduler.yaml""
[etcd] Wrote Static Pod manifest for a local etcd instance to ""/etc/kubernetes/manifests/etcd.yaml""
[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory ""/etc/kubernetes/manifests""
[init] This often takes around a minute; or longer if the control plane images have to be pulled.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz' failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.
...
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz' failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.

Unfortunately, an error has occurred:
        timed out waiting for the condition

This error is likely caused by that:
        - The kubelet is not running
        - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)
        - There is no internet connection; so the kubelet can't pull the following control plane images:
                - gcr.io/google_containers/kube-apiserver-amd64:v1.8.1
                - gcr.io/google_containers/kube-controller-manager-amd64:v1.8.1
                - gcr.io/google_containers/kube-scheduler-amd64:v1.8.1

You can troubleshoot this for example with the following commands if you're on a systemd-powered system:
        - 'systemctl status kubelet'
        - 'journalctl -xeu kubelet'
couldn't initialize a Kubernetes cluster
```

**Anything else we need to know?**:
Would it be possible to just allow the timeout to be configurable?
I noticed it was mentioned in the pull request for the timing out behaviour (#51369)

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.1"", GitCommit:""f38e43b221d08850172a9a4ea785a86a3ffa3b3a"", GitTreeState:""clean"", BuildDate:""2017-10-11T23:27:35Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd
64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.1"", GitCommit:""f38e43b221d08850172a9a4ea785a86a3ffa3b3a"", GitTreeState:""clean"", BuildDate:""2017-10-11T23:16:41Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd
64""}
```

- Cloud provider or hardware configuration**:
Locally, within a Vagrant VM (CoreOS, Vagrant version 1.9.7).
2GB RAM, 1-core of a 3GHz CPU, Australian internet connection

- OS (e.g. from /etc/os-release):
```
NAME=""Container Linux by CoreOS""
ID=coreos
VERSION=1548.0.0
VERSION_ID=1548.0.0
BUILD_ID=2017-09-27-0012
PRETTY_NAME=""Container Linux by CoreOS 1548.0.0 (Ladybug)""
ANSI_COLOR=""38;5;75""
HOME_URL=""https://coreos.com/""
BUG_REPORT_URL=""https://issues.coreos.com""
COREOS_BOARD=""amd64-usr""
```

- Kernel (e.g. `uname -a`):
```
Linux core-01 4.13.3-coreos-r1 #1 SMP Tue Sep 26 23:51:59 UTC 2017 x86_64 Intel(R) Core(TM) i7-5700HQ CPU @ 2.70GHz GenuineIntel GNU/Linux
```",kubelet-wrapper[1268,https://github.com/kubernetes/kubernetes
143,53853,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug

**What happened**:
`kubeadm init` timed out before kubelet finished downloading and initializing.

**What you expected to happen**:
`kubeadm init` would wait for the kubelet to completely initialize (or fail) before exiting

**How to reproduce it (as minimally and precisely as possible)**:
(A prerequisite is probably a bad (or manually restricted) internet connection, I'm testing on ~8-10Mb/s)

You just need to run `kubeadm init` and wait for the timeout to elapse.
Eventually the `kubelet` service will download and set itself up, but not until after `kubeadm` has given up.

```
Oct 13 00:48:02 core-01 kubelet-wrapper[1268]: gpg key fingerprint is: BFF3 13CD AA56 0B16 A898  7B8F 72AB F5F6 799D 33BC
Oct 13 00:48:02 core-01 kubelet-wrapper[1268]:         Quay.io ACI Converter (ACI conversion signing key) <support@quay.io>
Oct 13 00:48:02 core-01 kubelet-wrapper[1268]: Trusting ""https://quay.io/aci-signing-key"" for prefix ""quay.io/coreos/hyperkube"" without fingerprint review.
Oct 13 00:48:02 core-01 kubelet-wrapper[1268]: Added key for prefix ""quay.io/coreos/hyperkube"" at ""/etc/rkt/trustedkeys/prefix.d/quay.io/coreos/hyperkube/bff313cdaa560b16a8987b8f72abf5f6799d33bc""
Oct 13 00:48:04 core-01 kubelet-wrapper[1268]: Downloading signature:  0 B/473 B
Oct 13 00:48:04 core-01 kubelet-wrapper[1268]: Downloading signature:  473 B/473 B
Oct 13 00:48:10 core-01 kubelet-wrapper[1268]: Downloading ACI:  0 B/158 MB
Oct 13 00:48:10 core-01 kubelet-wrapper[1268]: Downloading ACI:  8.95 KB/158 MB
Oct 13 00:48:12 core-01 kubelet-wrapper[1268]: Downloading ACI:  470 KB/158 MB
Oct 13 00:48:13 core-01 kubelet-wrapper[1268]: Downloading ACI:  1.06 MB/158 MB
...
Oct 13 00:52:05 core-01 kubelet-wrapper[1268]: Downloading ACI:  156 MB/158 MB
Oct 13 00:52:06 core-01 kubelet-wrapper[1268]: Downloading ACI:  157 MB/158 MB
Oct 13 00:52:07 core-01 kubelet-wrapper[1268]: Downloading ACI:  158 MB/158 MB
Oct 13 00:52:08 core-01 kubelet-wrapper[1268]: Downloading ACI:  158 MB/158 MB
Oct 13 00:52:09 core-01 kubelet-wrapper[1268]: image: signature verified:
Oct 13 00:52:09 core-01 kubelet-wrapper[1268]:   Quay.io ACI Converter (ACI conversion signing key) <support@quay.io>
Oct 13 00:52:32 core-01 kubelet-wrapper[1268]: Flag --require-kubeconfig has been deprecated, You no longer need to use --require-kubeconfig. This will be removed in a future version. Providing --kubeconfig enables API server mode, omitting --kubeconfig enables standalone mode unless --require-kubeconfig=true is also set. In the latter case, the legacy default kubeconfig path will be used until --require-kubeconfig is removed.
```

```
core@core-01 ~ $ systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled)
   Active: active (running) since Fri 2017-10-13 00:47:56 UTC; 23min ago
     Docs: http://kubernetes.io/docs/
 Main PID: 1268 (kubelet)
    Tasks: 16 (limit: 32768)
   Memory: 253.5M
      CPU: 52.096s
   CGroup: /system.slice/kubelet.service
           └─1268 /kubelet --kubeconfig=/etc/kubernetes/kubelet.conf --require-kubeconfig=true --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cn

core@core-01 ~ $ kubectl get all --all-namespaces
NAMESPACE     NAME                                 READY     STATUS             RESTARTS   AGE
kube-system   po/etcd-core-01                      1/1       Running            0          17m
kube-system   po/kube-apiserver-core-01            1/1       Running            0          17m
kube-system   po/kube-controller-manager-core-01   1/1       Running   0          18m
kube-system   po/kube-scheduler-core-01            1/1       Running            0          18m

NAMESPACE   NAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
default     svc/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   18m
```

```
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[init] Using Kubernetes version: v1.8.1
[init] Using Authorization modes: [Node RBAC]
[preflight] Running pre-flight checks
[preflight] WARNING: docker version is greater than the most recently validated version. Docker version: 17.06.2-ce. Max validated version: 17.03
[preflight] WARNING: hostname ""core-01"" could not be reached
[preflight] WARNING: hostname ""core-01"" lookup core-01 on 10.0.2.3:53: no such host
[preflight] WARNING: socat not found in system path
[kubeadm] WARNING: starting in 1.8, tokens expire after 24 hours by default (if you require a non-expiring token use --token-ttl 0)
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [core-01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.17.8.101]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Valid certificates and keys now exist in ""/etc/kubernetes/pki""
[kubeconfig] Wrote KubeConfig file to disk: ""admin.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""kubelet.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""controller-manager.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""scheduler.conf""
[controlplane] Wrote Static Pod manifest for component kube-apiserver to ""/etc/kubernetes/manifests/kube-apiserver.yaml""
[controlplane] Wrote Static Pod manifest for component kube-controller-manager to ""/etc/kubernetes/manifests/kube-controller-manager.yaml""
[controlplane] Wrote Static Pod manifest for component kube-scheduler to ""/etc/kubernetes/manifests/kube-scheduler.yaml""
[etcd] Wrote Static Pod manifest for a local etcd instance to ""/etc/kubernetes/manifests/etcd.yaml""
[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory ""/etc/kubernetes/manifests""
[init] This often takes around a minute; or longer if the control plane images have to be pulled.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz' failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.
...
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz' failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.

Unfortunately, an error has occurred:
        timed out waiting for the condition

This error is likely caused by that:
        - The kubelet is not running
        - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)
        - There is no internet connection; so the kubelet can't pull the following control plane images:
                - gcr.io/google_containers/kube-apiserver-amd64:v1.8.1
                - gcr.io/google_containers/kube-controller-manager-amd64:v1.8.1
                - gcr.io/google_containers/kube-scheduler-amd64:v1.8.1

You can troubleshoot this for example with the following commands if you're on a systemd-powered system:
        - 'systemctl status kubelet'
        - 'journalctl -xeu kubelet'
couldn't initialize a Kubernetes cluster
```

**Anything else we need to know?**:
Would it be possible to just allow the timeout to be configurable?
I noticed it was mentioned in the pull request for the timing out behaviour (#51369)

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.1"", GitCommit:""f38e43b221d08850172a9a4ea785a86a3ffa3b3a"", GitTreeState:""clean"", BuildDate:""2017-10-11T23:27:35Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd
64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.1"", GitCommit:""f38e43b221d08850172a9a4ea785a86a3ffa3b3a"", GitTreeState:""clean"", BuildDate:""2017-10-11T23:16:41Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd
64""}
```

- Cloud provider or hardware configuration**:
Locally, within a Vagrant VM (CoreOS, Vagrant version 1.9.7).
2GB RAM, 1-core of a 3GHz CPU, Australian internet connection

- OS (e.g. from /etc/os-release):
```
NAME=""Container Linux by CoreOS""
ID=coreos
VERSION=1548.0.0
VERSION_ID=1548.0.0
BUILD_ID=2017-09-27-0012
PRETTY_NAME=""Container Linux by CoreOS 1548.0.0 (Ladybug)""
ANSI_COLOR=""38;5;75""
HOME_URL=""https://coreos.com/""
BUG_REPORT_URL=""https://issues.coreos.com""
COREOS_BOARD=""amd64-usr""
```

- Kernel (e.g. `uname -a`):
```
Linux core-01 4.13.3-coreos-r1 #1 SMP Tue Sep 26 23:51:59 UTC 2017 x86_64 Intel(R) Core(TM) i7-5700HQ CPU @ 2.70GHz GenuineIntel GNU/Linux
```",front-proxy-client,https://github.com/kubernetes/kubernetes
144,53373,"**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG REPORT

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:

Pod cannot be terminated because local volume with token cannot be unmounted.

**What you expected to happen**:

Pod is terminated correctly.

**How to reproduce it (as minimally and precisely as possible)**:

```
# kubectl describe pod kube-proxy-gngh8 -n kube-system
[...]
    Mounts:
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-79dz0 (ro)
[...]
```
```
# kubectl delete pod kube-proxy-gngh8 -n kube-system
```
kubelet log:
```
E1003 08:16:40.821333   11479 nestedpendingoperations.go:264] Operation for ""\""kubernetes.io/secret/df09b119-a772-11e7-82ce-5254009ef6db-default-token-79dz0\"" (\""df09b119-a772-11e7-82ce-5254009ef6db\"")"" failed. No retries permitted until 2017-10-03 08:18:42.821312669 +0000 UTC (durationBeforeRetry 2m2s). Error: UnmountVolume.TearDown failed for volume ""default-token-79dz0"" (UniqueName: ""kubernetes.io/secret/df09b119-a772-11e7-82ce-5254009ef6db-default-token-79dz0"") pod ""df09b119-a772-11e7-82ce-5254009ef6db"" (UID: ""df09b119-a772-11e7-82ce-5254009ef6db"") : remove /var/lib/kubelet/pods/df09b119-a772-11e7-82ce-5254009ef6db/volumes/kubernetes.io~secret/default-token-79dz0: device or resource busy
```
More kubelet logs:
https://gist.github.com/r0bj/2c7b4809a0a75c99f91589746ae95da9

Pod `kube-proxy-gngh8` stays forever in `Terminating` state.

**Anything else we need to know?**:
The same setup with k8s 1.7.x works fine.

kubelet config:
```
/usr/bin/kubelet \
    --register-with-taints=node-role.kubernetes.io/master="""":NoSchedule \
    --pod-manifest-path=/etc/kubernetes/manifests \
    --node-labels=k8s-role=k8s-controller,node-role.kubernetes.io/master="""" \
    --allow-privileged=true \
    --cloud-provider= \
    --cluster-dns=10.223.0.10 \
    --cluster-domain=cluster.local \
    --container-runtime=docker \
    --docker=unix:///var/run/docker.sock \
    --network-plugin=cni \
    --require-kubeconfig=true \
    --kubeconfig=/var/lib/kubelet/kubeconfig \
    --serialize-image-pulls=false \
    --tls-cert-file=/var/lib/kubernetes/kubelet-cert.pem \
    --tls-private-key-file=/var/lib/kubernetes/kubelet-key.pem \
    --eviction-soft=""memory.available<200Mi,nodefs.available<12%,nodefs.inodesFree<7%,imagefs.available<12%,imagefs.inodesFree<12%"" \
    --eviction-soft-grace-period=""memory.available=5m,nodefs.available=5m,nodefs.inodesFree=5m,imagefs.available=5m,imagefs.inodesFree=5m"" \
    --eviction-max-pod-grace-period=60 \
    --eviction-hard=""memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<10%,imagefs.inodesFree<10%"" \
    --v=2
```


**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""6e937839ac04a38cac63e6a7a306c5d035fe7b0a"", GitTreeState:""clean"", BuildDate:""2017-09-28T22:57:57Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""6e937839ac04a38cac63e6a7a306c5d035fe7b0a"", GitTreeState:""clean"", BuildDate:""2017-09-28T22:46:41Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration**:
bare matal
- OS (e.g. from /etc/os-release):
Ubuntu 14.04 LTS
- Kernel (e.g. `uname -a`):
Linux dev-k8s-controller-p1 4.4.0-53-generic #74~14.04.1-Ubuntu SMP Fri Dec 2 03:43:31 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
- Others:
calico version: v2.6.1
docker: 1.12.6
",df09b119-a772-11e7-82ce-5254009ef6db,https://github.com/kubernetes/kubernetes
145,53373,"**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG REPORT

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:

Pod cannot be terminated because local volume with token cannot be unmounted.

**What you expected to happen**:

Pod is terminated correctly.

**How to reproduce it (as minimally and precisely as possible)**:

```
# kubectl describe pod kube-proxy-gngh8 -n kube-system
[...]
    Mounts:
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-79dz0 (ro)
[...]
```
```
# kubectl delete pod kube-proxy-gngh8 -n kube-system
```
kubelet log:
```
E1003 08:16:40.821333   11479 nestedpendingoperations.go:264] Operation for ""\""kubernetes.io/secret/df09b119-a772-11e7-82ce-5254009ef6db-default-token-79dz0\"" (\""df09b119-a772-11e7-82ce-5254009ef6db\"")"" failed. No retries permitted until 2017-10-03 08:18:42.821312669 +0000 UTC (durationBeforeRetry 2m2s). Error: UnmountVolume.TearDown failed for volume ""default-token-79dz0"" (UniqueName: ""kubernetes.io/secret/df09b119-a772-11e7-82ce-5254009ef6db-default-token-79dz0"") pod ""df09b119-a772-11e7-82ce-5254009ef6db"" (UID: ""df09b119-a772-11e7-82ce-5254009ef6db"") : remove /var/lib/kubelet/pods/df09b119-a772-11e7-82ce-5254009ef6db/volumes/kubernetes.io~secret/default-token-79dz0: device or resource busy
```
More kubelet logs:
https://gist.github.com/r0bj/2c7b4809a0a75c99f91589746ae95da9

Pod `kube-proxy-gngh8` stays forever in `Terminating` state.

**Anything else we need to know?**:
The same setup with k8s 1.7.x works fine.

kubelet config:
```
/usr/bin/kubelet \
    --register-with-taints=node-role.kubernetes.io/master="""":NoSchedule \
    --pod-manifest-path=/etc/kubernetes/manifests \
    --node-labels=k8s-role=k8s-controller,node-role.kubernetes.io/master="""" \
    --allow-privileged=true \
    --cloud-provider= \
    --cluster-dns=10.223.0.10 \
    --cluster-domain=cluster.local \
    --container-runtime=docker \
    --docker=unix:///var/run/docker.sock \
    --network-plugin=cni \
    --require-kubeconfig=true \
    --kubeconfig=/var/lib/kubelet/kubeconfig \
    --serialize-image-pulls=false \
    --tls-cert-file=/var/lib/kubernetes/kubelet-cert.pem \
    --tls-private-key-file=/var/lib/kubernetes/kubelet-key.pem \
    --eviction-soft=""memory.available<200Mi,nodefs.available<12%,nodefs.inodesFree<7%,imagefs.available<12%,imagefs.inodesFree<12%"" \
    --eviction-soft-grace-period=""memory.available=5m,nodefs.available=5m,nodefs.inodesFree=5m,imagefs.available=5m,imagefs.inodesFree=5m"" \
    --eviction-max-pod-grace-period=60 \
    --eviction-hard=""memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<10%,imagefs.inodesFree<10%"" \
    --v=2
```


**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""6e937839ac04a38cac63e6a7a306c5d035fe7b0a"", GitTreeState:""clean"", BuildDate:""2017-09-28T22:57:57Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""6e937839ac04a38cac63e6a7a306c5d035fe7b0a"", GitTreeState:""clean"", BuildDate:""2017-09-28T22:46:41Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration**:
bare matal
- OS (e.g. from /etc/os-release):
Ubuntu 14.04 LTS
- Kernel (e.g. `uname -a`):
Linux dev-k8s-controller-p1 4.4.0-53-generic #74~14.04.1-Ubuntu SMP Fri Dec 2 03:43:31 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
- Others:
calico version: v2.6.1
docker: 1.12.6
",11e7-82ce-5254009ef6db,https://github.com/kubernetes/kubernetes
146,53373,"**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG REPORT

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:

Pod cannot be terminated because local volume with token cannot be unmounted.

**What you expected to happen**:

Pod is terminated correctly.

**How to reproduce it (as minimally and precisely as possible)**:

```
# kubectl describe pod kube-proxy-gngh8 -n kube-system
[...]
    Mounts:
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-79dz0 (ro)
[...]
```
```
# kubectl delete pod kube-proxy-gngh8 -n kube-system
```
kubelet log:
```
E1003 08:16:40.821333   11479 nestedpendingoperations.go:264] Operation for ""\""kubernetes.io/secret/df09b119-a772-11e7-82ce-5254009ef6db-default-token-79dz0\"" (\""df09b119-a772-11e7-82ce-5254009ef6db\"")"" failed. No retries permitted until 2017-10-03 08:18:42.821312669 +0000 UTC (durationBeforeRetry 2m2s). Error: UnmountVolume.TearDown failed for volume ""default-token-79dz0"" (UniqueName: ""kubernetes.io/secret/df09b119-a772-11e7-82ce-5254009ef6db-default-token-79dz0"") pod ""df09b119-a772-11e7-82ce-5254009ef6db"" (UID: ""df09b119-a772-11e7-82ce-5254009ef6db"") : remove /var/lib/kubelet/pods/df09b119-a772-11e7-82ce-5254009ef6db/volumes/kubernetes.io~secret/default-token-79dz0: device or resource busy
```
More kubelet logs:
https://gist.github.com/r0bj/2c7b4809a0a75c99f91589746ae95da9

Pod `kube-proxy-gngh8` stays forever in `Terminating` state.

**Anything else we need to know?**:
The same setup with k8s 1.7.x works fine.

kubelet config:
```
/usr/bin/kubelet \
    --register-with-taints=node-role.kubernetes.io/master="""":NoSchedule \
    --pod-manifest-path=/etc/kubernetes/manifests \
    --node-labels=k8s-role=k8s-controller,node-role.kubernetes.io/master="""" \
    --allow-privileged=true \
    --cloud-provider= \
    --cluster-dns=10.223.0.10 \
    --cluster-domain=cluster.local \
    --container-runtime=docker \
    --docker=unix:///var/run/docker.sock \
    --network-plugin=cni \
    --require-kubeconfig=true \
    --kubeconfig=/var/lib/kubelet/kubeconfig \
    --serialize-image-pulls=false \
    --tls-cert-file=/var/lib/kubernetes/kubelet-cert.pem \
    --tls-private-key-file=/var/lib/kubernetes/kubelet-key.pem \
    --eviction-soft=""memory.available<200Mi,nodefs.available<12%,nodefs.inodesFree<7%,imagefs.available<12%,imagefs.inodesFree<12%"" \
    --eviction-soft-grace-period=""memory.available=5m,nodefs.available=5m,nodefs.inodesFree=5m,imagefs.available=5m,imagefs.inodesFree=5m"" \
    --eviction-max-pod-grace-period=60 \
    --eviction-hard=""memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<10%,imagefs.inodesFree<10%"" \
    --v=2
```


**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""6e937839ac04a38cac63e6a7a306c5d035fe7b0a"", GitTreeState:""clean"", BuildDate:""2017-09-28T22:57:57Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""6e937839ac04a38cac63e6a7a306c5d035fe7b0a"", GitTreeState:""clean"", BuildDate:""2017-09-28T22:46:41Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration**:
bare matal
- OS (e.g. from /etc/os-release):
Ubuntu 14.04 LTS
- Kernel (e.g. `uname -a`):
Linux dev-k8s-controller-p1 4.4.0-53-generic #74~14.04.1-Ubuntu SMP Fri Dec 2 03:43:31 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
- Others:
calico version: v2.6.1
docker: 1.12.6
","12%,<7%,<12%,<12",https://github.com/kubernetes/kubernetes
147,53373,"**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG REPORT

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:

Pod cannot be terminated because local volume with token cannot be unmounted.

**What you expected to happen**:

Pod is terminated correctly.

**How to reproduce it (as minimally and precisely as possible)**:

```
# kubectl describe pod kube-proxy-gngh8 -n kube-system
[...]
    Mounts:
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-79dz0 (ro)
[...]
```
```
# kubectl delete pod kube-proxy-gngh8 -n kube-system
```
kubelet log:
```
E1003 08:16:40.821333   11479 nestedpendingoperations.go:264] Operation for ""\""kubernetes.io/secret/df09b119-a772-11e7-82ce-5254009ef6db-default-token-79dz0\"" (\""df09b119-a772-11e7-82ce-5254009ef6db\"")"" failed. No retries permitted until 2017-10-03 08:18:42.821312669 +0000 UTC (durationBeforeRetry 2m2s). Error: UnmountVolume.TearDown failed for volume ""default-token-79dz0"" (UniqueName: ""kubernetes.io/secret/df09b119-a772-11e7-82ce-5254009ef6db-default-token-79dz0"") pod ""df09b119-a772-11e7-82ce-5254009ef6db"" (UID: ""df09b119-a772-11e7-82ce-5254009ef6db"") : remove /var/lib/kubelet/pods/df09b119-a772-11e7-82ce-5254009ef6db/volumes/kubernetes.io~secret/default-token-79dz0: device or resource busy
```
More kubelet logs:
https://gist.github.com/r0bj/2c7b4809a0a75c99f91589746ae95da9

Pod `kube-proxy-gngh8` stays forever in `Terminating` state.

**Anything else we need to know?**:
The same setup with k8s 1.7.x works fine.

kubelet config:
```
/usr/bin/kubelet \
    --register-with-taints=node-role.kubernetes.io/master="""":NoSchedule \
    --pod-manifest-path=/etc/kubernetes/manifests \
    --node-labels=k8s-role=k8s-controller,node-role.kubernetes.io/master="""" \
    --allow-privileged=true \
    --cloud-provider= \
    --cluster-dns=10.223.0.10 \
    --cluster-domain=cluster.local \
    --container-runtime=docker \
    --docker=unix:///var/run/docker.sock \
    --network-plugin=cni \
    --require-kubeconfig=true \
    --kubeconfig=/var/lib/kubelet/kubeconfig \
    --serialize-image-pulls=false \
    --tls-cert-file=/var/lib/kubernetes/kubelet-cert.pem \
    --tls-private-key-file=/var/lib/kubernetes/kubelet-key.pem \
    --eviction-soft=""memory.available<200Mi,nodefs.available<12%,nodefs.inodesFree<7%,imagefs.available<12%,imagefs.inodesFree<12%"" \
    --eviction-soft-grace-period=""memory.available=5m,nodefs.available=5m,nodefs.inodesFree=5m,imagefs.available=5m,imagefs.inodesFree=5m"" \
    --eviction-max-pod-grace-period=60 \
    --eviction-hard=""memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<10%,imagefs.inodesFree<10%"" \
    --v=2
```


**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""6e937839ac04a38cac63e6a7a306c5d035fe7b0a"", GitTreeState:""clean"", BuildDate:""2017-09-28T22:57:57Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""6e937839ac04a38cac63e6a7a306c5d035fe7b0a"", GitTreeState:""clean"", BuildDate:""2017-09-28T22:46:41Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration**:
bare matal
- OS (e.g. from /etc/os-release):
Ubuntu 14.04 LTS
- Kernel (e.g. `uname -a`):
Linux dev-k8s-controller-p1 4.4.0-53-generic #74~14.04.1-Ubuntu SMP Fri Dec 2 03:43:31 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
- Others:
calico version: v2.6.1
docker: 1.12.6
",kube-proxy-gngh8,https://github.com/kubernetes/kubernetes
148,52022,"
**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug

**What happened**:

Apiserver proxy removes leading slash from path that is send to destination service or pod, effectively breaking applications.

Example tshark output:

Hypertext Transfer Protocol
    GET test HTTP/1.1\r\n
        [Expert Info (Chat/Sequence): GET test HTTP/1.1\r\n]
            [GET test HTTP/1.1\r\n]
            [Severity level: Chat]
            [Group: Sequence]
        Request Method: GET
        Request URI: test
        Request Version: HTTP/1.1
    Host: localhost:8080\r\n
    User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.101 Safari/537.36\r\n
    Accept-Encoding: gzip, deflate, br\r\n
    Accept-Language: en-US,en;q=0.8,uk;q=0.6,ru;q=0.4\r\n
    Cache-Control: no-cache\r\n
    Connection: Upgrade\r\n
    Origin: http://localhost:8080\r\n
    Pragma: no-cache\r\n
    Sec-Websocket-Extensions: permessage-deflate; client_max_window_bits\r\n
    Sec-Websocket-Key: rtBOKvTHtRAbTpz9vhyAng==\r\n
    Sec-Websocket-Version: 13\r\n
    Upgrade: websocket\r\n
    X-Forwarded-For: 127.0.0.1\r\n
    \r\n
    [Full request URI: **http://localhost:8080test**]
    [HTTP request 1/1]


**What you expected to happen**:

Proxied URLs should begin with slash.
Example:
`GET /test HTTP/1.1\r\n`


**How to reproduce it (as minimally and precisely as possible)**:

Use this guideline to construct proxy URL: https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#discovering-builtin-services

This request will not work (404 not found):
`curl http://localhost:8080/api/v1/namespaces/default/services/testservice:http/proxy/test`
backend result (wrong): GET test HTTP/1.1\r\n

Request with double slash will work:
`curl http://localhost:8080/api/v1/namespaces/default/services/testservice:http/proxy//test`
backend result (correct): GET /test HTTP/1.1\r\n

Also old-style proxy URL format works too:
`curl http://localhost:8080/api/v1/proxy/namespaces/default/services/testservice:http/test`
backend result (correct): GET /test HTTP/1.1\r\n

**Anything else we need to know?**:
First observed with 1.7 kubernetes release

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""8+"", GitVersion:""v1.8.0-alpha.3"", GitCommit:""6a4203eb4b5f59ac7a602e0c83023d15d991fd58"", GitTreeState:""clean"", BuildDate:""2017-08-23T22:58:52Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7+"", GitVersion:""v1.7.5-dirty"", GitCommit:""17d7182a7ccbb167074be7a87f0a68bd00d58d97"", GitTreeState:""dirty"", BuildDate:""2017-09-04T14:44:32Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration**:
Bare metal
- OS (e.g. from /etc/os-release):
CoreOS 1409.6.0
- Kernel (e.g. `uname -a`):
4.11.9-coreos
- Install tools:
- Others:
",Websocket-Version,https://github.com/kubernetes/kubernetes
149,50888,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug


**What happened**:
I was successfully able to `kubefed init` a federation control plane on a GKE k8s cluster, but upon attempting to join the cluster the following error occurs:

```
...
I0816 10:46:38.538848    5942 join.go:538] Creating service account in joining cluster
I0816 10:46:38.539061    5942 request.go:991] Request Body: {""kind"":""ServiceAccount"",""apiVersion"":""v1"",""metadata"":{""name"":""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"",""namespace"":""federation-system"",""creationTimestamp"":null,""annotations"":{""federation.alpha.kubernetes.io/cluster-name"":""gce-us-west1"",""federation.alpha.kubernetes.io/federation-name"":""federation""}}}
I0816 10:46:38.539134    5942 round_trippers.go:386] curl -k -v -XPOST  -H ""Accept: application/json, */*"" -H ""Content-Type: application/json"" -H ""User-Agent: kubefed/v1.7.3 (linux/amd64) kubernetes/2c2fe6e"" -H ""Authorization: Basic ABCDEFGHIJKLMNOPQRSTUVWXYZabcd=="" https://35.197.74.218/api/v1/namespaces/federation-system/serviceaccounts
I0816 10:46:38.576175    5942 round_trippers.go:405] POST https://35.197.74.218/api/v1/namespaces/federation-system/serviceaccounts 422 Unprocessable Entity in 37 milliseconds
I0816 10:46:38.576211    5942 round_trippers.go:411] Response Headers:
I0816 10:46:38.576225    5942 round_trippers.go:414]     Date: Wed, 16 Aug 2017 17:46:38 GMT
I0816 10:46:38.576236    5942 round_trippers.go:414]     Content-Type: application/json
I0816 10:46:38.576245    5942 round_trippers.go:414]     Content-Length: 1038
I0816 10:46:38.576515    5942 request.go:991] Response Body: {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""ServiceAccount \""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1\"" is invalid: metadata.name: Invalid value: \""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1\"": a DNS-1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')"",""reason"":""Invalid"",""details"":{""name"":""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"",""kind"":""ServiceAccount"",""causes"":[{""reason"":""FieldValueInvalid"",""message"":""Invalid value: \""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1\"": a DNS-1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')"",""field"":""metadata.name""}]},""code"":422}
I0816 10:46:38.576711    5942 join.go:541] Error creating service account in joining cluster: ServiceAccount ""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"" is invalid: metadata.name: Invalid value: ""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"": a DNS-1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')
I0816 10:46:38.576736    5942 join.go:234] Could not create cluster credentials secret: ServiceAccount ""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"" is invalid: metadata.name: Invalid value: ""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"": a DNS-1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')
```
This appears to be caused by the context name `gke_<project>_us-west1-b_gce-us-west1` which is using the default context name for a GKE cluster that is automatically imported to the kubeconfig file by `gcloud`.

If I update the kubeconfig context to get rid of the `_` to something like `gce-us-west1`, I can get past the above error but then subsequently hit the following error:

```
I0817 18:12:26.855195   14103 join.go:538] Creating service account in joining cluster
I0817 18:12:26.855329   14103 request.go:991] Request Body: {""kind"":""ServiceAccount"",""apiVersion"":""v1"",""metadata"":{""name"":""gce-us-west1-gce-us-west1"",""namespace"":""federation-system"",""creationTimestamp"":null,""annotations"":{""federation.alpha.kubernetes.io/cluster-name"":""gce-us-west1"",""federation.alpha.kubernetes.io/federation-name"":""federation""}}}
I0817 18:12:26.855376   14103 round_trippers.go:386] curl -k -v -XPOST  -H ""Accept: application/json, */*"" -H ""Content-Type: application/json"" -H ""User-Agent: kubefed/v1.7.3 (linux/amd64) kubernetes/2c2fe6e"" -H ""Authorization: Basic ABCDEFGHIJKLMNOPQRSTUVWXYZabcd=="" https://35.197.74.218/api/v1/namespaces/federation-system/serviceaccounts
I0817 18:12:26.885490   14103 round_trippers.go:405] POST https://35.197.74.218/api/v1/namespaces/federation-system/serviceaccounts 201 Created in 30 milliseconds
I0817 18:12:26.885536   14103 round_trippers.go:411] Response Headers:
I0817 18:12:26.885550   14103 round_trippers.go:414]     Content-Length: 470
I0817 18:12:26.885560   14103 round_trippers.go:414]     Date: Fri, 18 Aug 2017 01:12:26 GMT
I0817 18:12:26.885570   14103 round_trippers.go:414]     Content-Type: application/json
I0817 18:12:26.886617   14103 request.go:991] Response Body: {""kind"":""ServiceAccount"",""apiVersion"":""v1"",""metadata"":{""name"":""gce-us-west1-gce-us-west1"",""namespace"":""federation-system"",""selfLink"":""/api/v1/namespaces/federation-system/serviceaccounts/gce-us-west1-gce-us-west1"",""uid"":""4c7d5fb9-83b2-11e7-bda1-42010a8a01d8"",""resourceVersion"":""220285"",""creationTimestamp"":""2017-08-18T01:12:26Z"",""annotations"":{""federation.alpha.kubernetes.io/cluster-name"":""gce-us-west1"",""federation.alpha.kubernetes.io/federation-name"":""federation""}}}
I0817 18:12:26.886800   14103 join.go:544] Created service account in joining cluster
I0817 18:12:26.886817   14103 join.go:546] Creating role binding for service account in joining cluster
I0817 18:12:26.887024   14103 request.go:991] Request Body: {""kind"":""ClusterRole"",""apiVersion"":""rbac.authorization.k8s.io/v1beta1"",""metadata"":{""name"":""federation-controller-manager:federation-gce-us-west1-gce-us-west1"",""namespace"":""federation-system"",""creationTimestamp"":null,""annotations"":{""federation.alpha.kubernetes.io/cluster-name"":""gce-us-west1"",""federation.alpha.kubernetes.io/federation-name"":""federation""}},""rules"":[{""verbs"":[""*""],""apiGroups"":[""*""],""resources"":[""*""]},{""verbs"":[""get""],""nonResourceURLs"":[""/healthz""]}]}
I0817 18:12:26.887093   14103 round_trippers.go:386] curl -k -v -XPOST  -H ""Accept: application/json, */*"" -H ""Content-Type: application/json"" -H ""User-Agent: kubefed/v1.7.3 (linux/amd64) kubernetes/2c2fe6e"" -H ""Authorization: Basic ABCDEFGHIJKLMNOPQRSTUVWXYZabcd=="" https://35.197.74.218/apis/rbac.authorization.k8s.io/v1beta1/clusterroles
I0817 18:12:26.917114   14103 round_trippers.go:405] POST https://35.197.74.218/apis/rbac.authorization.k8s.io/v1beta1/clusterroles 409 Conflict in 29 milliseconds
I0817 18:12:26.917149   14103 round_trippers.go:411] Response Headers:
I0817 18:12:26.917161   14103 round_trippers.go:414]     Content-Length: 388
I0817 18:12:26.917167   14103 round_trippers.go:414]     Date: Fri, 18 Aug 2017 01:12:26 GMT
I0817 18:12:26.917173   14103 round_trippers.go:414]     Content-Type: application/json
I0817 18:12:26.917214   14103 request.go:991] Response Body: {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""clusterroles.rbac.authorization.k8s.io \""federation-controller-manager:federation-gce-us-west1-gce-us-west1\"" already exists"",""reason"":""AlreadyExists"",""details"":{""name"":""federation-controller-manager:federation-gce-us-west1-gce-us-west1"",""group"":""rbac.authorization.k8s.io"",""kind"":""clusterroles""},""code"":409}
I0817 18:12:26.917356   14103 join.go:630] Could not create role for service account in joining cluster: clusterroles.rbac.authorization.k8s.io ""federation-controller-manager:federation-gce-us-west1-gce-us-west1"" already exists
I0817 18:12:26.917375   14103 join.go:549] Error creating role binding for service account in joining cluster: clusterroles.rbac.authorization.k8s.io ""federation-controller-manager:federation-gce-us-west1-gce-us-west1"" already exists
I0817 18:12:26.917390   14103 join.go:234] Could not create cluster credentials secret: clusterroles.rbac.authorization.k8s.io ""federation-controller-manager:federation-gce-us-west1-gce-us-west1"" already exists
I0817 18:12:26.917446   14103 helpers.go:207] server response object: [{
  ""metadata"": {},
  ""status"": ""Failure"",
  ""message"": ""clusterroles.rbac.authorization.k8s.io \""federation-controller-manager:federation-gce-us-west1-gce-us-west1\"" already exists"",
  ""reason"": ""AlreadyExists"",
  ""details"": {
    ""name"": ""federation-controller-manager:federation-gce-us-west1-gce-us-west1"",
    ""group"": ""rbac.authorization.k8s.io"",
    ""kind"": ""clusterroles""
  },
  ""code"": 409
}]
F0817 18:12:26.917481   14103 helpers.go:120] Error from server (AlreadyExists): clusterroles.rbac.authorization.k8s.io ""federation-controller-manager:federation-gce-us-west1-gce-us-west1"" already exists
```

**What you expected to happen**:
Properly be able to `kubefed join` the cluster into the federation.

**How to reproduce it (as minimally and precisely as possible)**:
```
1. gcloud container clusters create --cluster-version=1.7.3 <cluster_name> --zone=<zone> --scopes ""cloud-platform,storage-ro,logging-write,monitoring-write,service-control,service-management,https://www.googleapis.com/auth/ndev.clouddns.readwrite""
2. kubefed init federation --host-cluster-context=<cluster_context> --dns-provider='google-clouddns' --dns-zone-name=<dns_zone_name> --controllermanager-arg-overrides=""--v=10"" --apiserver-arg-overrides=""--v=10"" --v=10
3. kubefed join <cluster_name> --host-cluster-context=<cluster_context> --cluster-context=<cluster_context> --v=10
```

**Environment**:
- Kubernetes version (use `kubectl version`):
```
→ kubefed version
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T07:00:21Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T06:43:48Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration**: GKE

cc: @kubernetes/sig-federation-bugs",west1-gke__us-west1-b_gce-us-west1,https://github.com/kubernetes/kubernetes
150,50888,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug


**What happened**:
I was successfully able to `kubefed init` a federation control plane on a GKE k8s cluster, but upon attempting to join the cluster the following error occurs:

```
...
I0816 10:46:38.538848    5942 join.go:538] Creating service account in joining cluster
I0816 10:46:38.539061    5942 request.go:991] Request Body: {""kind"":""ServiceAccount"",""apiVersion"":""v1"",""metadata"":{""name"":""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"",""namespace"":""federation-system"",""creationTimestamp"":null,""annotations"":{""federation.alpha.kubernetes.io/cluster-name"":""gce-us-west1"",""federation.alpha.kubernetes.io/federation-name"":""federation""}}}
I0816 10:46:38.539134    5942 round_trippers.go:386] curl -k -v -XPOST  -H ""Accept: application/json, */*"" -H ""Content-Type: application/json"" -H ""User-Agent: kubefed/v1.7.3 (linux/amd64) kubernetes/2c2fe6e"" -H ""Authorization: Basic ABCDEFGHIJKLMNOPQRSTUVWXYZabcd=="" https://35.197.74.218/api/v1/namespaces/federation-system/serviceaccounts
I0816 10:46:38.576175    5942 round_trippers.go:405] POST https://35.197.74.218/api/v1/namespaces/federation-system/serviceaccounts 422 Unprocessable Entity in 37 milliseconds
I0816 10:46:38.576211    5942 round_trippers.go:411] Response Headers:
I0816 10:46:38.576225    5942 round_trippers.go:414]     Date: Wed, 16 Aug 2017 17:46:38 GMT
I0816 10:46:38.576236    5942 round_trippers.go:414]     Content-Type: application/json
I0816 10:46:38.576245    5942 round_trippers.go:414]     Content-Length: 1038
I0816 10:46:38.576515    5942 request.go:991] Response Body: {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""ServiceAccount \""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1\"" is invalid: metadata.name: Invalid value: \""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1\"": a DNS-1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')"",""reason"":""Invalid"",""details"":{""name"":""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"",""kind"":""ServiceAccount"",""causes"":[{""reason"":""FieldValueInvalid"",""message"":""Invalid value: \""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1\"": a DNS-1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')"",""field"":""metadata.name""}]},""code"":422}
I0816 10:46:38.576711    5942 join.go:541] Error creating service account in joining cluster: ServiceAccount ""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"" is invalid: metadata.name: Invalid value: ""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"": a DNS-1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')
I0816 10:46:38.576736    5942 join.go:234] Could not create cluster credentials secret: ServiceAccount ""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"" is invalid: metadata.name: Invalid value: ""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"": a DNS-1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')
```
This appears to be caused by the context name `gke_<project>_us-west1-b_gce-us-west1` which is using the default context name for a GKE cluster that is automatically imported to the kubeconfig file by `gcloud`.

If I update the kubeconfig context to get rid of the `_` to something like `gce-us-west1`, I can get past the above error but then subsequently hit the following error:

```
I0817 18:12:26.855195   14103 join.go:538] Creating service account in joining cluster
I0817 18:12:26.855329   14103 request.go:991] Request Body: {""kind"":""ServiceAccount"",""apiVersion"":""v1"",""metadata"":{""name"":""gce-us-west1-gce-us-west1"",""namespace"":""federation-system"",""creationTimestamp"":null,""annotations"":{""federation.alpha.kubernetes.io/cluster-name"":""gce-us-west1"",""federation.alpha.kubernetes.io/federation-name"":""federation""}}}
I0817 18:12:26.855376   14103 round_trippers.go:386] curl -k -v -XPOST  -H ""Accept: application/json, */*"" -H ""Content-Type: application/json"" -H ""User-Agent: kubefed/v1.7.3 (linux/amd64) kubernetes/2c2fe6e"" -H ""Authorization: Basic ABCDEFGHIJKLMNOPQRSTUVWXYZabcd=="" https://35.197.74.218/api/v1/namespaces/federation-system/serviceaccounts
I0817 18:12:26.885490   14103 round_trippers.go:405] POST https://35.197.74.218/api/v1/namespaces/federation-system/serviceaccounts 201 Created in 30 milliseconds
I0817 18:12:26.885536   14103 round_trippers.go:411] Response Headers:
I0817 18:12:26.885550   14103 round_trippers.go:414]     Content-Length: 470
I0817 18:12:26.885560   14103 round_trippers.go:414]     Date: Fri, 18 Aug 2017 01:12:26 GMT
I0817 18:12:26.885570   14103 round_trippers.go:414]     Content-Type: application/json
I0817 18:12:26.886617   14103 request.go:991] Response Body: {""kind"":""ServiceAccount"",""apiVersion"":""v1"",""metadata"":{""name"":""gce-us-west1-gce-us-west1"",""namespace"":""federation-system"",""selfLink"":""/api/v1/namespaces/federation-system/serviceaccounts/gce-us-west1-gce-us-west1"",""uid"":""4c7d5fb9-83b2-11e7-bda1-42010a8a01d8"",""resourceVersion"":""220285"",""creationTimestamp"":""2017-08-18T01:12:26Z"",""annotations"":{""federation.alpha.kubernetes.io/cluster-name"":""gce-us-west1"",""federation.alpha.kubernetes.io/federation-name"":""federation""}}}
I0817 18:12:26.886800   14103 join.go:544] Created service account in joining cluster
I0817 18:12:26.886817   14103 join.go:546] Creating role binding for service account in joining cluster
I0817 18:12:26.887024   14103 request.go:991] Request Body: {""kind"":""ClusterRole"",""apiVersion"":""rbac.authorization.k8s.io/v1beta1"",""metadata"":{""name"":""federation-controller-manager:federation-gce-us-west1-gce-us-west1"",""namespace"":""federation-system"",""creationTimestamp"":null,""annotations"":{""federation.alpha.kubernetes.io/cluster-name"":""gce-us-west1"",""federation.alpha.kubernetes.io/federation-name"":""federation""}},""rules"":[{""verbs"":[""*""],""apiGroups"":[""*""],""resources"":[""*""]},{""verbs"":[""get""],""nonResourceURLs"":[""/healthz""]}]}
I0817 18:12:26.887093   14103 round_trippers.go:386] curl -k -v -XPOST  -H ""Accept: application/json, */*"" -H ""Content-Type: application/json"" -H ""User-Agent: kubefed/v1.7.3 (linux/amd64) kubernetes/2c2fe6e"" -H ""Authorization: Basic ABCDEFGHIJKLMNOPQRSTUVWXYZabcd=="" https://35.197.74.218/apis/rbac.authorization.k8s.io/v1beta1/clusterroles
I0817 18:12:26.917114   14103 round_trippers.go:405] POST https://35.197.74.218/apis/rbac.authorization.k8s.io/v1beta1/clusterroles 409 Conflict in 29 milliseconds
I0817 18:12:26.917149   14103 round_trippers.go:411] Response Headers:
I0817 18:12:26.917161   14103 round_trippers.go:414]     Content-Length: 388
I0817 18:12:26.917167   14103 round_trippers.go:414]     Date: Fri, 18 Aug 2017 01:12:26 GMT
I0817 18:12:26.917173   14103 round_trippers.go:414]     Content-Type: application/json
I0817 18:12:26.917214   14103 request.go:991] Response Body: {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""clusterroles.rbac.authorization.k8s.io \""federation-controller-manager:federation-gce-us-west1-gce-us-west1\"" already exists"",""reason"":""AlreadyExists"",""details"":{""name"":""federation-controller-manager:federation-gce-us-west1-gce-us-west1"",""group"":""rbac.authorization.k8s.io"",""kind"":""clusterroles""},""code"":409}
I0817 18:12:26.917356   14103 join.go:630] Could not create role for service account in joining cluster: clusterroles.rbac.authorization.k8s.io ""federation-controller-manager:federation-gce-us-west1-gce-us-west1"" already exists
I0817 18:12:26.917375   14103 join.go:549] Error creating role binding for service account in joining cluster: clusterroles.rbac.authorization.k8s.io ""federation-controller-manager:federation-gce-us-west1-gce-us-west1"" already exists
I0817 18:12:26.917390   14103 join.go:234] Could not create cluster credentials secret: clusterroles.rbac.authorization.k8s.io ""federation-controller-manager:federation-gce-us-west1-gce-us-west1"" already exists
I0817 18:12:26.917446   14103 helpers.go:207] server response object: [{
  ""metadata"": {},
  ""status"": ""Failure"",
  ""message"": ""clusterroles.rbac.authorization.k8s.io \""federation-controller-manager:federation-gce-us-west1-gce-us-west1\"" already exists"",
  ""reason"": ""AlreadyExists"",
  ""details"": {
    ""name"": ""federation-controller-manager:federation-gce-us-west1-gce-us-west1"",
    ""group"": ""rbac.authorization.k8s.io"",
    ""kind"": ""clusterroles""
  },
  ""code"": 409
}]
F0817 18:12:26.917481   14103 helpers.go:120] Error from server (AlreadyExists): clusterroles.rbac.authorization.k8s.io ""federation-controller-manager:federation-gce-us-west1-gce-us-west1"" already exists
```

**What you expected to happen**:
Properly be able to `kubefed join` the cluster into the federation.

**How to reproduce it (as minimally and precisely as possible)**:
```
1. gcloud container clusters create --cluster-version=1.7.3 <cluster_name> --zone=<zone> --scopes ""cloud-platform,storage-ro,logging-write,monitoring-write,service-control,service-management,https://www.googleapis.com/auth/ndev.clouddns.readwrite""
2. kubefed init federation --host-cluster-context=<cluster_context> --dns-provider='google-clouddns' --dns-zone-name=<dns_zone_name> --controllermanager-arg-overrides=""--v=10"" --apiserver-arg-overrides=""--v=10"" --v=10
3. kubefed join <cluster_name> --host-cluster-context=<cluster_context> --cluster-context=<cluster_context> --v=10
```

**Environment**:
- Kubernetes version (use `kubectl version`):
```
→ kubefed version
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T07:00:21Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T06:43:48Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration**: GKE

cc: @kubernetes/sig-federation-bugs",manager:federation-gce-us-west1-gce-us-west1,https://github.com/kubernetes/kubernetes
151,50857,"/kind bug
/sig api-machinery

kubernetes is sending a broken response when watching logs over HTTP 1.1. It sends a chunked response without sending the the size of the chunk first.

[Relevant RFC for reference.](https://tools.ietf.org/html/rfc7230#section-4.1)

Example:
```
curl --http1.1 -kv 'https://192.168.99.100:8443/api/v1/namespaces/default/pods/nginx-1423793266-mq53r/log?follow=true' -H ""Authorization: Bearer $TOKEN""
*   Trying 192.168.99.100...
* TCP_NODELAY set
* Connected to 192.168.99.100 (192.168.99.100) port 8443 (#0)
* Initializing NSS with certpath: sql:/etc/pki/nssdb
* skipping SSL peer certificate verification
* NSS: client certificate not found (nickname not specified)
* ALPN, server accepted to use http/1.1
* SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
* Server certificate:
* 	subject: CN=minikube,O=system:masters
* 	start date: Aug 17 17:04:24 2017 GMT
* 	expire date: Aug 17 17:04:24 2018 GMT
* 	common name: minikube
* 	issuer: CN=minikubeCA
> GET /api/v1/namespaces/default/pods/nginx-1423793266-mq53r/log?follow=true HTTP/1.1
> Host: 192.168.99.100:8443
> User-Agent: curl/7.53.1
> Accept: */*
> Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tZjl2bHAiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjI1MDRlOWFkLTgzNmUtMTFlNy05MWIwLTA4MDAyN2MzY2I1NyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.QFajWXR2ZFg72giS_kuudL0Lz5yPGZ_sQdaYRrwAyOjfVwcPWqjnFOwyqCesKrqMsdQkAWLNYiCb1h4SxpBW9HPjymAisqcO3Dq8azL9zMpFHwbZNtboUrSz5XuWtun3zOgbG3JhK-KZTuD2yP241Aaf0QAFZ80k-5ZT4chELQ-gQzCJco_8kv1trhCmveKk0aozMApKYQOJPpN0Z-cPCHEOxUsEMk2mtJEd2IvaFl6QerHKA-guUPuYMkbWAM_GMZh_D5IGcsAj9x9gJb7d5EvbMx4qYW_hW0SGbDIcwxdjAY4_Nv0xrOmYWIh789RcmM2ikCH5gOpesF8hhdUWjg
> 
< HTTP/1.1 200 OK
< Content-Type: text/plain
< Date: Thu, 17 Aug 2017 17:45:03 GMT
< Transfer-Encoding: chunked
< 
172.17.0.1 - - [17/Aug/2017:17:43:54 +0000] ""GET / HTTP/1.1"" 200 612 ""-"" ""curl/7.53.1"" ""-""
```

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.0+695f48a16f"", GitCommit:""d2e5420"", GitTreeState:""clean"", BuildDate:""2017-08-10T18:57:36Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.0"", GitCommit:""d3ada0119e776222f11ec7945e6d860061339aad"", GitTreeState:""clean"", BuildDate:""2017-07-26T00:12:31Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```",KZTuD2yP241Aaf0QAFZ80k-5ZT4chELQ-gQzCJco_8kv1trhCmveKk0aozMApKYQOJPpN0Z-cPCHEOxUsEMk2mtJEd2IvaFl6QerHKA-guUPuYMkbWAM_GMZh_D5IGcsAj9x9gJb7d5EvbMx4qYW_hW0SGbDIcwxdjAY4_Nv0xrOmYWIh789RcmM2ikCH5gOpesF8hhdUWjg,https://github.com/kubernetes/kubernetes
152,50432,"**Is this a BUG REPORT or FEATURE REQUEST?**: BUG REPORT


**What happened**:
I have configured oidc with kubernetes. But when i try `kubectl --user=spnzip@gmail.com get nodes` i get `error from server (Forbidden): User ""system:anonymous"" cannot list nodes at the cluster scope. (get nodes)
`

**What you expected to happen**:
The command should authenticate user and display the nodes 

**How to reproduce it (as minimally and precisely as possible)**:
1. Install single node kubernetes cluster with kubeadm
2. Obtain client secrets with the k8s-oidc-helper tool
k8s-oidc-helper -c <path where user's client id and secret is stored)
3. copy paste the code generated to ~/.kube/config file
```
- name: spnzig@gmail.com
  user:
    auth-provider:
      config:
        client-id: xxxxxxxxxx-xxxxxxx.apps.googleusercontent.com
        client-secret: xxxxxxxxxxxxxxxxxxxxxxx
        id-token: eyJhbGciOiJSUzI1NiIsImtpZCI6IjczMTdkOTM1MWQ1Y .... njZaIWlMaAFEfIrGPy-49TQ
        idp-issuer-url: https://accounts.google.com
        refresh-token: 1/3XPbQPD...g8PuNPs
      name: oidc
```

4. Set the context for the new user in ~/.kube/config file
```
- context:
    cluster: kubernetes
    user: spnzig@gmail.com
  name: spnzig@kubernetes
```

5. Switch context from admin to new user
`kubectl config use-context spnzig@kubernetes `
 
6. Run kubectl --user=spnzig@gmail.com get nodes to see the error 
`kubectl --user=spnzig@gmail.com get nodes`

**Anything else we need to know?**:
I have installed kubernetes on Ubuntu 16.04LTS.

**Environment**:
- Kubernetes version (use `kubectl version`): v1.7.3
- Cloud provider or hardware configuration**: hardware 
- OS (e.g. from /etc/os-release): Ubuntu 16.04
- Install tools: kubeadm, kubernetes, k8s-oidc-helper, go

",eyJhbGciOiJSUzI1NiIsImtpZCI6IjczMTdkOTM1MWQ1Y,https://github.com/kubernetes/kubernetes
153,50432,"**Is this a BUG REPORT or FEATURE REQUEST?**: BUG REPORT


**What happened**:
I have configured oidc with kubernetes. But when i try `kubectl --user=spnzip@gmail.com get nodes` i get `error from server (Forbidden): User ""system:anonymous"" cannot list nodes at the cluster scope. (get nodes)
`

**What you expected to happen**:
The command should authenticate user and display the nodes 

**How to reproduce it (as minimally and precisely as possible)**:
1. Install single node kubernetes cluster with kubeadm
2. Obtain client secrets with the k8s-oidc-helper tool
k8s-oidc-helper -c <path where user's client id and secret is stored)
3. copy paste the code generated to ~/.kube/config file
```
- name: spnzig@gmail.com
  user:
    auth-provider:
      config:
        client-id: xxxxxxxxxx-xxxxxxx.apps.googleusercontent.com
        client-secret: xxxxxxxxxxxxxxxxxxxxxxx
        id-token: eyJhbGciOiJSUzI1NiIsImtpZCI6IjczMTdkOTM1MWQ1Y .... njZaIWlMaAFEfIrGPy-49TQ
        idp-issuer-url: https://accounts.google.com
        refresh-token: 1/3XPbQPD...g8PuNPs
      name: oidc
```

4. Set the context for the new user in ~/.kube/config file
```
- context:
    cluster: kubernetes
    user: spnzig@gmail.com
  name: spnzig@kubernetes
```

5. Switch context from admin to new user
`kubectl config use-context spnzig@kubernetes `
 
6. Run kubectl --user=spnzig@gmail.com get nodes to see the error 
`kubectl --user=spnzig@gmail.com get nodes`

**Anything else we need to know?**:
I have installed kubernetes on Ubuntu 16.04LTS.

**Environment**:
- Kubernetes version (use `kubectl version`): v1.7.3
- Cloud provider or hardware configuration**: hardware 
- OS (e.g. from /etc/os-release): Ubuntu 16.04
- Install tools: kubeadm, kubernetes, k8s-oidc-helper, go

",xxxxxxxxxxxxxxxxxxxxxxx,https://github.com/kubernetes/kubernetes
154,49415,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**: Running `kubeadm init --config=<path>` fails.

**What you expected to happen**: `kubeadm init --config=<path>` configures based on the provided file.

**How to reproduce it (as minimally and precisely as possible)**:

Create a file `/tmp/kubeadm.yaml` with the below content:

```
---
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
token: m52zte.m52ztet6s1jknlpg
cloudProvider: aws
kubernetesVersion: v1.7.1
apiServerCertSANs:
- plombardi.kubernaut.io
```

Run `kubeadm init --config=/tmp/kubeadm.yaml`

Get error: 

```
kubeadm init --config /tmp/kubeadm.yaml
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[init] Using Kubernetes version: v1.7.1
[init] Using Authorization modes: [Node RBAC]
[init] WARNING: For cloudprovider integrations to work --cloud-provider must be set for all kubelets in the cluster.
	(/etc/systemd/system/kubelet.service.d/10-kubeadm.conf should be edited for this purpose)
[preflight] Running pre-flight checks
[preflight] WARNING: docker version is greater than the most recently validated version. Docker version: 17.06.0-ce. Max validated version: 1.12
can not mix '--config' with other arguments

```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`): NA
- Cloud provider or hardware configuration**: AWS
- OS (e.g. from /etc/os-release): CentOS 7.3 1703_01
- Kernel (e.g. `uname -a`): Linux ip-10-10-0-216.ec2.internal 3.10.0-514.26.2.el7.x86_64 #1 SMP Tue Jul 4 15:04:05 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

- Install tools:
- Others: Kubeadm Version = `kubeadm version: &version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.1"", GitCommit:""1dc5c66f5dd61da08412a74221ecc79208c2165b"", GitTreeState:""clean"", BuildDate:""2017-07-14T01:48:01Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}`
",kubernetesVersion,https://github.com/kubernetes/kubernetes
155,46460,"## Additions

Allows providing a configuration file (using flag `--experimental-encryption-provider-config`) to use the existing AEAD transformer (with multiple keys) by composing mutable transformer, prefix transformer (for parsing providerId), another prefix transformer (for parsing keyId), and AES-GCM transformers (one for each key). Multiple providers can be configured using the configuration file.

Example configuration:
```
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
    - namespaces
    providers:
    - aes:
        keys:
        - name: key1
          secret: c2vjcmv0iglzihnly3vyzq==
        - name: key2
          secret: dghpcybpcybwyxnzd29yza==
    - identity: {}
```

Need for configuration discussed in:
#41939
[Encryption](https://github.com/destijl/community/blob/3418b4e4c6358f5dc747a37b90a97bc792f159ee/contributors/design-proposals/encryption.md)

**Pathway of a read/write request**:
1. MutableTransformer
2. PrefixTransformer reads the provider-id, and passes the request further if that matches.
3. PrefixTransformer reads the key-id, and passes the request further if that matches.
4. GCMTransformer tries decrypting and authenticating the cipher text in case of reads. Similarly for writes.

## Caveats
1. To keep the command line parameter parsing independent of the individual transformer's configuration, we need to convert the configuration to an `interface{}` and manually parse it in the transformer. Suggestions on better ways to do this are welcome.

2. Flags `--encryption-provider` and `--encrypt-resource` (both mentioned in [this document](https://github.com/destijl/community/blob/3418b4e4c6358f5dc747a37b90a97bc792f159ee/contributors/design-proposals/encryption.md) ) are not supported in this because they do not allow more than one provider, and the current format for the configuration file possibly supersedes their functionality.

3. Currently, it can be tested by adding `--experimental-encryption-provider-config=config.yml` to `hack/local-up-cluster.sh` on line 511, and placing the above configuration in `config.yml` in the root project directory.

Previous discussion on these changes:
https://github.com/sakshamsharma/kubernetes/pull/1

@jcbsmpsn @destijl @smarterclayton

## TODO
1. Investigate if we need to store keys on disk (per [encryption.md](https://github.com/destijl/community/blob/3418b4e4c6358f5dc747a37b90a97bc792f159ee/contributors/design-proposals/encryption.md#option-1-simple-list-of-keys-on-disk))
2. Look at [alpha flag conventions](https://github.com/kubernetes/kubernetes/blob/master/pkg/features/kube_features.go)
3. Need to reserve `k8s:enc` prefix formally for encrypted data. Else find a better way to detect transformed data.",c2vjcmv0iglzihnly3vyzq,https://github.com/kubernetes/kubernetes
156,46460,"## Additions

Allows providing a configuration file (using flag `--experimental-encryption-provider-config`) to use the existing AEAD transformer (with multiple keys) by composing mutable transformer, prefix transformer (for parsing providerId), another prefix transformer (for parsing keyId), and AES-GCM transformers (one for each key). Multiple providers can be configured using the configuration file.

Example configuration:
```
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
    - namespaces
    providers:
    - aes:
        keys:
        - name: key1
          secret: c2vjcmv0iglzihnly3vyzq==
        - name: key2
          secret: dghpcybpcybwyxnzd29yza==
    - identity: {}
```

Need for configuration discussed in:
#41939
[Encryption](https://github.com/destijl/community/blob/3418b4e4c6358f5dc747a37b90a97bc792f159ee/contributors/design-proposals/encryption.md)

**Pathway of a read/write request**:
1. MutableTransformer
2. PrefixTransformer reads the provider-id, and passes the request further if that matches.
3. PrefixTransformer reads the key-id, and passes the request further if that matches.
4. GCMTransformer tries decrypting and authenticating the cipher text in case of reads. Similarly for writes.

## Caveats
1. To keep the command line parameter parsing independent of the individual transformer's configuration, we need to convert the configuration to an `interface{}` and manually parse it in the transformer. Suggestions on better ways to do this are welcome.

2. Flags `--encryption-provider` and `--encrypt-resource` (both mentioned in [this document](https://github.com/destijl/community/blob/3418b4e4c6358f5dc747a37b90a97bc792f159ee/contributors/design-proposals/encryption.md) ) are not supported in this because they do not allow more than one provider, and the current format for the configuration file possibly supersedes their functionality.

3. Currently, it can be tested by adding `--experimental-encryption-provider-config=config.yml` to `hack/local-up-cluster.sh` on line 511, and placing the above configuration in `config.yml` in the root project directory.

Previous discussion on these changes:
https://github.com/sakshamsharma/kubernetes/pull/1

@jcbsmpsn @destijl @smarterclayton

## TODO
1. Investigate if we need to store keys on disk (per [encryption.md](https://github.com/destijl/community/blob/3418b4e4c6358f5dc747a37b90a97bc792f159ee/contributors/design-proposals/encryption.md#option-1-simple-list-of-keys-on-disk))
2. Look at [alpha flag conventions](https://github.com/kubernetes/kubernetes/blob/master/pkg/features/kube_features.go)
3. Need to reserve `k8s:enc` prefix formally for encrypted data. Else find a better way to detect transformed data.",dghpcybpcybwyxnzd29yza,https://github.com/kubernetes/kubernetes
157,43435,"**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""6+"", GitVersion:""v1.6.0-beta.1-dirty"", GitCommit:""23cded36d1d20a538f97e0da05c1d2b62a6be700"", GitTreeState:""dirty"", BuildDate:""2017-03-20T09:11:01Z"", GoVersion:""go1.7"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""6+"", GitVersion:""v1.6.0-beta.3"", GitCommit:""0cd5ed469508e6dfc807ee6681561c845828917e"", GitTreeState:""clean"", BuildDate:""2017-03-11T04:09:08Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""linux/amd64""}

**What happened**:
We are creating TPRs inside the pod using incluster authentication. Previously (version 1.5.4 and before that) they were successfully created, but using trunk kubernetes i am getting Forbidden error with following logs:

```
I0321 07:50:52.754466       1 request.go:558] Request Body: ""{\""kind\"":\""ThirdPartyResource\"",\""apiVersion\"":\""extensions/v1beta1\"",\""metadata\"":{\""name\"":\""ip-node.ipcontroller.ext\"",\""creationTimestamp\"":null},\""versions\"":[{\""name\"":\""v1\""}]}\n""
I0321 07:50:52.754539       1 round_trippers.go:299] curl -k -v -XPOST  -H ""Accept: application/json, */*"" -H ""Content-Type: application/json"" -H ""User-Agent: ipmanager/v0.0.0 (linux/amd64) kubernetes/$Format"" -H ""Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tcnIyMzMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImMyZGIyNGFjLTBkN2EtMTFlNy1iZjZkLTAyNDIwYWMwMDAwMiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.DGDxUqIpcDhdgqvfRDoA3QP3wj5d94Z6mZ1Vs0-hlV5_652E6-TIMfwQu2yjPGlokgOzox4zUVzswCiVipLyg-ZDWNlaL3qrQLY5h0_iXcB3elO1XLzBXxhRhahKX9V34ECSjk1ztgeGkWpdnFBFCaVcCO8RzaqVyzcsHQGj_WMwoGVON_6c9a2e1X5mRGI-vI2V9gXilm_1Ct92Nx84zOo6tT7xz1iUXJh9WF6s1QQNYGQmr4QOQkeZXGvcwaCezfg3V_Aj7YXHTfMDZDXfYpKNBXE1TCG4wlrbXKNWZZuPsWxhKUvf86DEYUktUVpoqTmybSiyZlUGPWQ18Gzz5w"" https://10.96.0.1:443/apis/extensions/v1beta1/thirdpartyresources
I0321 07:50:52.754976       1 round_trippers.go:318] POST https://10.96.0.1:443/apis/extensions/v1beta1/thirdpartyresources 403 Forbidden in 0 milliseconds
I0321 07:50:52.754986       1 round_trippers.go:324] Response Headers:
I0321 07:50:52.754990       1 round_trippers.go:327]     Content-Type: text/plain
I0321 07:50:52.754995       1 round_trippers.go:327]     X-Content-Type-Options: nosniff
I0321 07:50:52.755000       1 round_trippers.go:327]     Content-Length: 111
I0321 07:50:52.755002       1 round_trippers.go:327]     Date: Tue, 21 Mar 2017 07:50:52 GMT
I0321 07:50:52.755017       1 request.go:896] Response Body: User ""system:serviceaccount:default:default"" cannot create thirdpartyresources.extensions at the cluster scope.
I0321 07:50:52.755024       1 request.go:986] Response Body: ""User \""system:serviceaccount:default:default\"" cannot create thirdpartyresources.extensions at the cluster scope.""
```

**What you expected to happen**:
Resources will be created

**How to reproduce it** (as minimally and precisely as possible):
Create any TPR using incluster auth

",hlV5_652E6-TIMfwQu2yjPGlokgOzox4zUVzswCiVipLyg-ZDWNlaL3qrQLY5h0_iXcB3elO1XLzBXxhRhahKX9V34ECSjk1ztgeGkWpdnFBFCaVcCO8RzaqVyzcsHQGj_WMwoGVON_6c9a2e1X5mRGI-vI2V9gXilm_1Ct92Nx84zOo6tT7xz1iUXJh9WF6s1QQNYGQmr4QOQkeZXGvcwaCezfg3V_Aj7YXHTfMDZDXfYpKNBXE1TCG4wlrbXKNWZZuPsWxhKUvf86DEYUktUVpoqTmybSiyZlUGPWQ18Gzz5w,https://github.com/kubernetes/kubernetes
158,42780,"Generate NPD token during upgrade.

I could not fully verify this change because of https://github.com/kubernetes/kubernetes/issues/42199. However, at least I tried upgrade master, and the corresponding environment variables are correctly generated.
```
...
ENABLE_NODE_PROBLEM_DETECTOR: 'standalone'
...
KUBELET_TOKEN: 'PKNgAaVXeL3VojND2s0KMleELjzGK0oW'
```

@maisem @dchen1107 ",PKNgAaVXeL3VojND2s0KMleELjzGK0oW,https://github.com/kubernetes/kubernetes
159,41388,"ref to https://github.com/kubernetes/kubeadm/issues/157

```
#kubeadm init
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[init] Using Kubernetes version: v1.6.0-alpha.1
[init] Using Authorization mode: RBAC
[init] A token has not been provided, generating one
[preflight] Running pre-flight checks
[preflight] Starting the kubelet service
[certificates] Generated CA certificate and key.
[certificates] Generated API server certificate and key.
[certificates] Generated API server kubelet client certificate and key.
[certificates] Generated service account token signing key.
[certificates] Generated service account token signing public key.
[certificates] Valid certificates and keys now exist in ""/etc/kubernetes/pki""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/admin.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/kubelet.conf""
[apiclient] Created API client, waiting for the control plane to become ready
[apiclient] Waiting for API server authorization
[apiclient] Waiting for API server authorization
[apiclient] Waiting for API server authorization
[apiclient] All control plane components are healthy after 18.260284 seconds
[apiclient] Waiting for at least one node to register and become ready
[apiclient] First node is ready after 3.509666 seconds
[apiclient] Test deployment succeeded
[apiconfig] Created RBAC rules
[token-discovery] Using token: e4b248:0563c2e3d0635b02
[token-discovery] Created the kube-discovery deployment, waiting for it to become ready
[token-discovery] kube-discovery is ready after 2.503299 seconds
[addons] Created essential addon: kube-proxy
[addons] Created essential addon: kube-dns

Your Kubernetes master has initialized successfully!

You should now deploy a pod network to the cluster.
Run ""kubectl apply -f [podnetwork].yaml"" with one of the options listed at:
    http://kubernetes.io/docs/admin/addons/

You can now join any number of machines by running the following on each node:

kubeadm join --discovery token://e4b248:0563c2e3d0635b02@192.168.122.100:9898
```",0563c2e3d0635b02,https://github.com/kubernetes/kubernetes
160,40008,"**What this PR does / why we need it**: `kubeadm init` must validate or generate a token before anything else. Otherwise, if token validation or generation fail, one will need to run `kubeadm reset && systemctl restart kubelet` before re-running `kubeadm init`.

**Which issue this PR fixes**: fixes kubernetes/kubeadm#112

**Special notes for your reviewer**: /cc @luxas

Tested manually.

### With no token

```
$ sudo ./kubeadm init
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[preflight] Running pre-flight checks
[init] Using Kubernetes version: v1.5.2
[token-discovery] A token has not been provided, generating one
[certificates] Generated Certificate Authority key and certificate.
[certificates] Generated API Server key and certificate
[certificates] Generated Service Account signing keys
[certificates] Created keys and certificates in ""/etc/kubernetes/pki""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/admin.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/kubelet.conf""
[apiclient] Created API client, waiting for the control plane to become ready
[apiclient] All control plane components are healthy after 7.762803 seconds
[apiclient] Waiting for at least one node to register and become ready
[apiclient] First node is ready after 1.003148 seconds
[apiclient] Creating a test deployment
[apiclient] Test deployment succeeded
[token-discovery] Using token: 8321b6:a535ba541af7623c
[token-discovery] Created the kube-discovery deployment, waiting for it to become ready
[token-discovery] kube-discovery is ready after 1.003423 seconds
[addons] Created essential addon: kube-proxy
[addons] Created essential addon: kube-dns
Your Kubernetes master has initialized successfully!
You should now deploy a pod network to the cluster.
Run ""kubectl apply -f [podnetwork].yaml"" with one of the options listed at:
    http://kubernetes.io/docs/admin/addons/
You can now join any number of machines by running the following on each node:
kubeadm join --discovery token://8321b6:a535ba541af7623c@10.142.0.6:9898
```

### With invalid token

```
$ sudo ./kubeadm init --discovery token://12345:12345
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[preflight] Running pre-flight checks
[init] Using Kubernetes version: v1.5.2
[token-discovery] A token has been provided, validating [&{ID:12345 Secret:12345 Addresses:[]}]
token [""12345:12345""] was not of form [""^([a-z0-9]{6})\\:([a-z0-9]{16})$""]
```

### With valid token

```
$ sudo ./kubeadm ex token generate
cd540e:c0e0318e2f4a63b1

$ sudo ./kubeadm init --discovery token://cd540e:c0e0318e2f4a63b1
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[preflight] Running pre-flight checks
[init] Using Kubernetes version: v1.5.2
[token-discovery] A token has been provided, validating [&{ID:cd540e Secret:c0e0318e2f4a63b1 Addresses:[]}]
[certificates] Generated Certificate Authority key and certificate.
[certificates] Generated API Server key and certificate
[certificates] Generated Service Account signing keys
[certificates] Created keys and certificates in ""/etc/kubernetes/pki""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/admin.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/kubelet.conf""
[apiclient] Created API client, waiting for the control plane to become ready
[apiclient] All control plane components are healthy after 13.513305 seconds
[apiclient] Waiting for at least one node to register and become ready
[apiclient] First node is ready after 0.502656 seconds
[apiclient] Creating a test deployment
[apiclient] Test deployment succeeded
[token-discovery] Using token: cd540e:c0e0318e2f4a63b1
[token-discovery] Created the kube-discovery deployment, waiting for it to become ready
[token-discovery] kube-discovery is ready after 2.002457 seconds
[addons] Created essential addon: kube-proxy
[addons] Created essential addon: kube-dns
Your Kubernetes master has initialized successfully!
You should now deploy a pod network to the cluster.
Run ""kubectl apply -f [podnetwork].yaml"" with one of the options listed at:
    http://kubernetes.io/docs/admin/addons/
You can now join any number of machines by running the following on each node:
kubeadm join --discovery token://cd540e:c0e0318e2f4a63b1@10.142.0.6:9898
```

**Release note**:
```release-note
NONE
```
",c0e0318e2f4a63b1,https://github.com/kubernetes/kubernetes
161,40008,"**What this PR does / why we need it**: `kubeadm init` must validate or generate a token before anything else. Otherwise, if token validation or generation fail, one will need to run `kubeadm reset && systemctl restart kubelet` before re-running `kubeadm init`.

**Which issue this PR fixes**: fixes kubernetes/kubeadm#112

**Special notes for your reviewer**: /cc @luxas

Tested manually.

### With no token

```
$ sudo ./kubeadm init
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[preflight] Running pre-flight checks
[init] Using Kubernetes version: v1.5.2
[token-discovery] A token has not been provided, generating one
[certificates] Generated Certificate Authority key and certificate.
[certificates] Generated API Server key and certificate
[certificates] Generated Service Account signing keys
[certificates] Created keys and certificates in ""/etc/kubernetes/pki""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/admin.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/kubelet.conf""
[apiclient] Created API client, waiting for the control plane to become ready
[apiclient] All control plane components are healthy after 7.762803 seconds
[apiclient] Waiting for at least one node to register and become ready
[apiclient] First node is ready after 1.003148 seconds
[apiclient] Creating a test deployment
[apiclient] Test deployment succeeded
[token-discovery] Using token: 8321b6:a535ba541af7623c
[token-discovery] Created the kube-discovery deployment, waiting for it to become ready
[token-discovery] kube-discovery is ready after 1.003423 seconds
[addons] Created essential addon: kube-proxy
[addons] Created essential addon: kube-dns
Your Kubernetes master has initialized successfully!
You should now deploy a pod network to the cluster.
Run ""kubectl apply -f [podnetwork].yaml"" with one of the options listed at:
    http://kubernetes.io/docs/admin/addons/
You can now join any number of machines by running the following on each node:
kubeadm join --discovery token://8321b6:a535ba541af7623c@10.142.0.6:9898
```

### With invalid token

```
$ sudo ./kubeadm init --discovery token://12345:12345
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[preflight] Running pre-flight checks
[init] Using Kubernetes version: v1.5.2
[token-discovery] A token has been provided, validating [&{ID:12345 Secret:12345 Addresses:[]}]
token [""12345:12345""] was not of form [""^([a-z0-9]{6})\\:([a-z0-9]{16})$""]
```

### With valid token

```
$ sudo ./kubeadm ex token generate
cd540e:c0e0318e2f4a63b1

$ sudo ./kubeadm init --discovery token://cd540e:c0e0318e2f4a63b1
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[preflight] Running pre-flight checks
[init] Using Kubernetes version: v1.5.2
[token-discovery] A token has been provided, validating [&{ID:cd540e Secret:c0e0318e2f4a63b1 Addresses:[]}]
[certificates] Generated Certificate Authority key and certificate.
[certificates] Generated API Server key and certificate
[certificates] Generated Service Account signing keys
[certificates] Created keys and certificates in ""/etc/kubernetes/pki""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/admin.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/kubelet.conf""
[apiclient] Created API client, waiting for the control plane to become ready
[apiclient] All control plane components are healthy after 13.513305 seconds
[apiclient] Waiting for at least one node to register and become ready
[apiclient] First node is ready after 0.502656 seconds
[apiclient] Creating a test deployment
[apiclient] Test deployment succeeded
[token-discovery] Using token: cd540e:c0e0318e2f4a63b1
[token-discovery] Created the kube-discovery deployment, waiting for it to become ready
[token-discovery] kube-discovery is ready after 2.002457 seconds
[addons] Created essential addon: kube-proxy
[addons] Created essential addon: kube-dns
Your Kubernetes master has initialized successfully!
You should now deploy a pod network to the cluster.
Run ""kubectl apply -f [podnetwork].yaml"" with one of the options listed at:
    http://kubernetes.io/docs/admin/addons/
You can now join any number of machines by running the following on each node:
kubeadm join --discovery token://cd540e:c0e0318e2f4a63b1@10.142.0.6:9898
```

**Release note**:
```release-note
NONE
```
",9]{6})\\:([a-z0-9]{16,https://github.com/kubernetes/kubernetes
162,40008,"**What this PR does / why we need it**: `kubeadm init` must validate or generate a token before anything else. Otherwise, if token validation or generation fail, one will need to run `kubeadm reset && systemctl restart kubelet` before re-running `kubeadm init`.

**Which issue this PR fixes**: fixes kubernetes/kubeadm#112

**Special notes for your reviewer**: /cc @luxas

Tested manually.

### With no token

```
$ sudo ./kubeadm init
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[preflight] Running pre-flight checks
[init] Using Kubernetes version: v1.5.2
[token-discovery] A token has not been provided, generating one
[certificates] Generated Certificate Authority key and certificate.
[certificates] Generated API Server key and certificate
[certificates] Generated Service Account signing keys
[certificates] Created keys and certificates in ""/etc/kubernetes/pki""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/admin.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/kubelet.conf""
[apiclient] Created API client, waiting for the control plane to become ready
[apiclient] All control plane components are healthy after 7.762803 seconds
[apiclient] Waiting for at least one node to register and become ready
[apiclient] First node is ready after 1.003148 seconds
[apiclient] Creating a test deployment
[apiclient] Test deployment succeeded
[token-discovery] Using token: 8321b6:a535ba541af7623c
[token-discovery] Created the kube-discovery deployment, waiting for it to become ready
[token-discovery] kube-discovery is ready after 1.003423 seconds
[addons] Created essential addon: kube-proxy
[addons] Created essential addon: kube-dns
Your Kubernetes master has initialized successfully!
You should now deploy a pod network to the cluster.
Run ""kubectl apply -f [podnetwork].yaml"" with one of the options listed at:
    http://kubernetes.io/docs/admin/addons/
You can now join any number of machines by running the following on each node:
kubeadm join --discovery token://8321b6:a535ba541af7623c@10.142.0.6:9898
```

### With invalid token

```
$ sudo ./kubeadm init --discovery token://12345:12345
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[preflight] Running pre-flight checks
[init] Using Kubernetes version: v1.5.2
[token-discovery] A token has been provided, validating [&{ID:12345 Secret:12345 Addresses:[]}]
token [""12345:12345""] was not of form [""^([a-z0-9]{6})\\:([a-z0-9]{16})$""]
```

### With valid token

```
$ sudo ./kubeadm ex token generate
cd540e:c0e0318e2f4a63b1

$ sudo ./kubeadm init --discovery token://cd540e:c0e0318e2f4a63b1
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[preflight] Running pre-flight checks
[init] Using Kubernetes version: v1.5.2
[token-discovery] A token has been provided, validating [&{ID:cd540e Secret:c0e0318e2f4a63b1 Addresses:[]}]
[certificates] Generated Certificate Authority key and certificate.
[certificates] Generated API Server key and certificate
[certificates] Generated Service Account signing keys
[certificates] Created keys and certificates in ""/etc/kubernetes/pki""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/admin.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/kubelet.conf""
[apiclient] Created API client, waiting for the control plane to become ready
[apiclient] All control plane components are healthy after 13.513305 seconds
[apiclient] Waiting for at least one node to register and become ready
[apiclient] First node is ready after 0.502656 seconds
[apiclient] Creating a test deployment
[apiclient] Test deployment succeeded
[token-discovery] Using token: cd540e:c0e0318e2f4a63b1
[token-discovery] Created the kube-discovery deployment, waiting for it to become ready
[token-discovery] kube-discovery is ready after 2.002457 seconds
[addons] Created essential addon: kube-proxy
[addons] Created essential addon: kube-dns
Your Kubernetes master has initialized successfully!
You should now deploy a pod network to the cluster.
Run ""kubectl apply -f [podnetwork].yaml"" with one of the options listed at:
    http://kubernetes.io/docs/admin/addons/
You can now join any number of machines by running the following on each node:
kubeadm join --discovery token://cd540e:c0e0318e2f4a63b1@10.142.0.6:9898
```

**Release note**:
```release-note
NONE
```
",a535ba541af7623c,https://github.com/kubernetes/kubernetes
163,39508,"**Kubernetes version** (use `kubectl version`):
```
kubectl version
Client Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.6"", GitCommit:""e569a27d02001e343cb68086bc06d47804f62af6"", GitTreeState:""clean"", BuildDate:""2016-11-12T05:22:15Z"", GoVersion:""go1.7.1"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.6"", GitCommit:""e569a27d02001e343cb68086bc06d47804f62af6"", GitTreeState:""clean"", BuildDate:""2016-11-12T05:16:27Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: Azure CS
- **OS** (e.g. from /etc/os-release): Ubuntu 16.04 LTS 
- **Kernel** (e.g. `uname -a`): 4.4.0-57-generic 
- **Install tools**: Azure cli (2 agents, standard d2)

**What happened**:
Everything were working, until I decided to update deployment (using `replace`). After that pods stop starting and staid on `ContainerCreating` status.
In 'kubectl describe pod` I got:
```
  28m		40s		22	{kubelet k8s-agent-941b4238-0}			Warning		FailedMount	MountVolume.SetUp failed for volume ""kubernetes.io/secret/71af8ad3-d3c6-11e6-9251-000d3a216b93-default-token-i9xv0"" (spec.Name: ""default-token-i9xv0"") pod ""71af8ad3-d3c6-11e6-9251-000d3a216b93"" (UID: ""71af8ad3-d3c6-11e6-9251-000d3a216b93"") with: mount failed: exit status 32
Mounting arguments: tmpfs /var/lib/kubelet/pods/71af8ad3-d3c6-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0 tmpfs []
Output: mount: mount tmpfs on /var/lib/kubelet/pods/71af8ad3-d3c6-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0 failed: No space left on device


  26m	23s	13	{kubelet k8s-agent-941b4238-0}		Warning	FailedMount	Unable to mount volumes for pod ""router-4284723421-h9de2_default(71af8ad3-d3c6-11e6-9251-000d3a216b93)"": timeout expired waiting for volumes to attach/mount for pod ""router-4284723421-h9de2""/""default"". list of unattached/unmounted volumes=[default-token-i9xv0]
  26m	23s	13	{kubelet k8s-agent-941b4238-0}		Warning	FailedSync	Error syncing pod, skipping: timeout expired waiting for volumes to attach/mount for pod ""router-4284723421-h9de2""/""default"". list of unattached/unmounted volumes=[default-token-i9xv0]
```

However, on agents there is definitely enough space:
```
Filesystem      Size  Used Avail Use% Mounted on
udev            3.4G     0  3.4G   0% /dev
tmpfs           697M   17M  680M   3% /run
/dev/sda1        29G  7.6G   21G  27% /
tmpfs           3.5G     0  3.5G   0% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs           3.5G     0  3.5G   0% /sys/fs/cgroup
none             64K     0   64K   0% /etc/network/interfaces.dynamic.d
/dev/sdb1        14G   40M   14G   1% /mnt
tmpfs           100K     0  100K   0% /run/lxcfs/controllers
tmpfs           3.5G   12K  3.5G   1% /var/lib/kubelet/pods/b7abae8f-cefc-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-zk98l
shm              64M     0   64M   0% /var/lib/docker/containers/70f1efbd4a5564208ca79a329dfc3828f0315aba5d7b5034b6784e9341e96a10/shm
tmpfs           3.5G   12K  3.5G   1% /var/lib/kubelet/pods/9c1dfead-d393-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0
shm              64M     0   64M   0% /var/lib/docker/containers/b050aafce252bb14f65db18315ff2957ae3c08854806a65693d96534749f9d73/shm
shm              64M     0   64M   0% /var/lib/docker/containers/7643840c8ea7465697ccbc3966a6ce7d5a6b852ae924787b1892945458e4695e/shm
tmpfs           697M     0  697M   0% /run/user/1000
```
(I even see that token was mounted)

It looks very close to https://github.com/kubernetes/kubernetes/issues/13174 but seems it is somehow Azure related...

Also on same agent I have another pod running with a volume mounted:
```
Volumes:
  default-token-i9xv0:
    Type:	Secret (a volume populated by a Secret)
    SecretName:	default-token-i9xv0
```",71af8ad3-d3c6-11e6-9251-000d3a216b93,https://github.com/kubernetes/kubernetes
164,39508,"**Kubernetes version** (use `kubectl version`):
```
kubectl version
Client Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.6"", GitCommit:""e569a27d02001e343cb68086bc06d47804f62af6"", GitTreeState:""clean"", BuildDate:""2016-11-12T05:22:15Z"", GoVersion:""go1.7.1"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.6"", GitCommit:""e569a27d02001e343cb68086bc06d47804f62af6"", GitTreeState:""clean"", BuildDate:""2016-11-12T05:16:27Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: Azure CS
- **OS** (e.g. from /etc/os-release): Ubuntu 16.04 LTS 
- **Kernel** (e.g. `uname -a`): 4.4.0-57-generic 
- **Install tools**: Azure cli (2 agents, standard d2)

**What happened**:
Everything were working, until I decided to update deployment (using `replace`). After that pods stop starting and staid on `ContainerCreating` status.
In 'kubectl describe pod` I got:
```
  28m		40s		22	{kubelet k8s-agent-941b4238-0}			Warning		FailedMount	MountVolume.SetUp failed for volume ""kubernetes.io/secret/71af8ad3-d3c6-11e6-9251-000d3a216b93-default-token-i9xv0"" (spec.Name: ""default-token-i9xv0"") pod ""71af8ad3-d3c6-11e6-9251-000d3a216b93"" (UID: ""71af8ad3-d3c6-11e6-9251-000d3a216b93"") with: mount failed: exit status 32
Mounting arguments: tmpfs /var/lib/kubelet/pods/71af8ad3-d3c6-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0 tmpfs []
Output: mount: mount tmpfs on /var/lib/kubelet/pods/71af8ad3-d3c6-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0 failed: No space left on device


  26m	23s	13	{kubelet k8s-agent-941b4238-0}		Warning	FailedMount	Unable to mount volumes for pod ""router-4284723421-h9de2_default(71af8ad3-d3c6-11e6-9251-000d3a216b93)"": timeout expired waiting for volumes to attach/mount for pod ""router-4284723421-h9de2""/""default"". list of unattached/unmounted volumes=[default-token-i9xv0]
  26m	23s	13	{kubelet k8s-agent-941b4238-0}		Warning	FailedSync	Error syncing pod, skipping: timeout expired waiting for volumes to attach/mount for pod ""router-4284723421-h9de2""/""default"". list of unattached/unmounted volumes=[default-token-i9xv0]
```

However, on agents there is definitely enough space:
```
Filesystem      Size  Used Avail Use% Mounted on
udev            3.4G     0  3.4G   0% /dev
tmpfs           697M   17M  680M   3% /run
/dev/sda1        29G  7.6G   21G  27% /
tmpfs           3.5G     0  3.5G   0% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs           3.5G     0  3.5G   0% /sys/fs/cgroup
none             64K     0   64K   0% /etc/network/interfaces.dynamic.d
/dev/sdb1        14G   40M   14G   1% /mnt
tmpfs           100K     0  100K   0% /run/lxcfs/controllers
tmpfs           3.5G   12K  3.5G   1% /var/lib/kubelet/pods/b7abae8f-cefc-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-zk98l
shm              64M     0   64M   0% /var/lib/docker/containers/70f1efbd4a5564208ca79a329dfc3828f0315aba5d7b5034b6784e9341e96a10/shm
tmpfs           3.5G   12K  3.5G   1% /var/lib/kubelet/pods/9c1dfead-d393-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0
shm              64M     0   64M   0% /var/lib/docker/containers/b050aafce252bb14f65db18315ff2957ae3c08854806a65693d96534749f9d73/shm
shm              64M     0   64M   0% /var/lib/docker/containers/7643840c8ea7465697ccbc3966a6ce7d5a6b852ae924787b1892945458e4695e/shm
tmpfs           697M     0  697M   0% /run/user/1000
```
(I even see that token was mounted)

It looks very close to https://github.com/kubernetes/kubernetes/issues/13174 but seems it is somehow Azure related...

Also on same agent I have another pod running with a volume mounted:
```
Volumes:
  default-token-i9xv0:
    Type:	Secret (a volume populated by a Secret)
    SecretName:	default-token-i9xv0
```",agent-941b4238-0,https://github.com/kubernetes/kubernetes
165,39508,"**Kubernetes version** (use `kubectl version`):
```
kubectl version
Client Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.6"", GitCommit:""e569a27d02001e343cb68086bc06d47804f62af6"", GitTreeState:""clean"", BuildDate:""2016-11-12T05:22:15Z"", GoVersion:""go1.7.1"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.6"", GitCommit:""e569a27d02001e343cb68086bc06d47804f62af6"", GitTreeState:""clean"", BuildDate:""2016-11-12T05:16:27Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: Azure CS
- **OS** (e.g. from /etc/os-release): Ubuntu 16.04 LTS 
- **Kernel** (e.g. `uname -a`): 4.4.0-57-generic 
- **Install tools**: Azure cli (2 agents, standard d2)

**What happened**:
Everything were working, until I decided to update deployment (using `replace`). After that pods stop starting and staid on `ContainerCreating` status.
In 'kubectl describe pod` I got:
```
  28m		40s		22	{kubelet k8s-agent-941b4238-0}			Warning		FailedMount	MountVolume.SetUp failed for volume ""kubernetes.io/secret/71af8ad3-d3c6-11e6-9251-000d3a216b93-default-token-i9xv0"" (spec.Name: ""default-token-i9xv0"") pod ""71af8ad3-d3c6-11e6-9251-000d3a216b93"" (UID: ""71af8ad3-d3c6-11e6-9251-000d3a216b93"") with: mount failed: exit status 32
Mounting arguments: tmpfs /var/lib/kubelet/pods/71af8ad3-d3c6-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0 tmpfs []
Output: mount: mount tmpfs on /var/lib/kubelet/pods/71af8ad3-d3c6-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0 failed: No space left on device


  26m	23s	13	{kubelet k8s-agent-941b4238-0}		Warning	FailedMount	Unable to mount volumes for pod ""router-4284723421-h9de2_default(71af8ad3-d3c6-11e6-9251-000d3a216b93)"": timeout expired waiting for volumes to attach/mount for pod ""router-4284723421-h9de2""/""default"". list of unattached/unmounted volumes=[default-token-i9xv0]
  26m	23s	13	{kubelet k8s-agent-941b4238-0}		Warning	FailedSync	Error syncing pod, skipping: timeout expired waiting for volumes to attach/mount for pod ""router-4284723421-h9de2""/""default"". list of unattached/unmounted volumes=[default-token-i9xv0]
```

However, on agents there is definitely enough space:
```
Filesystem      Size  Used Avail Use% Mounted on
udev            3.4G     0  3.4G   0% /dev
tmpfs           697M   17M  680M   3% /run
/dev/sda1        29G  7.6G   21G  27% /
tmpfs           3.5G     0  3.5G   0% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs           3.5G     0  3.5G   0% /sys/fs/cgroup
none             64K     0   64K   0% /etc/network/interfaces.dynamic.d
/dev/sdb1        14G   40M   14G   1% /mnt
tmpfs           100K     0  100K   0% /run/lxcfs/controllers
tmpfs           3.5G   12K  3.5G   1% /var/lib/kubelet/pods/b7abae8f-cefc-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-zk98l
shm              64M     0   64M   0% /var/lib/docker/containers/70f1efbd4a5564208ca79a329dfc3828f0315aba5d7b5034b6784e9341e96a10/shm
tmpfs           3.5G   12K  3.5G   1% /var/lib/kubelet/pods/9c1dfead-d393-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0
shm              64M     0   64M   0% /var/lib/docker/containers/b050aafce252bb14f65db18315ff2957ae3c08854806a65693d96534749f9d73/shm
shm              64M     0   64M   0% /var/lib/docker/containers/7643840c8ea7465697ccbc3966a6ce7d5a6b852ae924787b1892945458e4695e/shm
tmpfs           697M     0  697M   0% /run/user/1000
```
(I even see that token was mounted)

It looks very close to https://github.com/kubernetes/kubernetes/issues/13174 but seems it is somehow Azure related...

Also on same agent I have another pod running with a volume mounted:
```
Volumes:
  default-token-i9xv0:
    Type:	Secret (a volume populated by a Secret)
    SecretName:	default-token-i9xv0
```",default-token-i9xv0,https://github.com/kubernetes/kubernetes
166,39508,"**Kubernetes version** (use `kubectl version`):
```
kubectl version
Client Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.6"", GitCommit:""e569a27d02001e343cb68086bc06d47804f62af6"", GitTreeState:""clean"", BuildDate:""2016-11-12T05:22:15Z"", GoVersion:""go1.7.1"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.6"", GitCommit:""e569a27d02001e343cb68086bc06d47804f62af6"", GitTreeState:""clean"", BuildDate:""2016-11-12T05:16:27Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: Azure CS
- **OS** (e.g. from /etc/os-release): Ubuntu 16.04 LTS 
- **Kernel** (e.g. `uname -a`): 4.4.0-57-generic 
- **Install tools**: Azure cli (2 agents, standard d2)

**What happened**:
Everything were working, until I decided to update deployment (using `replace`). After that pods stop starting and staid on `ContainerCreating` status.
In 'kubectl describe pod` I got:
```
  28m		40s		22	{kubelet k8s-agent-941b4238-0}			Warning		FailedMount	MountVolume.SetUp failed for volume ""kubernetes.io/secret/71af8ad3-d3c6-11e6-9251-000d3a216b93-default-token-i9xv0"" (spec.Name: ""default-token-i9xv0"") pod ""71af8ad3-d3c6-11e6-9251-000d3a216b93"" (UID: ""71af8ad3-d3c6-11e6-9251-000d3a216b93"") with: mount failed: exit status 32
Mounting arguments: tmpfs /var/lib/kubelet/pods/71af8ad3-d3c6-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0 tmpfs []
Output: mount: mount tmpfs on /var/lib/kubelet/pods/71af8ad3-d3c6-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0 failed: No space left on device


  26m	23s	13	{kubelet k8s-agent-941b4238-0}		Warning	FailedMount	Unable to mount volumes for pod ""router-4284723421-h9de2_default(71af8ad3-d3c6-11e6-9251-000d3a216b93)"": timeout expired waiting for volumes to attach/mount for pod ""router-4284723421-h9de2""/""default"". list of unattached/unmounted volumes=[default-token-i9xv0]
  26m	23s	13	{kubelet k8s-agent-941b4238-0}		Warning	FailedSync	Error syncing pod, skipping: timeout expired waiting for volumes to attach/mount for pod ""router-4284723421-h9de2""/""default"". list of unattached/unmounted volumes=[default-token-i9xv0]
```

However, on agents there is definitely enough space:
```
Filesystem      Size  Used Avail Use% Mounted on
udev            3.4G     0  3.4G   0% /dev
tmpfs           697M   17M  680M   3% /run
/dev/sda1        29G  7.6G   21G  27% /
tmpfs           3.5G     0  3.5G   0% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs           3.5G     0  3.5G   0% /sys/fs/cgroup
none             64K     0   64K   0% /etc/network/interfaces.dynamic.d
/dev/sdb1        14G   40M   14G   1% /mnt
tmpfs           100K     0  100K   0% /run/lxcfs/controllers
tmpfs           3.5G   12K  3.5G   1% /var/lib/kubelet/pods/b7abae8f-cefc-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-zk98l
shm              64M     0   64M   0% /var/lib/docker/containers/70f1efbd4a5564208ca79a329dfc3828f0315aba5d7b5034b6784e9341e96a10/shm
tmpfs           3.5G   12K  3.5G   1% /var/lib/kubelet/pods/9c1dfead-d393-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0
shm              64M     0   64M   0% /var/lib/docker/containers/b050aafce252bb14f65db18315ff2957ae3c08854806a65693d96534749f9d73/shm
shm              64M     0   64M   0% /var/lib/docker/containers/7643840c8ea7465697ccbc3966a6ce7d5a6b852ae924787b1892945458e4695e/shm
tmpfs           697M     0  697M   0% /run/user/1000
```
(I even see that token was mounted)

It looks very close to https://github.com/kubernetes/kubernetes/issues/13174 but seems it is somehow Azure related...

Also on same agent I have another pod running with a volume mounted:
```
Volumes:
  default-token-i9xv0:
    Type:	Secret (a volume populated by a Secret)
    SecretName:	default-token-i9xv0
```",9251-000d3a216b93,https://github.com/kubernetes/kubernetes
167,36856,"From my current understanding, once I create a secret using something like the following, it is available to all pods in the namespace if they mount it into an environment variable or volume:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  password: MWYyZDFlMmU2N2Rm
  username: YWRtaW4=
```

It would be nice if we could scope secrets to a deployment. In this case, the secrets can only be mounted into a volume or environment variables for the deployment. When the deployment is deleted, the secrets are also deleted. This way, another pod would not be able to get access to secrets owned by the deployment.

The current work around is to use an environment variable directly in the deployment configuration, but environment variables are very insecure because a user can easily see the secret in plain text if he/she retrieves the deployment as a yaml file from the api server.",MWYyZDFlMmU2N2Rm,https://github.com/kubernetes/kubernetes
168,36856,"From my current understanding, once I create a secret using something like the following, it is available to all pods in the namespace if they mount it into an environment variable or volume:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  password: MWYyZDFlMmU2N2Rm
  username: YWRtaW4=
```

It would be nice if we could scope secrets to a deployment. In this case, the secrets can only be mounted into a volume or environment variables for the deployment. When the deployment is deleted, the secrets are also deleted. This way, another pod would not be able to get access to secrets owned by the deployment.

The current work around is to use an environment variable directly in the deployment configuration, but environment variables are very insecure because a user can easily see the secret in plain text if he/she retrieves the deployment as a yaml file from the api server.",YWRtaW4=,https://github.com/kubernetes/kubernetes
169,36143,"It happens once in a while in different tests. This is a meta issue for failures like
```
Type: ""UnexpectedServerResponse"",
                        Message: ""Error: 'No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-d33fce984358c2fdde6b\""?'\nTrying to reach: 'https://gke-jenkins-e2e-default-pool-f5e91d0a-2zb3:10250/metrics'"",
                        Field: """",
```

cc @kubernetes/goog-gke @wojtek-t",d33fce984358c2fdde6b\?\nTrying,https://github.com/kubernetes/kubernetes
170,35378,"Hi, 

I'm trying to build a test environment which in the end will be deployed to our production server; however I encountered some issues that, for now, I consider bugs.

Please find my install scripts @ https://github.com/itmcdev/solaris.

Following the valid code from `provision.sh`, I followed the instructions from http://kubernetes.io/docs/getting-started-guides/kubeadm/ running:

``` bash
# for master
kubeadm init --api-advertise-addresses=10.0.3.10

# for workers
kubeadm join --token e36420.92c6e1d4959fb8eb 10.0.3.10

# than, for master
kubectl apply -f https://git.io/weave-kube
```

Following this, I noticed that kube-dns and weave-kube images were persisting in ContainerCreating and CrashLoopBackOff statuses. I will let you decide further, whether this is a bug, or a mistake generated by me. To note, I'm a total noob under the process of learning.

``` bash
root@solaris-master:~# kubectl get nodes
NAME               STATUS    AGE
solaris-master     Ready     52m
solaris-worker-1   Ready     50m
solaris-worker-2   Ready     50m
```

``` bash
root@solaris-master:~# kubectl get pods --all-namespaces
NAMESPACE     NAME                                     READY     STATUS              RESTARTS   AGE
kube-system   etcd-solaris-master                      1/1       Running             0          51m
kube-system   kube-apiserver-solaris-master            1/1       Running             0          52m
kube-system   kube-controller-manager-solaris-master   1/1       Running             0          52m
kube-system   kube-discovery-982812725-wm3o2           1/1       Running             0          29m
kube-system   kube-dns-2247936740-ubzbm                0/3       ContainerCreating   0          51m
kube-system   kube-proxy-amd64-3jabb                   1/1       Running             0          50m
kube-system   kube-proxy-amd64-rt2f9                   1/1       Running             0          50m
kube-system   kube-proxy-amd64-ygc2p                   1/1       Running             0          51m
kube-system   kube-scheduler-solaris-master            1/1       Running             0          51m
kube-system   weave-net-4c3a4                          1/2       CrashLoopBackOff    11         37m
kube-system   weave-net-yvv71                          2/2       Running             0          37m
kube-system   weave-net-z9aqe                          1/2       CrashLoopBackOff    11         37m
```

``` bash
root@solaris-master:~# kubectl describe pod weave-net-4c3a4 --namespace=kube-system
Name:           weave-net-4c3a4
Namespace:      kube-system
Node:           solaris-worker-1/10.0.2.15
Start Time:     Sat, 22 Oct 2016 16:33:32 -0400
Labels:         name=weave-net
Status:         Running
IP:             10.0.2.15
Controllers:    DaemonSet/weave-net
Containers:
  weave:
    Container ID:       docker://a254dce85d8a027d9d3de1de080ca6322806e05fda2bc72a87f1d5b3cc936f35
    Image:              weaveworks/weave-kube:1.7.2
    Image ID:           docker://sha256:9a2aa48020f51f3bc736e63a888441979dbcba394debdc1abb8e43cf95449168
    Port:
    Command:
      /home/weave/launch.sh
    Requests:
      cpu:              10m
    State:              Waiting
      Reason:           CrashLoopBackOff
    Last State:         Terminated
      Reason:           Error
      Exit Code:        1
      Started:          Sat, 22 Oct 2016 17:11:32 -0400
      Finished:         Sat, 22 Oct 2016 17:11:32 -0400
    Ready:              False
    Restart Count:      12
    Liveness:           http-get http://127.0.0.1:6784/status delay=30s timeout=1s period=10s #success=1 #failure=3
    Volume Mounts:
      /etc from cni-conf (rw)
      /host_home from cni-bin2 (rw)
      /opt from cni-bin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
      /weavedb from weavedb (rw)
    Environment Variables:      <none>
  weave-npc:
    Container ID:       docker://2395225b64692a0a4caf63903ec5cd4ca55b0fc60989623c165a747acd456068
    Image:              weaveworks/weave-npc:1.7.2
    Image ID:           docker://sha256:63a7347dde435cccc849d0eacc1a11c17733b6e557e67ce56efd8264f11e5d8c
    Port:
    Requests:
      cpu:              10m
    State:              Running
      Started:          Sat, 22 Oct 2016 16:34:49 -0400
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
Conditions:
  Type          Status
  Initialized   True
  Ready         False
  PodScheduled  True
Volumes:
  weavedb:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
  cni-bin:
    Type:       HostPath (bare host directory volume)
    Path:       /opt
  cni-bin2:
    Type:       HostPath (bare host directory volume)
    Path:       /home
  cni-conf:
    Type:       HostPath (bare host directory volume)
    Path:       /etc
  default-token-eejd4:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-eejd4
QoS Class:      Burstable
Tolerations:    dedicated=master:Equal:NoSchedule
Events:
  FirstSeen     LastSeen        Count   From                            SubobjectPath                   Type            Reason          Message
  ---------     --------        -----   ----                            -------------                   --------        ------          -------
  38m           38m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Pulling         pulling image ""weaveworks/weave-kube:1.7.2""
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Pulled          Successfully pulled image ""weaveworks/weave-kube:1.7.2""
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Created         Created container with docker id 06f090f2775f; Security:[seccomp=unconfined]
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Started         Started container with docker id 06f090f2775f
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Pulling         pulling image ""weaveworks/weave-npc:1.7.2""
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Pulled          Successfully pulled image ""weaveworks/weave-npc:1.7.2""

  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Created         Created container with docker id 2395225b6469; Security:[seccomp=unconfined]
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Started         Started container with docker id 2395225b6469
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Created         Created container with docker id 8c4256f19b85; Security:[seccomp=unconfined]
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Started         Started container with docker id 8c4256f19b85
  36m           36m             3       {kubelet solaris-worker-1}                                      Warning         FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 10s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 8bba1d69d16b
  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 8bba1d69d16b; Security:[seccomp=unconfined]
  36m   36m     2       {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 20s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 88c6006d13c3
  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 88c6006d13c3; Security:[seccomp=unconfined]
  36m   35m     5       {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 40s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  35m   35m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 06dad5fefd0c; Security:[seccomp=unconfined]
  35m   35m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 06dad5fefd0c
  35m   34m     7       {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 1m20s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  34m   34m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id d8e315b09473; Security:[seccomp=unconfined]
  34m   34m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id d8e315b09473
  33m   31m     14      {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 2m40s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  31m   31m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 49ca69fe4aa7; Security:[seccomp=unconfined]
  31m   31m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 49ca69fe4aa7
  26m   26m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 5f4f6e79bf1b; Security:[seccomp=unconfined]
  26m   26m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 5f4f6e79bf1b
  20m   16s     5       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         (events with common reason combined)
  20m   16s     5       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         (events with common reason combined)
  36m   16s     12      {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Pulled          Container image ""weaveworks/weave-kube:1.7.2"" already present on machine
  31m   6s      142     {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 5m0s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  36m   6s      173     {kubelet solaris-worker-1}      spec.containers{weave}  Warning BackOff Back-off restarting failed docker container
```

``` bash
root@solaris-master:~# kubectl logs weave-net-4c3a4 --namespace=kube-system
Error from server: a container name must be specified for pod weave-net-4c3a4, choose one of: [weave weave-npc]
```

``` bash
root@solaris-master:~# kubectl describe pod kube-dns-2247936740-ubzbm --namespace=kube-system
Name:           kube-dns-2247936740-ubzbm
Namespace:      kube-system
Node:           solaris-master/10.0.2.15
Start Time:     Sat, 22 Oct 2016 16:19:37 -0400
Labels:         component=kube-dns
                k8s-app=kube-dns
                kubernetes.io/cluster-service=true
                name=kube-dns
                pod-template-hash=2247936740
                tier=node
Status:         Pending
IP:
Controllers:    ReplicaSet/kube-dns-2247936740
Containers:
  kube-dns:
    Container ID:
    Image:              gcr.io/google_containers/kubedns-amd64:1.7
    Image ID:
    Ports:              10053/UDP, 10053/TCP
    Args:
      --domain=cluster.local
      --dns-port=10053
    Limits:
      cpu:      100m
      memory:   170Mi
    Requests:
      cpu:              100m
      memory:           170Mi
    State:              Waiting
      Reason:           ContainerCreating
    Ready:              False
    Restart Count:      0
    Liveness:           http-get http://:8080/healthz delay=60s timeout=5s period=10s #success=1 #failure=1
    Readiness:          http-get http://:8081/readiness delay=30s timeout=5s period=10s #success=1 #failure=3
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
  dnsmasq:
    Container ID:
    Image:              gcr.io/google_containers/kube-dnsmasq-amd64:1.3
    Image ID:
    Ports:              53/UDP, 53/TCP
    Args:
      --cache-size=1000
      --no-resolv
      --server=127.0.0.1#10053
    Limits:
      cpu:      100m
      memory:   170Mi
    Requests:
      cpu:              100m
      memory:           170Mi
    State:              Waiting
      Reason:           ContainerCreating
    Ready:              False
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
  healthz:
    Container ID:
    Image:              gcr.io/google_containers/exechealthz-amd64:1.1
    Image ID:
    Port:               8080/TCP
    Args:
      -cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:53 >/dev/null && nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 >/dev/null
      -port=8080
      -quiet
    Limits:
      cpu:      10m
      memory:   50Mi
    Requests:
      cpu:              10m
      memory:           50Mi
    State:              Waiting
      Reason:           ContainerCreating
    Ready:              False
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
Conditions:
  Type          Status
  Initialized   True
  Ready         False
  PodScheduled  True
Volumes:
  default-token-eejd4:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-eejd4
QoS Class:      Guaranteed
Tolerations:    dedicated=master:NoSchedule
Events:
  FirstSeen     LastSeen        Count   From                            SubobjectPath   Type            Reason          Message
  ---------     --------        -----   ----                            -------------   --------        ------          -------
  55m           55m             1       {default-scheduler }                            Normal          Scheduled       Successfully assigned kube-dns-2247936740-ubzbm to solaris-master
  55m           39m             694     {kubelet solaris-master}                        Warning         FailedSync      Error syncing pod, skipping: failed to ""SetupNetwork"" for ""kube-dns-2247936740-ubzbm_kube-system"" with SetupNetworkError: ""Failed to setup network for pod \""kube-dns-2247936740-ubzbm_kube-system(dac6ff63-9894-11e6-b6af-080027d4fd28)\"" using network plugins \""cni\"": cni config unintialized; Skipping pod""
```

``` bash
root@solaris-master:~# kubectl logs kube-dns-2247936740-ubzbm --namespace=kube-system
Error from server: a container name must be specified for pod kube-dns-2247936740-ubzbm, choose one of: [kube-dns dnsmasq healthz]
```
",cc47b290-9896-11e6-b6af-080027d4fd28,https://github.com/kubernetes/kubernetes
171,35378,"Hi, 

I'm trying to build a test environment which in the end will be deployed to our production server; however I encountered some issues that, for now, I consider bugs.

Please find my install scripts @ https://github.com/itmcdev/solaris.

Following the valid code from `provision.sh`, I followed the instructions from http://kubernetes.io/docs/getting-started-guides/kubeadm/ running:

``` bash
# for master
kubeadm init --api-advertise-addresses=10.0.3.10

# for workers
kubeadm join --token e36420.92c6e1d4959fb8eb 10.0.3.10

# than, for master
kubectl apply -f https://git.io/weave-kube
```

Following this, I noticed that kube-dns and weave-kube images were persisting in ContainerCreating and CrashLoopBackOff statuses. I will let you decide further, whether this is a bug, or a mistake generated by me. To note, I'm a total noob under the process of learning.

``` bash
root@solaris-master:~# kubectl get nodes
NAME               STATUS    AGE
solaris-master     Ready     52m
solaris-worker-1   Ready     50m
solaris-worker-2   Ready     50m
```

``` bash
root@solaris-master:~# kubectl get pods --all-namespaces
NAMESPACE     NAME                                     READY     STATUS              RESTARTS   AGE
kube-system   etcd-solaris-master                      1/1       Running             0          51m
kube-system   kube-apiserver-solaris-master            1/1       Running             0          52m
kube-system   kube-controller-manager-solaris-master   1/1       Running             0          52m
kube-system   kube-discovery-982812725-wm3o2           1/1       Running             0          29m
kube-system   kube-dns-2247936740-ubzbm                0/3       ContainerCreating   0          51m
kube-system   kube-proxy-amd64-3jabb                   1/1       Running             0          50m
kube-system   kube-proxy-amd64-rt2f9                   1/1       Running             0          50m
kube-system   kube-proxy-amd64-ygc2p                   1/1       Running             0          51m
kube-system   kube-scheduler-solaris-master            1/1       Running             0          51m
kube-system   weave-net-4c3a4                          1/2       CrashLoopBackOff    11         37m
kube-system   weave-net-yvv71                          2/2       Running             0          37m
kube-system   weave-net-z9aqe                          1/2       CrashLoopBackOff    11         37m
```

``` bash
root@solaris-master:~# kubectl describe pod weave-net-4c3a4 --namespace=kube-system
Name:           weave-net-4c3a4
Namespace:      kube-system
Node:           solaris-worker-1/10.0.2.15
Start Time:     Sat, 22 Oct 2016 16:33:32 -0400
Labels:         name=weave-net
Status:         Running
IP:             10.0.2.15
Controllers:    DaemonSet/weave-net
Containers:
  weave:
    Container ID:       docker://a254dce85d8a027d9d3de1de080ca6322806e05fda2bc72a87f1d5b3cc936f35
    Image:              weaveworks/weave-kube:1.7.2
    Image ID:           docker://sha256:9a2aa48020f51f3bc736e63a888441979dbcba394debdc1abb8e43cf95449168
    Port:
    Command:
      /home/weave/launch.sh
    Requests:
      cpu:              10m
    State:              Waiting
      Reason:           CrashLoopBackOff
    Last State:         Terminated
      Reason:           Error
      Exit Code:        1
      Started:          Sat, 22 Oct 2016 17:11:32 -0400
      Finished:         Sat, 22 Oct 2016 17:11:32 -0400
    Ready:              False
    Restart Count:      12
    Liveness:           http-get http://127.0.0.1:6784/status delay=30s timeout=1s period=10s #success=1 #failure=3
    Volume Mounts:
      /etc from cni-conf (rw)
      /host_home from cni-bin2 (rw)
      /opt from cni-bin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
      /weavedb from weavedb (rw)
    Environment Variables:      <none>
  weave-npc:
    Container ID:       docker://2395225b64692a0a4caf63903ec5cd4ca55b0fc60989623c165a747acd456068
    Image:              weaveworks/weave-npc:1.7.2
    Image ID:           docker://sha256:63a7347dde435cccc849d0eacc1a11c17733b6e557e67ce56efd8264f11e5d8c
    Port:
    Requests:
      cpu:              10m
    State:              Running
      Started:          Sat, 22 Oct 2016 16:34:49 -0400
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
Conditions:
  Type          Status
  Initialized   True
  Ready         False
  PodScheduled  True
Volumes:
  weavedb:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
  cni-bin:
    Type:       HostPath (bare host directory volume)
    Path:       /opt
  cni-bin2:
    Type:       HostPath (bare host directory volume)
    Path:       /home
  cni-conf:
    Type:       HostPath (bare host directory volume)
    Path:       /etc
  default-token-eejd4:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-eejd4
QoS Class:      Burstable
Tolerations:    dedicated=master:Equal:NoSchedule
Events:
  FirstSeen     LastSeen        Count   From                            SubobjectPath                   Type            Reason          Message
  ---------     --------        -----   ----                            -------------                   --------        ------          -------
  38m           38m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Pulling         pulling image ""weaveworks/weave-kube:1.7.2""
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Pulled          Successfully pulled image ""weaveworks/weave-kube:1.7.2""
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Created         Created container with docker id 06f090f2775f; Security:[seccomp=unconfined]
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Started         Started container with docker id 06f090f2775f
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Pulling         pulling image ""weaveworks/weave-npc:1.7.2""
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Pulled          Successfully pulled image ""weaveworks/weave-npc:1.7.2""

  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Created         Created container with docker id 2395225b6469; Security:[seccomp=unconfined]
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Started         Started container with docker id 2395225b6469
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Created         Created container with docker id 8c4256f19b85; Security:[seccomp=unconfined]
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Started         Started container with docker id 8c4256f19b85
  36m           36m             3       {kubelet solaris-worker-1}                                      Warning         FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 10s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 8bba1d69d16b
  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 8bba1d69d16b; Security:[seccomp=unconfined]
  36m   36m     2       {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 20s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 88c6006d13c3
  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 88c6006d13c3; Security:[seccomp=unconfined]
  36m   35m     5       {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 40s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  35m   35m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 06dad5fefd0c; Security:[seccomp=unconfined]
  35m   35m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 06dad5fefd0c
  35m   34m     7       {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 1m20s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  34m   34m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id d8e315b09473; Security:[seccomp=unconfined]
  34m   34m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id d8e315b09473
  33m   31m     14      {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 2m40s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  31m   31m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 49ca69fe4aa7; Security:[seccomp=unconfined]
  31m   31m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 49ca69fe4aa7
  26m   26m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 5f4f6e79bf1b; Security:[seccomp=unconfined]
  26m   26m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 5f4f6e79bf1b
  20m   16s     5       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         (events with common reason combined)
  20m   16s     5       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         (events with common reason combined)
  36m   16s     12      {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Pulled          Container image ""weaveworks/weave-kube:1.7.2"" already present on machine
  31m   6s      142     {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 5m0s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  36m   6s      173     {kubelet solaris-worker-1}      spec.containers{weave}  Warning BackOff Back-off restarting failed docker container
```

``` bash
root@solaris-master:~# kubectl logs weave-net-4c3a4 --namespace=kube-system
Error from server: a container name must be specified for pod weave-net-4c3a4, choose one of: [weave weave-npc]
```

``` bash
root@solaris-master:~# kubectl describe pod kube-dns-2247936740-ubzbm --namespace=kube-system
Name:           kube-dns-2247936740-ubzbm
Namespace:      kube-system
Node:           solaris-master/10.0.2.15
Start Time:     Sat, 22 Oct 2016 16:19:37 -0400
Labels:         component=kube-dns
                k8s-app=kube-dns
                kubernetes.io/cluster-service=true
                name=kube-dns
                pod-template-hash=2247936740
                tier=node
Status:         Pending
IP:
Controllers:    ReplicaSet/kube-dns-2247936740
Containers:
  kube-dns:
    Container ID:
    Image:              gcr.io/google_containers/kubedns-amd64:1.7
    Image ID:
    Ports:              10053/UDP, 10053/TCP
    Args:
      --domain=cluster.local
      --dns-port=10053
    Limits:
      cpu:      100m
      memory:   170Mi
    Requests:
      cpu:              100m
      memory:           170Mi
    State:              Waiting
      Reason:           ContainerCreating
    Ready:              False
    Restart Count:      0
    Liveness:           http-get http://:8080/healthz delay=60s timeout=5s period=10s #success=1 #failure=1
    Readiness:          http-get http://:8081/readiness delay=30s timeout=5s period=10s #success=1 #failure=3
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
  dnsmasq:
    Container ID:
    Image:              gcr.io/google_containers/kube-dnsmasq-amd64:1.3
    Image ID:
    Ports:              53/UDP, 53/TCP
    Args:
      --cache-size=1000
      --no-resolv
      --server=127.0.0.1#10053
    Limits:
      cpu:      100m
      memory:   170Mi
    Requests:
      cpu:              100m
      memory:           170Mi
    State:              Waiting
      Reason:           ContainerCreating
    Ready:              False
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
  healthz:
    Container ID:
    Image:              gcr.io/google_containers/exechealthz-amd64:1.1
    Image ID:
    Port:               8080/TCP
    Args:
      -cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:53 >/dev/null && nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 >/dev/null
      -port=8080
      -quiet
    Limits:
      cpu:      10m
      memory:   50Mi
    Requests:
      cpu:              10m
      memory:           50Mi
    State:              Waiting
      Reason:           ContainerCreating
    Ready:              False
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
Conditions:
  Type          Status
  Initialized   True
  Ready         False
  PodScheduled  True
Volumes:
  default-token-eejd4:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-eejd4
QoS Class:      Guaranteed
Tolerations:    dedicated=master:NoSchedule
Events:
  FirstSeen     LastSeen        Count   From                            SubobjectPath   Type            Reason          Message
  ---------     --------        -----   ----                            -------------   --------        ------          -------
  55m           55m             1       {default-scheduler }                            Normal          Scheduled       Successfully assigned kube-dns-2247936740-ubzbm to solaris-master
  55m           39m             694     {kubelet solaris-master}                        Warning         FailedSync      Error syncing pod, skipping: failed to ""SetupNetwork"" for ""kube-dns-2247936740-ubzbm_kube-system"" with SetupNetworkError: ""Failed to setup network for pod \""kube-dns-2247936740-ubzbm_kube-system(dac6ff63-9894-11e6-b6af-080027d4fd28)\"" using network plugins \""cni\"": cni config unintialized; Skipping pod""
```

``` bash
root@solaris-master:~# kubectl logs kube-dns-2247936740-ubzbm --namespace=kube-system
Error from server: a container name must be specified for pod kube-dns-2247936740-ubzbm, choose one of: [kube-dns dnsmasq healthz]
```
",dac6ff63-9894-11e6-b6af-080027d4fd28,https://github.com/kubernetes/kubernetes
172,35378,"Hi, 

I'm trying to build a test environment which in the end will be deployed to our production server; however I encountered some issues that, for now, I consider bugs.

Please find my install scripts @ https://github.com/itmcdev/solaris.

Following the valid code from `provision.sh`, I followed the instructions from http://kubernetes.io/docs/getting-started-guides/kubeadm/ running:

``` bash
# for master
kubeadm init --api-advertise-addresses=10.0.3.10

# for workers
kubeadm join --token e36420.92c6e1d4959fb8eb 10.0.3.10

# than, for master
kubectl apply -f https://git.io/weave-kube
```

Following this, I noticed that kube-dns and weave-kube images were persisting in ContainerCreating and CrashLoopBackOff statuses. I will let you decide further, whether this is a bug, or a mistake generated by me. To note, I'm a total noob under the process of learning.

``` bash
root@solaris-master:~# kubectl get nodes
NAME               STATUS    AGE
solaris-master     Ready     52m
solaris-worker-1   Ready     50m
solaris-worker-2   Ready     50m
```

``` bash
root@solaris-master:~# kubectl get pods --all-namespaces
NAMESPACE     NAME                                     READY     STATUS              RESTARTS   AGE
kube-system   etcd-solaris-master                      1/1       Running             0          51m
kube-system   kube-apiserver-solaris-master            1/1       Running             0          52m
kube-system   kube-controller-manager-solaris-master   1/1       Running             0          52m
kube-system   kube-discovery-982812725-wm3o2           1/1       Running             0          29m
kube-system   kube-dns-2247936740-ubzbm                0/3       ContainerCreating   0          51m
kube-system   kube-proxy-amd64-3jabb                   1/1       Running             0          50m
kube-system   kube-proxy-amd64-rt2f9                   1/1       Running             0          50m
kube-system   kube-proxy-amd64-ygc2p                   1/1       Running             0          51m
kube-system   kube-scheduler-solaris-master            1/1       Running             0          51m
kube-system   weave-net-4c3a4                          1/2       CrashLoopBackOff    11         37m
kube-system   weave-net-yvv71                          2/2       Running             0          37m
kube-system   weave-net-z9aqe                          1/2       CrashLoopBackOff    11         37m
```

``` bash
root@solaris-master:~# kubectl describe pod weave-net-4c3a4 --namespace=kube-system
Name:           weave-net-4c3a4
Namespace:      kube-system
Node:           solaris-worker-1/10.0.2.15
Start Time:     Sat, 22 Oct 2016 16:33:32 -0400
Labels:         name=weave-net
Status:         Running
IP:             10.0.2.15
Controllers:    DaemonSet/weave-net
Containers:
  weave:
    Container ID:       docker://a254dce85d8a027d9d3de1de080ca6322806e05fda2bc72a87f1d5b3cc936f35
    Image:              weaveworks/weave-kube:1.7.2
    Image ID:           docker://sha256:9a2aa48020f51f3bc736e63a888441979dbcba394debdc1abb8e43cf95449168
    Port:
    Command:
      /home/weave/launch.sh
    Requests:
      cpu:              10m
    State:              Waiting
      Reason:           CrashLoopBackOff
    Last State:         Terminated
      Reason:           Error
      Exit Code:        1
      Started:          Sat, 22 Oct 2016 17:11:32 -0400
      Finished:         Sat, 22 Oct 2016 17:11:32 -0400
    Ready:              False
    Restart Count:      12
    Liveness:           http-get http://127.0.0.1:6784/status delay=30s timeout=1s period=10s #success=1 #failure=3
    Volume Mounts:
      /etc from cni-conf (rw)
      /host_home from cni-bin2 (rw)
      /opt from cni-bin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
      /weavedb from weavedb (rw)
    Environment Variables:      <none>
  weave-npc:
    Container ID:       docker://2395225b64692a0a4caf63903ec5cd4ca55b0fc60989623c165a747acd456068
    Image:              weaveworks/weave-npc:1.7.2
    Image ID:           docker://sha256:63a7347dde435cccc849d0eacc1a11c17733b6e557e67ce56efd8264f11e5d8c
    Port:
    Requests:
      cpu:              10m
    State:              Running
      Started:          Sat, 22 Oct 2016 16:34:49 -0400
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
Conditions:
  Type          Status
  Initialized   True
  Ready         False
  PodScheduled  True
Volumes:
  weavedb:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
  cni-bin:
    Type:       HostPath (bare host directory volume)
    Path:       /opt
  cni-bin2:
    Type:       HostPath (bare host directory volume)
    Path:       /home
  cni-conf:
    Type:       HostPath (bare host directory volume)
    Path:       /etc
  default-token-eejd4:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-eejd4
QoS Class:      Burstable
Tolerations:    dedicated=master:Equal:NoSchedule
Events:
  FirstSeen     LastSeen        Count   From                            SubobjectPath                   Type            Reason          Message
  ---------     --------        -----   ----                            -------------                   --------        ------          -------
  38m           38m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Pulling         pulling image ""weaveworks/weave-kube:1.7.2""
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Pulled          Successfully pulled image ""weaveworks/weave-kube:1.7.2""
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Created         Created container with docker id 06f090f2775f; Security:[seccomp=unconfined]
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Started         Started container with docker id 06f090f2775f
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Pulling         pulling image ""weaveworks/weave-npc:1.7.2""
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Pulled          Successfully pulled image ""weaveworks/weave-npc:1.7.2""

  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Created         Created container with docker id 2395225b6469; Security:[seccomp=unconfined]
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Started         Started container with docker id 2395225b6469
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Created         Created container with docker id 8c4256f19b85; Security:[seccomp=unconfined]
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Started         Started container with docker id 8c4256f19b85
  36m           36m             3       {kubelet solaris-worker-1}                                      Warning         FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 10s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 8bba1d69d16b
  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 8bba1d69d16b; Security:[seccomp=unconfined]
  36m   36m     2       {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 20s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 88c6006d13c3
  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 88c6006d13c3; Security:[seccomp=unconfined]
  36m   35m     5       {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 40s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  35m   35m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 06dad5fefd0c; Security:[seccomp=unconfined]
  35m   35m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 06dad5fefd0c
  35m   34m     7       {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 1m20s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  34m   34m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id d8e315b09473; Security:[seccomp=unconfined]
  34m   34m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id d8e315b09473
  33m   31m     14      {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 2m40s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  31m   31m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 49ca69fe4aa7; Security:[seccomp=unconfined]
  31m   31m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 49ca69fe4aa7
  26m   26m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 5f4f6e79bf1b; Security:[seccomp=unconfined]
  26m   26m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 5f4f6e79bf1b
  20m   16s     5       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         (events with common reason combined)
  20m   16s     5       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         (events with common reason combined)
  36m   16s     12      {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Pulled          Container image ""weaveworks/weave-kube:1.7.2"" already present on machine
  31m   6s      142     {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 5m0s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  36m   6s      173     {kubelet solaris-worker-1}      spec.containers{weave}  Warning BackOff Back-off restarting failed docker container
```

``` bash
root@solaris-master:~# kubectl logs weave-net-4c3a4 --namespace=kube-system
Error from server: a container name must be specified for pod weave-net-4c3a4, choose one of: [weave weave-npc]
```

``` bash
root@solaris-master:~# kubectl describe pod kube-dns-2247936740-ubzbm --namespace=kube-system
Name:           kube-dns-2247936740-ubzbm
Namespace:      kube-system
Node:           solaris-master/10.0.2.15
Start Time:     Sat, 22 Oct 2016 16:19:37 -0400
Labels:         component=kube-dns
                k8s-app=kube-dns
                kubernetes.io/cluster-service=true
                name=kube-dns
                pod-template-hash=2247936740
                tier=node
Status:         Pending
IP:
Controllers:    ReplicaSet/kube-dns-2247936740
Containers:
  kube-dns:
    Container ID:
    Image:              gcr.io/google_containers/kubedns-amd64:1.7
    Image ID:
    Ports:              10053/UDP, 10053/TCP
    Args:
      --domain=cluster.local
      --dns-port=10053
    Limits:
      cpu:      100m
      memory:   170Mi
    Requests:
      cpu:              100m
      memory:           170Mi
    State:              Waiting
      Reason:           ContainerCreating
    Ready:              False
    Restart Count:      0
    Liveness:           http-get http://:8080/healthz delay=60s timeout=5s period=10s #success=1 #failure=1
    Readiness:          http-get http://:8081/readiness delay=30s timeout=5s period=10s #success=1 #failure=3
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
  dnsmasq:
    Container ID:
    Image:              gcr.io/google_containers/kube-dnsmasq-amd64:1.3
    Image ID:
    Ports:              53/UDP, 53/TCP
    Args:
      --cache-size=1000
      --no-resolv
      --server=127.0.0.1#10053
    Limits:
      cpu:      100m
      memory:   170Mi
    Requests:
      cpu:              100m
      memory:           170Mi
    State:              Waiting
      Reason:           ContainerCreating
    Ready:              False
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
  healthz:
    Container ID:
    Image:              gcr.io/google_containers/exechealthz-amd64:1.1
    Image ID:
    Port:               8080/TCP
    Args:
      -cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:53 >/dev/null && nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 >/dev/null
      -port=8080
      -quiet
    Limits:
      cpu:      10m
      memory:   50Mi
    Requests:
      cpu:              10m
      memory:           50Mi
    State:              Waiting
      Reason:           ContainerCreating
    Ready:              False
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
Conditions:
  Type          Status
  Initialized   True
  Ready         False
  PodScheduled  True
Volumes:
  default-token-eejd4:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-eejd4
QoS Class:      Guaranteed
Tolerations:    dedicated=master:NoSchedule
Events:
  FirstSeen     LastSeen        Count   From                            SubobjectPath   Type            Reason          Message
  ---------     --------        -----   ----                            -------------   --------        ------          -------
  55m           55m             1       {default-scheduler }                            Normal          Scheduled       Successfully assigned kube-dns-2247936740-ubzbm to solaris-master
  55m           39m             694     {kubelet solaris-master}                        Warning         FailedSync      Error syncing pod, skipping: failed to ""SetupNetwork"" for ""kube-dns-2247936740-ubzbm_kube-system"" with SetupNetworkError: ""Failed to setup network for pod \""kube-dns-2247936740-ubzbm_kube-system(dac6ff63-9894-11e6-b6af-080027d4fd28)\"" using network plugins \""cni\"": cni config unintialized; Skipping pod""
```

``` bash
root@solaris-master:~# kubectl logs kube-dns-2247936740-ubzbm --namespace=kube-system
Error from server: a container name must be specified for pod kube-dns-2247936740-ubzbm, choose one of: [kube-dns dnsmasq healthz]
```
",default-token-eejd4,https://github.com/kubernetes/kubernetes
173,33617,"It hits the apiserver proxy on GKE, which fails with: 

```
'http://10.244.0.6:80/guestbook.php?cmd=get&key=messages&value='"") has prevented the request from succeeding (get services frontend), response: 
Sep 27 15:36:19.157: INFO: Failed to get response from guestbook. err: an error on the server (""Error: 'No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-4b1917fb2f43ed3cbed7\""?'\nTrying to reach: 'http://10.244.0.6:80/guestbook.php?cmd=get&key=messages&value='"") has prevented the request from succeeding (get services frontend), response: 
Sep 27 15:36:24.162: INFO: Failed to get response from guestbook. err: an error on the server (""Error: 'No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-4b1917fb2f43ed3cbed7\""?'\nTrying to reach: 'http://10.244.0.6:80/guestbook.php?cmd=get&key=messages&value='"") has prevented the request from succeeding (get services frontend), response: 
Sep 27 15:36:29.166: INFO: Failed to get response from guestbook. err: an error on the server (""Error: 'No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-4b1917fb2f43ed3cbed7\""?'\nTrying to reach: 'http://10.244.0.6:80/guestbook.php?cmd=get&key=messages&value='"") has prevented the request from succeeding (get services frontend), response: 
Sep 27 15:36:34.173: INFO: Failed to get response from guestbook. err: an error on the server (""Error: 'No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-4b1917fb2f43ed3cbed7\""?'\nTrying to reach: 'http://10.244.1.6:80/guestbook.php?cmd=get&key=messages&value='"") has prevented the request from succeeding (get services frontend), response: 
Sep 27 15:36:39.179: INFO: Failed to get response from guestbook. err: an error on the server (""Error: 'No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-4b1917fb2f43ed3cbed7\""?'\nTrying to reach: 'http://10.244.1.6:80/guestbook.php?cmd=get&key=messages&value='"") has prevented the request from succeeding (get services frontend), response: 
Sep 27 15:36:44.184: INFO: Failed to get response from guestbook. err: an error on the server (""Error: 'No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-4b1917fb2f43ed3cbed7\""?'\nTrying to reach: 'http://10.244.1.6:80/guestbook.php?cmd=get&key=messages&value='"") has prevented the request from succeeding (get services frontend), response: 
```

We could just not hit the apiserver proxy but create a service of type=lb instead. That is actually more representative of real world app usage, but the proxy issue seems specific to gke and ssh-tunnels. 
",4b1917fb2f43ed3cbed7\?\nTrying,https://github.com/kubernetes/kubernetes
174,28617,"Syptom: loading environment variables from keys within a secret does not allow loading of keys with a value that is a BOOL or an INT of any size.

Version:  1.2.4 confirmed

Explaination:

sample secret:
testbool=true
testint=8080
teststring=hello

```
apiVersion: v1
kind: Secret
metadata:
  name: test.secret
type: Opaque
data:
  testbool: dHJ1ZQ==
  testint: ODA4MA==
  teststring: aGVsbG8=
```

in kubernetes:

```
apiVersion: v1
data:
  testbool: dHJ1ZQ==
  testint: ODA4MA==
  teststring: aGVsbG8=
kind: Secret
metadata:
  annotations:
  name: test.secret
  namespace: default
  resourceVersion: ""4178761""
  selfLink: /api/v1/namespaces/default/secrets/test.secret
  uid: b258f770-42e2-11e6-9fee-0ac53f35edc7
type: Opaque
```

Sample deployment:

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: test
  labels:
    app: test
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: test
        tier: dev
    spec:
      imagePullSecrets:
      - name: mypullsecret
      containers:
      - name: testapp1
        image: us.gcr.io/account-name/app:version
        ports:
        - containerPort: 8080
        env:
          - name: testbool
            valueFrom:
              secretKeyRef:
                name: test.secret
                key: testbool
          - name: testint
            valueFrom:
              secretKeyRef:
                name: test.secret
                key: testint
          - name: teststring
            valueFrom:
              secretKeyRef:
                name: test.secret
                key: teststring
```

in Kubernetes:

```
metadata:
  generation: 104
  labels:
    app: test
  name: test
  namespace: default
  resourceVersion: ""4178941""
  selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/test
  uid: f74be004-3ca4-11e6-b887-06500d373dfb
spec:
  replicas: 3
  selector:
    matchLabels:
      app: test
      tier: dev
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: test
        tier: dev
    spec:
      containers:
        env:
          - name: teststring
            valueFrom:
              secretKeyRef:
                name: test.secret
                key: teststring
        image: us.gcr.io/account-name/app:version
        imagePullPolicy: IfNotPresent
        name: test1
        ports:
        - containerPort: 8080
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
      dnsPolicy: ClusterFirst
      imagePullSecrets:
      - name: mypullsecret
      restartPolicy: Always
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 3
  observedGeneration: 104
  replicas: 3
  updatedReplicas: 3
```

As you can see, using a kubectl apply -f to upload my deployment, the deployment was created completely correctly except they environment variables that were pure INT or BOOL types. The secret is uploaded correctly as seen above, and the deployment works flawlessly except it does not pass those environment vars to the container. I have logged into the container and verified it does not set them at all.

On top of this, Kubectl does not mention any errors with the deployment or the secret- as well as the logs for Kubernetes saying nothing at all bout anything. I have no idea where the vars are being dropped, in all honestly.

This is very repeatable, I can change the bool to ""false"" and it does the same, and well as changing the ""int"" to any other number of any size (verified random numbers up to 1000000000000). Understandably, raw strings and bools wouldn't typically be considered secrets, however we have a software key we need to consume that is a pure string, as well as us wishing to keep certain true/false configurations sensitive at times.

I can load these BOOLs and INTs on secret volumes, but that is far from ideal.
",teststring=hello,https://github.com/kubernetes/kubernetes
175,23149,"**KubeProxy should test kube-proxy [Slow]**

_/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubeproxy.go:105 Expected error: <errors.errorString | 0xc208554840>: { s: ""Error running &{/jenkins-master-data/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.105.26 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/.kube/config exec --namespace=e2e-tests-e2e-kubeproxy-dtjqf host-test-container-pod -- /bin/sh -c for i in $(seq 1 5); do echo 'hostName' | timeout -t 3 nc -w 1 -u 10.180.0.5 8081; echo; sleep 1s; done | grep -v '^\s*$' |sort | uniq -c | wc -l] [] Error from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-c5d9e45b81848219014b\""?\n [] 0xc2084e48e0 exit status 1 true [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82e0 0xc2083e8300] [0x95de00 0x95de00] 0xc2083d5f20}:\nCommand stdout:\n\nstderr:\nError from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-c5d9e45b81848219014b\""?\n\nerror:\nexit status 1\n"", } Error running &{/jenkins-master-data/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.105.26 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/.kube/config exec --namespace=e2e-tests-e2e-kubeproxy-dtjqf host-test-container-pod -- /bin/sh -c for i in $(seq 1 5); do echo 'hostName' | timeout -t 3 nc -w 1 -u 10.180.0.5 8081; echo; sleep 1s; done | grep -v '^\s*$' |sort | uniq -c | wc -l] [] Error from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user ""gke-c5d9e45b81848219014b""? [] 0xc2084e48e0 exit status 1 true [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82e0 0xc2083e8300] [0x95de00 0x95de00] 0xc2083d5f20}: Command stdout: stderr: Error from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user ""gke-c5d9e45b81848219014b""? error: exit status 1 not to have occurred_

e.g.:
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/97
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/80
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/55
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/46
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/14
",c5d9e45b81848219014b\?\n,https://github.com/kubernetes/kubernetes
176,23149,"**KubeProxy should test kube-proxy [Slow]**

_/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubeproxy.go:105 Expected error: <errors.errorString | 0xc208554840>: { s: ""Error running &{/jenkins-master-data/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.105.26 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/.kube/config exec --namespace=e2e-tests-e2e-kubeproxy-dtjqf host-test-container-pod -- /bin/sh -c for i in $(seq 1 5); do echo 'hostName' | timeout -t 3 nc -w 1 -u 10.180.0.5 8081; echo; sleep 1s; done | grep -v '^\s*$' |sort | uniq -c | wc -l] [] Error from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-c5d9e45b81848219014b\""?\n [] 0xc2084e48e0 exit status 1 true [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82e0 0xc2083e8300] [0x95de00 0x95de00] 0xc2083d5f20}:\nCommand stdout:\n\nstderr:\nError from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-c5d9e45b81848219014b\""?\n\nerror:\nexit status 1\n"", } Error running &{/jenkins-master-data/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.105.26 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/.kube/config exec --namespace=e2e-tests-e2e-kubeproxy-dtjqf host-test-container-pod -- /bin/sh -c for i in $(seq 1 5); do echo 'hostName' | timeout -t 3 nc -w 1 -u 10.180.0.5 8081; echo; sleep 1s; done | grep -v '^\s*$' |sort | uniq -c | wc -l] [] Error from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user ""gke-c5d9e45b81848219014b""? [] 0xc2084e48e0 exit status 1 true [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82e0 0xc2083e8300] [0x95de00 0x95de00] 0xc2083d5f20}: Command stdout: stderr: Error from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user ""gke-c5d9e45b81848219014b""? error: exit status 1 not to have occurred_

e.g.:
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/97
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/80
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/55
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/46
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/14
",\?\n\nerror:\nexit,https://github.com/kubernetes/kubernetes
177,23149,"**KubeProxy should test kube-proxy [Slow]**

_/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubeproxy.go:105 Expected error: <errors.errorString | 0xc208554840>: { s: ""Error running &{/jenkins-master-data/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.105.26 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/.kube/config exec --namespace=e2e-tests-e2e-kubeproxy-dtjqf host-test-container-pod -- /bin/sh -c for i in $(seq 1 5); do echo 'hostName' | timeout -t 3 nc -w 1 -u 10.180.0.5 8081; echo; sleep 1s; done | grep -v '^\s*$' |sort | uniq -c | wc -l] [] Error from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-c5d9e45b81848219014b\""?\n [] 0xc2084e48e0 exit status 1 true [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82e0 0xc2083e8300] [0x95de00 0x95de00] 0xc2083d5f20}:\nCommand stdout:\n\nstderr:\nError from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-c5d9e45b81848219014b\""?\n\nerror:\nexit status 1\n"", } Error running &{/jenkins-master-data/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.105.26 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/.kube/config exec --namespace=e2e-tests-e2e-kubeproxy-dtjqf host-test-container-pod -- /bin/sh -c for i in $(seq 1 5); do echo 'hostName' | timeout -t 3 nc -w 1 -u 10.180.0.5 8081; echo; sleep 1s; done | grep -v '^\s*$' |sort | uniq -c | wc -l] [] Error from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user ""gke-c5d9e45b81848219014b""? [] 0xc2084e48e0 exit status 1 true [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82e0 0xc2083e8300] [0x95de00 0x95de00] 0xc2083d5f20}: Command stdout: stderr: Error from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user ""gke-c5d9e45b81848219014b""? error: exit status 1 not to have occurred_

e.g.:
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/97
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/80
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/55
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/46
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/14
",c5d9e45b81848219014b,https://github.com/kubernetes/kubernetes
178,21530,"[https://console.cloud.google.com/m/cloudstorage/b/kubernetes-jenkins/o/pr-logs/pull/21369/kubernetes-pull-build-test-e2e-gce/29340/build-log.txt](https://console.cloud.google.com/m/cloudstorage/b/kubernetes-jenkins/o/pr-logs/pull/21369/kubernetes-pull-build-test-e2e-gce/29340/build-log.txt)

```
鈥� Failure [11.605 seconds]
ServiceAccounts
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/service_accounts.go:124
  should mount an API token into pods [Conformance] [It]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/service_accounts.go:123

  ""content of file \""/var/run/secrets/kubernetes.io/serviceaccount/token\"": eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJlMmUtdGVzdHMtc3ZjYWNjb3VudHMtY2pkZDIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiZGVmYXVsdC10b2tlbi1ldWE1ZiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYTQyMjI4ZjEtZDZkMi0xMWU1LWE0MmYtNDIwMTBhZjAwMDAyIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmUyZS10ZXN0cy1zdmNhY2NvdW50cy1jamRkMjpkZWZhdWx0In0.T_dLwS5K_x3rrmuC3iP-LY_lmPPiTxTnjais6tQkR-wkSR5XGzTnJW1xBCwsBXS2uGZi97Dug_l6i29cWt8TvRITMgQjLF5IT5Fi68FyRB0JfrBkCcDKRhPP8tH69rSNMWkuntpVZs3cXARlWPPuEXmxUPxUfPuhM8H1baYLg7i0hJveHVqNY5tlWEgYe7ujPUdxNvwUwTWtG3Lna-MgMtq7m5A5iEe9yh5ixeFM8URA_kHoAEQlxdKMR83XVkCfbKJxAUGuTvuA8PQ5fi8Wr6vFz1sAX8_NRTk0hstIYtCh-Y5xa5aHot75bKLkBFcGSFqgtUGX2_4beGIIOxZ3Cg"" in container output
  Expected
      <string>: content of file ""/var/run/secrets/kubernetes.io/serviceaccount/token"": eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJlMmUtdGVzdHMtc3ZjYWNjb3VudHMtY2pkZDIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiZGVmYXVsdC10b2tlbi1vMzZ5OCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYTQyMjI4ZjEtZDZkMi0xMWU1LWE0MmYtNDIwMTBhZjAwMDAyIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmUyZS10ZXN0cy1zdmNhY2NvdW50cy1jamRkMjpkZWZhdWx0In0.Wy5Iksh4SgXa32ERH5VRVbg-DsZ4SCEdfXGRdM6Nh_a5uhYRRyhCo5w6vRdi39dKeiVTFBwiHVJap2RQUQ6ivMo8_0O_lWlEsbVtFO7CC2cWKIEm-Pjx5958_3sv6kw8QW3ZM6-CWk7669idz-ApGzx-hkrlREvle0HoO2wErPAR4s_X5VIveOjDT5eW1shzazAKSWs1FjxHIw_oeVntyxcRW6i_PDMEDpXOJTC3vjIY2bTYJ9eHDEblXNgiwuIvb3lTQbTnV-1GDs0OPkhB52-jyHMOhK0PzkQp8jerGSF3fffiTu6h47riUWE5UfuCLHAU9RM0AKSZtDx5Z64xPA

  to contain substring
      <string>: content of file ""/var/run/secrets/kubernetes.io/serviceaccount/token"": eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJlMmUtdGVzdHMtc3ZjYWNjb3VudHMtY2pkZDIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiZGVmYXVsdC10b2tlbi1ldWE1ZiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYTQyMjI4ZjEtZDZkMi0xMWU1LWE0MmYtNDIwMTBhZjAwMDAyIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmUyZS10ZXN0cy1zdmNhY2NvdW50cy1jamRkMjpkZWZhdWx0In0.T_dLwS5K_x3rrmuC3iP-LY_lmPPiTxTnjais6tQkR-wkSR5XGzTnJW1xBCwsBXS2uGZi97Dug_l6i29cWt8TvRITMgQjLF5IT5Fi68FyRB0JfrBkCcDKRhPP8tH69rSNMWkuntpVZs3cXARlWPPuEXmxUPxUfPuhM8H1baYLg7i0hJveHVqNY5tlWEgYe7ujPUdxNvwUwTWtG3Lna-MgMtq7m5A5iEe9yh5ixeFM8URA_kHoAEQlxdKMR83XVkCfbKJxAUGuTvuA8PQ5fi8Wr6vFz1sAX8_NRTk0hstIYtCh-Y5xa5aHot75bKLkBFcGSFqgtUGX2_4beGIIOxZ3Cg
```
",LY_lmPPiTxTnjais6tQkR-wkSR5XGzTnJW1xBCwsBXS2uGZi97Dug_l6i29cWt8TvRITMgQjLF5IT5Fi68FyRB0JfrBkCcDKRhPP8tH69rSNMWkuntpVZs3cXARlWPPuEXmxUPxUfPuhM8H1baYLg7i0hJveHVqNY5tlWEgYe7ujPUdxNvwUwTWtG3Lna-MgMtq7m5A5iEe9yh5ixeFM8URA_kHoAEQlxdKMR83XVkCfbKJxAUGuTvuA8PQ5fi8Wr6vFz1sAX8_NRTk0hstIYtCh-Y5xa5aHot75bKLkBFcGSFqgtUGX2_4beGIIOxZ3Cg,https://github.com/kubernetes/kubernetes
179,20787,"https://console.developers.google.com/storage/browser/kubernetes-jenkins/logs/kubernetes-e2e-gce/11138/

```
• Failure [44.213 seconds]
ServiceAccounts
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/service_accounts.go:104
  should mount an API token into pods [Conformance] [It]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/service_accounts.go:103

  ""content of file \""/var/run/secrets/kubernetes.io/serviceaccount/token\"": eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJlMmUtdGVzdHMtc3ZjYWNjb3VudHMtNWQxZjMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiZGVmYXVsdC10b2tlbi15NGJ2aiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZTQyMDA3YWEtY2RiMS0xMWU1LWFlYmUtNDIwMTBhZjAwMDAyIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmUyZS10ZXN0cy1zdmNhY2NvdW50cy01ZDFmMzpkZWZhdWx0In0.ey9Ak0dgOEIsSwjEUwi__bOk999g2iU5wT-FDAegxF6Dlk4z3e01LTQYD6xx0B0OulAP_G3RI32QgP1lmIPtiChe3YXmGwLyez4mPzU_BnWcZu2X9jsXxjWNqsU2yZeG4wKlJJBmWmIqZDJDZplmcqR7pCnBeoQwqkFgbf00DhCRr6b9RX5UlUUayqQNf6kK9ewpGkCUxbeQ1F8Y401VhcUJSXAOJSxYWpk1TN0NL88kboQZ1kDa2958EFc7voObAtC4loSooJk1AsWGiL6d3YdoNdiPWICOvPlO_J2I-fGv8dkuap0y9Jp6O0YmwDsJjUrqZ8_FmhUJugQ5CyCRvA"" in container output
  Expected
      <string>: content of file ""/var/run/secrets/kubernetes.io/serviceaccount/token"": eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJlMmUtdGVzdHMtc3ZjYWNjb3VudHMtNWQxZjMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiZGVmYXVsdC10b2tlbi05emx2NiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZTQyMDA3YWEtY2RiMS0xMWU1LWFlYmUtNDIwMTBhZjAwMDAyIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmUyZS10ZXN0cy1zdmNhY2NvdW50cy01ZDFmMzpkZWZhdWx0In0.RFby-5cko4yvIoyY0cQwSvSi2sFzWIJRI5NryI1bAhcoQlSez51--beWUe1JWh8i3pV_CLN3P4qE_M_47o3sT6wNgQ1GuZ1UDY6z5xaNwpSQOvfpFOT5QxxP0ZD-s6kFluKsslt_A9tG1l4UV2YODmT_5CEO_xo0rKih6HHeYn3q965zLa47lY8nzv7-__dNoYdwDeCxmiErsm-31uedw3zsOj9FqH71Ij0L_9Tax72BYssP0Hey4WF_q8Xq0N-WPNPROCqiFYHePIyfSBfj5LkKi2jTt59bm8Lqp-2bc29ayt1YsY8Ofjnz98XPCapX5WG30O6ed3-OJOqHJFCooQ

  to contain substring
      <string>: content of file ""/var/run/secrets/kubernetes.io/serviceaccount/token"": eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJlMmUtdGVzdHMtc3ZjYWNjb3VudHMtNWQxZjMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiZGVmYXVsdC10b2tlbi15NGJ2aiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZTQyMDA3YWEtY2RiMS0xMWU1LWFlYmUtNDIwMTBhZjAwMDAyIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmUyZS10ZXN0cy1zdmNhY2NvdW50cy01ZDFmMzpkZWZhdWx0In0.ey9Ak0dgOEIsSwjEUwi__bOk999g2iU5wT-FDAegxF6Dlk4z3e01LTQYD6xx0B0OulAP_G3RI32QgP1lmIPtiChe3YXmGwLyez4mPzU_BnWcZu2X9jsXxjWNqsU2yZeG4wKlJJBmWmIqZDJDZplmcqR7pCnBeoQwqkFgbf00DhCRr6b9RX5UlUUayqQNf6kK9ewpGkCUxbeQ1F8Y401VhcUJSXAOJSxYWpk1TN0NL88kboQZ1kDa2958EFc7voObAtC4loSooJk1AsWGiL6d3YdoNdiPWICOvPlO_J2I-fGv8dkuap0y9Jp6O0YmwDsJjUrqZ8_FmhUJugQ5CyCRvA

```

Maybe similar to #20494 ?

`/var/run/secrets/kubernetes.io/serviceaccount/token` does contain a token, but it looks like it is a different token.
",FDAegxF6Dlk4z3e01LTQYD6xx0B0OulAP_G3RI32QgP1lmIPtiChe3YXmGwLyez4mPzU_BnWcZu2X9jsXxjWNqsU2yZeG4wKlJJBmWmIqZDJDZplmcqR7pCnBeoQwqkFgbf00DhCRr6b9RX5UlUUayqQNf6kK9ewpGkCUxbeQ1F8Y401VhcUJSXAOJSxYWpk1TN0NL88kboQZ1kDa2958EFc7voObAtC4loSooJk1AsWGiL6d3YdoNdiPWICOvPlO_J2I-fGv8dkuap0y9Jp6O0YmwDsJjUrqZ8_FmhUJugQ5CyCRvA,https://github.com/kubernetes/kubernetes
180,20787,"https://console.developers.google.com/storage/browser/kubernetes-jenkins/logs/kubernetes-e2e-gce/11138/

```
• Failure [44.213 seconds]
ServiceAccounts
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/service_accounts.go:104
  should mount an API token into pods [Conformance] [It]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/service_accounts.go:103

  ""content of file \""/var/run/secrets/kubernetes.io/serviceaccount/token\"": eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJlMmUtdGVzdHMtc3ZjYWNjb3VudHMtNWQxZjMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiZGVmYXVsdC10b2tlbi15NGJ2aiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZTQyMDA3YWEtY2RiMS0xMWU1LWFlYmUtNDIwMTBhZjAwMDAyIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmUyZS10ZXN0cy1zdmNhY2NvdW50cy01ZDFmMzpkZWZhdWx0In0.ey9Ak0dgOEIsSwjEUwi__bOk999g2iU5wT-FDAegxF6Dlk4z3e01LTQYD6xx0B0OulAP_G3RI32QgP1lmIPtiChe3YXmGwLyez4mPzU_BnWcZu2X9jsXxjWNqsU2yZeG4wKlJJBmWmIqZDJDZplmcqR7pCnBeoQwqkFgbf00DhCRr6b9RX5UlUUayqQNf6kK9ewpGkCUxbeQ1F8Y401VhcUJSXAOJSxYWpk1TN0NL88kboQZ1kDa2958EFc7voObAtC4loSooJk1AsWGiL6d3YdoNdiPWICOvPlO_J2I-fGv8dkuap0y9Jp6O0YmwDsJjUrqZ8_FmhUJugQ5CyCRvA"" in container output
  Expected
      <string>: content of file ""/var/run/secrets/kubernetes.io/serviceaccount/token"": eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJlMmUtdGVzdHMtc3ZjYWNjb3VudHMtNWQxZjMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiZGVmYXVsdC10b2tlbi05emx2NiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZTQyMDA3YWEtY2RiMS0xMWU1LWFlYmUtNDIwMTBhZjAwMDAyIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmUyZS10ZXN0cy1zdmNhY2NvdW50cy01ZDFmMzpkZWZhdWx0In0.RFby-5cko4yvIoyY0cQwSvSi2sFzWIJRI5NryI1bAhcoQlSez51--beWUe1JWh8i3pV_CLN3P4qE_M_47o3sT6wNgQ1GuZ1UDY6z5xaNwpSQOvfpFOT5QxxP0ZD-s6kFluKsslt_A9tG1l4UV2YODmT_5CEO_xo0rKih6HHeYn3q965zLa47lY8nzv7-__dNoYdwDeCxmiErsm-31uedw3zsOj9FqH71Ij0L_9Tax72BYssP0Hey4WF_q8Xq0N-WPNPROCqiFYHePIyfSBfj5LkKi2jTt59bm8Lqp-2bc29ayt1YsY8Ofjnz98XPCapX5WG30O6ed3-OJOqHJFCooQ

  to contain substring
      <string>: content of file ""/var/run/secrets/kubernetes.io/serviceaccount/token"": eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJlMmUtdGVzdHMtc3ZjYWNjb3VudHMtNWQxZjMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiZGVmYXVsdC10b2tlbi15NGJ2aiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZTQyMDA3YWEtY2RiMS0xMWU1LWFlYmUtNDIwMTBhZjAwMDAyIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmUyZS10ZXN0cy1zdmNhY2NvdW50cy01ZDFmMzpkZWZhdWx0In0.ey9Ak0dgOEIsSwjEUwi__bOk999g2iU5wT-FDAegxF6Dlk4z3e01LTQYD6xx0B0OulAP_G3RI32QgP1lmIPtiChe3YXmGwLyez4mPzU_BnWcZu2X9jsXxjWNqsU2yZeG4wKlJJBmWmIqZDJDZplmcqR7pCnBeoQwqkFgbf00DhCRr6b9RX5UlUUayqQNf6kK9ewpGkCUxbeQ1F8Y401VhcUJSXAOJSxYWpk1TN0NL88kboQZ1kDa2958EFc7voObAtC4loSooJk1AsWGiL6d3YdoNdiPWICOvPlO_J2I-fGv8dkuap0y9Jp6O0YmwDsJjUrqZ8_FmhUJugQ5CyCRvA

```

Maybe similar to #20494 ?

`/var/run/secrets/kubernetes.io/serviceaccount/token` does contain a token, but it looks like it is a different token.
",00DhCRr6b9RX5UlUUayqQNf6kK9ewpGkCUxbeQ1F8Y,https://github.com/kubernetes/kubernetes
181,14701,"The api doc shows:Kubernetes uses client certificates, tokens, or http basic auth to authenticate users for API calls.
So I tried to use basic auth to authenticate kubelet by set the --kubeconfig as config.yaml which generted by kubectl under ~/.kube/:

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: http://master:8080
  name: ubuntu
contexts:
- context:
    cluster: """"
    user: """"
  name: development
- context:
    cluster: ubuntu
    user: ubuntu
  name: ubuntu
current-context: ubuntu
kind: Config
preferences: {}
users:
- name: ubuntu
  user:
    password: wpJjc2rKyCoiP7tb
    username: admin
```

But it did not work, so I thought whether the kubelet can use basic auth?
",wpJjc2rKyCoiP7tb,https://github.com/kubernetes/kubernetes
182,14701,"The api doc shows:Kubernetes uses client certificates, tokens, or http basic auth to authenticate users for API calls.
So I tried to use basic auth to authenticate kubelet by set the --kubeconfig as config.yaml which generted by kubectl under ~/.kube/:

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: http://master:8080
  name: ubuntu
contexts:
- context:
    cluster: """"
    user: """"
  name: development
- context:
    cluster: ubuntu
    user: ubuntu
  name: ubuntu
current-context: ubuntu
kind: Config
preferences: {}
users:
- name: ubuntu
  user:
    password: wpJjc2rKyCoiP7tb
    username: admin
```

But it did not work, so I thought whether the kubelet can use basic auth?
",username,https://github.com/kubernetes/kubernetes
183,10247,"```
$ kubectl config view --minify
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://173.255.113.219
  name: kubernetes-satnam_e2e-test-satnam
contexts:
- context:
    cluster: kubernetes-satnam_e2e-test-satnam
    user: kubernetes-satnam_e2e-test-satnam
  name: kubernetes-satnam_e2e-test-satnam
current-context: kubernetes-satnam_e2e-test-satnam
kind: Config
preferences: {}
users:
- name: kubernetes-satnam_e2e-test-satnam
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
    token: LsUe2Z4cXqa61UQqQ2qWGGf7nOSLw9np
```
",LsUe2Z4cXqa61UQqQ2qWGGf7nOSLw9np,https://github.com/kubernetes/kubernetes
184,9483,"It seems creating a secret is only allowed if it's base64 encoded?  I have a secret that's PEM encoded, e.g.

```
apiVersion: v1beta3
kind: Secret
metadata:
  name: kube-keypair
data:
  kube-registry.crt: ""-----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm kvGkVwkfj+gR8NomUZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wY XYrhLEM357BxFRM1/G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ ltdI07XEPIgrGr5wvRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7 PutHNg3TIPSM4F4pmmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNb oHUT2RArHR1zJV47+NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O2 1+NM4DsjAgMBAAECggEAWNpciaJDWX2CUAMHsBbNfQgv1V02JeHaUebr4LvDJMCw tl0j/zdVwtby4ZbKcX96UdnmshTosPuZTUlPt2anmRv+0hrDX2pbUxTrUZ7U4yXc WIggQLebgP5pAS2qJOWS5Sk5K+kszJVE3PJuj3QIQ+by7ABahZPnS14usOyi2QUg eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB d7+9eypks7NA48hJt+NZZC9ZZXvNsRHdlVTueExtM9lJPZEDgkAUsd8wTx49pIs3 T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr 16cwb5Q38qu01o7R5pZ6KkigRt0FlMDn92hEnipPd/zJesoSTKGSeoTu7SsGF9IC kg7y6cO7RrWNQNwbM7dwN9jIq/kJN45oxClVNW73lxEwenYy4AIx5boeALoktToz tJ2Zj36b/LvDA//kyznqBa+JEwKBgQDD2gU8B46Jika6H2g+5Hmk9mNtHj6ZZIHc ijwkDXiq5uLLRwa+RX1OhhVFnStVoJ3QrhFJAGqJbz0GHiDnTu3yfQL7iQE/Oc0K HMk415o3pjk3VIYchOENlaYx6mfr5Og6+ZXlcgXUvN4lRdwDphwyM3L4EKrfnc3v C5TYv/9XsQKBgB7UWJeEz+mbPv1KuWjXpEBz22kHjyQq8hpIFbCex96AbrteuocV R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus 0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ UDJMfO0Tci8NbA7QBuRZnr5a5PNOfT3IO2ZcAGM94GKEznHSWt2d9yOZHc/xiBAn S39t7eoDxnzTzGjvP1qgGvMJUP6F09U2fEhkQ2ebWadHTkZ5srSW/WhYGK2veN81 A7Tuf9gkm/2OhwI01bIZdfbGo7refKz7btSPvcECgYEApN3h9grnhhZ9lvrGjPgj XAS6oAKmRZtLvlmvbKlZNdQceHDBDzUEn7JA4ASgGaq+WPQPimfmzdoXmRfEltUA BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2 Y9QiC6fdz+koJqq33yMwCy0= -----END PRIVATE KEY-----""
  kube-registry.key: ""-----BEGIN CERTIFICATE----- MIIDUDCCAjigAwIBAgIJAK87AEpK+g1OMA0GCSqGSIb3DQEBCwUAMB8xHTAbBgNV BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC95VjiVBvZCZLmkvGkVwkfj+gR8Nom UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1 /G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ltdI07XEPIgrGr5w vRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7PutHNg3TIPSM4F4p mmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNboHUT2RArHR1zJV47 +NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O21+NM4DsjAgMBAAGj gY4wgYswHQYDVR0OBBYEFBe5fmodCcmMKa3VU/dvLmf3AK7RME8GA1UdIwRIMEaA FBe5fmodCcmMKa3VU/dvLmf3AK7RoSOkITAfMR0wGwYDVQQDFBQxMjcuMC4wLjFA MTQzMjg0MDgzMoIJAK87AEpK+g1OMAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEG MA0GCSqGSIb3DQEBCwUAA4IBAQAQ2wCPmH9kTXC5JGNN/w6qOhVe4O48EnmI3ceL XNvb5aPHztZ73uP+NH5vrdJOr+86w3m6QeiAiivUlr5GmcDWutgaWhElrrwo9tY/ oJpJf3E8zzCGixONYlFQEFtDA1oUl8Nz8yvO3Eq58YJr5Gqwmbmf3CjxAuBA+GX5 YDPXr3YWu/AMekA6pcNZg92mYfDdNI8GzAkGXmGgSZfUELCXIDPpn2XIjuQvN+4J tg12xMOnDPAoFGXmqnQzwhHy68aMYzGUeHcDuGbYNUgd+AMey71uau6j76NW0JFl 9wZ2hsIRH18oZwAJFbOHPkHaRhXSvzFauKqYc6k629yUF0s+ -----END CERTIFICATE-----""
type: Opaque
```

kubectl create reports error:

```
error: unable to load file ""/tmp/secret.yaml"": unable to load ""/tmp/secret.yaml"": illegal base64 data at input byte 0
```

BTW, I'm not sure if puting certificate in secret is the right approach in k8s (or will work).  My use case is to mount certificates as secret volume, and set ssl_certificate directive in nginx to use that certificates. e.g.

```
      containers:
        - name: kube-nginx
          image: kube-nginx
          volumeMounts:
            - name: kube-keypair
              readOnly: true
              mountPath: /etc/nginx/ssl
      volumes:
        - name: kube-keypair
          secret:
            secretName: kube-keypair
```

and nginx conf

```
  ssl_certificate /etc/nginx/ssl/kube-registry.crt;
  ssl_certificate_key /etc/nginx/ssl/kube-registry.key;
```
",MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm,https://github.com/kubernetes/kubernetes
185,9483,"It seems creating a secret is only allowed if it's base64 encoded?  I have a secret that's PEM encoded, e.g.

```
apiVersion: v1beta3
kind: Secret
metadata:
  name: kube-keypair
data:
  kube-registry.crt: ""-----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm kvGkVwkfj+gR8NomUZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wY XYrhLEM357BxFRM1/G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ ltdI07XEPIgrGr5wvRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7 PutHNg3TIPSM4F4pmmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNb oHUT2RArHR1zJV47+NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O2 1+NM4DsjAgMBAAECggEAWNpciaJDWX2CUAMHsBbNfQgv1V02JeHaUebr4LvDJMCw tl0j/zdVwtby4ZbKcX96UdnmshTosPuZTUlPt2anmRv+0hrDX2pbUxTrUZ7U4yXc WIggQLebgP5pAS2qJOWS5Sk5K+kszJVE3PJuj3QIQ+by7ABahZPnS14usOyi2QUg eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB d7+9eypks7NA48hJt+NZZC9ZZXvNsRHdlVTueExtM9lJPZEDgkAUsd8wTx49pIs3 T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr 16cwb5Q38qu01o7R5pZ6KkigRt0FlMDn92hEnipPd/zJesoSTKGSeoTu7SsGF9IC kg7y6cO7RrWNQNwbM7dwN9jIq/kJN45oxClVNW73lxEwenYy4AIx5boeALoktToz tJ2Zj36b/LvDA//kyznqBa+JEwKBgQDD2gU8B46Jika6H2g+5Hmk9mNtHj6ZZIHc ijwkDXiq5uLLRwa+RX1OhhVFnStVoJ3QrhFJAGqJbz0GHiDnTu3yfQL7iQE/Oc0K HMk415o3pjk3VIYchOENlaYx6mfr5Og6+ZXlcgXUvN4lRdwDphwyM3L4EKrfnc3v C5TYv/9XsQKBgB7UWJeEz+mbPv1KuWjXpEBz22kHjyQq8hpIFbCex96AbrteuocV R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus 0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ UDJMfO0Tci8NbA7QBuRZnr5a5PNOfT3IO2ZcAGM94GKEznHSWt2d9yOZHc/xiBAn S39t7eoDxnzTzGjvP1qgGvMJUP6F09U2fEhkQ2ebWadHTkZ5srSW/WhYGK2veN81 A7Tuf9gkm/2OhwI01bIZdfbGo7refKz7btSPvcECgYEApN3h9grnhhZ9lvrGjPgj XAS6oAKmRZtLvlmvbKlZNdQceHDBDzUEn7JA4ASgGaq+WPQPimfmzdoXmRfEltUA BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2 Y9QiC6fdz+koJqq33yMwCy0= -----END PRIVATE KEY-----""
  kube-registry.key: ""-----BEGIN CERTIFICATE----- MIIDUDCCAjigAwIBAgIJAK87AEpK+g1OMA0GCSqGSIb3DQEBCwUAMB8xHTAbBgNV BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC95VjiVBvZCZLmkvGkVwkfj+gR8Nom UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1 /G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ltdI07XEPIgrGr5w vRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7PutHNg3TIPSM4F4p mmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNboHUT2RArHR1zJV47 +NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O21+NM4DsjAgMBAAGj gY4wgYswHQYDVR0OBBYEFBe5fmodCcmMKa3VU/dvLmf3AK7RME8GA1UdIwRIMEaA FBe5fmodCcmMKa3VU/dvLmf3AK7RoSOkITAfMR0wGwYDVQQDFBQxMjcuMC4wLjFA MTQzMjg0MDgzMoIJAK87AEpK+g1OMAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEG MA0GCSqGSIb3DQEBCwUAA4IBAQAQ2wCPmH9kTXC5JGNN/w6qOhVe4O48EnmI3ceL XNvb5aPHztZ73uP+NH5vrdJOr+86w3m6QeiAiivUlr5GmcDWutgaWhElrrwo9tY/ oJpJf3E8zzCGixONYlFQEFtDA1oUl8Nz8yvO3Eq58YJr5Gqwmbmf3CjxAuBA+GX5 YDPXr3YWu/AMekA6pcNZg92mYfDdNI8GzAkGXmGgSZfUELCXIDPpn2XIjuQvN+4J tg12xMOnDPAoFGXmqnQzwhHy68aMYzGUeHcDuGbYNUgd+AMey71uau6j76NW0JFl 9wZ2hsIRH18oZwAJFbOHPkHaRhXSvzFauKqYc6k629yUF0s+ -----END CERTIFICATE-----""
type: Opaque
```

kubectl create reports error:

```
error: unable to load file ""/tmp/secret.yaml"": unable to load ""/tmp/secret.yaml"": illegal base64 data at input byte 0
```

BTW, I'm not sure if puting certificate in secret is the right approach in k8s (or will work).  My use case is to mount certificates as secret volume, and set ssl_certificate directive in nginx to use that certificates. e.g.

```
      containers:
        - name: kube-nginx
          image: kube-nginx
          volumeMounts:
            - name: kube-keypair
              readOnly: true
              mountPath: /etc/nginx/ssl
      volumes:
        - name: kube-keypair
          secret:
            secretName: kube-keypair
```

and nginx conf

```
  ssl_certificate /etc/nginx/ssl/kube-registry.crt;
  ssl_certificate_key /etc/nginx/ssl/kube-registry.key;
```
",0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ,https://github.com/kubernetes/kubernetes
186,9483,"It seems creating a secret is only allowed if it's base64 encoded?  I have a secret that's PEM encoded, e.g.

```
apiVersion: v1beta3
kind: Secret
metadata:
  name: kube-keypair
data:
  kube-registry.crt: ""-----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm kvGkVwkfj+gR8NomUZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wY XYrhLEM357BxFRM1/G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ ltdI07XEPIgrGr5wvRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7 PutHNg3TIPSM4F4pmmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNb oHUT2RArHR1zJV47+NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O2 1+NM4DsjAgMBAAECggEAWNpciaJDWX2CUAMHsBbNfQgv1V02JeHaUebr4LvDJMCw tl0j/zdVwtby4ZbKcX96UdnmshTosPuZTUlPt2anmRv+0hrDX2pbUxTrUZ7U4yXc WIggQLebgP5pAS2qJOWS5Sk5K+kszJVE3PJuj3QIQ+by7ABahZPnS14usOyi2QUg eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB d7+9eypks7NA48hJt+NZZC9ZZXvNsRHdlVTueExtM9lJPZEDgkAUsd8wTx49pIs3 T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr 16cwb5Q38qu01o7R5pZ6KkigRt0FlMDn92hEnipPd/zJesoSTKGSeoTu7SsGF9IC kg7y6cO7RrWNQNwbM7dwN9jIq/kJN45oxClVNW73lxEwenYy4AIx5boeALoktToz tJ2Zj36b/LvDA//kyznqBa+JEwKBgQDD2gU8B46Jika6H2g+5Hmk9mNtHj6ZZIHc ijwkDXiq5uLLRwa+RX1OhhVFnStVoJ3QrhFJAGqJbz0GHiDnTu3yfQL7iQE/Oc0K HMk415o3pjk3VIYchOENlaYx6mfr5Og6+ZXlcgXUvN4lRdwDphwyM3L4EKrfnc3v C5TYv/9XsQKBgB7UWJeEz+mbPv1KuWjXpEBz22kHjyQq8hpIFbCex96AbrteuocV R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus 0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ UDJMfO0Tci8NbA7QBuRZnr5a5PNOfT3IO2ZcAGM94GKEznHSWt2d9yOZHc/xiBAn S39t7eoDxnzTzGjvP1qgGvMJUP6F09U2fEhkQ2ebWadHTkZ5srSW/WhYGK2veN81 A7Tuf9gkm/2OhwI01bIZdfbGo7refKz7btSPvcECgYEApN3h9grnhhZ9lvrGjPgj XAS6oAKmRZtLvlmvbKlZNdQceHDBDzUEn7JA4ASgGaq+WPQPimfmzdoXmRfEltUA BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2 Y9QiC6fdz+koJqq33yMwCy0= -----END PRIVATE KEY-----""
  kube-registry.key: ""-----BEGIN CERTIFICATE----- MIIDUDCCAjigAwIBAgIJAK87AEpK+g1OMA0GCSqGSIb3DQEBCwUAMB8xHTAbBgNV BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC95VjiVBvZCZLmkvGkVwkfj+gR8Nom UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1 /G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ltdI07XEPIgrGr5w vRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7PutHNg3TIPSM4F4p mmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNboHUT2RArHR1zJV47 +NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O21+NM4DsjAgMBAAGj gY4wgYswHQYDVR0OBBYEFBe5fmodCcmMKa3VU/dvLmf3AK7RME8GA1UdIwRIMEaA FBe5fmodCcmMKa3VU/dvLmf3AK7RoSOkITAfMR0wGwYDVQQDFBQxMjcuMC4wLjFA MTQzMjg0MDgzMoIJAK87AEpK+g1OMAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEG MA0GCSqGSIb3DQEBCwUAA4IBAQAQ2wCPmH9kTXC5JGNN/w6qOhVe4O48EnmI3ceL XNvb5aPHztZ73uP+NH5vrdJOr+86w3m6QeiAiivUlr5GmcDWutgaWhElrrwo9tY/ oJpJf3E8zzCGixONYlFQEFtDA1oUl8Nz8yvO3Eq58YJr5Gqwmbmf3CjxAuBA+GX5 YDPXr3YWu/AMekA6pcNZg92mYfDdNI8GzAkGXmGgSZfUELCXIDPpn2XIjuQvN+4J tg12xMOnDPAoFGXmqnQzwhHy68aMYzGUeHcDuGbYNUgd+AMey71uau6j76NW0JFl 9wZ2hsIRH18oZwAJFbOHPkHaRhXSvzFauKqYc6k629yUF0s+ -----END CERTIFICATE-----""
type: Opaque
```

kubectl create reports error:

```
error: unable to load file ""/tmp/secret.yaml"": unable to load ""/tmp/secret.yaml"": illegal base64 data at input byte 0
```

BTW, I'm not sure if puting certificate in secret is the right approach in k8s (or will work).  My use case is to mount certificates as secret volume, and set ssl_certificate directive in nginx to use that certificates. e.g.

```
      containers:
        - name: kube-nginx
          image: kube-nginx
          volumeMounts:
            - name: kube-keypair
              readOnly: true
              mountPath: /etc/nginx/ssl
      volumes:
        - name: kube-keypair
          secret:
            secretName: kube-keypair
```

and nginx conf

```
  ssl_certificate /etc/nginx/ssl/kube-registry.crt;
  ssl_certificate_key /etc/nginx/ssl/kube-registry.key;
```
",BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2,https://github.com/kubernetes/kubernetes
187,9483,"It seems creating a secret is only allowed if it's base64 encoded?  I have a secret that's PEM encoded, e.g.

```
apiVersion: v1beta3
kind: Secret
metadata:
  name: kube-keypair
data:
  kube-registry.crt: ""-----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm kvGkVwkfj+gR8NomUZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wY XYrhLEM357BxFRM1/G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ ltdI07XEPIgrGr5wvRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7 PutHNg3TIPSM4F4pmmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNb oHUT2RArHR1zJV47+NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O2 1+NM4DsjAgMBAAECggEAWNpciaJDWX2CUAMHsBbNfQgv1V02JeHaUebr4LvDJMCw tl0j/zdVwtby4ZbKcX96UdnmshTosPuZTUlPt2anmRv+0hrDX2pbUxTrUZ7U4yXc WIggQLebgP5pAS2qJOWS5Sk5K+kszJVE3PJuj3QIQ+by7ABahZPnS14usOyi2QUg eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB d7+9eypks7NA48hJt+NZZC9ZZXvNsRHdlVTueExtM9lJPZEDgkAUsd8wTx49pIs3 T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr 16cwb5Q38qu01o7R5pZ6KkigRt0FlMDn92hEnipPd/zJesoSTKGSeoTu7SsGF9IC kg7y6cO7RrWNQNwbM7dwN9jIq/kJN45oxClVNW73lxEwenYy4AIx5boeALoktToz tJ2Zj36b/LvDA//kyznqBa+JEwKBgQDD2gU8B46Jika6H2g+5Hmk9mNtHj6ZZIHc ijwkDXiq5uLLRwa+RX1OhhVFnStVoJ3QrhFJAGqJbz0GHiDnTu3yfQL7iQE/Oc0K HMk415o3pjk3VIYchOENlaYx6mfr5Og6+ZXlcgXUvN4lRdwDphwyM3L4EKrfnc3v C5TYv/9XsQKBgB7UWJeEz+mbPv1KuWjXpEBz22kHjyQq8hpIFbCex96AbrteuocV R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus 0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ UDJMfO0Tci8NbA7QBuRZnr5a5PNOfT3IO2ZcAGM94GKEznHSWt2d9yOZHc/xiBAn S39t7eoDxnzTzGjvP1qgGvMJUP6F09U2fEhkQ2ebWadHTkZ5srSW/WhYGK2veN81 A7Tuf9gkm/2OhwI01bIZdfbGo7refKz7btSPvcECgYEApN3h9grnhhZ9lvrGjPgj XAS6oAKmRZtLvlmvbKlZNdQceHDBDzUEn7JA4ASgGaq+WPQPimfmzdoXmRfEltUA BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2 Y9QiC6fdz+koJqq33yMwCy0= -----END PRIVATE KEY-----""
  kube-registry.key: ""-----BEGIN CERTIFICATE----- MIIDUDCCAjigAwIBAgIJAK87AEpK+g1OMA0GCSqGSIb3DQEBCwUAMB8xHTAbBgNV BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC95VjiVBvZCZLmkvGkVwkfj+gR8Nom UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1 /G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ltdI07XEPIgrGr5w vRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7PutHNg3TIPSM4F4p mmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNboHUT2RArHR1zJV47 +NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O21+NM4DsjAgMBAAGj gY4wgYswHQYDVR0OBBYEFBe5fmodCcmMKa3VU/dvLmf3AK7RME8GA1UdIwRIMEaA FBe5fmodCcmMKa3VU/dvLmf3AK7RoSOkITAfMR0wGwYDVQQDFBQxMjcuMC4wLjFA MTQzMjg0MDgzMoIJAK87AEpK+g1OMAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEG MA0GCSqGSIb3DQEBCwUAA4IBAQAQ2wCPmH9kTXC5JGNN/w6qOhVe4O48EnmI3ceL XNvb5aPHztZ73uP+NH5vrdJOr+86w3m6QeiAiivUlr5GmcDWutgaWhElrrwo9tY/ oJpJf3E8zzCGixONYlFQEFtDA1oUl8Nz8yvO3Eq58YJr5Gqwmbmf3CjxAuBA+GX5 YDPXr3YWu/AMekA6pcNZg92mYfDdNI8GzAkGXmGgSZfUELCXIDPpn2XIjuQvN+4J tg12xMOnDPAoFGXmqnQzwhHy68aMYzGUeHcDuGbYNUgd+AMey71uau6j76NW0JFl 9wZ2hsIRH18oZwAJFbOHPkHaRhXSvzFauKqYc6k629yUF0s+ -----END CERTIFICATE-----""
type: Opaque
```

kubectl create reports error:

```
error: unable to load file ""/tmp/secret.yaml"": unable to load ""/tmp/secret.yaml"": illegal base64 data at input byte 0
```

BTW, I'm not sure if puting certificate in secret is the right approach in k8s (or will work).  My use case is to mount certificates as secret volume, and set ssl_certificate directive in nginx to use that certificates. e.g.

```
      containers:
        - name: kube-nginx
          image: kube-nginx
          volumeMounts:
            - name: kube-keypair
              readOnly: true
              mountPath: /etc/nginx/ssl
      volumes:
        - name: kube-keypair
          secret:
            secretName: kube-keypair
```

and nginx conf

```
  ssl_certificate /etc/nginx/ssl/kube-registry.crt;
  ssl_certificate_key /etc/nginx/ssl/kube-registry.key;
```
",BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy,https://github.com/kubernetes/kubernetes
188,9483,"It seems creating a secret is only allowed if it's base64 encoded?  I have a secret that's PEM encoded, e.g.

```
apiVersion: v1beta3
kind: Secret
metadata:
  name: kube-keypair
data:
  kube-registry.crt: ""-----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm kvGkVwkfj+gR8NomUZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wY XYrhLEM357BxFRM1/G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ ltdI07XEPIgrGr5wvRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7 PutHNg3TIPSM4F4pmmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNb oHUT2RArHR1zJV47+NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O2 1+NM4DsjAgMBAAECggEAWNpciaJDWX2CUAMHsBbNfQgv1V02JeHaUebr4LvDJMCw tl0j/zdVwtby4ZbKcX96UdnmshTosPuZTUlPt2anmRv+0hrDX2pbUxTrUZ7U4yXc WIggQLebgP5pAS2qJOWS5Sk5K+kszJVE3PJuj3QIQ+by7ABahZPnS14usOyi2QUg eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB d7+9eypks7NA48hJt+NZZC9ZZXvNsRHdlVTueExtM9lJPZEDgkAUsd8wTx49pIs3 T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr 16cwb5Q38qu01o7R5pZ6KkigRt0FlMDn92hEnipPd/zJesoSTKGSeoTu7SsGF9IC kg7y6cO7RrWNQNwbM7dwN9jIq/kJN45oxClVNW73lxEwenYy4AIx5boeALoktToz tJ2Zj36b/LvDA//kyznqBa+JEwKBgQDD2gU8B46Jika6H2g+5Hmk9mNtHj6ZZIHc ijwkDXiq5uLLRwa+RX1OhhVFnStVoJ3QrhFJAGqJbz0GHiDnTu3yfQL7iQE/Oc0K HMk415o3pjk3VIYchOENlaYx6mfr5Og6+ZXlcgXUvN4lRdwDphwyM3L4EKrfnc3v C5TYv/9XsQKBgB7UWJeEz+mbPv1KuWjXpEBz22kHjyQq8hpIFbCex96AbrteuocV R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus 0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ UDJMfO0Tci8NbA7QBuRZnr5a5PNOfT3IO2ZcAGM94GKEznHSWt2d9yOZHc/xiBAn S39t7eoDxnzTzGjvP1qgGvMJUP6F09U2fEhkQ2ebWadHTkZ5srSW/WhYGK2veN81 A7Tuf9gkm/2OhwI01bIZdfbGo7refKz7btSPvcECgYEApN3h9grnhhZ9lvrGjPgj XAS6oAKmRZtLvlmvbKlZNdQceHDBDzUEn7JA4ASgGaq+WPQPimfmzdoXmRfEltUA BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2 Y9QiC6fdz+koJqq33yMwCy0= -----END PRIVATE KEY-----""
  kube-registry.key: ""-----BEGIN CERTIFICATE----- MIIDUDCCAjigAwIBAgIJAK87AEpK+g1OMA0GCSqGSIb3DQEBCwUAMB8xHTAbBgNV BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC95VjiVBvZCZLmkvGkVwkfj+gR8Nom UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1 /G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ltdI07XEPIgrGr5w vRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7PutHNg3TIPSM4F4p mmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNboHUT2RArHR1zJV47 +NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O21+NM4DsjAgMBAAGj gY4wgYswHQYDVR0OBBYEFBe5fmodCcmMKa3VU/dvLmf3AK7RME8GA1UdIwRIMEaA FBe5fmodCcmMKa3VU/dvLmf3AK7RoSOkITAfMR0wGwYDVQQDFBQxMjcuMC4wLjFA MTQzMjg0MDgzMoIJAK87AEpK+g1OMAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEG MA0GCSqGSIb3DQEBCwUAA4IBAQAQ2wCPmH9kTXC5JGNN/w6qOhVe4O48EnmI3ceL XNvb5aPHztZ73uP+NH5vrdJOr+86w3m6QeiAiivUlr5GmcDWutgaWhElrrwo9tY/ oJpJf3E8zzCGixONYlFQEFtDA1oUl8Nz8yvO3Eq58YJr5Gqwmbmf3CjxAuBA+GX5 YDPXr3YWu/AMekA6pcNZg92mYfDdNI8GzAkGXmGgSZfUELCXIDPpn2XIjuQvN+4J tg12xMOnDPAoFGXmqnQzwhHy68aMYzGUeHcDuGbYNUgd+AMey71uau6j76NW0JFl 9wZ2hsIRH18oZwAJFbOHPkHaRhXSvzFauKqYc6k629yUF0s+ -----END CERTIFICATE-----""
type: Opaque
```

kubectl create reports error:

```
error: unable to load file ""/tmp/secret.yaml"": unable to load ""/tmp/secret.yaml"": illegal base64 data at input byte 0
```

BTW, I'm not sure if puting certificate in secret is the right approach in k8s (or will work).  My use case is to mount certificates as secret volume, and set ssl_certificate directive in nginx to use that certificates. e.g.

```
      containers:
        - name: kube-nginx
          image: kube-nginx
          volumeMounts:
            - name: kube-keypair
              readOnly: true
              mountPath: /etc/nginx/ssl
      volumes:
        - name: kube-keypair
          secret:
            secretName: kube-keypair
```

and nginx conf

```
  ssl_certificate /etc/nginx/ssl/kube-registry.crt;
  ssl_certificate_key /etc/nginx/ssl/kube-registry.key;
```
",UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1,https://github.com/kubernetes/kubernetes
189,9483,"It seems creating a secret is only allowed if it's base64 encoded?  I have a secret that's PEM encoded, e.g.

```
apiVersion: v1beta3
kind: Secret
metadata:
  name: kube-keypair
data:
  kube-registry.crt: ""-----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm kvGkVwkfj+gR8NomUZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wY XYrhLEM357BxFRM1/G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ ltdI07XEPIgrGr5wvRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7 PutHNg3TIPSM4F4pmmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNb oHUT2RArHR1zJV47+NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O2 1+NM4DsjAgMBAAECggEAWNpciaJDWX2CUAMHsBbNfQgv1V02JeHaUebr4LvDJMCw tl0j/zdVwtby4ZbKcX96UdnmshTosPuZTUlPt2anmRv+0hrDX2pbUxTrUZ7U4yXc WIggQLebgP5pAS2qJOWS5Sk5K+kszJVE3PJuj3QIQ+by7ABahZPnS14usOyi2QUg eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB d7+9eypks7NA48hJt+NZZC9ZZXvNsRHdlVTueExtM9lJPZEDgkAUsd8wTx49pIs3 T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr 16cwb5Q38qu01o7R5pZ6KkigRt0FlMDn92hEnipPd/zJesoSTKGSeoTu7SsGF9IC kg7y6cO7RrWNQNwbM7dwN9jIq/kJN45oxClVNW73lxEwenYy4AIx5boeALoktToz tJ2Zj36b/LvDA//kyznqBa+JEwKBgQDD2gU8B46Jika6H2g+5Hmk9mNtHj6ZZIHc ijwkDXiq5uLLRwa+RX1OhhVFnStVoJ3QrhFJAGqJbz0GHiDnTu3yfQL7iQE/Oc0K HMk415o3pjk3VIYchOENlaYx6mfr5Og6+ZXlcgXUvN4lRdwDphwyM3L4EKrfnc3v C5TYv/9XsQKBgB7UWJeEz+mbPv1KuWjXpEBz22kHjyQq8hpIFbCex96AbrteuocV R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus 0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ UDJMfO0Tci8NbA7QBuRZnr5a5PNOfT3IO2ZcAGM94GKEznHSWt2d9yOZHc/xiBAn S39t7eoDxnzTzGjvP1qgGvMJUP6F09U2fEhkQ2ebWadHTkZ5srSW/WhYGK2veN81 A7Tuf9gkm/2OhwI01bIZdfbGo7refKz7btSPvcECgYEApN3h9grnhhZ9lvrGjPgj XAS6oAKmRZtLvlmvbKlZNdQceHDBDzUEn7JA4ASgGaq+WPQPimfmzdoXmRfEltUA BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2 Y9QiC6fdz+koJqq33yMwCy0= -----END PRIVATE KEY-----""
  kube-registry.key: ""-----BEGIN CERTIFICATE----- MIIDUDCCAjigAwIBAgIJAK87AEpK+g1OMA0GCSqGSIb3DQEBCwUAMB8xHTAbBgNV BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC95VjiVBvZCZLmkvGkVwkfj+gR8Nom UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1 /G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ltdI07XEPIgrGr5w vRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7PutHNg3TIPSM4F4p mmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNboHUT2RArHR1zJV47 +NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O21+NM4DsjAgMBAAGj gY4wgYswHQYDVR0OBBYEFBe5fmodCcmMKa3VU/dvLmf3AK7RME8GA1UdIwRIMEaA FBe5fmodCcmMKa3VU/dvLmf3AK7RoSOkITAfMR0wGwYDVQQDFBQxMjcuMC4wLjFA MTQzMjg0MDgzMoIJAK87AEpK+g1OMAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEG MA0GCSqGSIb3DQEBCwUAA4IBAQAQ2wCPmH9kTXC5JGNN/w6qOhVe4O48EnmI3ceL XNvb5aPHztZ73uP+NH5vrdJOr+86w3m6QeiAiivUlr5GmcDWutgaWhElrrwo9tY/ oJpJf3E8zzCGixONYlFQEFtDA1oUl8Nz8yvO3Eq58YJr5Gqwmbmf3CjxAuBA+GX5 YDPXr3YWu/AMekA6pcNZg92mYfDdNI8GzAkGXmGgSZfUELCXIDPpn2XIjuQvN+4J tg12xMOnDPAoFGXmqnQzwhHy68aMYzGUeHcDuGbYNUgd+AMey71uau6j76NW0JFl 9wZ2hsIRH18oZwAJFbOHPkHaRhXSvzFauKqYc6k629yUF0s+ -----END CERTIFICATE-----""
type: Opaque
```

kubectl create reports error:

```
error: unable to load file ""/tmp/secret.yaml"": unable to load ""/tmp/secret.yaml"": illegal base64 data at input byte 0
```

BTW, I'm not sure if puting certificate in secret is the right approach in k8s (or will work).  My use case is to mount certificates as secret volume, and set ssl_certificate directive in nginx to use that certificates. e.g.

```
      containers:
        - name: kube-nginx
          image: kube-nginx
          volumeMounts:
            - name: kube-keypair
              readOnly: true
              mountPath: /etc/nginx/ssl
      volumes:
        - name: kube-keypair
          secret:
            secretName: kube-keypair
```

and nginx conf

```
  ssl_certificate /etc/nginx/ssl/kube-registry.crt;
  ssl_certificate_key /etc/nginx/ssl/kube-registry.key;
```
",NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G,https://github.com/kubernetes/kubernetes
190,9483,"It seems creating a secret is only allowed if it's base64 encoded?  I have a secret that's PEM encoded, e.g.

```
apiVersion: v1beta3
kind: Secret
metadata:
  name: kube-keypair
data:
  kube-registry.crt: ""-----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm kvGkVwkfj+gR8NomUZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wY XYrhLEM357BxFRM1/G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ ltdI07XEPIgrGr5wvRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7 PutHNg3TIPSM4F4pmmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNb oHUT2RArHR1zJV47+NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O2 1+NM4DsjAgMBAAECggEAWNpciaJDWX2CUAMHsBbNfQgv1V02JeHaUebr4LvDJMCw tl0j/zdVwtby4ZbKcX96UdnmshTosPuZTUlPt2anmRv+0hrDX2pbUxTrUZ7U4yXc WIggQLebgP5pAS2qJOWS5Sk5K+kszJVE3PJuj3QIQ+by7ABahZPnS14usOyi2QUg eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB d7+9eypks7NA48hJt+NZZC9ZZXvNsRHdlVTueExtM9lJPZEDgkAUsd8wTx49pIs3 T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr 16cwb5Q38qu01o7R5pZ6KkigRt0FlMDn92hEnipPd/zJesoSTKGSeoTu7SsGF9IC kg7y6cO7RrWNQNwbM7dwN9jIq/kJN45oxClVNW73lxEwenYy4AIx5boeALoktToz tJ2Zj36b/LvDA//kyznqBa+JEwKBgQDD2gU8B46Jika6H2g+5Hmk9mNtHj6ZZIHc ijwkDXiq5uLLRwa+RX1OhhVFnStVoJ3QrhFJAGqJbz0GHiDnTu3yfQL7iQE/Oc0K HMk415o3pjk3VIYchOENlaYx6mfr5Og6+ZXlcgXUvN4lRdwDphwyM3L4EKrfnc3v C5TYv/9XsQKBgB7UWJeEz+mbPv1KuWjXpEBz22kHjyQq8hpIFbCex96AbrteuocV R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus 0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ UDJMfO0Tci8NbA7QBuRZnr5a5PNOfT3IO2ZcAGM94GKEznHSWt2d9yOZHc/xiBAn S39t7eoDxnzTzGjvP1qgGvMJUP6F09U2fEhkQ2ebWadHTkZ5srSW/WhYGK2veN81 A7Tuf9gkm/2OhwI01bIZdfbGo7refKz7btSPvcECgYEApN3h9grnhhZ9lvrGjPgj XAS6oAKmRZtLvlmvbKlZNdQceHDBDzUEn7JA4ASgGaq+WPQPimfmzdoXmRfEltUA BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2 Y9QiC6fdz+koJqq33yMwCy0= -----END PRIVATE KEY-----""
  kube-registry.key: ""-----BEGIN CERTIFICATE----- MIIDUDCCAjigAwIBAgIJAK87AEpK+g1OMA0GCSqGSIb3DQEBCwUAMB8xHTAbBgNV BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC95VjiVBvZCZLmkvGkVwkfj+gR8Nom UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1 /G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ltdI07XEPIgrGr5w vRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7PutHNg3TIPSM4F4p mmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNboHUT2RArHR1zJV47 +NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O21+NM4DsjAgMBAAGj gY4wgYswHQYDVR0OBBYEFBe5fmodCcmMKa3VU/dvLmf3AK7RME8GA1UdIwRIMEaA FBe5fmodCcmMKa3VU/dvLmf3AK7RoSOkITAfMR0wGwYDVQQDFBQxMjcuMC4wLjFA MTQzMjg0MDgzMoIJAK87AEpK+g1OMAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEG MA0GCSqGSIb3DQEBCwUAA4IBAQAQ2wCPmH9kTXC5JGNN/w6qOhVe4O48EnmI3ceL XNvb5aPHztZ73uP+NH5vrdJOr+86w3m6QeiAiivUlr5GmcDWutgaWhElrrwo9tY/ oJpJf3E8zzCGixONYlFQEFtDA1oUl8Nz8yvO3Eq58YJr5Gqwmbmf3CjxAuBA+GX5 YDPXr3YWu/AMekA6pcNZg92mYfDdNI8GzAkGXmGgSZfUELCXIDPpn2XIjuQvN+4J tg12xMOnDPAoFGXmqnQzwhHy68aMYzGUeHcDuGbYNUgd+AMey71uau6j76NW0JFl 9wZ2hsIRH18oZwAJFbOHPkHaRhXSvzFauKqYc6k629yUF0s+ -----END CERTIFICATE-----""
type: Opaque
```

kubectl create reports error:

```
error: unable to load file ""/tmp/secret.yaml"": unable to load ""/tmp/secret.yaml"": illegal base64 data at input byte 0
```

BTW, I'm not sure if puting certificate in secret is the right approach in k8s (or will work).  My use case is to mount certificates as secret volume, and set ssl_certificate directive in nginx to use that certificates. e.g.

```
      containers:
        - name: kube-nginx
          image: kube-nginx
          volumeMounts:
            - name: kube-keypair
              readOnly: true
              mountPath: /etc/nginx/ssl
      volumes:
        - name: kube-keypair
          secret:
            secretName: kube-keypair
```

and nginx conf

```
  ssl_certificate /etc/nginx/ssl/kube-registry.crt;
  ssl_certificate_key /etc/nginx/ssl/kube-registry.key;
```
",T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr,https://github.com/kubernetes/kubernetes
191,9483,"It seems creating a secret is only allowed if it's base64 encoded?  I have a secret that's PEM encoded, e.g.

```
apiVersion: v1beta3
kind: Secret
metadata:
  name: kube-keypair
data:
  kube-registry.crt: ""-----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm kvGkVwkfj+gR8NomUZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wY XYrhLEM357BxFRM1/G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ ltdI07XEPIgrGr5wvRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7 PutHNg3TIPSM4F4pmmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNb oHUT2RArHR1zJV47+NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O2 1+NM4DsjAgMBAAECggEAWNpciaJDWX2CUAMHsBbNfQgv1V02JeHaUebr4LvDJMCw tl0j/zdVwtby4ZbKcX96UdnmshTosPuZTUlPt2anmRv+0hrDX2pbUxTrUZ7U4yXc WIggQLebgP5pAS2qJOWS5Sk5K+kszJVE3PJuj3QIQ+by7ABahZPnS14usOyi2QUg eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB d7+9eypks7NA48hJt+NZZC9ZZXvNsRHdlVTueExtM9lJPZEDgkAUsd8wTx49pIs3 T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr 16cwb5Q38qu01o7R5pZ6KkigRt0FlMDn92hEnipPd/zJesoSTKGSeoTu7SsGF9IC kg7y6cO7RrWNQNwbM7dwN9jIq/kJN45oxClVNW73lxEwenYy4AIx5boeALoktToz tJ2Zj36b/LvDA//kyznqBa+JEwKBgQDD2gU8B46Jika6H2g+5Hmk9mNtHj6ZZIHc ijwkDXiq5uLLRwa+RX1OhhVFnStVoJ3QrhFJAGqJbz0GHiDnTu3yfQL7iQE/Oc0K HMk415o3pjk3VIYchOENlaYx6mfr5Og6+ZXlcgXUvN4lRdwDphwyM3L4EKrfnc3v C5TYv/9XsQKBgB7UWJeEz+mbPv1KuWjXpEBz22kHjyQq8hpIFbCex96AbrteuocV R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus 0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ UDJMfO0Tci8NbA7QBuRZnr5a5PNOfT3IO2ZcAGM94GKEznHSWt2d9yOZHc/xiBAn S39t7eoDxnzTzGjvP1qgGvMJUP6F09U2fEhkQ2ebWadHTkZ5srSW/WhYGK2veN81 A7Tuf9gkm/2OhwI01bIZdfbGo7refKz7btSPvcECgYEApN3h9grnhhZ9lvrGjPgj XAS6oAKmRZtLvlmvbKlZNdQceHDBDzUEn7JA4ASgGaq+WPQPimfmzdoXmRfEltUA BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2 Y9QiC6fdz+koJqq33yMwCy0= -----END PRIVATE KEY-----""
  kube-registry.key: ""-----BEGIN CERTIFICATE----- MIIDUDCCAjigAwIBAgIJAK87AEpK+g1OMA0GCSqGSIb3DQEBCwUAMB8xHTAbBgNV BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC95VjiVBvZCZLmkvGkVwkfj+gR8Nom UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1 /G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ltdI07XEPIgrGr5w vRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7PutHNg3TIPSM4F4p mmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNboHUT2RArHR1zJV47 +NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O21+NM4DsjAgMBAAGj gY4wgYswHQYDVR0OBBYEFBe5fmodCcmMKa3VU/dvLmf3AK7RME8GA1UdIwRIMEaA FBe5fmodCcmMKa3VU/dvLmf3AK7RoSOkITAfMR0wGwYDVQQDFBQxMjcuMC4wLjFA MTQzMjg0MDgzMoIJAK87AEpK+g1OMAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEG MA0GCSqGSIb3DQEBCwUAA4IBAQAQ2wCPmH9kTXC5JGNN/w6qOhVe4O48EnmI3ceL XNvb5aPHztZ73uP+NH5vrdJOr+86w3m6QeiAiivUlr5GmcDWutgaWhElrrwo9tY/ oJpJf3E8zzCGixONYlFQEFtDA1oUl8Nz8yvO3Eq58YJr5Gqwmbmf3CjxAuBA+GX5 YDPXr3YWu/AMekA6pcNZg92mYfDdNI8GzAkGXmGgSZfUELCXIDPpn2XIjuQvN+4J tg12xMOnDPAoFGXmqnQzwhHy68aMYzGUeHcDuGbYNUgd+AMey71uau6j76NW0JFl 9wZ2hsIRH18oZwAJFbOHPkHaRhXSvzFauKqYc6k629yUF0s+ -----END CERTIFICATE-----""
type: Opaque
```

kubectl create reports error:

```
error: unable to load file ""/tmp/secret.yaml"": unable to load ""/tmp/secret.yaml"": illegal base64 data at input byte 0
```

BTW, I'm not sure if puting certificate in secret is the right approach in k8s (or will work).  My use case is to mount certificates as secret volume, and set ssl_certificate directive in nginx to use that certificates. e.g.

```
      containers:
        - name: kube-nginx
          image: kube-nginx
          volumeMounts:
            - name: kube-keypair
              readOnly: true
              mountPath: /etc/nginx/ssl
      volumes:
        - name: kube-keypair
          secret:
            secretName: kube-keypair
```

and nginx conf

```
  ssl_certificate /etc/nginx/ssl/kube-registry.crt;
  ssl_certificate_key /etc/nginx/ssl/kube-registry.key;
```
",R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus,https://github.com/kubernetes/kubernetes
192,9483,"It seems creating a secret is only allowed if it's base64 encoded?  I have a secret that's PEM encoded, e.g.

```
apiVersion: v1beta3
kind: Secret
metadata:
  name: kube-keypair
data:
  kube-registry.crt: ""-----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm kvGkVwkfj+gR8NomUZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wY XYrhLEM357BxFRM1/G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ ltdI07XEPIgrGr5wvRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7 PutHNg3TIPSM4F4pmmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNb oHUT2RArHR1zJV47+NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O2 1+NM4DsjAgMBAAECggEAWNpciaJDWX2CUAMHsBbNfQgv1V02JeHaUebr4LvDJMCw tl0j/zdVwtby4ZbKcX96UdnmshTosPuZTUlPt2anmRv+0hrDX2pbUxTrUZ7U4yXc WIggQLebgP5pAS2qJOWS5Sk5K+kszJVE3PJuj3QIQ+by7ABahZPnS14usOyi2QUg eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB d7+9eypks7NA48hJt+NZZC9ZZXvNsRHdlVTueExtM9lJPZEDgkAUsd8wTx49pIs3 T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr 16cwb5Q38qu01o7R5pZ6KkigRt0FlMDn92hEnipPd/zJesoSTKGSeoTu7SsGF9IC kg7y6cO7RrWNQNwbM7dwN9jIq/kJN45oxClVNW73lxEwenYy4AIx5boeALoktToz tJ2Zj36b/LvDA//kyznqBa+JEwKBgQDD2gU8B46Jika6H2g+5Hmk9mNtHj6ZZIHc ijwkDXiq5uLLRwa+RX1OhhVFnStVoJ3QrhFJAGqJbz0GHiDnTu3yfQL7iQE/Oc0K HMk415o3pjk3VIYchOENlaYx6mfr5Og6+ZXlcgXUvN4lRdwDphwyM3L4EKrfnc3v C5TYv/9XsQKBgB7UWJeEz+mbPv1KuWjXpEBz22kHjyQq8hpIFbCex96AbrteuocV R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus 0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ UDJMfO0Tci8NbA7QBuRZnr5a5PNOfT3IO2ZcAGM94GKEznHSWt2d9yOZHc/xiBAn S39t7eoDxnzTzGjvP1qgGvMJUP6F09U2fEhkQ2ebWadHTkZ5srSW/WhYGK2veN81 A7Tuf9gkm/2OhwI01bIZdfbGo7refKz7btSPvcECgYEApN3h9grnhhZ9lvrGjPgj XAS6oAKmRZtLvlmvbKlZNdQceHDBDzUEn7JA4ASgGaq+WPQPimfmzdoXmRfEltUA BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2 Y9QiC6fdz+koJqq33yMwCy0= -----END PRIVATE KEY-----""
  kube-registry.key: ""-----BEGIN CERTIFICATE----- MIIDUDCCAjigAwIBAgIJAK87AEpK+g1OMA0GCSqGSIb3DQEBCwUAMB8xHTAbBgNV BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC95VjiVBvZCZLmkvGkVwkfj+gR8Nom UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1 /G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ltdI07XEPIgrGr5w vRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7PutHNg3TIPSM4F4p mmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNboHUT2RArHR1zJV47 +NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O21+NM4DsjAgMBAAGj gY4wgYswHQYDVR0OBBYEFBe5fmodCcmMKa3VU/dvLmf3AK7RME8GA1UdIwRIMEaA FBe5fmodCcmMKa3VU/dvLmf3AK7RoSOkITAfMR0wGwYDVQQDFBQxMjcuMC4wLjFA MTQzMjg0MDgzMoIJAK87AEpK+g1OMAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEG MA0GCSqGSIb3DQEBCwUAA4IBAQAQ2wCPmH9kTXC5JGNN/w6qOhVe4O48EnmI3ceL XNvb5aPHztZ73uP+NH5vrdJOr+86w3m6QeiAiivUlr5GmcDWutgaWhElrrwo9tY/ oJpJf3E8zzCGixONYlFQEFtDA1oUl8Nz8yvO3Eq58YJr5Gqwmbmf3CjxAuBA+GX5 YDPXr3YWu/AMekA6pcNZg92mYfDdNI8GzAkGXmGgSZfUELCXIDPpn2XIjuQvN+4J tg12xMOnDPAoFGXmqnQzwhHy68aMYzGUeHcDuGbYNUgd+AMey71uau6j76NW0JFl 9wZ2hsIRH18oZwAJFbOHPkHaRhXSvzFauKqYc6k629yUF0s+ -----END CERTIFICATE-----""
type: Opaque
```

kubectl create reports error:

```
error: unable to load file ""/tmp/secret.yaml"": unable to load ""/tmp/secret.yaml"": illegal base64 data at input byte 0
```

BTW, I'm not sure if puting certificate in secret is the right approach in k8s (or will work).  My use case is to mount certificates as secret volume, and set ssl_certificate directive in nginx to use that certificates. e.g.

```
      containers:
        - name: kube-nginx
          image: kube-nginx
          volumeMounts:
            - name: kube-keypair
              readOnly: true
              mountPath: /etc/nginx/ssl
      volumes:
        - name: kube-keypair
          secret:
            secretName: kube-keypair
```

and nginx conf

```
  ssl_certificate /etc/nginx/ssl/kube-registry.crt;
  ssl_certificate_key /etc/nginx/ssl/kube-registry.key;
```
",eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB,https://github.com/kubernetes/kubernetes
193,9483,"It seems creating a secret is only allowed if it's base64 encoded?  I have a secret that's PEM encoded, e.g.

```
apiVersion: v1beta3
kind: Secret
metadata:
  name: kube-keypair
data:
  kube-registry.crt: ""-----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm kvGkVwkfj+gR8NomUZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wY XYrhLEM357BxFRM1/G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ ltdI07XEPIgrGr5wvRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7 PutHNg3TIPSM4F4pmmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNb oHUT2RArHR1zJV47+NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O2 1+NM4DsjAgMBAAECggEAWNpciaJDWX2CUAMHsBbNfQgv1V02JeHaUebr4LvDJMCw tl0j/zdVwtby4ZbKcX96UdnmshTosPuZTUlPt2anmRv+0hrDX2pbUxTrUZ7U4yXc WIggQLebgP5pAS2qJOWS5Sk5K+kszJVE3PJuj3QIQ+by7ABahZPnS14usOyi2QUg eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB d7+9eypks7NA48hJt+NZZC9ZZXvNsRHdlVTueExtM9lJPZEDgkAUsd8wTx49pIs3 T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr 16cwb5Q38qu01o7R5pZ6KkigRt0FlMDn92hEnipPd/zJesoSTKGSeoTu7SsGF9IC kg7y6cO7RrWNQNwbM7dwN9jIq/kJN45oxClVNW73lxEwenYy4AIx5boeALoktToz tJ2Zj36b/LvDA//kyznqBa+JEwKBgQDD2gU8B46Jika6H2g+5Hmk9mNtHj6ZZIHc ijwkDXiq5uLLRwa+RX1OhhVFnStVoJ3QrhFJAGqJbz0GHiDnTu3yfQL7iQE/Oc0K HMk415o3pjk3VIYchOENlaYx6mfr5Og6+ZXlcgXUvN4lRdwDphwyM3L4EKrfnc3v C5TYv/9XsQKBgB7UWJeEz+mbPv1KuWjXpEBz22kHjyQq8hpIFbCex96AbrteuocV R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus 0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ UDJMfO0Tci8NbA7QBuRZnr5a5PNOfT3IO2ZcAGM94GKEznHSWt2d9yOZHc/xiBAn S39t7eoDxnzTzGjvP1qgGvMJUP6F09U2fEhkQ2ebWadHTkZ5srSW/WhYGK2veN81 A7Tuf9gkm/2OhwI01bIZdfbGo7refKz7btSPvcECgYEApN3h9grnhhZ9lvrGjPgj XAS6oAKmRZtLvlmvbKlZNdQceHDBDzUEn7JA4ASgGaq+WPQPimfmzdoXmRfEltUA BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2 Y9QiC6fdz+koJqq33yMwCy0= -----END PRIVATE KEY-----""
  kube-registry.key: ""-----BEGIN CERTIFICATE----- MIIDUDCCAjigAwIBAgIJAK87AEpK+g1OMA0GCSqGSIb3DQEBCwUAMB8xHTAbBgNV BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC95VjiVBvZCZLmkvGkVwkfj+gR8Nom UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1 /G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ltdI07XEPIgrGr5w vRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7PutHNg3TIPSM4F4p mmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNboHUT2RArHR1zJV47 +NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O21+NM4DsjAgMBAAGj gY4wgYswHQYDVR0OBBYEFBe5fmodCcmMKa3VU/dvLmf3AK7RME8GA1UdIwRIMEaA FBe5fmodCcmMKa3VU/dvLmf3AK7RoSOkITAfMR0wGwYDVQQDFBQxMjcuMC4wLjFA MTQzMjg0MDgzMoIJAK87AEpK+g1OMAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEG MA0GCSqGSIb3DQEBCwUAA4IBAQAQ2wCPmH9kTXC5JGNN/w6qOhVe4O48EnmI3ceL XNvb5aPHztZ73uP+NH5vrdJOr+86w3m6QeiAiivUlr5GmcDWutgaWhElrrwo9tY/ oJpJf3E8zzCGixONYlFQEFtDA1oUl8Nz8yvO3Eq58YJr5Gqwmbmf3CjxAuBA+GX5 YDPXr3YWu/AMekA6pcNZg92mYfDdNI8GzAkGXmGgSZfUELCXIDPpn2XIjuQvN+4J tg12xMOnDPAoFGXmqnQzwhHy68aMYzGUeHcDuGbYNUgd+AMey71uau6j76NW0JFl 9wZ2hsIRH18oZwAJFbOHPkHaRhXSvzFauKqYc6k629yUF0s+ -----END CERTIFICATE-----""
type: Opaque
```

kubectl create reports error:

```
error: unable to load file ""/tmp/secret.yaml"": unable to load ""/tmp/secret.yaml"": illegal base64 data at input byte 0
```

BTW, I'm not sure if puting certificate in secret is the right approach in k8s (or will work).  My use case is to mount certificates as secret volume, and set ssl_certificate directive in nginx to use that certificates. e.g.

```
      containers:
        - name: kube-nginx
          image: kube-nginx
          volumeMounts:
            - name: kube-keypair
              readOnly: true
              mountPath: /etc/nginx/ssl
      volumes:
        - name: kube-keypair
          secret:
            secretName: kube-keypair
```

and nginx conf

```
  ssl_certificate /etc/nginx/ssl/kube-registry.crt;
  ssl_certificate_key /etc/nginx/ssl/kube-registry.key;
```
",#NAME?,https://github.com/kubernetes/kubernetes
194,8362,"kubectl cannot process the following listed objects (copied from [here](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/secrets.md#use-case-pods-with-prod--test-credentials))

```
[{
  ""apiVersion"": ""v1beta2"",
  ""kind"": ""Secret"",
  ""id"": ""prod-db-secret"",
  ""data"": {
    ""username"": ""dmFsdWUtMQ0K"",
    ""password"": ""dmFsdWUtMg0KDQo=""
  }
},
{
  ""apiVersion"": ""v1beta2"",
  ""kind"": ""Secret"",
  ""id"": ""test-db-secret"",
  ""data"": {
    ""username"": ""dmFsdWUtMQ0K"",
    ""password"": ""dmFsdWUtMg0KDQo=""
  }
}]
```

```
$ kubectl create -f ...
Error: unable to get type info from ""double.json"": couldn't get version/kind: json: cannot unmarshal array into Go value of type struct { APIVersion string ""json:\""apiVersion,omitempty\""""; Kind string ""json:\""kind,omitempty\"""" }
```
#7257 states ""Group related objects together in a single file. This currently requires a List object in YAML, but it's still better than separate files. Almost nobody knows about this feature."" Does this feature only work for YAML but not JSON? Thanks.

@bgrant0607 @krousey @nikhiljindal 
",apiVersion,https://github.com/kubernetes/kubernetes
195,8362,"kubectl cannot process the following listed objects (copied from [here](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/secrets.md#use-case-pods-with-prod--test-credentials))

```
[{
  ""apiVersion"": ""v1beta2"",
  ""kind"": ""Secret"",
  ""id"": ""prod-db-secret"",
  ""data"": {
    ""username"": ""dmFsdWUtMQ0K"",
    ""password"": ""dmFsdWUtMg0KDQo=""
  }
},
{
  ""apiVersion"": ""v1beta2"",
  ""kind"": ""Secret"",
  ""id"": ""test-db-secret"",
  ""data"": {
    ""username"": ""dmFsdWUtMQ0K"",
    ""password"": ""dmFsdWUtMg0KDQo=""
  }
}]
```

```
$ kubectl create -f ...
Error: unable to get type info from ""double.json"": couldn't get version/kind: json: cannot unmarshal array into Go value of type struct { APIVersion string ""json:\""apiVersion,omitempty\""""; Kind string ""json:\""kind,omitempty\"""" }
```
#7257 states ""Group related objects together in a single file. This currently requires a List object in YAML, but it's still better than separate files. Almost nobody knows about this feature."" Does this feature only work for YAML but not JSON? Thanks.

@bgrant0607 @krousey @nikhiljindal 
",dmFsdWUtMg0KDQo=,https://github.com/kubernetes/kubernetes
196,8147,"In the step 1) of docs https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/getting-started-guides/ubuntu.md

Clone repo kubernetes via ssh would fail:

```
$ git clone git@github.com:GoogleCloudPlatform/kubernetes.git
Cloning into 'kubernetes'...
The authenticity of host 'github.com (192.30.252.129)' can't be established.
RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'github.com,192.30.252.129' (RSA) to the list of known hosts.
Permission denied (publickey).
fatal: Could not read from remote repository.

Please make sure you have the correct access rights
and the repository exists.
```

Better to change it to the [https one](https://github.com/GoogleCloudPlatform/kubernetes.git).
",63:1b:56:4d:eb:df:a6:48,https://github.com/kubernetes/kubernetes
197,6387,"Accessing `https://<master-ip>/api/v1beta1/proxy/services/monitoring-heapster/` redirects to `https://<master-ip>/validate/` instead of ` `https://<master-ip>/api/v1beta1/proxy/services/monitoring-heapster/validate`

Here is a snapshot of the request and response headers:

```
Remote Address:104.197.10.177:443
Request URL:https://104.197.10.177/api/v1beta1/proxy/services/monitoring-heapster/
Request Method:GET
Status Code:307 Temporary Redirect
Request Headersview source
Accept:text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8
Accept-Encoding:gzip, deflate, sdch
Accept-Language:en-US,en;q=0.8
Authorization:Basic YWRtaW46SXMwQkV0RDFrTmVRTENEcw==
Cache-Control:no-cache
Connection:keep-alive
DNT:1
Host:104.197.10.177
Pragma:no-cache
User-Agent:Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36

Response Headers
Connection:keep-alive
Content-Length:46
Content-Type:text/html; charset=utf-8
Date:Thu, 02 Apr 2015 21:21:32 GMT
Location:/validate/
Server:nginx/1.2.1
```
",YWRtaW46SXMwQkV0RDFrTmVRTENEcw,https://github.com/kubernetes/kubernetes
198,1212,"The vCenter Server Appliance (VCSA) is automatically configured with self-signed certificates which default to localhost.localdomain and may not include the final hostname which would yield a mismatch in the x509 Certificates. In case a new certificate is not re-generated, we should perhaps consider feature enhancement to allow override, this maybe quite common for home lab/dev environments. 

```
└─[0] cluster/kube-up.sh
Starting cluster using provider: vsphere
Using password: admin:ez3hgL2DU7WuIHTh

Starting master VM (this can take a minute)...
Error: Post https://vcsa.primp-industries.com/sdk: x509: certificate is valid for localhost.primp-industries.com, localhost, not vcsa.primp-industries.com
```
",ez3hgL2DU7WuIHTh,https://github.com/kubernetes/kubernetes
199,1212,"The vCenter Server Appliance (VCSA) is automatically configured with self-signed certificates which default to localhost.localdomain and may not include the final hostname which would yield a mismatch in the x509 Certificates. In case a new certificate is not re-generated, we should perhaps consider feature enhancement to allow override, this maybe quite common for home lab/dev environments. 

```
└─[0] cluster/kube-up.sh
Starting cluster using provider: vsphere
Using password: admin:ez3hgL2DU7WuIHTh

Starting master VM (this can take a minute)...
Error: Post https://vcsa.primp-industries.com/sdk: x509: certificate is valid for localhost.primp-industries.com, localhost, not vcsa.primp-industries.com
```
",Starting,https://github.com/kubernetes/kubernetes
