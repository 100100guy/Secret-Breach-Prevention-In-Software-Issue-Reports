Unnamed: 0,Issue ID,Issue Body,Candidate String,Repolink
0,12352.0,"Trying to create a SQL VM with auto backup enabled, but getting a 400 error with code `BackupScheduleTypeNotSet`, despite `backupScheduleType` being specified in the request.

## Request

PUT https://management.azure.com/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/acctestRG-mssql-210108235644463787/providers/Microsoft.SqlVirtualMachine/sqlVirtualMachines/acctest-VM-210108235644463787?api-version=2017-03-01-preview
```json
{
	""location"": ""westeurope"",
	""properties"": {
		""autoBackupSettings"": {
			""enable"": true,
			""enableEncryption"": false,
			""retentionPeriod"": 21,
			""storageAccountUrl"": ""https://unlikely23exst2acctpiakc.blob.core.windows.net/"",
			""storageAccessKey"": ""mDCN2IFsCkM250yvHFRJv4UZEPJ+CPAM8q/mWDNiENdXarhuSBRD8QoDhkBVZIhUDIDEv61LQtD8oi75D4Uwng=="",
			""password"": """",
			""backupSystemDbs"": false,
			""backupScheduleType"": ""Automated"",
			""fullBackupFrequency"": ""Weekly"",
			""fullBackupStartTime"": 0,
			""fullBackupWindowHours"": 0,
			""logBackupFrequency"": 0
		},
		""serverConfigurationsManagementSettings"": {
			""sqlConnectivityUpdateSettings"": {
				""connectivityType"": ""PRIVATE"",
				""port"": 1433,
				""sqlAuthUpdateUserName"": """",
				""sqlAuthUpdatePassword"": """"
			},
			""additionalFeaturesServerConfigurations"": {
				""isRServicesEnabled"": false
			}
		},
		""sqlManagement"": ""Full"",
		""sqlServerLicenseType"": ""PAYG"",
		""virtualMachineResourceId"": ""/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/acctestRG-mssql-210108235644463787/providers/Microsoft.Compute/virtualMachines/acctest-VM-210108235644463787""
	},
	""tags"": {}
}
```

## Response

400 Bad Request
```json
{
	""error"": {
		""code"": ""BackupScheduleTypeNotSet"",
		""message"": ""Backup schedule type needs to be set.""
	}
}
```",backupScheduleType,https://github.com/microsoft/api-guidelines
1,12352.0,"Trying to create a SQL VM with auto backup enabled, but getting a 400 error with code `BackupScheduleTypeNotSet`, despite `backupScheduleType` being specified in the request.

## Request

PUT https://management.azure.com/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/acctestRG-mssql-210108235644463787/providers/Microsoft.SqlVirtualMachine/sqlVirtualMachines/acctest-VM-210108235644463787?api-version=2017-03-01-preview
```json
{
	""location"": ""westeurope"",
	""properties"": {
		""autoBackupSettings"": {
			""enable"": true,
			""enableEncryption"": false,
			""retentionPeriod"": 21,
			""storageAccountUrl"": ""https://unlikely23exst2acctpiakc.blob.core.windows.net/"",
			""storageAccessKey"": ""mDCN2IFsCkM250yvHFRJv4UZEPJ+CPAM8q/mWDNiENdXarhuSBRD8QoDhkBVZIhUDIDEv61LQtD8oi75D4Uwng=="",
			""password"": """",
			""backupSystemDbs"": false,
			""backupScheduleType"": ""Automated"",
			""fullBackupFrequency"": ""Weekly"",
			""fullBackupStartTime"": 0,
			""fullBackupWindowHours"": 0,
			""logBackupFrequency"": 0
		},
		""serverConfigurationsManagementSettings"": {
			""sqlConnectivityUpdateSettings"": {
				""connectivityType"": ""PRIVATE"",
				""port"": 1433,
				""sqlAuthUpdateUserName"": """",
				""sqlAuthUpdatePassword"": """"
			},
			""additionalFeaturesServerConfigurations"": {
				""isRServicesEnabled"": false
			}
		},
		""sqlManagement"": ""Full"",
		""sqlServerLicenseType"": ""PAYG"",
		""virtualMachineResourceId"": ""/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/acctestRG-mssql-210108235644463787/providers/Microsoft.Compute/virtualMachines/acctest-VM-210108235644463787""
	},
	""tags"": {}
}
```

## Response

400 Bad Request
```json
{
	""error"": {
		""code"": ""BackupScheduleTypeNotSet"",
		""message"": ""Backup schedule type needs to be set.""
	}
}
```",mDCN2IFsCkM250yvHFRJv4UZEPJ+CPAM8q,https://github.com/microsoft/api-guidelines
2,12352.0,"Trying to create a SQL VM with auto backup enabled, but getting a 400 error with code `BackupScheduleTypeNotSet`, despite `backupScheduleType` being specified in the request.

## Request

PUT https://management.azure.com/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/acctestRG-mssql-210108235644463787/providers/Microsoft.SqlVirtualMachine/sqlVirtualMachines/acctest-VM-210108235644463787?api-version=2017-03-01-preview
```json
{
	""location"": ""westeurope"",
	""properties"": {
		""autoBackupSettings"": {
			""enable"": true,
			""enableEncryption"": false,
			""retentionPeriod"": 21,
			""storageAccountUrl"": ""https://unlikely23exst2acctpiakc.blob.core.windows.net/"",
			""storageAccessKey"": ""mDCN2IFsCkM250yvHFRJv4UZEPJ+CPAM8q/mWDNiENdXarhuSBRD8QoDhkBVZIhUDIDEv61LQtD8oi75D4Uwng=="",
			""password"": """",
			""backupSystemDbs"": false,
			""backupScheduleType"": ""Automated"",
			""fullBackupFrequency"": ""Weekly"",
			""fullBackupStartTime"": 0,
			""fullBackupWindowHours"": 0,
			""logBackupFrequency"": 0
		},
		""serverConfigurationsManagementSettings"": {
			""sqlConnectivityUpdateSettings"": {
				""connectivityType"": ""PRIVATE"",
				""port"": 1433,
				""sqlAuthUpdateUserName"": """",
				""sqlAuthUpdatePassword"": """"
			},
			""additionalFeaturesServerConfigurations"": {
				""isRServicesEnabled"": false
			}
		},
		""sqlManagement"": ""Full"",
		""sqlServerLicenseType"": ""PAYG"",
		""virtualMachineResourceId"": ""/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/acctestRG-mssql-210108235644463787/providers/Microsoft.Compute/virtualMachines/acctest-VM-210108235644463787""
	},
	""tags"": {}
}
```

## Response

400 Bad Request
```json
{
	""error"": {
		""code"": ""BackupScheduleTypeNotSet"",
		""message"": ""Backup schedule type needs to be set.""
	}
}
```",additionalFeaturesServerConfigurations,https://github.com/microsoft/api-guidelines
0,2254.0,"https://github.com/googleapis/google-api-nodejs-client/blob/master/samples/drive/list.js doesn't work

```nodejs
root@docker:~/gg# cat oauth2.keys.json
{
  ""web"" : {
""redirect_uris"": [
  ""https://hello.example.net""
],
    ""client_id"": ""10xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxontent.com"",
    ""client_secret"": ""5fS6xxxxxxxxxxxxxxxxxxf0w"",
    ""project_id"": ""maxxxxxxxxxxxxxx76211""
  }
}
root@docker:~/gg# node e.js
Error: The provided keyfile does not define a valid
redirect URI. There must be at least one redirect URI defined, and this sample
assumes it redirects to 'http://localhost:3000/oauth2callback'.  Please edit
your keyfile, and add a 'redirect_uris' section.  For example:

""redirect_uris"": [
  ""http://localhost:3000/oauth2callback""
]

    at authenticate (/root/gg/node_modules/@google-cloud/local-auth/build/src/index.js:56:15)
    at runSample (/root/gg/e.js:11:22)
    at Object.<anonymous> (/root/gg/e.js:25:3)
    at Module._compile (internal/modules/cjs/loader.js:778:30)
    at Object.Module._extensions..js (internal/modules/cjs/loader.js:789:10)
    at Module.load (internal/modules/cjs/loader.js:653:32)
    at tryModuleLoad (internal/modules/cjs/loader.js:593:12)
    at Function.Module._load (internal/modules/cjs/loader.js:585:3)
    at Function.Module.runMain (internal/modules/cjs/loader.js:831:12)
    at startup (internal/bootstrap/node.js:283:19)
```

what's wrong?!",5fS6xxxxxxxxxxxxxxxxxxf0w,https://github.com/googleapis/google-api-nodejs-client
1,530.0,"I am trying to integrate Google drive resumable file upload/update with my application. But when i update the file, file is updating in encoded format it is not taking the actual content. Encoded format is working for multiplepart uploadType but same content is not working for Resumable upload. Please find the below details

Step 1 : Start the resumable session

```js
function uploadFile(fileData) {

    var accessToken = 'ya29.nwI5Em6UnYGHvVzVx7lBk5tD-xzFl4_JG3_c-_t4FJ3owll_8i_rL5M17LFV6VlF7QE';

    const boundary = '-------314159265358979323846';
    const delimiter = ""\r\n--"" + boundary + ""\r\n"";
    const close_delim = ""\r\n--"" + boundary + ""--"";

   var contentType = fileData.type || 'application/octet-stream';
   var metadata = {
            'name': fileData.name,
            'mimeType': contentType,
            'Content-Type': contentType,
            'Content-Length': fileData.size
        };

   var request = gapi.client.request({
        'path' : 'upload/drive/v3/files',
        'method' : 'POST',
        'params' : {'uploadType':'resumable'},
        'headers' : {
          'X-Upload-Content-Type' : fileData.type,
          'Content-Type': 'application/json; charset=UTF-8',
          'Authorization': 'Bearer ' + accessToken,
        },
        'body' : metadata
    });

   request.execute(function(resp, raw_resp) {
      var locationUrl =   JSON.parse(raw_resp).gapiRequest.data.headers.location;
      console.log(locationUrl);
      uploadToLocationUrl(locationUrl, fileData);
   });
}
```

Upto here it's fine I am getting Location Url and then calling a function to upload the file.

Step 2 : Resumable session initiation request

```js
     function uploadToLocationUrl(locationUrl, fileData)
       {
            var reader = new FileReader();
            reader.readAsBinaryString(fileData);
            reader.onload = function (e) {
            var contentType = fileData.type || 'application/octet-stream';
            var metadata = {
                'name': fileData.name,
                'mimeType': contentType,
                'Content-Type': contentType,
                'Content-Length': fileData.size
            };

              var base64Data = btoa(reader.result);
              var multipartRequestBody =
                  delimiter +
                  'Content-Type: application/json\r\n\r\n' +
                  JSON.stringify(metadata) +
                  delimiter +
                  'Content-Type: ' + contentType + '\r\n' +
                  'Content-Transfer-Encoding: base64\r\n' +
                  '\r\n' +
                  base64Data +
                  close_delim;

           var requestPost = gapi.client.request({
                'path' : locationUrl,
                'method' : 'PUT',
                'headers' : {
                  'X-Upload-Content-Length' : fileData.size
                },
                'body' : multipartRequestBody
              });
            console.log(requestPost);

            requestPost.execute(function(resp, raw_resp) {
              console.log(resp);
            });
         }
}
```

Result : Updated file in google drive

```
---------314159265358979323846
Content-Type: application/json

{""name"":""api.txt"",""mimeType"":""text/plain""}
---------314159265358979323846
Content-Type: text/plain
Content-Transfer-Encoding: base64

MSkgTmVlZCBhbiBhcGkgd2hpY2ggd2lsbCByZXR1cm4gYWxsIGxlYWRzIGVtYWlsIGlkLg0KMikgTmVlZCBhbiBhcGkgdG8gY29udmVydCBtdWx0aXBsZSBjb250YWN0IGludG8gbGVhZC4NCjMpIE5lZWQgYW4gYXBpIGZvciBnb29nbGUgc2lnbiBpbi4vLyBkb24ndCBkaXNjdXNzIGFib3V0IHRoaXMgb25lIG5vdywgZmlyc3Qgd2Ugd2lsbCBkaXNjdXNzIGFib3V0IHRoaXMgQVBJLg==
---------314159265358979323846--
```

Thank you.
",MSkgTmVlZCBhbiBhcGkgd2hpY2ggd2lsbCByZXR1cm4gYWxsIGxlYWRzIGVtYWlsIGlkLg0KM,https://github.com/googleapis/google-api-nodejs-client
2,530.0,"I am trying to integrate Google drive resumable file upload/update with my application. But when i update the file, file is updating in encoded format it is not taking the actual content. Encoded format is working for multiplepart uploadType but same content is not working for Resumable upload. Please find the below details

Step 1 : Start the resumable session

```js
function uploadFile(fileData) {

    var accessToken = 'ya29.nwI5Em6UnYGHvVzVx7lBk5tD-xzFl4_JG3_c-_t4FJ3owll_8i_rL5M17LFV6VlF7QE';

    const boundary = '-------314159265358979323846';
    const delimiter = ""\r\n--"" + boundary + ""\r\n"";
    const close_delim = ""\r\n--"" + boundary + ""--"";

   var contentType = fileData.type || 'application/octet-stream';
   var metadata = {
            'name': fileData.name,
            'mimeType': contentType,
            'Content-Type': contentType,
            'Content-Length': fileData.size
        };

   var request = gapi.client.request({
        'path' : 'upload/drive/v3/files',
        'method' : 'POST',
        'params' : {'uploadType':'resumable'},
        'headers' : {
          'X-Upload-Content-Type' : fileData.type,
          'Content-Type': 'application/json; charset=UTF-8',
          'Authorization': 'Bearer ' + accessToken,
        },
        'body' : metadata
    });

   request.execute(function(resp, raw_resp) {
      var locationUrl =   JSON.parse(raw_resp).gapiRequest.data.headers.location;
      console.log(locationUrl);
      uploadToLocationUrl(locationUrl, fileData);
   });
}
```

Upto here it's fine I am getting Location Url and then calling a function to upload the file.

Step 2 : Resumable session initiation request

```js
     function uploadToLocationUrl(locationUrl, fileData)
       {
            var reader = new FileReader();
            reader.readAsBinaryString(fileData);
            reader.onload = function (e) {
            var contentType = fileData.type || 'application/octet-stream';
            var metadata = {
                'name': fileData.name,
                'mimeType': contentType,
                'Content-Type': contentType,
                'Content-Length': fileData.size
            };

              var base64Data = btoa(reader.result);
              var multipartRequestBody =
                  delimiter +
                  'Content-Type: application/json\r\n\r\n' +
                  JSON.stringify(metadata) +
                  delimiter +
                  'Content-Type: ' + contentType + '\r\n' +
                  'Content-Transfer-Encoding: base64\r\n' +
                  '\r\n' +
                  base64Data +
                  close_delim;

           var requestPost = gapi.client.request({
                'path' : locationUrl,
                'method' : 'PUT',
                'headers' : {
                  'X-Upload-Content-Length' : fileData.size
                },
                'body' : multipartRequestBody
              });
            console.log(requestPost);

            requestPost.execute(function(resp, raw_resp) {
              console.log(resp);
            });
         }
}
```

Result : Updated file in google drive

```
---------314159265358979323846
Content-Type: application/json

{""name"":""api.txt"",""mimeType"":""text/plain""}
---------314159265358979323846
Content-Type: text/plain
Content-Transfer-Encoding: base64

MSkgTmVlZCBhbiBhcGkgd2hpY2ggd2lsbCByZXR1cm4gYWxsIGxlYWRzIGVtYWlsIGlkLg0KMikgTmVlZCBhbiBhcGkgdG8gY29udmVydCBtdWx0aXBsZSBjb250YWN0IGludG8gbGVhZC4NCjMpIE5lZWQgYW4gYXBpIGZvciBnb29nbGUgc2lnbiBpbi4vLyBkb24ndCBkaXNjdXNzIGFib3V0IHRoaXMgb25lIG5vdywgZmlyc3Qgd2Ugd2lsbCBkaXNjdXNzIGFib3V0IHRoaXMgQVBJLg==
---------314159265358979323846--
```

Thank you.
",MSkgTmVlZCBhbiBhcGkgd2hpY2ggd2lsbCByZXR1cm4gYWxsIGxlYWRzIGVtYWlsIGlkLg0KMikgTmVlZCBhbiBhcGkgdG8gY29udmVydCBtdWx0aXBsZSBjb250YWN0IGludG8gbGVhZC4NCjMpIE5lZWQgYW4gYXBpIGZvciBnb29nbGUgc2lnbiBpbi4vLyBkb24ndCBkaXNjdXNzIGFib3V0IHRoaXMgb25lIG5vdywgZmlyc3Qgd2Ugd2lsbCBkaXNjdXNzIGFib3V0IHRoaXMgQVBJLg,https://github.com/googleapis/google-api-nodejs-client
3,530.0,"I am trying to integrate Google drive resumable file upload/update with my application. But when i update the file, file is updating in encoded format it is not taking the actual content. Encoded format is working for multiplepart uploadType but same content is not working for Resumable upload. Please find the below details

Step 1 : Start the resumable session

```js
function uploadFile(fileData) {

    var accessToken = 'ya29.nwI5Em6UnYGHvVzVx7lBk5tD-xzFl4_JG3_c-_t4FJ3owll_8i_rL5M17LFV6VlF7QE';

    const boundary = '-------314159265358979323846';
    const delimiter = ""\r\n--"" + boundary + ""\r\n"";
    const close_delim = ""\r\n--"" + boundary + ""--"";

   var contentType = fileData.type || 'application/octet-stream';
   var metadata = {
            'name': fileData.name,
            'mimeType': contentType,
            'Content-Type': contentType,
            'Content-Length': fileData.size
        };

   var request = gapi.client.request({
        'path' : 'upload/drive/v3/files',
        'method' : 'POST',
        'params' : {'uploadType':'resumable'},
        'headers' : {
          'X-Upload-Content-Type' : fileData.type,
          'Content-Type': 'application/json; charset=UTF-8',
          'Authorization': 'Bearer ' + accessToken,
        },
        'body' : metadata
    });

   request.execute(function(resp, raw_resp) {
      var locationUrl =   JSON.parse(raw_resp).gapiRequest.data.headers.location;
      console.log(locationUrl);
      uploadToLocationUrl(locationUrl, fileData);
   });
}
```

Upto here it's fine I am getting Location Url and then calling a function to upload the file.

Step 2 : Resumable session initiation request

```js
     function uploadToLocationUrl(locationUrl, fileData)
       {
            var reader = new FileReader();
            reader.readAsBinaryString(fileData);
            reader.onload = function (e) {
            var contentType = fileData.type || 'application/octet-stream';
            var metadata = {
                'name': fileData.name,
                'mimeType': contentType,
                'Content-Type': contentType,
                'Content-Length': fileData.size
            };

              var base64Data = btoa(reader.result);
              var multipartRequestBody =
                  delimiter +
                  'Content-Type: application/json\r\n\r\n' +
                  JSON.stringify(metadata) +
                  delimiter +
                  'Content-Type: ' + contentType + '\r\n' +
                  'Content-Transfer-Encoding: base64\r\n' +
                  '\r\n' +
                  base64Data +
                  close_delim;

           var requestPost = gapi.client.request({
                'path' : locationUrl,
                'method' : 'PUT',
                'headers' : {
                  'X-Upload-Content-Length' : fileData.size
                },
                'body' : multipartRequestBody
              });
            console.log(requestPost);

            requestPost.execute(function(resp, raw_resp) {
              console.log(resp);
            });
         }
}
```

Result : Updated file in google drive

```
---------314159265358979323846
Content-Type: application/json

{""name"":""api.txt"",""mimeType"":""text/plain""}
---------314159265358979323846
Content-Type: text/plain
Content-Transfer-Encoding: base64

MSkgTmVlZCBhbiBhcGkgd2hpY2ggd2lsbCByZXR1cm4gYWxsIGxlYWRzIGVtYWlsIGlkLg0KMikgTmVlZCBhbiBhcGkgdG8gY29udmVydCBtdWx0aXBsZSBjb250YWN0IGludG8gbGVhZC4NCjMpIE5lZWQgYW4gYXBpIGZvciBnb29nbGUgc2lnbiBpbi4vLyBkb24ndCBkaXNjdXNzIGFib3V0IHRoaXMgb25lIG5vdywgZmlyc3Qgd2Ugd2lsbCBkaXNjdXNzIGFib3V0IHRoaXMgQVBJLg==
---------314159265358979323846--
```

Thank you.
",_t4FJ3owll_8i_rL5M17LFV6VlF7QE,https://github.com/googleapis/google-api-nodejs-client
4,164.0,"This may be the wrong place to post this, but I'm having difficulty reaching anyone familiar with google apis on either android-developers or google-apis-explorer-users forums.

I'm having a problem deciphering the root cause of a cryptic ""invalid value"" response for the `androidpublisher.inapppurchases.get` api. OAuth is working fine, and I'm able to get the access_token as well as refresh_token.

I'm guessing something in the parameters are malformed or missing but cannot decipher where. The dev console project seems to be connected correctly because for an unauthorized user, querying on my packageName throws a permission denied error. 

I also suspected the productId may be malformed, and I tried all sorts of combinations such as `subs:{packageName}:pro_monthly_15`, `{packageName}:pro_monthly_15`, etc. with all the same invalid response.

Can anyone help me shed some light on this?

```
oauth2Client.credentials = {
            access_token: '{removed}',
            refresh_token: '{removed}'
        };

        googleapis
            .discover('androidpublisher', 'v1.1')
            .execute(function(err, client) {

                console.log(['discovered', err, client.androidpublisher.apiMeta]);

            var params = {
                packageName: '{removed}',
                productId:   'pro_monthly_15',
                token:       'glhannifclifbhdgbpalegib.AO-J1OyeEpe0JagpGtG588_Jor3mtqjp_CRB-xGdq55kqMMWqGyGd2YlesHdazWPnOC4CoB0EVP-o_j1LT7taDJE8vUxg7UcjzeMPZ4WHi79aTYdv3FalrvqKAFTvWZqJqwjecdGaTpa'
            };
            client
                .androidpublisher.inapppurchases.get(params)
                .withAuthClient(oauth2Client)
                .execute(function (err, response) {
                    console.log([err, response]);

                    res.json({
                        response: response,
                        error: err
                    });
            });
        });
```

Cryptic response:

```
{
  ""response"": null,
  ""error"": {
    ""errors"": [
      {
        ""domain"": ""global"",
        ""reason"": ""invalid"",
        ""message"": ""Invalid Value""
      }
    ],
    ""code"": 400,
    ""message"": ""Invalid Value""
  }
}
```
",J1OyeEpe0JagpGtG588_Jor3mtqjp_CRB-xGdq55kqMMWqGyGd2YlesHdazWPnOC4CoB0EVP-o_j1LT7taDJE8vUxg7UcjzeMPZ4WHi79aTYdv3FalrvqKAFTvWZqJqwjecdGaTpa,https://github.com/googleapis/google-api-nodejs-client
5,164.0,"This may be the wrong place to post this, but I'm having difficulty reaching anyone familiar with google apis on either android-developers or google-apis-explorer-users forums.

I'm having a problem deciphering the root cause of a cryptic ""invalid value"" response for the `androidpublisher.inapppurchases.get` api. OAuth is working fine, and I'm able to get the access_token as well as refresh_token.

I'm guessing something in the parameters are malformed or missing but cannot decipher where. The dev console project seems to be connected correctly because for an unauthorized user, querying on my packageName throws a permission denied error. 

I also suspected the productId may be malformed, and I tried all sorts of combinations such as `subs:{packageName}:pro_monthly_15`, `{packageName}:pro_monthly_15`, etc. with all the same invalid response.

Can anyone help me shed some light on this?

```
oauth2Client.credentials = {
            access_token: '{removed}',
            refresh_token: '{removed}'
        };

        googleapis
            .discover('androidpublisher', 'v1.1')
            .execute(function(err, client) {

                console.log(['discovered', err, client.androidpublisher.apiMeta]);

            var params = {
                packageName: '{removed}',
                productId:   'pro_monthly_15',
                token:       'glhannifclifbhdgbpalegib.AO-J1OyeEpe0JagpGtG588_Jor3mtqjp_CRB-xGdq55kqMMWqGyGd2YlesHdazWPnOC4CoB0EVP-o_j1LT7taDJE8vUxg7UcjzeMPZ4WHi79aTYdv3FalrvqKAFTvWZqJqwjecdGaTpa'
            };
            client
                .androidpublisher.inapppurchases.get(params)
                .withAuthClient(oauth2Client)
                .execute(function (err, response) {
                    console.log([err, response]);

                    res.json({
                        response: response,
                        error: err
                    });
            });
        });
```

Cryptic response:

```
{
  ""response"": null,
  ""error"": {
    ""errors"": [
      {
        ""domain"": ""global"",
        ""reason"": ""invalid"",
        ""message"": ""Invalid Value""
      }
    ],
    ""code"": 400,
    ""message"": ""Invalid Value""
  }
}
```
",J1OyeEpe0JagpGtG588_Jor3mtqjp_CRB-,https://github.com/googleapis/google-api-nodejs-client
6,59.0,"Using googleapis 0.2.13-alpha. I'm having trouble using this module and trying to call the recently released drive.files.watch endpoint. Other endpoints appear to work correctly, but this one is causing the error. The class is correctly discovered, and called using:

```
  var subscription = {
    id: ""id-1373896667820""
    token: ""101852559274654726533"",
    type: 'web_hook',
    address: 'https://prisoner.com/dev/glass-drive/incoming/file'
  };
  var params = {
    fileId: '14lITDEjLp-YGZ8tkZGLT19ouAD5rhuDrYDRt-J6o1RE'
  };
  client.drive.files.watch( params, subscription )
  .withAuthClient(user.auth)
  .execute(function(err,data){
    console.log( 'drive.files.watch', err, data, user.auth );
  });
```

user.auth is a valid object containing the clientId, clientSecret, and credentials containing the access_token and refresh_token.

Adding some debugging to BaseRequest.prototype.execute (request.js, after line 125) I can confirm that the authClient has the correct user.auth information. The requestOpts object is set to

```
{ method: 'POST',
  uri: 'https://www.googleapis.com/rpc',
  json: 
   [ { jsonrpc: '2.0',
       id: 0,
       method: 'drive.files.watch',
       params: [Object],
       apiVersion: 'v2' } ] }
```

which appears correct. The params field contains

```
{ fileId: '14lITDEjLp-YGZ8tkZGLT19ouAD5rhuDrYDRt-J6o1RE',
  resource: 
   { id: 'id-1373896667820',
     token: '101852559274654726533',
     type: 'web_hook',
     address: 'https://prisoner.com/dev/glass-drive/incoming/file' } }
```

which also appears correct.

The error returned is:

```
{ code: 500,
  message: 'Internal Error',
  data: 
   [ { domain: 'global',
       reason: 'internalError',
       message: 'Internal Error' } ] }
```

Using the exact same parameters, and the exact same access_token, I have run the following curl command:

```
curl \
  --header ""Authorization: Bearer valid-access-token-redacted"" \
  --header ""Content-type: application/json"" \
  --data '{""id"": ""id-1373896667820"", ""type"": ""web_hook"", ""address"": ""https://pri
soner.com/dev/glass-drive/incoming/file"", ""token"": ""101852559274654726533""}' \
  https://www.googleapis.com/drive/v2/files/14lITDEjLp-YGZ8tkZGLT19ouAD5rhuDrYDR
t-J6o1RE/watch
```

Which works correctly.
",1.01853E+20,https://github.com/googleapis/google-api-nodejs-client
0,26405.0,"### Before reporting an issue

- [X] I have read and understood the above terms for submitting issues, and I understand that my issue may be closed without action if I do not follow them.

### Area

core

### Describe the bug

Previously I upgraded key cloak from 11.0 to 23.0, and found a redirect error like #8959 

After my investigation, I found it seems not so simple.



### Version

main

### How to Reproduce?

this bug can be easily reproduced with nightly image, steps are following:
1. setup keycloak server 
2. open network tool in browser, enable persist log to see all requests, filter for HTML only
3. navigate to admin console like `http://localhost:8080/admin/master/console/`
4. login normally


### Expected behavior

keycloak would return a 302 response , redirecting user to a redirect uri that is identical to the uri user provided previously

### Actual behavior

the two redirect uri does not match

the login process would be : 
1. admin console redirect user keycloak login page `http://localhost:8080/realms/master/protocol/openid-connect/auth?client_id=security-admin-console&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2Fadmin%2Fmaster%2Fconsole%2F&state=361ed6b9-afea-44b9-9f16-86ac91f0f153&response_mode=fragment&response_type=code&scope=openid&nonce=a354386d-d08b-4957-92e8-ae8412ce8130&prompt=none&code_challenge=K41x3hNMzRzLiJt1DZcR6aolGopt7vJi1glMdx_LCbg&code_challenge_method=S256` the url decoded parameter redirect_uri is `http://localhost:8080/admin/master/console/` which is normal
2. user login with authenticate api 
3. keycloak return 302 response with headers
```
HTTP/1.1 302 Found
Cache-Control: no-store, must-revalidate, max-age=0
Location: http://localhost:8080/admin/master/console/#state=21f0e380-1941-40c2-8ba0-66dc710bf106&session_state=78b4fb0f-781f-4686-91e8-58ead1f797ff&iss=http%3A%2F%2Flocalhost%3A8080%2Frealms%2Fmaster&code=b2d3f76c-a0fb-41e4-a39c-eb2ffac51630.78b4fb0f-781f-4686-91e8-58ead1f797ff.a9b337bb-03a1-4b3f-8ec9-fb56c093f909
```

please notice that the Location header contains an URI that connect URI path and parameter with `#` instead of `?`

### Anything else?

After upgrade, I found strange redirect like #8959  in many SPA apps, but none of the backend projects are involved

I also tried to request for a auth with url like `http://localhost:8080/realms/master/protocol/openid-connect/auth?client_id=test&redirect_uri=http%3A%2F%2Flocalhost%3A9090%2F%23%2F&response_type=code` 

notice the redirect_uri is `http://localhost:9090/#/` which contains a `#`

So I'm lost now, I don't know how to trigger this bug preciously.",78b4fb0f-781f-4686-91e8-58ead1f797ff,https://github.com/keycloak/keycloak
1,26405.0,"### Before reporting an issue

- [X] I have read and understood the above terms for submitting issues, and I understand that my issue may be closed without action if I do not follow them.

### Area

core

### Describe the bug

Previously I upgraded key cloak from 11.0 to 23.0, and found a redirect error like #8959 

After my investigation, I found it seems not so simple.



### Version

main

### How to Reproduce?

this bug can be easily reproduced with nightly image, steps are following:
1. setup keycloak server 
2. open network tool in browser, enable persist log to see all requests, filter for HTML only
3. navigate to admin console like `http://localhost:8080/admin/master/console/`
4. login normally


### Expected behavior

keycloak would return a 302 response , redirecting user to a redirect uri that is identical to the uri user provided previously

### Actual behavior

the two redirect uri does not match

the login process would be : 
1. admin console redirect user keycloak login page `http://localhost:8080/realms/master/protocol/openid-connect/auth?client_id=security-admin-console&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2Fadmin%2Fmaster%2Fconsole%2F&state=361ed6b9-afea-44b9-9f16-86ac91f0f153&response_mode=fragment&response_type=code&scope=openid&nonce=a354386d-d08b-4957-92e8-ae8412ce8130&prompt=none&code_challenge=K41x3hNMzRzLiJt1DZcR6aolGopt7vJi1glMdx_LCbg&code_challenge_method=S256` the url decoded parameter redirect_uri is `http://localhost:8080/admin/master/console/` which is normal
2. user login with authenticate api 
3. keycloak return 302 response with headers
```
HTTP/1.1 302 Found
Cache-Control: no-store, must-revalidate, max-age=0
Location: http://localhost:8080/admin/master/console/#state=21f0e380-1941-40c2-8ba0-66dc710bf106&session_state=78b4fb0f-781f-4686-91e8-58ead1f797ff&iss=http%3A%2F%2Flocalhost%3A8080%2Frealms%2Fmaster&code=b2d3f76c-a0fb-41e4-a39c-eb2ffac51630.78b4fb0f-781f-4686-91e8-58ead1f797ff.a9b337bb-03a1-4b3f-8ec9-fb56c093f909
```

please notice that the Location header contains an URI that connect URI path and parameter with `#` instead of `?`

### Anything else?

After upgrade, I found strange redirect like #8959  in many SPA apps, but none of the backend projects are involved

I also tried to request for a auth with url like `http://localhost:8080/realms/master/protocol/openid-connect/auth?client_id=test&redirect_uri=http%3A%2F%2Flocalhost%3A9090%2F%23%2F&response_type=code` 

notice the redirect_uri is `http://localhost:9090/#/` which contains a `#`

So I'm lost now, I don't know how to trigger this bug preciously.",b2d3f76c-a0fb-41e4-a39c--781f-4686-91e8--03a1-4b3f-8ec9-fb56c093f909,https://github.com/keycloak/keycloak
2,26405.0,"### Before reporting an issue

- [X] I have read and understood the above terms for submitting issues, and I understand that my issue may be closed without action if I do not follow them.

### Area

core

### Describe the bug

Previously I upgraded key cloak from 11.0 to 23.0, and found a redirect error like #8959 

After my investigation, I found it seems not so simple.



### Version

main

### How to Reproduce?

this bug can be easily reproduced with nightly image, steps are following:
1. setup keycloak server 
2. open network tool in browser, enable persist log to see all requests, filter for HTML only
3. navigate to admin console like `http://localhost:8080/admin/master/console/`
4. login normally


### Expected behavior

keycloak would return a 302 response , redirecting user to a redirect uri that is identical to the uri user provided previously

### Actual behavior

the two redirect uri does not match

the login process would be : 
1. admin console redirect user keycloak login page `http://localhost:8080/realms/master/protocol/openid-connect/auth?client_id=security-admin-console&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2Fadmin%2Fmaster%2Fconsole%2F&state=361ed6b9-afea-44b9-9f16-86ac91f0f153&response_mode=fragment&response_type=code&scope=openid&nonce=a354386d-d08b-4957-92e8-ae8412ce8130&prompt=none&code_challenge=K41x3hNMzRzLiJt1DZcR6aolGopt7vJi1glMdx_LCbg&code_challenge_method=S256` the url decoded parameter redirect_uri is `http://localhost:8080/admin/master/console/` which is normal
2. user login with authenticate api 
3. keycloak return 302 response with headers
```
HTTP/1.1 302 Found
Cache-Control: no-store, must-revalidate, max-age=0
Location: http://localhost:8080/admin/master/console/#state=21f0e380-1941-40c2-8ba0-66dc710bf106&session_state=78b4fb0f-781f-4686-91e8-58ead1f797ff&iss=http%3A%2F%2Flocalhost%3A8080%2Frealms%2Fmaster&code=b2d3f76c-a0fb-41e4-a39c-eb2ffac51630.78b4fb0f-781f-4686-91e8-58ead1f797ff.a9b337bb-03a1-4b3f-8ec9-fb56c093f909
```

please notice that the Location header contains an URI that connect URI path and parameter with `#` instead of `?`

### Anything else?

After upgrade, I found strange redirect like #8959  in many SPA apps, but none of the backend projects are involved

I also tried to request for a auth with url like `http://localhost:8080/realms/master/protocol/openid-connect/auth?client_id=test&redirect_uri=http%3A%2F%2Flocalhost%3A9090%2F%23%2F&response_type=code` 

notice the redirect_uri is `http://localhost:9090/#/` which contains a `#`

So I'm lost now, I don't know how to trigger this bug preciously.",a39c--781f-4686-91e8--03a1-4b3f-8ec9-fb56c093f909,https://github.com/keycloak/keycloak
3,26082.0,"## Description
A weakness was found in Keycloak core where possible error messages could be logged or transmitted to a user when performing a bad action against the server (such as failing some parameter in the URL). With that in place a malicious user could benefit from that message and possibly jeopardize the environment based on the error sent.

## Steps to reproduce

1. Go to Login page

2. Change the redirect_uri value in URL as below and hit enter key.
```
/auth/realms/master/protocol/openid-connect/auth?client_id=security-admin-
console&redirect_uri=%00&state=32128ce4-0b7f-415f-888b-
274e5a060a3b&response_mode=fragment&response_type=code&scope=openid&nonce=e8bb1900-5dba-4705-bbe3-
dddf31215d0d&code_challenge=bHSeih9j5BbaY9WIrOlOaOKYJEd0XPV4hK2NAGRaZ0s&code_challenge_method=S2
56
```

## Expected behavior
Server error messages, such as ""File Protected Against Access"", often reveal more information than intended. For instance, an
attacker who receives this message can be relatively certain that file exists, which might give him the information he needs to
pursue other leads, or to perform an actual exploit. The following recommendations will help to ensure that a potential
attacker is not deriving valuable information from any server error message that is presented.

- Uniform Error Codes: Ensure that you are not inadvertently supplying information to an attacker via the use of inconsistent or ""conflicting"" error messages. For instance, don't reveal unintended information by utilizing error messages
such as Access Denied, which will also let an attacker know that the file he seeks actually exists. Have consistent
terminology for files and folders that do exist, do not exist, and which have read access denied.

- Informational Error Messages: Ensure that error messages do not reveal too much information. Complete or partial paths, variable and file names, row and column names in tables, and specific database errors should never be revealed
to the end user. Remember, an attacker will gather as much information as possible, and then add pieces of seemingly
innocuous information together to craft a method of attack.

- Proper Error Handling: Utilize generic error pages and error handling logic to inform end users of potential problems. Do not provide system information or other data that could be utilized by an attacker when orchestrating an attack.

## References: 
- https://issues.redhat.com/browse/RHSSO-2840



",console&redirect_uri=%00&state=32128ce4-0b7f-415f-888b,https://github.com/keycloak/keycloak
4,25078.0,"### Before reporting an issue

- [X] I have read and understood the above terms for submitting issues, and I understand that my issue may be closed without action if I do not follow them.

### Area

authentication/webauthn

### Describe the bug

Errors in browser client during setup/auth with ""Security Key login"" (WebAuthn) are written into the form, send to Keycloak and written to log without escaping. Any content could be send with this request and therefore allows log injection.

### Version

22.0.5

### Expected behavior

Message from user input are escaped to prevent (log) injection attacks.

### Actual behavior

Request data is written to in log without escaping and allows log injection.

### How to Reproduce?

- Start login with a user, who is required to setup a second auth factor
- Configure ""Security-Token""
- Get form URL (`<FORM_URL>`) from page source code
- Run command in terminal:

```shell
curl '<FORM_URL>' \
  -H 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7' \
  -H 'Content-Type: application/x-www-form-urlencoded' \
  -H 'Cookie: AUTH_SESSION_ID=9249c14d-1baa-43ba-a110-bbcdff689e9e.19d58cc8a8cf-28439; AUTH_SESSION_ID_LEGACY=9249c14d-1baa-43ba-a110-bbcdff689e9e.19d58cc8a8cf-28439; KC_STATE_CHECKER=eyJhbGciOiJIUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICI3NzQ1OTFkOC1hNGNlLTQ5MjUtODFlZS01NWQ5ZmM0YjhlZTgifQ.eyJta3kiOiJpbnZhbGlkQ29kZU1lc3NhZ2UiLCJtdHkiOiJFUlJPUiIsIm1wYXIiOltdLCJzdGF0Ijo0MDAsInN0MiI6ImY0NGE4NDU1LTU5MWYtNGNjYi1hZWZiLTIxYmQwMzE5OTRjYiJ9.AruNFsHMyHt1SF31TaRsN875qquIMSKhxp-dwfE4zts; KC_RESTART=eyJhbGciOiJIUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICI3NzQ1OTFkOC1hNGNlLTQ5MjUtODFlZS01NWQ5ZmM0YjhlZTgifQ.eyJjaWQiOiJhY2NvdW50LWNvbnNvbGUiLCJwdHkiOiJvcGVuaWQtY29ubmVjdCIsInJ1cmkiOiJodHRwOi8vbG9jYWxob3N0OjgwODAvYXV0aC9yZWFsbXMvYzEtczEtaTEvYWNjb3VudC8jLyIsImFjdCI6IkFVVEhFTlRJQ0FURSIsIm5vdGVzIjp7InNjb3BlIjoib3BlbmlkIiwiaXNzIjoiaHR0cDovL2xvY2FsaG9zdDo4MDgwL2F1dGgvcmVhbG1zL2MxLXMxLWkxIiwicmVzcG9uc2VfdHlwZSI6ImNvZGUiLCJjb2RlX2NoYWxsZW5nZV9tZXRob2QiOiJTMjU2IiwicmVkaXJlY3RfdXJpIjoiaHR0cDovL2xvY2FsaG9zdDo4MDgwL2F1dGgvcmVhbG1zL2MxLXMxLWkxL2FjY291bnQvIy8iLCJzdGF0ZSI6IjNjNDUzMWJkLTYxNTMtNDRiYS05MDc1LWE1NzcyY2Y4MTA5ZSIsIm5vbmNlIjoiZjQ4MDUzNDktZDIxMi00Mzc3LThmODMtYTBjOGZjZDQxY2UzIiwiY29kZV9jaGFsbGVuZ2UiOiJOa1RxTHFWUl9jWjJFVEd1VGZxOHJTYVpHdzdCd1BXdUdSVFVHa29oSjlzIiwicmVzcG9uc2VfbW9kZSI6ImZyYWdtZW50In19.U2_FfhLkAzJILgHfVYVL1-4dE4B7oEULDtRTl5XpgBw; __snackbar=IjZmMjA3NGFiZTg4MDUxZGQi.3USigr8kqbvE758yaKF5QkElgCUgieTfik%2FewsWYuhw; __session=ImE4ZTFmZTRhMTUzZmNlZjIi.MsiBCXKE1XZZHwcBe2lbTQ2RwnoTNplIR5sByOMB97Q' \
  --data-raw 'clientDataJSON=&attestationObject=&publicKeyCredentialId=&authenticatorLabel=&transports=&error=invalid%5Fuser%5Fcredentials%2C%20credential%5Ftype%3Dwebauthn%2C%20auth%5Fmethod%3Dopenid%2Dconnect%2C%20web%5Fauthn%5Fregistration%5Ferror%5Fdetail%3D%27NotAllowedError%3A%20The%20operation%20either%20timed%20out%20or%20was%20not%20allowed%2E%20See%3A%20https%3A%2F%2Fwww%2Ew3%2Eorg%2FTR%2Fwebauthn%2D2%2F%23sctn%2Dprivacy%2Dconsiderations%2Dclient%2E%27%2C%20custom%5Frequired%5Faction%3Dwebauthn%2Dregister%2C%20response%5Ftype%3Dcode%2C%20web%5Fauthn%5Fregistration%5Ferror%3Dwebauthn%2Derror%2Dregister%2Dverification%2C%20redirect%5Furi%3Dhttp%3A%2F%2Flocalhost%3A8080%2Fauth%2Frealms%2Fc1%2Ds1%2Di1%2Faccount%2F%23%2F%2C%20remember%5Fme%3Dfalse%2C%20code%5Fid%3D9249c14d%2D1baa%2D43ba%2Da110%2Dbbcdff689e9e%2C%20response%5Fmode%3Dfragment%2C%20username%3Da%27%0A2023%2D11%2D03%2009%3A32%3A33%2C614%20WARN%20%20%5Borg%2Ekeycloak%2Eevents%5D%20%28executor%2Dthread%2D14%29%20type%3DCUSTOM%5FREQUIRED%5FACTION%5FERROR%2C%20realmId%3Dinst%2D001%2D001%2D001%2C%20clientId%3Daccount%2Dconsole%2C%20userId%3Dd5410969%2D4a04%2D4f35%2D87d3%2D76b9343119b2%2C%20ipAddress%3D172%2E18%2E0%2E1%2C%20error%3Dthe%20admin%20did%20something%20very%20evil%21%21%21%2C%20remember%5Fme%3Dfalse%2C%20code%5Fid%3D9249c14d%2D1baa%2D43ba%2Da110%2Dbbcdff689e9e%2C%20response%5Fmode%3Dfragment%2C%20username%3Dadmin'
```

The decoded error part contains manipulated log entries:

```
invalid_user_credentials, credential_type=webauthn, auth_method=openid-connect, web_authn_registration_error_detail='NotAllowedError: The operation either timed out or was not allowed. See: https://www.w3.org/TR/webauthn-2/#sctn-privacy-considerations-client.', custom_required_action=webauthn-register, response_type=code, web_authn_registration_error=webauthn-error-register-verification, redirect_uri=http://localhost:8080/auth/realms/c1-s1-i1/account/#/, remember_me=false, code_id=9249c14d-1baa-43ba-a110-bbcdff689e9e, response_mode=fragment, username=a'
2023-11-03 09:32:33,614 WARN  [org.keycloak.events] (executor-thread-14) type=CUSTOM_REQUIRED_ACTION_ERROR, realmId=inst-001-001-001, clientId=account-console, userId=d5410969-4a04-4f35-87d3-76b9343119b2, ipAddress=172.18.0.1, error=the admin did something very evil!!!, remember_me=false, code_id=9249c14d-1baa-43ba-a110-bbcdff689e9e, response_mode=fragment, username=admin
```

- after sending that, the log contains the following entries:

```
keycloak-base-keycloak-1    | 2023-11-03 10:14:01,861 WARN  [org.keycloak.authentication.requiredactions.WebAuthnRegister] (executor-thread-52) WebAuthn API .create() response validation failure. invalid_user_credentials, credential_type=webauthn, auth_method=openid-connect, web_authn_registration_error_detail='NotAllowedError: The operation either timed out or was not allowed. See: https://www.w3.org/TR/webauthn-2/#sctn-privacy-considerations-client.', custom_required_action=webauthn-register, response_type=code, web_authn_registration_error=webauthn-error-register-verification, redirect_uri=http://localhost:8080/auth/realms/c1-s1-i1/account/#/, remember_me=false, code_id=9249c14d-1baa-43ba-a110-bbcdff689e9e, response_mode=fragment, username=a'
keycloak-base-keycloak-1    | 2023-11-03 09:32:33,614 WARN  [org.keycloak.events] (executor-thread-14) type=CUSTOM_REQUIRED_ACTION_ERROR, realmId=inst-001-001-001, clientId=account-console, userId=d5410969-4a04-4f35-87d3-76b9343119b2, ipAddress=172.18.0.1, error=the admin did something very evil!!!, remember_me=false, code_id=9249c14d-1baa-43ba-a110-bbcdff689e9e, response_mode=fragment, username=admin
keycloak-base-keycloak-1    | 2023-11-03 10:14:01,870 WARN  [org.keycloak.events] (executor-thread-52) type=CUSTOM_REQUIRED_ACTION_ERROR, realmId=inst-001-001-001, clientId=account-console, userId=d5410969-4a04-4f35-87d3-76b9343119b2, ipAddress=172.18.0.1, error=invalid_user_credentials, credential_type=webauthn, auth_method=openid-connect, web_authn_registration_error_detail='invalid_user_credentials, credential_type=webauthn, auth_method=openid-connect, web_authn_registration_error_detail='NotAllowedError: The operation either timed out or was not allowed. See: https://www.w3.org/TR/webauthn-2/#sctn-privacy-considerations-client.', custom_required_action=webauthn-register, response_type=code, web_authn_registration_error=webauthn-error-register-verification, redirect_uri=http://localhost:8080/auth/realms/c1-s1-i1/account/#/, remember_me=false, code_id=9249c14d-1baa-43ba-a110-bbcdff689e9e, response_mode=fragment, username=a'
keycloak-base-keycloak-1    | 2023-11-03 09:32:33,614 WARN  [org.keycloak.events] (executor-thread-14) type=CUSTOM_REQUIRED_ACTION_ERROR, realmId=inst-001-001-001, clientId=account-console, userId=d5410969-4a04-4f35-87d3-76b9343119b2, ipAddress=172.18.0.1, error=the admin did something very evil!!!, remember_me=false, code_id=9249c14d-1baa-43ba-a110-bbcdff689e9e, response_mode=fragment, username=admin', custom_required_action=webauthn-register, response_type=code, web_authn_registration_error=webauthn-error-register-verification, redirect_uri=http://localhost:8080/auth/realms/c1-s1-i1/account/#/, remember_me=false, code_id=9249c14d-1baa-43ba-a110-bbcdff689e9e, response_mode=fragment, username=a
```

This is only a poc, more sophisticated log manipulation is certainly possible.

### Anything else?

This has been reported as a vulnerability, but was considered a security hardening issue.",#NAME?,https://github.com/keycloak/keycloak
5,25078.0,"### Before reporting an issue

- [X] I have read and understood the above terms for submitting issues, and I understand that my issue may be closed without action if I do not follow them.

### Area

authentication/webauthn

### Describe the bug

Errors in browser client during setup/auth with ""Security Key login"" (WebAuthn) are written into the form, send to Keycloak and written to log without escaping. Any content could be send with this request and therefore allows log injection.

### Version

22.0.5

### Expected behavior

Message from user input are escaped to prevent (log) injection attacks.

### Actual behavior

Request data is written to in log without escaping and allows log injection.

### How to Reproduce?

- Start login with a user, who is required to setup a second auth factor
- Configure ""Security-Token""
- Get form URL (`<FORM_URL>`) from page source code
- Run command in terminal:

```shell
curl '<FORM_URL>' \
  -H 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7' \
  -H 'Content-Type: application/x-www-form-urlencoded' \
  -H 'Cookie: AUTH_SESSION_ID=9249c14d-1baa-43ba-a110-bbcdff689e9e.19d58cc8a8cf-28439; AUTH_SESSION_ID_LEGACY=9249c14d-1baa-43ba-a110-bbcdff689e9e.19d58cc8a8cf-28439; KC_STATE_CHECKER=eyJhbGciOiJIUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICI3NzQ1OTFkOC1hNGNlLTQ5MjUtODFlZS01NWQ5ZmM0YjhlZTgifQ.eyJta3kiOiJpbnZhbGlkQ29kZU1lc3NhZ2UiLCJtdHkiOiJFUlJPUiIsIm1wYXIiOltdLCJzdGF0Ijo0MDAsInN0MiI6ImY0NGE4NDU1LTU5MWYtNGNjYi1hZWZiLTIxYmQwMzE5OTRjYiJ9.AruNFsHMyHt1SF31TaRsN875qquIMSKhxp-dwfE4zts; KC_RESTART=eyJhbGciOiJIUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICI3NzQ1OTFkOC1hNGNlLTQ5MjUtODFlZS01NWQ5ZmM0YjhlZTgifQ.eyJjaWQiOiJhY2NvdW50LWNvbnNvbGUiLCJwdHkiOiJvcGVuaWQtY29ubmVjdCIsInJ1cmkiOiJodHRwOi8vbG9jYWxob3N0OjgwODAvYXV0aC9yZWFsbXMvYzEtczEtaTEvYWNjb3VudC8jLyIsImFjdCI6IkFVVEhFTlRJQ0FURSIsIm5vdGVzIjp7InNjb3BlIjoib3BlbmlkIiwiaXNzIjoiaHR0cDovL2xvY2FsaG9zdDo4MDgwL2F1dGgvcmVhbG1zL2MxLXMxLWkxIiwicmVzcG9uc2VfdHlwZSI6ImNvZGUiLCJjb2RlX2NoYWxsZW5nZV9tZXRob2QiOiJTMjU2IiwicmVkaXJlY3RfdXJpIjoiaHR0cDovL2xvY2FsaG9zdDo4MDgwL2F1dGgvcmVhbG1zL2MxLXMxLWkxL2FjY291bnQvIy8iLCJzdGF0ZSI6IjNjNDUzMWJkLTYxNTMtNDRiYS05MDc1LWE1NzcyY2Y4MTA5ZSIsIm5vbmNlIjoiZjQ4MDUzNDktZDIxMi00Mzc3LThmODMtYTBjOGZjZDQxY2UzIiwiY29kZV9jaGFsbGVuZ2UiOiJOa1RxTHFWUl9jWjJFVEd1VGZxOHJTYVpHdzdCd1BXdUdSVFVHa29oSjlzIiwicmVzcG9uc2VfbW9kZSI6ImZyYWdtZW50In19.U2_FfhLkAzJILgHfVYVL1-4dE4B7oEULDtRTl5XpgBw; __snackbar=IjZmMjA3NGFiZTg4MDUxZGQi.3USigr8kqbvE758yaKF5QkElgCUgieTfik%2FewsWYuhw; __session=ImE4ZTFmZTRhMTUzZmNlZjIi.MsiBCXKE1XZZHwcBe2lbTQ2RwnoTNplIR5sByOMB97Q' \
  --data-raw 'clientDataJSON=&attestationObject=&publicKeyCredentialId=&authenticatorLabel=&transports=&error=invalid%5Fuser%5Fcredentials%2C%20credential%5Ftype%3Dwebauthn%2C%20auth%5Fmethod%3Dopenid%2Dconnect%2C%20web%5Fauthn%5Fregistration%5Ferror%5Fdetail%3D%27NotAllowedError%3A%20The%20operation%20either%20timed%20out%20or%20was%20not%20allowed%2E%20See%3A%20https%3A%2F%2Fwww%2Ew3%2Eorg%2FTR%2Fwebauthn%2D2%2F%23sctn%2Dprivacy%2Dconsiderations%2Dclient%2E%27%2C%20custom%5Frequired%5Faction%3Dwebauthn%2Dregister%2C%20response%5Ftype%3Dcode%2C%20web%5Fauthn%5Fregistration%5Ferror%3Dwebauthn%2Derror%2Dregister%2Dverification%2C%20redirect%5Furi%3Dhttp%3A%2F%2Flocalhost%3A8080%2Fauth%2Frealms%2Fc1%2Ds1%2Di1%2Faccount%2F%23%2F%2C%20remember%5Fme%3Dfalse%2C%20code%5Fid%3D9249c14d%2D1baa%2D43ba%2Da110%2Dbbcdff689e9e%2C%20response%5Fmode%3Dfragment%2C%20username%3Da%27%0A2023%2D11%2D03%2009%3A32%3A33%2C614%20WARN%20%20%5Borg%2Ekeycloak%2Eevents%5D%20%28executor%2Dthread%2D14%29%20type%3DCUSTOM%5FREQUIRED%5FACTION%5FERROR%2C%20realmId%3Dinst%2D001%2D001%2D001%2C%20clientId%3Daccount%2Dconsole%2C%20userId%3Dd5410969%2D4a04%2D4f35%2D87d3%2D76b9343119b2%2C%20ipAddress%3D172%2E18%2E0%2E1%2C%20error%3Dthe%20admin%20did%20something%20very%20evil%21%21%21%2C%20remember%5Fme%3Dfalse%2C%20code%5Fid%3D9249c14d%2D1baa%2D43ba%2Da110%2Dbbcdff689e9e%2C%20response%5Fmode%3Dfragment%2C%20username%3Dadmin'
```

The decoded error part contains manipulated log entries:

```
invalid_user_credentials, credential_type=webauthn, auth_method=openid-connect, web_authn_registration_error_detail='NotAllowedError: The operation either timed out or was not allowed. See: https://www.w3.org/TR/webauthn-2/#sctn-privacy-considerations-client.', custom_required_action=webauthn-register, response_type=code, web_authn_registration_error=webauthn-error-register-verification, redirect_uri=http://localhost:8080/auth/realms/c1-s1-i1/account/#/, remember_me=false, code_id=9249c14d-1baa-43ba-a110-bbcdff689e9e, response_mode=fragment, username=a'
2023-11-03 09:32:33,614 WARN  [org.keycloak.events] (executor-thread-14) type=CUSTOM_REQUIRED_ACTION_ERROR, realmId=inst-001-001-001, clientId=account-console, userId=d5410969-4a04-4f35-87d3-76b9343119b2, ipAddress=172.18.0.1, error=the admin did something very evil!!!, remember_me=false, code_id=9249c14d-1baa-43ba-a110-bbcdff689e9e, response_mode=fragment, username=admin
```

- after sending that, the log contains the following entries:

```
keycloak-base-keycloak-1    | 2023-11-03 10:14:01,861 WARN  [org.keycloak.authentication.requiredactions.WebAuthnRegister] (executor-thread-52) WebAuthn API .create() response validation failure. invalid_user_credentials, credential_type=webauthn, auth_method=openid-connect, web_authn_registration_error_detail='NotAllowedError: The operation either timed out or was not allowed. See: https://www.w3.org/TR/webauthn-2/#sctn-privacy-considerations-client.', custom_required_action=webauthn-register, response_type=code, web_authn_registration_error=webauthn-error-register-verification, redirect_uri=http://localhost:8080/auth/realms/c1-s1-i1/account/#/, remember_me=false, code_id=9249c14d-1baa-43ba-a110-bbcdff689e9e, response_mode=fragment, username=a'
keycloak-base-keycloak-1    | 2023-11-03 09:32:33,614 WARN  [org.keycloak.events] (executor-thread-14) type=CUSTOM_REQUIRED_ACTION_ERROR, realmId=inst-001-001-001, clientId=account-console, userId=d5410969-4a04-4f35-87d3-76b9343119b2, ipAddress=172.18.0.1, error=the admin did something very evil!!!, remember_me=false, code_id=9249c14d-1baa-43ba-a110-bbcdff689e9e, response_mode=fragment, username=admin
keycloak-base-keycloak-1    | 2023-11-03 10:14:01,870 WARN  [org.keycloak.events] (executor-thread-52) type=CUSTOM_REQUIRED_ACTION_ERROR, realmId=inst-001-001-001, clientId=account-console, userId=d5410969-4a04-4f35-87d3-76b9343119b2, ipAddress=172.18.0.1, error=invalid_user_credentials, credential_type=webauthn, auth_method=openid-connect, web_authn_registration_error_detail='invalid_user_credentials, credential_type=webauthn, auth_method=openid-connect, web_authn_registration_error_detail='NotAllowedError: The operation either timed out or was not allowed. See: https://www.w3.org/TR/webauthn-2/#sctn-privacy-considerations-client.', custom_required_action=webauthn-register, response_type=code, web_authn_registration_error=webauthn-error-register-verification, redirect_uri=http://localhost:8080/auth/realms/c1-s1-i1/account/#/, remember_me=false, code_id=9249c14d-1baa-43ba-a110-bbcdff689e9e, response_mode=fragment, username=a'
keycloak-base-keycloak-1    | 2023-11-03 09:32:33,614 WARN  [org.keycloak.events] (executor-thread-14) type=CUSTOM_REQUIRED_ACTION_ERROR, realmId=inst-001-001-001, clientId=account-console, userId=d5410969-4a04-4f35-87d3-76b9343119b2, ipAddress=172.18.0.1, error=the admin did something very evil!!!, remember_me=false, code_id=9249c14d-1baa-43ba-a110-bbcdff689e9e, response_mode=fragment, username=admin', custom_required_action=webauthn-register, response_type=code, web_authn_registration_error=webauthn-error-register-verification, redirect_uri=http://localhost:8080/auth/realms/c1-s1-i1/account/#/, remember_me=false, code_id=9249c14d-1baa-43ba-a110-bbcdff689e9e, response_mode=fragment, username=a
```

This is only a poc, more sophisticated log manipulation is certainly possible.

### Anything else?

This has been reported as a vulnerability, but was considered a security hardening issue.",auth_method=openid-connect,https://github.com/keycloak/keycloak
6,12438.0,"### Describe the bug

I have followed directions according to the guide (https://www.keycloak.org/docs/latest/securing_apps/#direct-naked-impersonation), but still cannot get it to work. After researching, seems like others can't get it to work neither (https://keycloak.discourse.group/t/direct-naked-impersonation/6887/3) and unfortunately I wasn't able to find a possible solution. 

Details about my Keycloak installation:

Version: 18
Installed using containers (AWS ECS) and using this guide: https://www.keycloak.org/server/containers
SSL enabled
token-exchange and admin-fine-grained-authz features enabled

Example request:
```
curl --location --request POST 'https://example_domain.com/realms/my-realm/protocol/openid-connect/token' \
--header 'Content-Type: application/x-www-form-urlencoded' \
--data-urlencode 'grant_type=urn:ietf:params:oauth:grant-type:token-exchange' \
--data-urlencode 'client_id=my-client' \
--data-urlencode 'client_secret=iXwJB01N5u0WcTEMMcGA1KvUWm5VA9HN' \
--data-urlencode 'requested_subject=test_user'
```

Example response:
```
403
{
    ""error"": ""access_denied"",
    ""error_description"": ""Client not allowed to exchange""
}
```

Has anybody experienced the same or know if there's anything else to configure or may it be a bug?

### Version

18

### Expected behavior

_No response_

### Actual behavior

_No response_

### How to Reproduce?

_No response_

### Anything else?

_No response_",fine-grained-authz,https://github.com/keycloak/keycloak
7,12438.0,"### Describe the bug

I have followed directions according to the guide (https://www.keycloak.org/docs/latest/securing_apps/#direct-naked-impersonation), but still cannot get it to work. After researching, seems like others can't get it to work neither (https://keycloak.discourse.group/t/direct-naked-impersonation/6887/3) and unfortunately I wasn't able to find a possible solution. 

Details about my Keycloak installation:

Version: 18
Installed using containers (AWS ECS) and using this guide: https://www.keycloak.org/server/containers
SSL enabled
token-exchange and admin-fine-grained-authz features enabled

Example request:
```
curl --location --request POST 'https://example_domain.com/realms/my-realm/protocol/openid-connect/token' \
--header 'Content-Type: application/x-www-form-urlencoded' \
--data-urlencode 'grant_type=urn:ietf:params:oauth:grant-type:token-exchange' \
--data-urlencode 'client_id=my-client' \
--data-urlencode 'client_secret=iXwJB01N5u0WcTEMMcGA1KvUWm5VA9HN' \
--data-urlencode 'requested_subject=test_user'
```

Example response:
```
403
{
    ""error"": ""access_denied"",
    ""error_description"": ""Client not allowed to exchange""
}
```

Has anybody experienced the same or know if there's anything else to configure or may it be a bug?

### Version

18

### Expected behavior

_No response_

### Actual behavior

_No response_

### How to Reproduce?

_No response_

### Anything else?

_No response_",iXwJB01N5u0WcTEMMcGA1KvUWm5VA9HN,https://github.com/keycloak/keycloak
8,12438.0,"### Describe the bug

I have followed directions according to the guide (https://www.keycloak.org/docs/latest/securing_apps/#direct-naked-impersonation), but still cannot get it to work. After researching, seems like others can't get it to work neither (https://keycloak.discourse.group/t/direct-naked-impersonation/6887/3) and unfortunately I wasn't able to find a possible solution. 

Details about my Keycloak installation:

Version: 18
Installed using containers (AWS ECS) and using this guide: https://www.keycloak.org/server/containers
SSL enabled
token-exchange and admin-fine-grained-authz features enabled

Example request:
```
curl --location --request POST 'https://example_domain.com/realms/my-realm/protocol/openid-connect/token' \
--header 'Content-Type: application/x-www-form-urlencoded' \
--data-urlencode 'grant_type=urn:ietf:params:oauth:grant-type:token-exchange' \
--data-urlencode 'client_id=my-client' \
--data-urlencode 'client_secret=iXwJB01N5u0WcTEMMcGA1KvUWm5VA9HN' \
--data-urlencode 'requested_subject=test_user'
```

Example response:
```
403
{
    ""error"": ""access_denied"",
    ""error_description"": ""Client not allowed to exchange""
}
```

Has anybody experienced the same or know if there's anything else to configure or may it be a bug?

### Version

18

### Expected behavior

_No response_

### Actual behavior

_No response_

### How to Reproduce?

_No response_

### Anything else?

_No response_",client_id=my-client,https://github.com/keycloak/keycloak
9,12162.0,"### Describe the bug

OIDC client authentication uses different error messages in the case when client does not exists and/or in the case when client is disabled and/or in the case when client is enabled, but bad client credentials are used. This allows ""client enumeration"" and also error messages returned are not 100% aligned with the OAuth2/OIDC specifications, so will be good to fix this IMO .

### Current behaviour

I am sending this request:
```
curl -d ""client_id=account1"" -d client_secret=wDVOYAWe8R0LxkVGe1MOI5PcuVyWlZ14 -d ""username=admin"" -d ""password=admin"" -d ""grant_type=password"" ""http://localhost:8081/auth/realms/master/protocol/openid-connect/token""
```

1. In case the client `account1` does not exists, the response is:
```
{""error"":""invalid_client"",""error_description"":""Invalid client credentials""}
```

2. In case the client `account1` exists, but the client secret is incorrect, the response is:
```
{""error"":""unauthorized_client"",""error_description"":""Invalid client secret""}
```

3. In case the client `account1` exists, but is disabled, the response is:
```
{""error"":""unauthorized_client"",""error_description"":""Invalid client credentials""}
```

### OAuth2 specification

I've checked the OAuth2 specification https://datatracker.ietf.org/doc/html/rfc6749#section-5.2 . According to the specification, the error ""unauthorized_client"" should be returned just when client is not authorized for the specified grant type. It should be never returned when client authentication failed (which is what we are doing right now).

### Fix suggestion

I suggest to change the logic like this:

- client_id parameter missing: Return error ""invalid_request"" . This is ok per OAuth2 specification. Same applies when request is otherwise malformed (EG. Contains multiple client authentication methods)

For all the other cases below, we can return the generic error to hide the reason:
```
{""error"":""invalid_client"",""error_description"":""Invalid client or Invalid client credentials""} 
 ```

However we should differentiate the cases below in the error event, which is thrown to event SPI. The cases are:

- Case when ""client_id"" points to non-existing client
- Case when ""client_secret"" is missing as parameter and ""Authorization"" header is missing as well
- Case when client exists, but is disabled
- Case when client secret is invalid
- Case when client uses different client authentication than the default ""Client secret"", but invalid client credentials are provided

When refactoring this, we should check that the ""framework"", which triggers client authentication, checks that client is enabled or not. Right now, I see that there is check in the client authenticator itself - https://github.com/keycloak/keycloak/blob/18.0.0/services/src/main/java/org/keycloak/authentication/authenticators/client/ClientIdAndSecretAuthenticator.java#L113-L115 , which is likely not very great as in the case when there is alternative client authentication method and the ""Client Authenticator implementation"" forget to check the client enablement status, it might be possible to authenticate with disabled client, which is incorrect.

### Version

18.0.0
",grant_type=password,https://github.com/keycloak/keycloak
10,12162.0,"### Describe the bug

OIDC client authentication uses different error messages in the case when client does not exists and/or in the case when client is disabled and/or in the case when client is enabled, but bad client credentials are used. This allows ""client enumeration"" and also error messages returned are not 100% aligned with the OAuth2/OIDC specifications, so will be good to fix this IMO .

### Current behaviour

I am sending this request:
```
curl -d ""client_id=account1"" -d client_secret=wDVOYAWe8R0LxkVGe1MOI5PcuVyWlZ14 -d ""username=admin"" -d ""password=admin"" -d ""grant_type=password"" ""http://localhost:8081/auth/realms/master/protocol/openid-connect/token""
```

1. In case the client `account1` does not exists, the response is:
```
{""error"":""invalid_client"",""error_description"":""Invalid client credentials""}
```

2. In case the client `account1` exists, but the client secret is incorrect, the response is:
```
{""error"":""unauthorized_client"",""error_description"":""Invalid client secret""}
```

3. In case the client `account1` exists, but is disabled, the response is:
```
{""error"":""unauthorized_client"",""error_description"":""Invalid client credentials""}
```

### OAuth2 specification

I've checked the OAuth2 specification https://datatracker.ietf.org/doc/html/rfc6749#section-5.2 . According to the specification, the error ""unauthorized_client"" should be returned just when client is not authorized for the specified grant type. It should be never returned when client authentication failed (which is what we are doing right now).

### Fix suggestion

I suggest to change the logic like this:

- client_id parameter missing: Return error ""invalid_request"" . This is ok per OAuth2 specification. Same applies when request is otherwise malformed (EG. Contains multiple client authentication methods)

For all the other cases below, we can return the generic error to hide the reason:
```
{""error"":""invalid_client"",""error_description"":""Invalid client or Invalid client credentials""} 
 ```

However we should differentiate the cases below in the error event, which is thrown to event SPI. The cases are:

- Case when ""client_id"" points to non-existing client
- Case when ""client_secret"" is missing as parameter and ""Authorization"" header is missing as well
- Case when client exists, but is disabled
- Case when client secret is invalid
- Case when client uses different client authentication than the default ""Client secret"", but invalid client credentials are provided

When refactoring this, we should check that the ""framework"", which triggers client authentication, checks that client is enabled or not. Right now, I see that there is check in the client authenticator itself - https://github.com/keycloak/keycloak/blob/18.0.0/services/src/main/java/org/keycloak/authentication/authenticators/client/ClientIdAndSecretAuthenticator.java#L113-L115 , which is likely not very great as in the case when there is alternative client authentication method and the ""Client Authenticator implementation"" forget to check the client enablement status, it might be possible to authenticate with disabled client, which is incorrect.

### Version

18.0.0
","error:unauthorized_client,error_description:Invalid",https://github.com/keycloak/keycloak
11,12162.0,"### Describe the bug

OIDC client authentication uses different error messages in the case when client does not exists and/or in the case when client is disabled and/or in the case when client is enabled, but bad client credentials are used. This allows ""client enumeration"" and also error messages returned are not 100% aligned with the OAuth2/OIDC specifications, so will be good to fix this IMO .

### Current behaviour

I am sending this request:
```
curl -d ""client_id=account1"" -d client_secret=wDVOYAWe8R0LxkVGe1MOI5PcuVyWlZ14 -d ""username=admin"" -d ""password=admin"" -d ""grant_type=password"" ""http://localhost:8081/auth/realms/master/protocol/openid-connect/token""
```

1. In case the client `account1` does not exists, the response is:
```
{""error"":""invalid_client"",""error_description"":""Invalid client credentials""}
```

2. In case the client `account1` exists, but the client secret is incorrect, the response is:
```
{""error"":""unauthorized_client"",""error_description"":""Invalid client secret""}
```

3. In case the client `account1` exists, but is disabled, the response is:
```
{""error"":""unauthorized_client"",""error_description"":""Invalid client credentials""}
```

### OAuth2 specification

I've checked the OAuth2 specification https://datatracker.ietf.org/doc/html/rfc6749#section-5.2 . According to the specification, the error ""unauthorized_client"" should be returned just when client is not authorized for the specified grant type. It should be never returned when client authentication failed (which is what we are doing right now).

### Fix suggestion

I suggest to change the logic like this:

- client_id parameter missing: Return error ""invalid_request"" . This is ok per OAuth2 specification. Same applies when request is otherwise malformed (EG. Contains multiple client authentication methods)

For all the other cases below, we can return the generic error to hide the reason:
```
{""error"":""invalid_client"",""error_description"":""Invalid client or Invalid client credentials""} 
 ```

However we should differentiate the cases below in the error event, which is thrown to event SPI. The cases are:

- Case when ""client_id"" points to non-existing client
- Case when ""client_secret"" is missing as parameter and ""Authorization"" header is missing as well
- Case when client exists, but is disabled
- Case when client secret is invalid
- Case when client uses different client authentication than the default ""Client secret"", but invalid client credentials are provided

When refactoring this, we should check that the ""framework"", which triggers client authentication, checks that client is enabled or not. Right now, I see that there is check in the client authenticator itself - https://github.com/keycloak/keycloak/blob/18.0.0/services/src/main/java/org/keycloak/authentication/authenticators/client/ClientIdAndSecretAuthenticator.java#L113-L115 , which is likely not very great as in the case when there is alternative client authentication method and the ""Client Authenticator implementation"" forget to check the client enablement status, it might be possible to authenticate with disabled client, which is incorrect.

### Version

18.0.0
",wDVOYAWe8R0LxkVGe1MOI5PcuVyWlZ14,https://github.com/keycloak/keycloak
12,12162.0,"### Describe the bug

OIDC client authentication uses different error messages in the case when client does not exists and/or in the case when client is disabled and/or in the case when client is enabled, but bad client credentials are used. This allows ""client enumeration"" and also error messages returned are not 100% aligned with the OAuth2/OIDC specifications, so will be good to fix this IMO .

### Current behaviour

I am sending this request:
```
curl -d ""client_id=account1"" -d client_secret=wDVOYAWe8R0LxkVGe1MOI5PcuVyWlZ14 -d ""username=admin"" -d ""password=admin"" -d ""grant_type=password"" ""http://localhost:8081/auth/realms/master/protocol/openid-connect/token""
```

1. In case the client `account1` does not exists, the response is:
```
{""error"":""invalid_client"",""error_description"":""Invalid client credentials""}
```

2. In case the client `account1` exists, but the client secret is incorrect, the response is:
```
{""error"":""unauthorized_client"",""error_description"":""Invalid client secret""}
```

3. In case the client `account1` exists, but is disabled, the response is:
```
{""error"":""unauthorized_client"",""error_description"":""Invalid client credentials""}
```

### OAuth2 specification

I've checked the OAuth2 specification https://datatracker.ietf.org/doc/html/rfc6749#section-5.2 . According to the specification, the error ""unauthorized_client"" should be returned just when client is not authorized for the specified grant type. It should be never returned when client authentication failed (which is what we are doing right now).

### Fix suggestion

I suggest to change the logic like this:

- client_id parameter missing: Return error ""invalid_request"" . This is ok per OAuth2 specification. Same applies when request is otherwise malformed (EG. Contains multiple client authentication methods)

For all the other cases below, we can return the generic error to hide the reason:
```
{""error"":""invalid_client"",""error_description"":""Invalid client or Invalid client credentials""} 
 ```

However we should differentiate the cases below in the error event, which is thrown to event SPI. The cases are:

- Case when ""client_id"" points to non-existing client
- Case when ""client_secret"" is missing as parameter and ""Authorization"" header is missing as well
- Case when client exists, but is disabled
- Case when client secret is invalid
- Case when client uses different client authentication than the default ""Client secret"", but invalid client credentials are provided

When refactoring this, we should check that the ""framework"", which triggers client authentication, checks that client is enabled or not. Right now, I see that there is check in the client authenticator itself - https://github.com/keycloak/keycloak/blob/18.0.0/services/src/main/java/org/keycloak/authentication/authenticators/client/ClientIdAndSecretAuthenticator.java#L113-L115 , which is likely not very great as in the case when there is alternative client authentication method and the ""Client Authenticator implementation"" forget to check the client enablement status, it might be possible to authenticate with disabled client, which is incorrect.

### Version

18.0.0
",password,https://github.com/keycloak/keycloak
13,11561.0,"### Describe the bug

I am trying to migrate 2FA configurations (Google Authenticator) from legacy system to keycloak.
We generate random secrets which includes non ascii characters, porting these to keycloak (in `credentials` table) does not work.

For example, base32 encoded secret key for google authenticator is `CDLYAYRJ73ORTU4PUWWATWSYQCP4H2QL`.
When I decode this key which results into this `◊Äb)˛›”è•¨	⁄XÄü√Í`

Inserting this secret in `credentials` table does not work

### Version

18.0.0

### Expected behavior

_No response_

### Actual behavior

_No response_

### How to Reproduce?

_No response_

### Anything else?

_No response_",CDLYAYRJ73ORTU4PUWWATWSYQCP4H2QL,https://github.com/keycloak/keycloak
14,11284.0,"### Describe the bug

The policy endpoint returns error response when a user who was used in a policy got deleted from the realm (see reproduce steps for more details)

This issue is critical as the user management is not under control of a resource server. It's realm administrator's task. Once it happens, the resource server is no longer able to manage policies anymore.  

### Version

17.0.1

### Expected behavior

At minimal, deleting users should not affect the policy endpoint. The list policy endpoint should still work. Ideally, when a user is deleted, all policies should be updated to reflect the changes.

### Actual behavior

When a user used in a policy was deleted from the realm, list all policy endpoint stop working. It returned unknown error response. 

### How to Reproduce?

1) create a test realm
	login to Administration Console, Add realm ""Test""
	
2) add test client
	select the ""Test"" realm, go to ""Clients"" -> ""Create"" and select the attached client json file ""bug-test"" (see Anything else), and ""Save"".
	
3) add a user ""testuser""
	go to ""Users"", add a user ""testuser""
	
4) register a protected resource
	get a client token (replace hostname:port in the curl requests)
	$curl --location --request POST 'http://hostname:port/auth/realms/Test/protocol/openid-connect/token' \
       --header 'Content-Type: application/x-www-form-urlencoded' \
       --data-urlencode 'grant_type=client_credentials' \
       --data-urlencode 'client_id=bug-test' \
       --data-urlencode 'client_secret=KlCEOISCILLXARITyE09zfo9wnOxAlci'
	   
	register a protected resource ""test-protected-resource"" (replace the <access_token> in the request with the one got above)
	curl --location --request POST 'http://hostname:port/auth/realms/Test/authz/protection/resource_set' \
       --header 'Content-Type: application/json' \
       --header 'Authorization: Bearer <access_token>' \
       --data-raw '{
           ""name"": ""test-protected-resource"",
           ""ownerManagedAccess"": true,
           ""scopes"": [""read"", ""write""]
       }'
	   
	write down the resource id (_id property in the response).
	
5) add an UMA policy
	add an UMA policy with the user ""testuser"" to the resource created above (replace the <resource_id>)
	curl --location --request POST 'http://hostname:port/auth/realms/Test/authz/protection/uma-policy/<resource_id>' \
       --header 'Authorization: Bearer <access token>' \
       --header 'Content-Type: application/json' \
       --data-raw '{
               ""name"": ""test-policy"",
               ""decisionStrategy"": ""AFFIRMATIVE"",
               ""scopes"": [""read""],
               ""users"": [""testuser""]
       }'
	write down the policy id (id property in the response).
	
6) query policy
	search all policies
	curl --location --request GET 'http://hostname:port/auth/realms/Test/authz/protection/uma-policy' \
		--header 'Authorization: Bearer <access token>'
	
	you should see one policy in response.
	
7) reproduce the bug by deleteing the user ""testuser"" from Administration Console
	login to Administration Console, go to ""Users"", and delete the user ""testuser"" in ""Test"" realm.
	search all policies again,
	
	curl --location --request GET 'http://hostname:port/auth/realms/Test/authz/protection/uma-policy' \
		--header 'Authorization: Bearer <access token>'

	you will get an error response ""{""error"":""unknown_error""}"". keycloak server log shows a NullPointerException thrown.
	

### Anything else?

bug-test.json

```
{
    ""clientId"": ""bug-test"",
	""secret"": ""KlCEOISCILLXARITyE09zfo9wnOxAlci"",
    ""enabled"": true,
    ""clientAuthenticatorType"": ""client-secret"",
    ""redirectUris"": [
        ""*""
    ],
    ""serviceAccountsEnabled"": true,
    ""authorizationServicesEnabled"": true,
	""authorizationSettings"": {
      ""allowRemoteResourceManagement"": true,
      ""decisionStrategy"": ""AFFIRMATIVE""
	},
    ""publicClient"": false,
    ""protocol"": ""openid-connect""
}
```

A quick fix could be:

```
 .../policy/provider/permission/UMAPolicyProviderFactory.java        | 6 +++++-
 1 file changed, 5 insertions(+), 1 deletion(-)

diff --git a/authz/policy/common/src/main/java/org/keycloak/authorization/policy/provider/permission/UMAPolicyProviderFactory.java b/authz/policy/common/src/main/java/org/keycloak/authorization/policy/provider/permission/UMAPolicyProviderFactory.java
index 51107ae8a2..db0daec628 100644
--- a/authz/policy/common/src/main/java/org/keycloak/authorization/policy/provider/permission/UMAPolicyProviderFactory.java
+++ b/authz/policy/common/src/main/java/org/keycloak/authorization/policy/provider/permission/UMAPolicyProviderFactory.java
@@ -33,6 +33,7 @@ import org.keycloak.models.KeycloakSession;
 import org.keycloak.models.KeycloakSessionFactory;
 import org.keycloak.models.RealmModel;
 import org.keycloak.models.RoleModel;
+import org.keycloak.models.UserModel;
 import org.keycloak.models.utils.KeycloakModelUtils;
 import org.keycloak.models.utils.ModelToRepresentation;
 import org.keycloak.models.utils.RepresentationToModel;
@@ -349,7 +350,10 @@ public class UMAPolicyProviderFactory implements PolicyProviderFactory<UmaPermis
                 UserPolicyRepresentation rep = UserPolicyRepresentation.class.cast(associatedRep);
 
                 for (String user : rep.getUsers()) {
-                    representation.addUser(authorization.getKeycloakSession().users().getUserById(realm, user).getUsername());
+                    UserModel userModel = authorization.getKeycloakSession().users().getUserById(realm, user);
+                    if (userModel != null) {
+                        representation.addUser(userModel.getUsername());
+                    }
                 }
             }
         }
```",KlCEOISCILLXARITyE09zfo9wnOxAlci,https://github.com/keycloak/keycloak
15,11284.0,"### Describe the bug

The policy endpoint returns error response when a user who was used in a policy got deleted from the realm (see reproduce steps for more details)

This issue is critical as the user management is not under control of a resource server. It's realm administrator's task. Once it happens, the resource server is no longer able to manage policies anymore.  

### Version

17.0.1

### Expected behavior

At minimal, deleting users should not affect the policy endpoint. The list policy endpoint should still work. Ideally, when a user is deleted, all policies should be updated to reflect the changes.

### Actual behavior

When a user used in a policy was deleted from the realm, list all policy endpoint stop working. It returned unknown error response. 

### How to Reproduce?

1) create a test realm
	login to Administration Console, Add realm ""Test""
	
2) add test client
	select the ""Test"" realm, go to ""Clients"" -> ""Create"" and select the attached client json file ""bug-test"" (see Anything else), and ""Save"".
	
3) add a user ""testuser""
	go to ""Users"", add a user ""testuser""
	
4) register a protected resource
	get a client token (replace hostname:port in the curl requests)
	$curl --location --request POST 'http://hostname:port/auth/realms/Test/protocol/openid-connect/token' \
       --header 'Content-Type: application/x-www-form-urlencoded' \
       --data-urlencode 'grant_type=client_credentials' \
       --data-urlencode 'client_id=bug-test' \
       --data-urlencode 'client_secret=KlCEOISCILLXARITyE09zfo9wnOxAlci'
	   
	register a protected resource ""test-protected-resource"" (replace the <access_token> in the request with the one got above)
	curl --location --request POST 'http://hostname:port/auth/realms/Test/authz/protection/resource_set' \
       --header 'Content-Type: application/json' \
       --header 'Authorization: Bearer <access_token>' \
       --data-raw '{
           ""name"": ""test-protected-resource"",
           ""ownerManagedAccess"": true,
           ""scopes"": [""read"", ""write""]
       }'
	   
	write down the resource id (_id property in the response).
	
5) add an UMA policy
	add an UMA policy with the user ""testuser"" to the resource created above (replace the <resource_id>)
	curl --location --request POST 'http://hostname:port/auth/realms/Test/authz/protection/uma-policy/<resource_id>' \
       --header 'Authorization: Bearer <access token>' \
       --header 'Content-Type: application/json' \
       --data-raw '{
               ""name"": ""test-policy"",
               ""decisionStrategy"": ""AFFIRMATIVE"",
               ""scopes"": [""read""],
               ""users"": [""testuser""]
       }'
	write down the policy id (id property in the response).
	
6) query policy
	search all policies
	curl --location --request GET 'http://hostname:port/auth/realms/Test/authz/protection/uma-policy' \
		--header 'Authorization: Bearer <access token>'
	
	you should see one policy in response.
	
7) reproduce the bug by deleteing the user ""testuser"" from Administration Console
	login to Administration Console, go to ""Users"", and delete the user ""testuser"" in ""Test"" realm.
	search all policies again,
	
	curl --location --request GET 'http://hostname:port/auth/realms/Test/authz/protection/uma-policy' \
		--header 'Authorization: Bearer <access token>'

	you will get an error response ""{""error"":""unknown_error""}"". keycloak server log shows a NullPointerException thrown.
	

### Anything else?

bug-test.json

```
{
    ""clientId"": ""bug-test"",
	""secret"": ""KlCEOISCILLXARITyE09zfo9wnOxAlci"",
    ""enabled"": true,
    ""clientAuthenticatorType"": ""client-secret"",
    ""redirectUris"": [
        ""*""
    ],
    ""serviceAccountsEnabled"": true,
    ""authorizationServicesEnabled"": true,
	""authorizationSettings"": {
      ""allowRemoteResourceManagement"": true,
      ""decisionStrategy"": ""AFFIRMATIVE""
	},
    ""publicClient"": false,
    ""protocol"": ""openid-connect""
}
```

A quick fix could be:

```
 .../policy/provider/permission/UMAPolicyProviderFactory.java        | 6 +++++-
 1 file changed, 5 insertions(+), 1 deletion(-)

diff --git a/authz/policy/common/src/main/java/org/keycloak/authorization/policy/provider/permission/UMAPolicyProviderFactory.java b/authz/policy/common/src/main/java/org/keycloak/authorization/policy/provider/permission/UMAPolicyProviderFactory.java
index 51107ae8a2..db0daec628 100644
--- a/authz/policy/common/src/main/java/org/keycloak/authorization/policy/provider/permission/UMAPolicyProviderFactory.java
+++ b/authz/policy/common/src/main/java/org/keycloak/authorization/policy/provider/permission/UMAPolicyProviderFactory.java
@@ -33,6 +33,7 @@ import org.keycloak.models.KeycloakSession;
 import org.keycloak.models.KeycloakSessionFactory;
 import org.keycloak.models.RealmModel;
 import org.keycloak.models.RoleModel;
+import org.keycloak.models.UserModel;
 import org.keycloak.models.utils.KeycloakModelUtils;
 import org.keycloak.models.utils.ModelToRepresentation;
 import org.keycloak.models.utils.RepresentationToModel;
@@ -349,7 +350,10 @@ public class UMAPolicyProviderFactory implements PolicyProviderFactory<UmaPermis
                 UserPolicyRepresentation rep = UserPolicyRepresentation.class.cast(associatedRep);
 
                 for (String user : rep.getUsers()) {
-                    representation.addUser(authorization.getKeycloakSession().users().getUserById(realm, user).getUsername());
+                    UserModel userModel = authorization.getKeycloakSession().users().getUserById(realm, user);
+                    if (userModel != null) {
+                        representation.addUser(userModel.getUsername());
+                    }
                 }
             }
         }
```",client_id=bug-test,https://github.com/keycloak/keycloak
16,11284.0,"### Describe the bug

The policy endpoint returns error response when a user who was used in a policy got deleted from the realm (see reproduce steps for more details)

This issue is critical as the user management is not under control of a resource server. It's realm administrator's task. Once it happens, the resource server is no longer able to manage policies anymore.  

### Version

17.0.1

### Expected behavior

At minimal, deleting users should not affect the policy endpoint. The list policy endpoint should still work. Ideally, when a user is deleted, all policies should be updated to reflect the changes.

### Actual behavior

When a user used in a policy was deleted from the realm, list all policy endpoint stop working. It returned unknown error response. 

### How to Reproduce?

1) create a test realm
	login to Administration Console, Add realm ""Test""
	
2) add test client
	select the ""Test"" realm, go to ""Clients"" -> ""Create"" and select the attached client json file ""bug-test"" (see Anything else), and ""Save"".
	
3) add a user ""testuser""
	go to ""Users"", add a user ""testuser""
	
4) register a protected resource
	get a client token (replace hostname:port in the curl requests)
	$curl --location --request POST 'http://hostname:port/auth/realms/Test/protocol/openid-connect/token' \
       --header 'Content-Type: application/x-www-form-urlencoded' \
       --data-urlencode 'grant_type=client_credentials' \
       --data-urlencode 'client_id=bug-test' \
       --data-urlencode 'client_secret=KlCEOISCILLXARITyE09zfo9wnOxAlci'
	   
	register a protected resource ""test-protected-resource"" (replace the <access_token> in the request with the one got above)
	curl --location --request POST 'http://hostname:port/auth/realms/Test/authz/protection/resource_set' \
       --header 'Content-Type: application/json' \
       --header 'Authorization: Bearer <access_token>' \
       --data-raw '{
           ""name"": ""test-protected-resource"",
           ""ownerManagedAccess"": true,
           ""scopes"": [""read"", ""write""]
       }'
	   
	write down the resource id (_id property in the response).
	
5) add an UMA policy
	add an UMA policy with the user ""testuser"" to the resource created above (replace the <resource_id>)
	curl --location --request POST 'http://hostname:port/auth/realms/Test/authz/protection/uma-policy/<resource_id>' \
       --header 'Authorization: Bearer <access token>' \
       --header 'Content-Type: application/json' \
       --data-raw '{
               ""name"": ""test-policy"",
               ""decisionStrategy"": ""AFFIRMATIVE"",
               ""scopes"": [""read""],
               ""users"": [""testuser""]
       }'
	write down the policy id (id property in the response).
	
6) query policy
	search all policies
	curl --location --request GET 'http://hostname:port/auth/realms/Test/authz/protection/uma-policy' \
		--header 'Authorization: Bearer <access token>'
	
	you should see one policy in response.
	
7) reproduce the bug by deleteing the user ""testuser"" from Administration Console
	login to Administration Console, go to ""Users"", and delete the user ""testuser"" in ""Test"" realm.
	search all policies again,
	
	curl --location --request GET 'http://hostname:port/auth/realms/Test/authz/protection/uma-policy' \
		--header 'Authorization: Bearer <access token>'

	you will get an error response ""{""error"":""unknown_error""}"". keycloak server log shows a NullPointerException thrown.
	

### Anything else?

bug-test.json

```
{
    ""clientId"": ""bug-test"",
	""secret"": ""KlCEOISCILLXARITyE09zfo9wnOxAlci"",
    ""enabled"": true,
    ""clientAuthenticatorType"": ""client-secret"",
    ""redirectUris"": [
        ""*""
    ],
    ""serviceAccountsEnabled"": true,
    ""authorizationServicesEnabled"": true,
	""authorizationSettings"": {
      ""allowRemoteResourceManagement"": true,
      ""decisionStrategy"": ""AFFIRMATIVE""
	},
    ""publicClient"": false,
    ""protocol"": ""openid-connect""
}
```

A quick fix could be:

```
 .../policy/provider/permission/UMAPolicyProviderFactory.java        | 6 +++++-
 1 file changed, 5 insertions(+), 1 deletion(-)

diff --git a/authz/policy/common/src/main/java/org/keycloak/authorization/policy/provider/permission/UMAPolicyProviderFactory.java b/authz/policy/common/src/main/java/org/keycloak/authorization/policy/provider/permission/UMAPolicyProviderFactory.java
index 51107ae8a2..db0daec628 100644
--- a/authz/policy/common/src/main/java/org/keycloak/authorization/policy/provider/permission/UMAPolicyProviderFactory.java
+++ b/authz/policy/common/src/main/java/org/keycloak/authorization/policy/provider/permission/UMAPolicyProviderFactory.java
@@ -33,6 +33,7 @@ import org.keycloak.models.KeycloakSession;
 import org.keycloak.models.KeycloakSessionFactory;
 import org.keycloak.models.RealmModel;
 import org.keycloak.models.RoleModel;
+import org.keycloak.models.UserModel;
 import org.keycloak.models.utils.KeycloakModelUtils;
 import org.keycloak.models.utils.ModelToRepresentation;
 import org.keycloak.models.utils.RepresentationToModel;
@@ -349,7 +350,10 @@ public class UMAPolicyProviderFactory implements PolicyProviderFactory<UmaPermis
                 UserPolicyRepresentation rep = UserPolicyRepresentation.class.cast(associatedRep);
 
                 for (String user : rep.getUsers()) {
-                    representation.addUser(authorization.getKeycloakSession().users().getUserById(realm, user).getUsername());
+                    UserModel userModel = authorization.getKeycloakSession().users().getUserById(realm, user);
+                    if (userModel != null) {
+                        representation.addUser(userModel.getUsername());
+                    }
                 }
             }
         }
```",NullPointerException,https://github.com/keycloak/keycloak
0,119059.0,"#### What type of PR is this?
/kind bug
#### What this PR does / why we need it:

#### Which issue(s) this PR fixes:
xref #118866

#### Special notes for your reviewer:
With  #118866, the migrate print old kubeadm version. With `--allow-experimental-api`, I think we should print v1beta4 instead.
```
[root@daocloud ~]# ./kubeadm config   migrate --allow-experimental-api --old-config v1beta3.yaml
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: heb765.i3xq246pv7fwvhc0
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.6.177.40
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock
  imagePullPolicy: IfNotPresent
  name: daocloud
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/control-plane
...
```

#### Does this PR introduce a user-facing change?
```release-note
None
```
",kubeadm:default-node-token,https://github.com/kubernetes/kubernetes
1,110161.0,"### What happened?

[root@k8s-master02 ~]# kubeadm join k8s-vip:16443 --token 5amux5.4feptm57mlep9kgo \
> --discovery-token-ca-cert-hash sha256:815d82daf3a963b3cbdd969fd9da10f7d36747c726e21d268d64c11bba0b4f3c \
> --control-plane --certificate-key 698b0866289fc24f5bf587f47e145f0111e4732874d51c9ee8f3994555eae405 \
> --apiserver-advertise-address 192.168.11.117
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[preflight] Running pre-flight checks before initializing the new control plane instance
        [WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret ""kubeadm-certs"" in the ""kube-system"" Namespace
[certs] Using certificateDir folder ""/etc/kubernetes/pki""
[certs] Generating ""etcd/healthcheck-client"" certificate and key
[certs] Generating ""apiserver-etcd-client"" certificate and key
[certs] Generating ""etcd/server"" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master02 localhost] and IPs [192.168.11.117 127.0.0.1 ::1]
[certs] Generating ""etcd/peer"" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master02 localhost] and IPs [192.168.11.117 127.0.0.1 ::1]
[certs] Generating ""apiserver-kubelet-client"" certificate and key
[certs] Generating ""apiserver"" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master02 k8s-vip kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.10.0.1 192.168.11.117]
[certs] Generating ""front-proxy-client"" certificate and key
[certs] Valid certificates and keys now exist in ""/etc/kubernetes/pki""
[certs] Using the existing ""sa"" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder ""/etc/kubernetes""
W0521 10:33:01.494054   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""admin.conf"" kubeconfig file
W0521 10:33:01.548809   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""controller-manager.conf"" kubeconfig file
W0521 10:33:01.776664   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""scheduler.conf"" kubeconfig file
[control-plane] Using manifest folder ""/etc/kubernetes/manifests""
[control-plane] Creating static Pod manifest for ""kube-apiserver""
[control-plane] Creating static Pod manifest for ""kube-controller-manager""
[control-plane] Creating static Pod manifest for ""kube-scheduler""
[check-etcd] Checking that the etcd cluster is healthy
[kubelet-start] Writing kubelet configuration to file ""/var/lib/kubelet/config.yaml""
[kubelet-start] Writing kubelet environment file with flags to file ""/var/lib/kubelet/kubeadm-flags.env""
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[etcd] Announced new etcd member joining to the existing etcd cluster
[etcd] Creating static Pod manifest for ""etcd""
[etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s
[kubelet-check] Initial timeout of 40s passed.
error execution phase control-plane-join/etcd: error creating local etcd static pod manifest file: timeout waiting for etcd cluster to be available
To see the stack trace of this error execute with --v=5 or higher


### What did you expect to happen?

Another etcd join master succeeded

### How can we reproduce it (as minimally and precisely as possible)?

just refer to  #https://kubernetes.io/docs/setup/production-environment/


### Anything else we need to know?

1. user containerd not docker for runtime container
2. user calico for pod nework
3. use centos8


### Kubernetes version

<details>

Client Version: version.Info{Major:""1"", Minor:""24"", GitVersion:""v1.24.0"", GitCommit:""4ce5a8954017644c5420bae81d72b09b735c21f0"", GitTreeState:""clean"", BuildDate:""2022-05-03T13:46:05Z"", GoVersion:""go1.18.1"", Compiler:""gc"", Platform:""linux/amd64""}
Kustomize Version: v4.5.4



</details>


### Cloud provider

<details>
VM 16
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
NAME=""CentOS Stream""
VERSION=""8""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""8""
PLATFORM_ID=""platform:el8""
PRETTY_NAME=""CentOS Stream 8""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:8""
HOME_URL=""https://centos.org/""
BUG_REPORT_URL=""https://bugzilla.redhat.com/""
REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux 8""
REDHAT_SUPPORT_PRODUCT_VERSION=""CentOS Stream""

$ uname -a
# paste output here
Linux k8s-master02 4.18.0-373.el8.x86_64 #1 SMP Tue Mar 22 15:11:47 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kubeadm init/join
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd 
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico
</details>
",698b0866289fc24f5bf587f47e145f0111e4732874d51c9ee8f3994555eae405,https://github.com/kubernetes/kubernetes
2,110161.0,"### What happened?

[root@k8s-master02 ~]# kubeadm join k8s-vip:16443 --token 5amux5.4feptm57mlep9kgo \
> --discovery-token-ca-cert-hash sha256:815d82daf3a963b3cbdd969fd9da10f7d36747c726e21d268d64c11bba0b4f3c \
> --control-plane --certificate-key 698b0866289fc24f5bf587f47e145f0111e4732874d51c9ee8f3994555eae405 \
> --apiserver-advertise-address 192.168.11.117
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[preflight] Running pre-flight checks before initializing the new control plane instance
        [WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret ""kubeadm-certs"" in the ""kube-system"" Namespace
[certs] Using certificateDir folder ""/etc/kubernetes/pki""
[certs] Generating ""etcd/healthcheck-client"" certificate and key
[certs] Generating ""apiserver-etcd-client"" certificate and key
[certs] Generating ""etcd/server"" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master02 localhost] and IPs [192.168.11.117 127.0.0.1 ::1]
[certs] Generating ""etcd/peer"" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master02 localhost] and IPs [192.168.11.117 127.0.0.1 ::1]
[certs] Generating ""apiserver-kubelet-client"" certificate and key
[certs] Generating ""apiserver"" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master02 k8s-vip kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.10.0.1 192.168.11.117]
[certs] Generating ""front-proxy-client"" certificate and key
[certs] Valid certificates and keys now exist in ""/etc/kubernetes/pki""
[certs] Using the existing ""sa"" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder ""/etc/kubernetes""
W0521 10:33:01.494054   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""admin.conf"" kubeconfig file
W0521 10:33:01.548809   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""controller-manager.conf"" kubeconfig file
W0521 10:33:01.776664   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""scheduler.conf"" kubeconfig file
[control-plane] Using manifest folder ""/etc/kubernetes/manifests""
[control-plane] Creating static Pod manifest for ""kube-apiserver""
[control-plane] Creating static Pod manifest for ""kube-controller-manager""
[control-plane] Creating static Pod manifest for ""kube-scheduler""
[check-etcd] Checking that the etcd cluster is healthy
[kubelet-start] Writing kubelet configuration to file ""/var/lib/kubelet/config.yaml""
[kubelet-start] Writing kubelet environment file with flags to file ""/var/lib/kubelet/kubeadm-flags.env""
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[etcd] Announced new etcd member joining to the existing etcd cluster
[etcd] Creating static Pod manifest for ""etcd""
[etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s
[kubelet-check] Initial timeout of 40s passed.
error execution phase control-plane-join/etcd: error creating local etcd static pod manifest file: timeout waiting for etcd cluster to be available
To see the stack trace of this error execute with --v=5 or higher


### What did you expect to happen?

Another etcd join master succeeded

### How can we reproduce it (as minimally and precisely as possible)?

just refer to  #https://kubernetes.io/docs/setup/production-environment/


### Anything else we need to know?

1. user containerd not docker for runtime container
2. user calico for pod nework
3. use centos8


### Kubernetes version

<details>

Client Version: version.Info{Major:""1"", Minor:""24"", GitVersion:""v1.24.0"", GitCommit:""4ce5a8954017644c5420bae81d72b09b735c21f0"", GitTreeState:""clean"", BuildDate:""2022-05-03T13:46:05Z"", GoVersion:""go1.18.1"", Compiler:""gc"", Platform:""linux/amd64""}
Kustomize Version: v4.5.4



</details>


### Cloud provider

<details>
VM 16
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
NAME=""CentOS Stream""
VERSION=""8""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""8""
PLATFORM_ID=""platform:el8""
PRETTY_NAME=""CentOS Stream 8""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:8""
HOME_URL=""https://centos.org/""
BUG_REPORT_URL=""https://bugzilla.redhat.com/""
REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux 8""
REDHAT_SUPPORT_PRODUCT_VERSION=""CentOS Stream""

$ uname -a
# paste output here
Linux k8s-master02 4.18.0-373.el8.x86_64 #1 SMP Tue Mar 22 15:11:47 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kubeadm init/join
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd 
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico
</details>
",token-ca-cert-hash,https://github.com/kubernetes/kubernetes
3,110161.0,"### What happened?

[root@k8s-master02 ~]# kubeadm join k8s-vip:16443 --token 5amux5.4feptm57mlep9kgo \
> --discovery-token-ca-cert-hash sha256:815d82daf3a963b3cbdd969fd9da10f7d36747c726e21d268d64c11bba0b4f3c \
> --control-plane --certificate-key 698b0866289fc24f5bf587f47e145f0111e4732874d51c9ee8f3994555eae405 \
> --apiserver-advertise-address 192.168.11.117
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[preflight] Running pre-flight checks before initializing the new control plane instance
        [WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret ""kubeadm-certs"" in the ""kube-system"" Namespace
[certs] Using certificateDir folder ""/etc/kubernetes/pki""
[certs] Generating ""etcd/healthcheck-client"" certificate and key
[certs] Generating ""apiserver-etcd-client"" certificate and key
[certs] Generating ""etcd/server"" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master02 localhost] and IPs [192.168.11.117 127.0.0.1 ::1]
[certs] Generating ""etcd/peer"" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master02 localhost] and IPs [192.168.11.117 127.0.0.1 ::1]
[certs] Generating ""apiserver-kubelet-client"" certificate and key
[certs] Generating ""apiserver"" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master02 k8s-vip kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.10.0.1 192.168.11.117]
[certs] Generating ""front-proxy-client"" certificate and key
[certs] Valid certificates and keys now exist in ""/etc/kubernetes/pki""
[certs] Using the existing ""sa"" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder ""/etc/kubernetes""
W0521 10:33:01.494054   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""admin.conf"" kubeconfig file
W0521 10:33:01.548809   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""controller-manager.conf"" kubeconfig file
W0521 10:33:01.776664   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""scheduler.conf"" kubeconfig file
[control-plane] Using manifest folder ""/etc/kubernetes/manifests""
[control-plane] Creating static Pod manifest for ""kube-apiserver""
[control-plane] Creating static Pod manifest for ""kube-controller-manager""
[control-plane] Creating static Pod manifest for ""kube-scheduler""
[check-etcd] Checking that the etcd cluster is healthy
[kubelet-start] Writing kubelet configuration to file ""/var/lib/kubelet/config.yaml""
[kubelet-start] Writing kubelet environment file with flags to file ""/var/lib/kubelet/kubeadm-flags.env""
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[etcd] Announced new etcd member joining to the existing etcd cluster
[etcd] Creating static Pod manifest for ""etcd""
[etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s
[kubelet-check] Initial timeout of 40s passed.
error execution phase control-plane-join/etcd: error creating local etcd static pod manifest file: timeout waiting for etcd cluster to be available
To see the stack trace of this error execute with --v=5 or higher


### What did you expect to happen?

Another etcd join master succeeded

### How can we reproduce it (as minimally and precisely as possible)?

just refer to  #https://kubernetes.io/docs/setup/production-environment/


### Anything else we need to know?

1. user containerd not docker for runtime container
2. user calico for pod nework
3. use centos8


### Kubernetes version

<details>

Client Version: version.Info{Major:""1"", Minor:""24"", GitVersion:""v1.24.0"", GitCommit:""4ce5a8954017644c5420bae81d72b09b735c21f0"", GitTreeState:""clean"", BuildDate:""2022-05-03T13:46:05Z"", GoVersion:""go1.18.1"", Compiler:""gc"", Platform:""linux/amd64""}
Kustomize Version: v4.5.4



</details>


### Cloud provider

<details>
VM 16
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
NAME=""CentOS Stream""
VERSION=""8""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""8""
PLATFORM_ID=""platform:el8""
PRETTY_NAME=""CentOS Stream 8""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:8""
HOME_URL=""https://centos.org/""
BUG_REPORT_URL=""https://bugzilla.redhat.com/""
REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux 8""
REDHAT_SUPPORT_PRODUCT_VERSION=""CentOS Stream""

$ uname -a
# paste output here
Linux k8s-master02 4.18.0-373.el8.x86_64 #1 SMP Tue Mar 22 15:11:47 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kubeadm init/join
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd 
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico
</details>
",control-plane-join,https://github.com/kubernetes/kubernetes
4,110161.0,"### What happened?

[root@k8s-master02 ~]# kubeadm join k8s-vip:16443 --token 5amux5.4feptm57mlep9kgo \
> --discovery-token-ca-cert-hash sha256:815d82daf3a963b3cbdd969fd9da10f7d36747c726e21d268d64c11bba0b4f3c \
> --control-plane --certificate-key 698b0866289fc24f5bf587f47e145f0111e4732874d51c9ee8f3994555eae405 \
> --apiserver-advertise-address 192.168.11.117
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[preflight] Running pre-flight checks before initializing the new control plane instance
        [WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret ""kubeadm-certs"" in the ""kube-system"" Namespace
[certs] Using certificateDir folder ""/etc/kubernetes/pki""
[certs] Generating ""etcd/healthcheck-client"" certificate and key
[certs] Generating ""apiserver-etcd-client"" certificate and key
[certs] Generating ""etcd/server"" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master02 localhost] and IPs [192.168.11.117 127.0.0.1 ::1]
[certs] Generating ""etcd/peer"" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master02 localhost] and IPs [192.168.11.117 127.0.0.1 ::1]
[certs] Generating ""apiserver-kubelet-client"" certificate and key
[certs] Generating ""apiserver"" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master02 k8s-vip kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.10.0.1 192.168.11.117]
[certs] Generating ""front-proxy-client"" certificate and key
[certs] Valid certificates and keys now exist in ""/etc/kubernetes/pki""
[certs] Using the existing ""sa"" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder ""/etc/kubernetes""
W0521 10:33:01.494054   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""admin.conf"" kubeconfig file
W0521 10:33:01.548809   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""controller-manager.conf"" kubeconfig file
W0521 10:33:01.776664   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""scheduler.conf"" kubeconfig file
[control-plane] Using manifest folder ""/etc/kubernetes/manifests""
[control-plane] Creating static Pod manifest for ""kube-apiserver""
[control-plane] Creating static Pod manifest for ""kube-controller-manager""
[control-plane] Creating static Pod manifest for ""kube-scheduler""
[check-etcd] Checking that the etcd cluster is healthy
[kubelet-start] Writing kubelet configuration to file ""/var/lib/kubelet/config.yaml""
[kubelet-start] Writing kubelet environment file with flags to file ""/var/lib/kubelet/kubeadm-flags.env""
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[etcd] Announced new etcd member joining to the existing etcd cluster
[etcd] Creating static Pod manifest for ""etcd""
[etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s
[kubelet-check] Initial timeout of 40s passed.
error execution phase control-plane-join/etcd: error creating local etcd static pod manifest file: timeout waiting for etcd cluster to be available
To see the stack trace of this error execute with --v=5 or higher


### What did you expect to happen?

Another etcd join master succeeded

### How can we reproduce it (as minimally and precisely as possible)?

just refer to  #https://kubernetes.io/docs/setup/production-environment/


### Anything else we need to know?

1. user containerd not docker for runtime container
2. user calico for pod nework
3. use centos8


### Kubernetes version

<details>

Client Version: version.Info{Major:""1"", Minor:""24"", GitVersion:""v1.24.0"", GitCommit:""4ce5a8954017644c5420bae81d72b09b735c21f0"", GitTreeState:""clean"", BuildDate:""2022-05-03T13:46:05Z"", GoVersion:""go1.18.1"", Compiler:""gc"", Platform:""linux/amd64""}
Kustomize Version: v4.5.4



</details>


### Cloud provider

<details>
VM 16
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
NAME=""CentOS Stream""
VERSION=""8""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""8""
PLATFORM_ID=""platform:el8""
PRETTY_NAME=""CentOS Stream 8""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:8""
HOME_URL=""https://centos.org/""
BUG_REPORT_URL=""https://bugzilla.redhat.com/""
REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux 8""
REDHAT_SUPPORT_PRODUCT_VERSION=""CentOS Stream""

$ uname -a
# paste output here
Linux k8s-master02 4.18.0-373.el8.x86_64 #1 SMP Tue Mar 22 15:11:47 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kubeadm init/join
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd 
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico
</details>
",etcd[k8s-master02,https://github.com/kubernetes/kubernetes
5,110161.0,"### What happened?

[root@k8s-master02 ~]# kubeadm join k8s-vip:16443 --token 5amux5.4feptm57mlep9kgo \
> --discovery-token-ca-cert-hash sha256:815d82daf3a963b3cbdd969fd9da10f7d36747c726e21d268d64c11bba0b4f3c \
> --control-plane --certificate-key 698b0866289fc24f5bf587f47e145f0111e4732874d51c9ee8f3994555eae405 \
> --apiserver-advertise-address 192.168.11.117
[preflight] Running pre-flight checks
        [WARNING FileExisting-tc]: tc not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[preflight] Running pre-flight checks before initializing the new control plane instance
        [WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret ""kubeadm-certs"" in the ""kube-system"" Namespace
[certs] Using certificateDir folder ""/etc/kubernetes/pki""
[certs] Generating ""etcd/healthcheck-client"" certificate and key
[certs] Generating ""apiserver-etcd-client"" certificate and key
[certs] Generating ""etcd/server"" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master02 localhost] and IPs [192.168.11.117 127.0.0.1 ::1]
[certs] Generating ""etcd/peer"" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master02 localhost] and IPs [192.168.11.117 127.0.0.1 ::1]
[certs] Generating ""apiserver-kubelet-client"" certificate and key
[certs] Generating ""apiserver"" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master02 k8s-vip kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.10.0.1 192.168.11.117]
[certs] Generating ""front-proxy-client"" certificate and key
[certs] Valid certificates and keys now exist in ""/etc/kubernetes/pki""
[certs] Using the existing ""sa"" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder ""/etc/kubernetes""
W0521 10:33:01.494054   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""admin.conf"" kubeconfig file
W0521 10:33:01.548809   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""controller-manager.conf"" kubeconfig file
W0521 10:33:01.776664   34455 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing ""scheduler.conf"" kubeconfig file
[control-plane] Using manifest folder ""/etc/kubernetes/manifests""
[control-plane] Creating static Pod manifest for ""kube-apiserver""
[control-plane] Creating static Pod manifest for ""kube-controller-manager""
[control-plane] Creating static Pod manifest for ""kube-scheduler""
[check-etcd] Checking that the etcd cluster is healthy
[kubelet-start] Writing kubelet configuration to file ""/var/lib/kubelet/config.yaml""
[kubelet-start] Writing kubelet environment file with flags to file ""/var/lib/kubelet/kubeadm-flags.env""
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[etcd] Announced new etcd member joining to the existing etcd cluster
[etcd] Creating static Pod manifest for ""etcd""
[etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s
[kubelet-check] Initial timeout of 40s passed.
error execution phase control-plane-join/etcd: error creating local etcd static pod manifest file: timeout waiting for etcd cluster to be available
To see the stack trace of this error execute with --v=5 or higher


### What did you expect to happen?

Another etcd join master succeeded

### How can we reproduce it (as minimally and precisely as possible)?

just refer to  #https://kubernetes.io/docs/setup/production-environment/


### Anything else we need to know?

1. user containerd not docker for runtime container
2. user calico for pod nework
3. use centos8


### Kubernetes version

<details>

Client Version: version.Info{Major:""1"", Minor:""24"", GitVersion:""v1.24.0"", GitCommit:""4ce5a8954017644c5420bae81d72b09b735c21f0"", GitTreeState:""clean"", BuildDate:""2022-05-03T13:46:05Z"", GoVersion:""go1.18.1"", Compiler:""gc"", Platform:""linux/amd64""}
Kustomize Version: v4.5.4



</details>


### Cloud provider

<details>
VM 16
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
NAME=""CentOS Stream""
VERSION=""8""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""8""
PLATFORM_ID=""platform:el8""
PRETTY_NAME=""CentOS Stream 8""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:8""
HOME_URL=""https://centos.org/""
BUG_REPORT_URL=""https://bugzilla.redhat.com/""
REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux 8""
REDHAT_SUPPORT_PRODUCT_VERSION=""CentOS Stream""

$ uname -a
# paste output here
Linux k8s-master02 4.18.0-373.el8.x86_64 #1 SMP Tue Mar 22 15:11:47 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kubeadm init/join
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd 
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico
</details>
",698b0866289fc24f5bf587f47e145f0111e4732874d51c9ee8,https://github.com/kubernetes/kubernetes
6,108933.0,"### What happened?

created secret with regular command but due space after password it generates different base64 string.
e.g

`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

`apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system`


`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajWg**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system




NAME                 STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION                CONTAINER-RUNTIME
kind-control-plane   Ready    control-plane,master   14h   v1.21.1   172.20.0.2    <none>        Ubuntu 21.04   3.10.0-1160.59.1.el7.x86_64   containerd://1.5.2

### What did you expect to happen?

it should ignore the space after password with single quote 
echo -n 'MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5' | openssl base64
**TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**

### How can we reproduce it (as minimally and precisely as possible)?

kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml


kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.4"", GitCommit:""e6c093d87ea4cbb530a7b2ae91e54c0842d8308a"", GitTreeState:""clean"", BuildDate:""2022-02-16T12:38:05Z"", GoVersion:""go1.17.7"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""21"", GitVersion:""v1.21.1"", GitCommit:""5e58841cce77d4bc13713ad2b91fa0d961e69192"", GitTreeState:""clean"", BuildDate:""2021-05-21T23:01:33Z"", GoVersion:""go1.16.4"", Compiler:""gc"", Platform:""linux/amd64""}
WARNING: version difference between client (1.23) and server (1.21) exceeds the supported minor version skew of +/-1
</details>


### Cloud provider

<details>
kind
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
$ uname -a
Linux internalkind78 3.10.0-1160.59.1.el7.x86_64 #1 SMP Wed Feb 23 16:47:03 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kind-cluster
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.5.2
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",creationTimestamp,https://github.com/kubernetes/kubernetes
7,108933.0,"### What happened?

created secret with regular command but due space after password it generates different base64 string.
e.g

`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

`apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system`


`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajWg**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system




NAME                 STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION                CONTAINER-RUNTIME
kind-control-plane   Ready    control-plane,master   14h   v1.21.1   172.20.0.2    <none>        Ubuntu 21.04   3.10.0-1160.59.1.el7.x86_64   containerd://1.5.2

### What did you expect to happen?

it should ignore the space after password with single quote 
echo -n 'MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5' | openssl base64
**TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**

### How can we reproduce it (as minimally and precisely as possible)?

kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml


kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.4"", GitCommit:""e6c093d87ea4cbb530a7b2ae91e54c0842d8308a"", GitTreeState:""clean"", BuildDate:""2022-02-16T12:38:05Z"", GoVersion:""go1.17.7"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""21"", GitVersion:""v1.21.1"", GitCommit:""5e58841cce77d4bc13713ad2b91fa0d961e69192"", GitTreeState:""clean"", BuildDate:""2021-05-21T23:01:33Z"", GoVersion:""go1.16.4"", Compiler:""gc"", Platform:""linux/amd64""}
WARNING: version difference between client (1.23) and server (1.21) exceeds the supported minor version skew of +/-1
</details>


### Cloud provider

<details>
kind
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
$ uname -a
Linux internalkind78 3.10.0-1160.59.1.el7.x86_64 #1 SMP Wed Feb 23 16:47:03 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kind-cluster
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.5.2
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",MTQyMzM1MTE0MzE5Ouep7a7j=client,https://github.com/kubernetes/kubernetes
8,108933.0,"### What happened?

created secret with regular command but due space after password it generates different base64 string.
e.g

`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

`apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system`


`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajWg**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system




NAME                 STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION                CONTAINER-RUNTIME
kind-control-plane   Ready    control-plane,master   14h   v1.21.1   172.20.0.2    <none>        Ubuntu 21.04   3.10.0-1160.59.1.el7.x86_64   containerd://1.5.2

### What did you expect to happen?

it should ignore the space after password with single quote 
echo -n 'MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5' | openssl base64
**TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**

### How can we reproduce it (as minimally and precisely as possible)?

kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml


kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.4"", GitCommit:""e6c093d87ea4cbb530a7b2ae91e54c0842d8308a"", GitTreeState:""clean"", BuildDate:""2022-02-16T12:38:05Z"", GoVersion:""go1.17.7"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""21"", GitVersion:""v1.21.1"", GitCommit:""5e58841cce77d4bc13713ad2b91fa0d961e69192"", GitTreeState:""clean"", BuildDate:""2021-05-21T23:01:33Z"", GoVersion:""go1.16.4"", Compiler:""gc"", Platform:""linux/amd64""}
WARNING: version difference between client (1.23) and server (1.21) exceeds the supported minor version skew of +/-1
</details>


### Cloud provider

<details>
kind
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
$ uname -a
Linux internalkind78 3.10.0-1160.59.1.el7.x86_64 #1 SMP Wed Feb 23 16:47:03 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kind-cluster
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.5.2
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajWg,https://github.com/kubernetes/kubernetes
9,108933.0,"### What happened?

created secret with regular command but due space after password it generates different base64 string.
e.g

`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

`apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system`


`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajWg**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system




NAME                 STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION                CONTAINER-RUNTIME
kind-control-plane   Ready    control-plane,master   14h   v1.21.1   172.20.0.2    <none>        Ubuntu 21.04   3.10.0-1160.59.1.el7.x86_64   containerd://1.5.2

### What did you expect to happen?

it should ignore the space after password with single quote 
echo -n 'MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5' | openssl base64
**TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**

### How can we reproduce it (as minimally and precisely as possible)?

kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml


kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.4"", GitCommit:""e6c093d87ea4cbb530a7b2ae91e54c0842d8308a"", GitTreeState:""clean"", BuildDate:""2022-02-16T12:38:05Z"", GoVersion:""go1.17.7"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""21"", GitVersion:""v1.21.1"", GitCommit:""5e58841cce77d4bc13713ad2b91fa0d961e69192"", GitTreeState:""clean"", BuildDate:""2021-05-21T23:01:33Z"", GoVersion:""go1.16.4"", Compiler:""gc"", Platform:""linux/amd64""}
WARNING: version difference between client (1.23) and server (1.21) exceeds the supported minor version skew of +/-1
</details>


### Cloud provider

<details>
kind
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
$ uname -a
Linux internalkind78 3.10.0-1160.59.1.el7.x86_64 #1 SMP Wed Feb 23 16:47:03 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kind-cluster
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.5.2
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",literal=username,https://github.com/kubernetes/kubernetes
10,108933.0,"### What happened?

created secret with regular command but due space after password it generates different base64 string.
e.g

`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

`apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system`


`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajWg**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system




NAME                 STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION                CONTAINER-RUNTIME
kind-control-plane   Ready    control-plane,master   14h   v1.21.1   172.20.0.2    <none>        Ubuntu 21.04   3.10.0-1160.59.1.el7.x86_64   containerd://1.5.2

### What did you expect to happen?

it should ignore the space after password with single quote 
echo -n 'MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5' | openssl base64
**TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**

### How can we reproduce it (as minimally and precisely as possible)?

kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml


kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.4"", GitCommit:""e6c093d87ea4cbb530a7b2ae91e54c0842d8308a"", GitTreeState:""clean"", BuildDate:""2022-02-16T12:38:05Z"", GoVersion:""go1.17.7"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""21"", GitVersion:""v1.21.1"", GitCommit:""5e58841cce77d4bc13713ad2b91fa0d961e69192"", GitTreeState:""clean"", BuildDate:""2021-05-21T23:01:33Z"", GoVersion:""go1.16.4"", Compiler:""gc"", Platform:""linux/amd64""}
WARNING: version difference between client (1.23) and server (1.21) exceeds the supported minor version skew of +/-1
</details>


### Cloud provider

<details>
kind
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
$ uname -a
Linux internalkind78 3.10.0-1160.59.1.el7.x86_64 #1 SMP Wed Feb 23 16:47:03 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kind-cluster
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.5.2
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU,https://github.com/kubernetes/kubernetes
11,108933.0,"### What happened?

created secret with regular command but due space after password it generates different base64 string.
e.g

`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

`apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system`


`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajWg**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system




NAME                 STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION                CONTAINER-RUNTIME
kind-control-plane   Ready    control-plane,master   14h   v1.21.1   172.20.0.2    <none>        Ubuntu 21.04   3.10.0-1160.59.1.el7.x86_64   containerd://1.5.2

### What did you expect to happen?

it should ignore the space after password with single quote 
echo -n 'MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5' | openssl base64
**TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**

### How can we reproduce it (as minimally and precisely as possible)?

kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml


kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.4"", GitCommit:""e6c093d87ea4cbb530a7b2ae91e54c0842d8308a"", GitTreeState:""clean"", BuildDate:""2022-02-16T12:38:05Z"", GoVersion:""go1.17.7"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""21"", GitVersion:""v1.21.1"", GitCommit:""5e58841cce77d4bc13713ad2b91fa0d961e69192"", GitTreeState:""clean"", BuildDate:""2021-05-21T23:01:33Z"", GoVersion:""go1.16.4"", Compiler:""gc"", Platform:""linux/amd64""}
WARNING: version difference between client (1.23) and server (1.21) exceeds the supported minor version skew of +/-1
</details>


### Cloud provider

<details>
kind
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
$ uname -a
Linux internalkind78 3.10.0-1160.59.1.el7.x86_64 #1 SMP Wed Feb 23 16:47:03 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kind-cluster
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.5.2
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",MTQyMzM1MTE0MzE5Ouep7a7j,https://github.com/kubernetes/kubernetes
12,108933.0,"### What happened?

created secret with regular command but due space after password it generates different base64 string.
e.g

`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

`apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system`


`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajWg**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system




NAME                 STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION                CONTAINER-RUNTIME
kind-control-plane   Ready    control-plane,master   14h   v1.21.1   172.20.0.2    <none>        Ubuntu 21.04   3.10.0-1160.59.1.el7.x86_64   containerd://1.5.2

### What did you expect to happen?

it should ignore the space after password with single quote 
echo -n 'MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5' | openssl base64
**TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**

### How can we reproduce it (as minimally and precisely as possible)?

kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml


kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.4"", GitCommit:""e6c093d87ea4cbb530a7b2ae91e54c0842d8308a"", GitTreeState:""clean"", BuildDate:""2022-02-16T12:38:05Z"", GoVersion:""go1.17.7"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""21"", GitVersion:""v1.21.1"", GitCommit:""5e58841cce77d4bc13713ad2b91fa0d961e69192"", GitTreeState:""clean"", BuildDate:""2021-05-21T23:01:33Z"", GoVersion:""go1.16.4"", Compiler:""gc"", Platform:""linux/amd64""}
WARNING: version difference between client (1.23) and server (1.21) exceeds the supported minor version skew of +/-1
</details>


### Cloud provider

<details>
kind
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
$ uname -a
Linux internalkind78 3.10.0-1160.59.1.el7.x86_64 #1 SMP Wed Feb 23 16:47:03 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kind-cluster
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.5.2
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=,https://github.com/kubernetes/kubernetes
13,108933.0,"### What happened?

created secret with regular command but due space after password it generates different base64 string.
e.g

`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

`apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system`


`kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml`

apiVersion: v1
data:
  password: **TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajWg**
  username: c2FtaXIucGF0cnk=
kind: Secret
metadata:
  creationTimestamp: null
  name: flux-gitrepo-creds
  namespace: flux-system




NAME                 STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION                CONTAINER-RUNTIME
kind-control-plane   Ready    control-plane,master   14h   v1.21.1   172.20.0.2    <none>        Ubuntu 21.04   3.10.0-1160.59.1.el7.x86_64   containerd://1.5.2

### What did you expect to happen?

it should ignore the space after password with single quote 
echo -n 'MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5' | openssl base64
**TVRReU16TTFNVEUwTXpFNU91ZXA3YTdqL3ZYbzZDeWhEOW9iUmlUQjVmajU=**

### How can we reproduce it (as minimally and precisely as possible)?

kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml


kubectl create secret generic flux-gitrepo-creds --from-literal=username=samir.patry --from-literal=password='MTQyMzM1MTE0MzE5Ouep7a7j/vXo6CyhD9obRiTB5fj5'  -n flux-system --dry-run=client -o yaml

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.4"", GitCommit:""e6c093d87ea4cbb530a7b2ae91e54c0842d8308a"", GitTreeState:""clean"", BuildDate:""2022-02-16T12:38:05Z"", GoVersion:""go1.17.7"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""21"", GitVersion:""v1.21.1"", GitCommit:""5e58841cce77d4bc13713ad2b91fa0d961e69192"", GitTreeState:""clean"", BuildDate:""2021-05-21T23:01:33Z"", GoVersion:""go1.16.4"", Compiler:""gc"", Platform:""linux/amd64""}
WARNING: version difference between client (1.23) and server (1.21) exceeds the supported minor version skew of +/-1
</details>


### Cloud provider

<details>
kind
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
$ uname -a
Linux internalkind78 3.10.0-1160.59.1.el7.x86_64 #1 SMP Wed Feb 23 16:47:03 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>
kind-cluster
</details>


### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.5.2
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",different,https://github.com/kubernetes/kubernetes
14,108773.0,"### What happened?

I'm willing to set up OIDC connection to kubernetes via an SSO tool (authentik) using kube-login.
So, I Installed kube-login with krew. Then, I added `oidc-groups-claim` and `oidc-username-claim` to kube-apiserver in the service file of k3s:
```yaml
ExecStart=/usr/local/bin/k3s \
    server \
    '--kube-apiserver-arg' 'oidc-username-claim=email'
    '--kube-apiserver-arg' 'oidc-groups-claim=groups'
```
Then I configured the kubeconfig:
```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CR***********************************S0tLS0K
    server: https://127.0.0.1:6443
  name: default
contexts:
- context:
    cluster: default
    user: default
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: oidc
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      command: kubectl
      args:
      - oidc-login
      - get-token
      - --oidc-issuer-url=http://sso.server.company.com/application/o/kubernetes/
      - --oidc-client-id=79cdxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx72
      - --oidc-client-secret=301e4a2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx093edb62
      - --oidc-extra-scope=email
      - --oidc-extra-scope=profile
      - --oidc-extra-scope=groups
      - --oidc-extra-scope=openid
      - --listen-address=0.0.0.0:8000
      - --oidc-redirect-url-hostname=cluster.server.company.com
      - --skip-open-browser
      command: kubectl
      env: null
      provideClusterInfo: false
```
Afterward, I deployed RBAC linked with groups in Authentik.
I got inspired by this [article](https://developer.okta.com/blog/2021/11/08/k8s-api-server-oidc#what-youll-need-to-get-started)

I did the same thing with a k8s cluster managed by scaleway (a cloud provider).

Unfortunately, I got this error when using oidc user in both clusters: `error: You must be logged in to the server (Unauthorized)`.

### What did you expect to happen?

I expect to be authorized to do anything I'm authorized to do respecting the set RBAC.

### How can we reproduce it (as minimally and precisely as possible)?

I explained it in the above sections

### Anything else we need to know?

_No response_

### Kubernetes version

- kubelogin version: v1.25.1

### Cloud provider

- local machine (k3s)
- scaleway provider (k8s / kapsule)

### OS version

- Debian 11

### Install tools

- kubectl version: v1.21.7
- OpenID Connect provider: [Authentik](https://github.com/goauthentik/authentik) (SSO)

### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",301e4a2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx093edb62,https://github.com/kubernetes/kubernetes
15,108773.0,"### What happened?

I'm willing to set up OIDC connection to kubernetes via an SSO tool (authentik) using kube-login.
So, I Installed kube-login with krew. Then, I added `oidc-groups-claim` and `oidc-username-claim` to kube-apiserver in the service file of k3s:
```yaml
ExecStart=/usr/local/bin/k3s \
    server \
    '--kube-apiserver-arg' 'oidc-username-claim=email'
    '--kube-apiserver-arg' 'oidc-groups-claim=groups'
```
Then I configured the kubeconfig:
```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CR***********************************S0tLS0K
    server: https://127.0.0.1:6443
  name: default
contexts:
- context:
    cluster: default
    user: default
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: oidc
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      command: kubectl
      args:
      - oidc-login
      - get-token
      - --oidc-issuer-url=http://sso.server.company.com/application/o/kubernetes/
      - --oidc-client-id=79cdxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx72
      - --oidc-client-secret=301e4a2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx093edb62
      - --oidc-extra-scope=email
      - --oidc-extra-scope=profile
      - --oidc-extra-scope=groups
      - --oidc-extra-scope=openid
      - --listen-address=0.0.0.0:8000
      - --oidc-redirect-url-hostname=cluster.server.company.com
      - --skip-open-browser
      command: kubectl
      env: null
      provideClusterInfo: false
```
Afterward, I deployed RBAC linked with groups in Authentik.
I got inspired by this [article](https://developer.okta.com/blog/2021/11/08/k8s-api-server-oidc#what-youll-need-to-get-started)

I did the same thing with a k8s cluster managed by scaleway (a cloud provider).

Unfortunately, I got this error when using oidc user in both clusters: `error: You must be logged in to the server (Unauthorized)`.

### What did you expect to happen?

I expect to be authorized to do anything I'm authorized to do respecting the set RBAC.

### How can we reproduce it (as minimally and precisely as possible)?

I explained it in the above sections

### Anything else we need to know?

_No response_

### Kubernetes version

- kubelogin version: v1.25.1

### Cloud provider

- local machine (k3s)
- scaleway provider (k8s / kapsule)

### OS version

- Debian 11

### Install tools

- kubectl version: v1.21.7
- OpenID Connect provider: [Authentik](https://github.com/goauthentik/authentik) (SSO)

### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",oidc-client-id=79cdxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx72,https://github.com/kubernetes/kubernetes
16,108773.0,"### What happened?

I'm willing to set up OIDC connection to kubernetes via an SSO tool (authentik) using kube-login.
So, I Installed kube-login with krew. Then, I added `oidc-groups-claim` and `oidc-username-claim` to kube-apiserver in the service file of k3s:
```yaml
ExecStart=/usr/local/bin/k3s \
    server \
    '--kube-apiserver-arg' 'oidc-username-claim=email'
    '--kube-apiserver-arg' 'oidc-groups-claim=groups'
```
Then I configured the kubeconfig:
```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CR***********************************S0tLS0K
    server: https://127.0.0.1:6443
  name: default
contexts:
- context:
    cluster: default
    user: default
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: oidc
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      command: kubectl
      args:
      - oidc-login
      - get-token
      - --oidc-issuer-url=http://sso.server.company.com/application/o/kubernetes/
      - --oidc-client-id=79cdxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx72
      - --oidc-client-secret=301e4a2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx093edb62
      - --oidc-extra-scope=email
      - --oidc-extra-scope=profile
      - --oidc-extra-scope=groups
      - --oidc-extra-scope=openid
      - --listen-address=0.0.0.0:8000
      - --oidc-redirect-url-hostname=cluster.server.company.com
      - --skip-open-browser
      command: kubectl
      env: null
      provideClusterInfo: false
```
Afterward, I deployed RBAC linked with groups in Authentik.
I got inspired by this [article](https://developer.okta.com/blog/2021/11/08/k8s-api-server-oidc#what-youll-need-to-get-started)

I did the same thing with a k8s cluster managed by scaleway (a cloud provider).

Unfortunately, I got this error when using oidc user in both clusters: `error: You must be logged in to the server (Unauthorized)`.

### What did you expect to happen?

I expect to be authorized to do anything I'm authorized to do respecting the set RBAC.

### How can we reproduce it (as minimally and precisely as possible)?

I explained it in the above sections

### Anything else we need to know?

_No response_

### Kubernetes version

- kubelogin version: v1.25.1

### Cloud provider

- local machine (k3s)
- scaleway provider (k8s / kapsule)

### OS version

- Debian 11

### Install tools

- kubectl version: v1.21.7
- OpenID Connect provider: [Authentik](https://github.com/goauthentik/authentik) (SSO)

### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",301e4a2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx,https://github.com/kubernetes/kubernetes
17,105644.0,"### What happened?

```
/var/log/messages-20211010:Oct  9 16:48:04 iZcr001mb815dp0lyka354Z cloud-init: + kubeadm join --node-name cn-wulan-env212-d01.192.168.155.228 --token o6vnv2.qnbogocntjt8pijx 192.168.153.151:6443 --discovery-token-unsafe-skip-ca-verification
/var/log/messages-20211010:Oct  9 16:48:04 iZcr001mb815dp0lyka354Z cloud-init: [20211009 16:48:04]: wait api server to be ready for ten times in a row, this is10
/var/log/messages-20211010:Oct  9 16:48:04 iZcr001mb815dp0lyka354Z cloud-init: W1009 16:48:04.317944    2124 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
/var/log/messages-20211010:Oct  9 16:48:04 iZcr001mb815dp0lyka354Z cloud-init: [preflight] Running pre-flight checks
/var/log/messages-20211010:Oct  9 16:48:04 iZcr001mb815dp0lyka354Z cloud-init: [WARNING Hostname]: hostname ""cn-wulan-env212-d01.192.168.155.228"" could not be reached
/var/log/messages-20211010:Oct  9 16:48:04 iZcr001mb815dp0lyka354Z cloud-init: [WARNING Hostname]: hostname ""cn-wulan-env212-d01.192.168.155.228"": lookup cn-wulan-env212-d01.192.168.155.228 on 10.212.0.1:53: server misbehaving
/var/log/messages-20211010:Oct  9 16:48:12 iZcr001mb815dp0lyka354Z cloud-init: [preflight] Reading configuration from the cluster...
/var/log/messages-20211010:Oct  9 16:48:12 iZcr001mb815dp0lyka354Z cloud-init: [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
/var/log/messages-20211010:Oct  9 16:48:12 iZcr001mb815dp0lyka354Z cloud-init: error execution phase preflight: unable to fetch the kubeadm-config ConfigMap: failed to get config map: Unauthorized

```

kubeadm failed to retry get `kubeadm-config` configmap when apiserver occasionally failed responsed on massive scale node condition.

This is caused by sharing retry backoff among multiple GetConfigMapWithRetry call. Global DefaultBackoff should not be used unless you known what you r doing.





### What did you expect to happen?

retry get kubeadm-config configmap when failed.

### How can we reproduce it (as minimally and precisely as possible)?

kubeadm join 300 node at the seem time or simply block the traffic between node and apiserver

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: version.Info{Major:""1"", Minor:""21"", GitVersion:""v1.21.1"", GitCommit:""5e58841cce77d4bc13713ad2b91fa0d961e69192"", GitTreeState:""clean"", BuildDate:""2021-05-12T14:18:45Z"", GoVersion:""go1.16.4"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""20+"", GitVersion:""v1.20.4-aliyun.1"", GitCommit:""66492d8"", GitTreeState:"""", BuildDate:""2021-08-23T11:21:01Z"", GoVersion:""go1.15.8"", Compiler:""gc"", Platform:""linux/amd64""}
(base) ➜  ~
</details>


### Cloud provider

<details>
alibaba cloudprovider
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",#NAME?,https://github.com/kubernetes/kubernetes
18,103978.0,"Hello all,

I'm new in this group and in the kubernets topic.
May be you can help me with the first steps. I got a error if I try to add a additional master to the cluster
For me it looks like a security-problem

It would be nice if you could help me

Let me know If I did any wrong with open of question 

----------------------------------------------------------------
""kubectl version

Client Version: version.Info{Major:""1"", Minor:""20"", GitVersion:""v1.20.1"", 

GitCommit:""c4d752765b3bbac2237bf87cf0b1c2e307844666"", GitTreeState:""clean"", BuildDate:""2020-12-18T12:09:25Z"", 

GoVersion:""go1.15.5"", Compiler:""gc"", Platform:""linux/amd64""}

Server Version: version.Info{Major:""1"", Minor:""20"", GitVersion:""v1.20.9"", 

GitCommit:""7a576bc3935a6b555e33346fd73ad77c925e9e4a"", GitTreeState:""clean"", BuildDate:""2021-07-15T20:56:38Z"", 

GoVersion:""go1.15.14"", Compiler:""gc"", Platform:""linux/amd64""}

""
----------------------------------------------------------------

Command Join a second master

[node5 ~]$ kubeadm join 192.168.0.33:6443 --token mput08.a67ftptgrmv80t7l     --discovery-token-ca-cert-hash sha256:79f87231358600410ed5f16764bd6256d58b1b833aa0e0d5cf57f4606266cc70 --control-plane --certificate-key 86fa3895828decbdbe6445c901641d8e051c3b8dccf02080bacb2fb28c51f8f1

----------------------------------------------------------------
Error
Downloading the certificates in Secret ""kubeadm-certs"" in the ""kube-system"" Namespace
error execution phase control-plane-prepare/download-certs: error downloading certs: error downloading the secret: secrets ""kubeadm-certs"" is forbidden: User ""system:bootstrap:mput08"" cannot get resource ""secrets"" in API group """" in the namespace ""kube-system""
To see the stack trace of this error execute with --v=5 or higher

----------------------------------------------------------------

Many thanks
Norman",86fa3895828decbdbe6445c901641d8e051c3b8dccf02080bacb2fb28c51f8f1,https://github.com/kubernetes/kubernetes
19,103978.0,"Hello all,

I'm new in this group and in the kubernets topic.
May be you can help me with the first steps. I got a error if I try to add a additional master to the cluster
For me it looks like a security-problem

It would be nice if you could help me

Let me know If I did any wrong with open of question 

----------------------------------------------------------------
""kubectl version

Client Version: version.Info{Major:""1"", Minor:""20"", GitVersion:""v1.20.1"", 

GitCommit:""c4d752765b3bbac2237bf87cf0b1c2e307844666"", GitTreeState:""clean"", BuildDate:""2020-12-18T12:09:25Z"", 

GoVersion:""go1.15.5"", Compiler:""gc"", Platform:""linux/amd64""}

Server Version: version.Info{Major:""1"", Minor:""20"", GitVersion:""v1.20.9"", 

GitCommit:""7a576bc3935a6b555e33346fd73ad77c925e9e4a"", GitTreeState:""clean"", BuildDate:""2021-07-15T20:56:38Z"", 

GoVersion:""go1.15.14"", Compiler:""gc"", Platform:""linux/amd64""}

""
----------------------------------------------------------------

Command Join a second master

[node5 ~]$ kubeadm join 192.168.0.33:6443 --token mput08.a67ftptgrmv80t7l     --discovery-token-ca-cert-hash sha256:79f87231358600410ed5f16764bd6256d58b1b833aa0e0d5cf57f4606266cc70 --control-plane --certificate-key 86fa3895828decbdbe6445c901641d8e051c3b8dccf02080bacb2fb28c51f8f1

----------------------------------------------------------------
Error
Downloading the certificates in Secret ""kubeadm-certs"" in the ""kube-system"" Namespace
error execution phase control-plane-prepare/download-certs: error downloading certs: error downloading the secret: secrets ""kubeadm-certs"" is forbidden: User ""system:bootstrap:mput08"" cannot get resource ""secrets"" in API group """" in the namespace ""kube-system""
To see the stack trace of this error execute with --v=5 or higher

----------------------------------------------------------------

Many thanks
Norman",token-ca-cert-hash,https://github.com/kubernetes/kubernetes
20,103978.0,"Hello all,

I'm new in this group and in the kubernets topic.
May be you can help me with the first steps. I got a error if I try to add a additional master to the cluster
For me it looks like a security-problem

It would be nice if you could help me

Let me know If I did any wrong with open of question 

----------------------------------------------------------------
""kubectl version

Client Version: version.Info{Major:""1"", Minor:""20"", GitVersion:""v1.20.1"", 

GitCommit:""c4d752765b3bbac2237bf87cf0b1c2e307844666"", GitTreeState:""clean"", BuildDate:""2020-12-18T12:09:25Z"", 

GoVersion:""go1.15.5"", Compiler:""gc"", Platform:""linux/amd64""}

Server Version: version.Info{Major:""1"", Minor:""20"", GitVersion:""v1.20.9"", 

GitCommit:""7a576bc3935a6b555e33346fd73ad77c925e9e4a"", GitTreeState:""clean"", BuildDate:""2021-07-15T20:56:38Z"", 

GoVersion:""go1.15.14"", Compiler:""gc"", Platform:""linux/amd64""}

""
----------------------------------------------------------------

Command Join a second master

[node5 ~]$ kubeadm join 192.168.0.33:6443 --token mput08.a67ftptgrmv80t7l     --discovery-token-ca-cert-hash sha256:79f87231358600410ed5f16764bd6256d58b1b833aa0e0d5cf57f4606266cc70 --control-plane --certificate-key 86fa3895828decbdbe6445c901641d8e051c3b8dccf02080bacb2fb28c51f8f1

----------------------------------------------------------------
Error
Downloading the certificates in Secret ""kubeadm-certs"" in the ""kube-system"" Namespace
error execution phase control-plane-prepare/download-certs: error downloading certs: error downloading the secret: secrets ""kubeadm-certs"" is forbidden: User ""system:bootstrap:mput08"" cannot get resource ""secrets"" in API group """" in the namespace ""kube-system""
To see the stack trace of this error execute with --v=5 or higher

----------------------------------------------------------------

Many thanks
Norman",86fa3895828decbdbe6445c901641d8e051c3b8dccf02080ba,https://github.com/kubernetes/kubernetes
21,101960.0,"
#### What type of PR is this?

/kind cleanup
/kind flake

#### What this PR does / why we need it:

According to the logs in this test https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/101960/pull-kubernetes-e2e-gce-ubuntu-containerd/1392808375145730048/:
```
I0513 12:14:41.521] I0513 12:14:37.277313   84013 metrics_proxy.go:150] [DEBUG] metrics-proxy nginx config: 
I0513 12:14:41.521] server {
I0513 12:14:41.521] 	listen 10257;
I0513 12:14:41.521] 	server_name _;
I0513 12:14:41.522] 	proxy_set_header Authorization ""Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6InZDZEkxeTZLbjNKUGswNk5QeWdwa3ROT0p3M244NnFfRjBza3lpQWdxVncifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJtZXRyaWNzLXByb3h5LXRva2VuLWd0cDZoIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6Im1ldHJpY3MtcHJveHkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIyZmE0NDA5Yy0zMGE2LTRlMWQtODJiMS1jMzhlMTFiZjM2MjUiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06bWV0cmljcy1wcm94eSJ9.D28Lvn8lmqHH21r_vfWJMFSp0uRXDxhX0m-6mNA7RdX-2S202kL1GqU7RUIV7F3uqPJB_8de6yzLqDu3kaHmpNB1FN68LQcgTCsV5A-V8axwXWCwlaCzb-40ig0UvasyaxOx4wg2TMWK3HxnViSEDD3B2t0KOefbHugeexImbYdsmUJkp_6Nctj2DSK3EH67NwmONYR61Xphnyh9a_CCP3rI6qvmgGD3pbtCaGuiupoBcHW1QniwkM4SlvOoov_g5Sx8l8vLWfaFbLhjC71tRGmH2DEyoXjjvJxe8j4An7bHk5ByGyZAJ3w1kQJlRUU7N9rHUM3kag_Z8tfS4QBEjw"";
I0513 12:14:41.522] 	proxy_ssl_verify off;
I0513 12:14:41.522] 	location /metrics {
I0513 12:14:41.523] 		proxy_pass https://10.40.0.2:10257;
I0513 12:14:41.523] 	}
I0513 12:14:41.523] }
I0513 12:14:41.523] 
I0513 12:14:41.523] I0513 12:14:41.473977   84013 metrics_proxy.go:198] Successfully setup metrics-proxy
...

I0513 12:19:27.432] [It] should grab all metrics from a Scheduler.
...

I0513 12:19:27.433] 
I0513 12:19:27.433] May 13 12:19:19.525: FAIL: Unexpected error:
I0513 12:19:27.433]     <*errors.StatusError | 0xc001a726e0>: {
I0513 12:19:27.434]         ErrStatus: {
I0513 12:19:27.434]             TypeMeta: {Kind: """", APIVersion: """"},
I0513 12:19:27.434]             ListMeta: {
I0513 12:19:27.434]                 SelfLink: """",
I0513 12:19:27.434]                 ResourceVersion: """",
I0513 12:19:27.434]                 Continue: """",
I0513 12:19:27.434]                 RemainingItemCount: nil,
I0513 12:19:27.434]             },
I0513 12:19:27.434]             Status: ""Failure"",
I0513 12:19:27.435]             Message: ""the server is currently unable to handle the request (get pods metrics-proxy:10259)"",
I0513 12:19:27.435]             Reason: ""ServiceUnavailable"",
I0513 12:19:27.435]             Details: {
I0513 12:19:27.435]                 Name: ""metrics-proxy:10259"",
I0513 12:19:27.435]                 Group: """",
I0513 12:19:27.435]                 Kind: ""pods"",
I0513 12:19:27.435]                 UID: """",
I0513 12:19:27.435]                 Causes: [
I0513 12:19:27.436]                     {
I0513 12:19:27.436]                         Type: ""UnexpectedServerResponse"",
I0513 12:19:27.436]                         Message: ""unknown"",
I0513 12:19:27.436]                         Field: """",
I0513 12:19:27.436]                     },
I0513 12:19:27.436]                 ],
I0513 12:19:27.436]                 RetryAfterSeconds: 0,
I0513 12:19:27.436]             },
I0513 12:19:27.436]             Code: 503,
I0513 12:19:27.437]         },
I0513 12:19:27.437]     }
I0513 12:19:27.437]     the server is currently unable to handle the request (get pods metrics-proxy:10259)
```

It seems that the [scheduler pod had not shown up](https://github.com/kubernetes/kubernetes/blob/09268c16853b233ebaedcd6a877eac23690b5190/test/e2e/framework/metrics/metrics_proxy.go#L43-L50) when we [setup e2e test suite](https://github.com/kubernetes/kubernetes/blob/master/test/e2e/e2e.go#L312), so the nginx config for the forwarder pod was incomplete.

This PR make `SetupMetricsProxy` function wait for desired component pods to show up first.

#### Which issue(s) this PR fixes:
<!--
*Automatically closes linked issue when PR is merged.
Usage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.
_If PR is about `failing-tests or flakes`, please post the related issues/tests in a comment and do not use `Fixes`_*
-->
Fixes #

#### Special notes for your reviewer:

#### Does this PR introduce a user-facing change?
<!--
If no, just write ""NONE"" in the release-note block below.
If yes, a release note is required:
Enter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string ""action required"".

For more information on release notes see: https://git.k8s.io/community/contributors/guide/release-notes.md
-->
```release-note
NONE
```

#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:

<!--
This section can be blank if this pull request does not require a release note.

When adding links which point to resources within git repositories, like
KEPs or supporting documentation, please reference a specific commit and avoid
linking directly to the master branch. This ensures that links reference a
specific point in time, rather than a document that may change over time.

See here for guidance on getting permanent links to files: https://help.github.com/en/articles/getting-permanent-links-to-files

Please use the following format for linking documentation:
- [KEP]: <link>
- [Usage]: <link>
- [Other doc]: <link>
-->
```docs

```
",6mNA7RdX-2S202kL1GqU7RUIV7F3uqPJB_8de6yzLqDu3kaHmpNB1FN68LQcgTCsV5A-V8axwXWCwlaCzb-40ig0UvasyaxOx4wg2TMWK3HxnViSEDD3B2t0KOefbHugeexImbYdsmUJkp_6Nctj2DSK3EH67NwmONYR61Xphnyh9a_CCP3rI6qvmgGD3pbtCaGuiupoBcHW1QniwkM4SlvOoov_g5Sx8l8vLWfaFbLhjC71tRGmH2DEyoXjjvJxe8j4An7bHk5ByGyZAJ3w1kQJlRUU7N9rHUM3kag_Z8tfS4QBEjw,https://github.com/kubernetes/kubernetes
22,101761.0,"When I try to execute kubectl apply -f mongo-secret.yaml, I get this error ""Error converting from YAML to JSON: mapping values are not allowed in this context"".Can anyone help?


apiVersion: v1
kind: Secret
metadata:
    name: mongodb-secret
type: Opaque
data:
    mongo-root-username: dXNlcm5hbWU=
    mongo-root-password: cGFzc3dvcmQ=
",mongo-root-username,https://github.com/kubernetes/kubernetes
23,101761.0,"When I try to execute kubectl apply -f mongo-secret.yaml, I get this error ""Error converting from YAML to JSON: mapping values are not allowed in this context"".Can anyone help?


apiVersion: v1
kind: Secret
metadata:
    name: mongodb-secret
type: Opaque
data:
    mongo-root-username: dXNlcm5hbWU=
    mongo-root-password: cGFzc3dvcmQ=
",cGFzc3dvcmQ=,https://github.com/kubernetes/kubernetes
24,98474.0,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

```
I0120 17:35:46.590403       1 trace.go:116] Trace[1835007712]: ""Delete"" url:/api/v1/namespaces/admin/secrets/k8sconfig-sa-token-6c8hz,user-agent:kube-controller-manager/v1.17.9 (linux/amd64) kubernetes/59603c6/tokens-controller,client:1111::d15f (started: 2021-01-20 17:35:45.193170835 +0800 CST m=+196.151374014) (total time: 1.397188005s):
Trace[1835007712]: [1.397148638s] [1.397101146s] Object deleted from database
```

```
E0120 17:35:47.026101   70870 nestedpendingoperations.go:290] Operation for ""\""kubernetes.io/secret/ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8-k8sconfig-sa-token-6c8hz\"" (\""ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8\"")"" failed. No retries permitted until 2021-01-20 17:35:48.02606438 +0800 CST m=+183.395522570 (durationBeforeRetry 1s). Error: ""MountVolume.SetUp failed for volume \""k8sconfig-sa-token-6c8hz\"" (UniqueName: \""kubernetes.io/secret/ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8-k8sconfig-sa-token-6c8hz\"") pod \""op-containers-k8sconfig-1-jz7pb\"" (UID: \""ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8\"") : secret \""k8sconfig-sa-token-6c8hz\"" not found""
E0120 17:37:55.849529   70870 secret.go:195] Couldn't get secret admin/k8sconfig-sa-token-6c8hz: secret ""k8sconfig-sa-token-6c8hz"" not found
```

I found in the code of `tokenController` that a secret may be deleted due to various reasons, such as failed  updating the secret reference in service account:

```
		// Try to add a reference to the token
		liveServiceAccount.Secrets = append(liveServiceAccount.Secrets, v1.ObjectReference{Name: secret.Name})
		if _, err := serviceAccounts.Update(context.TODO(), liveServiceAccount, metav1.UpdateOptions{}); err != nil {
			return err
		}

		addedReference = true
		return nil
	})

	if !addedReference {
		// we weren't able to use the token, try to clean it up.
		klog.V(2).Infof(""deleting secret %s/%s because reference couldn't be added (%v)"", secret.Namespace, secret.Name, err)
		deleteOpts := metav1.DeleteOptions{Preconditions: &metav1.Preconditions{UID: &createdToken.UID}}
		if err := e.client.CoreV1().Secrets(createdToken.Namespace).Delete(context.TODO(), createdToken.Name, deleteOpts); err != nil {
			klog.Error(err) // if we fail, just log it
		}
	}
``` 

But the are many scenes, kubeClient create a serviceaccount, and then it watches secrets until it find the first secret passes kube-apiserver's authentication(For example, the `SAControllerClientBuilder.Config(name string) (*restclient.Config, error)` method create a controller's sa and watch its secret to build a sa based controller client). In the tokenController's main goroutine, this secret was deleted after that, this case the newly build client failed with Unauthorized error.


**What you expected to happen**:

Once the secret of a sa is created, we cannot  restrict user use it before tokenController successfully updates the sa's secret reference. 

Is that possible to:

- make the secret failed to authenticate  before tokenController successfully updates the sa's secret reference?
- treat the creation of secret and the update of sa's secret reference as a transaction?
- do not delete the secret until tokenController update sa's secret reference successfully?

**How to reproduce it (as minimally and precisely as possible)**:

This may happens etcd works at a low IO disks or some other processes squeeze the disk IO.

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):  v1.19.4  
- Cloud provider or hardware configuration:
- OS (e.g: `cat /etc/os-release`): Centos 7
- Kernel (e.g. `uname -a`): 3.10.0-693.21.1.el7.x86_64 #1 SMP Sat Dec 12 11:46:01 CST 2020 x86_64 x86_64 x86_64 GNU/Linux
- Install tools:
- Network plugin and version (if this is a network-related bug):
- Others:
",ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8,https://github.com/kubernetes/kubernetes
25,98474.0,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

```
I0120 17:35:46.590403       1 trace.go:116] Trace[1835007712]: ""Delete"" url:/api/v1/namespaces/admin/secrets/k8sconfig-sa-token-6c8hz,user-agent:kube-controller-manager/v1.17.9 (linux/amd64) kubernetes/59603c6/tokens-controller,client:1111::d15f (started: 2021-01-20 17:35:45.193170835 +0800 CST m=+196.151374014) (total time: 1.397188005s):
Trace[1835007712]: [1.397148638s] [1.397101146s] Object deleted from database
```

```
E0120 17:35:47.026101   70870 nestedpendingoperations.go:290] Operation for ""\""kubernetes.io/secret/ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8-k8sconfig-sa-token-6c8hz\"" (\""ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8\"")"" failed. No retries permitted until 2021-01-20 17:35:48.02606438 +0800 CST m=+183.395522570 (durationBeforeRetry 1s). Error: ""MountVolume.SetUp failed for volume \""k8sconfig-sa-token-6c8hz\"" (UniqueName: \""kubernetes.io/secret/ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8-k8sconfig-sa-token-6c8hz\"") pod \""op-containers-k8sconfig-1-jz7pb\"" (UID: \""ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8\"") : secret \""k8sconfig-sa-token-6c8hz\"" not found""
E0120 17:37:55.849529   70870 secret.go:195] Couldn't get secret admin/k8sconfig-sa-token-6c8hz: secret ""k8sconfig-sa-token-6c8hz"" not found
```

I found in the code of `tokenController` that a secret may be deleted due to various reasons, such as failed  updating the secret reference in service account:

```
		// Try to add a reference to the token
		liveServiceAccount.Secrets = append(liveServiceAccount.Secrets, v1.ObjectReference{Name: secret.Name})
		if _, err := serviceAccounts.Update(context.TODO(), liveServiceAccount, metav1.UpdateOptions{}); err != nil {
			return err
		}

		addedReference = true
		return nil
	})

	if !addedReference {
		// we weren't able to use the token, try to clean it up.
		klog.V(2).Infof(""deleting secret %s/%s because reference couldn't be added (%v)"", secret.Namespace, secret.Name, err)
		deleteOpts := metav1.DeleteOptions{Preconditions: &metav1.Preconditions{UID: &createdToken.UID}}
		if err := e.client.CoreV1().Secrets(createdToken.Namespace).Delete(context.TODO(), createdToken.Name, deleteOpts); err != nil {
			klog.Error(err) // if we fail, just log it
		}
	}
``` 

But the are many scenes, kubeClient create a serviceaccount, and then it watches secrets until it find the first secret passes kube-apiserver's authentication(For example, the `SAControllerClientBuilder.Config(name string) (*restclient.Config, error)` method create a controller's sa and watch its secret to build a sa based controller client). In the tokenController's main goroutine, this secret was deleted after that, this case the newly build client failed with Unauthorized error.


**What you expected to happen**:

Once the secret of a sa is created, we cannot  restrict user use it before tokenController successfully updates the sa's secret reference. 

Is that possible to:

- make the secret failed to authenticate  before tokenController successfully updates the sa's secret reference?
- treat the creation of secret and the update of sa's secret reference as a transaction?
- do not delete the secret until tokenController update sa's secret reference successfully?

**How to reproduce it (as minimally and precisely as possible)**:

This may happens etcd works at a low IO disks or some other processes squeeze the disk IO.

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):  v1.19.4  
- Cloud provider or hardware configuration:
- OS (e.g: `cat /etc/os-release`): Centos 7
- Kernel (e.g. `uname -a`): 3.10.0-693.21.1.el7.x86_64 #1 SMP Sat Dec 12 11:46:01 CST 2020 x86_64 x86_64 x86_64 GNU/Linux
- Install tools:
- Network plugin and version (if this is a network-related bug):
- Others:
",k8sconfig-sa-token-6c8hz,https://github.com/kubernetes/kubernetes
26,98474.0,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

```
I0120 17:35:46.590403       1 trace.go:116] Trace[1835007712]: ""Delete"" url:/api/v1/namespaces/admin/secrets/k8sconfig-sa-token-6c8hz,user-agent:kube-controller-manager/v1.17.9 (linux/amd64) kubernetes/59603c6/tokens-controller,client:1111::d15f (started: 2021-01-20 17:35:45.193170835 +0800 CST m=+196.151374014) (total time: 1.397188005s):
Trace[1835007712]: [1.397148638s] [1.397101146s] Object deleted from database
```

```
E0120 17:35:47.026101   70870 nestedpendingoperations.go:290] Operation for ""\""kubernetes.io/secret/ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8-k8sconfig-sa-token-6c8hz\"" (\""ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8\"")"" failed. No retries permitted until 2021-01-20 17:35:48.02606438 +0800 CST m=+183.395522570 (durationBeforeRetry 1s). Error: ""MountVolume.SetUp failed for volume \""k8sconfig-sa-token-6c8hz\"" (UniqueName: \""kubernetes.io/secret/ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8-k8sconfig-sa-token-6c8hz\"") pod \""op-containers-k8sconfig-1-jz7pb\"" (UID: \""ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8\"") : secret \""k8sconfig-sa-token-6c8hz\"" not found""
E0120 17:37:55.849529   70870 secret.go:195] Couldn't get secret admin/k8sconfig-sa-token-6c8hz: secret ""k8sconfig-sa-token-6c8hz"" not found
```

I found in the code of `tokenController` that a secret may be deleted due to various reasons, such as failed  updating the secret reference in service account:

```
		// Try to add a reference to the token
		liveServiceAccount.Secrets = append(liveServiceAccount.Secrets, v1.ObjectReference{Name: secret.Name})
		if _, err := serviceAccounts.Update(context.TODO(), liveServiceAccount, metav1.UpdateOptions{}); err != nil {
			return err
		}

		addedReference = true
		return nil
	})

	if !addedReference {
		// we weren't able to use the token, try to clean it up.
		klog.V(2).Infof(""deleting secret %s/%s because reference couldn't be added (%v)"", secret.Namespace, secret.Name, err)
		deleteOpts := metav1.DeleteOptions{Preconditions: &metav1.Preconditions{UID: &createdToken.UID}}
		if err := e.client.CoreV1().Secrets(createdToken.Namespace).Delete(context.TODO(), createdToken.Name, deleteOpts); err != nil {
			klog.Error(err) // if we fail, just log it
		}
	}
``` 

But the are many scenes, kubeClient create a serviceaccount, and then it watches secrets until it find the first secret passes kube-apiserver's authentication(For example, the `SAControllerClientBuilder.Config(name string) (*restclient.Config, error)` method create a controller's sa and watch its secret to build a sa based controller client). In the tokenController's main goroutine, this secret was deleted after that, this case the newly build client failed with Unauthorized error.


**What you expected to happen**:

Once the secret of a sa is created, we cannot  restrict user use it before tokenController successfully updates the sa's secret reference. 

Is that possible to:

- make the secret failed to authenticate  before tokenController successfully updates the sa's secret reference?
- treat the creation of secret and the update of sa's secret reference as a transaction?
- do not delete the secret until tokenController update sa's secret reference successfully?

**How to reproduce it (as minimally and precisely as possible)**:

This may happens etcd works at a low IO disks or some other processes squeeze the disk IO.

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):  v1.19.4  
- Cloud provider or hardware configuration:
- OS (e.g: `cat /etc/os-release`): Centos 7
- Kernel (e.g. `uname -a`): 3.10.0-693.21.1.el7.x86_64 #1 SMP Sat Dec 12 11:46:01 CST 2020 x86_64 x86_64 x86_64 GNU/Linux
- Install tools:
- Network plugin and version (if this is a network-related bug):
- Others:
",containers-k8sconfig-1-jz7pb,https://github.com/kubernetes/kubernetes
27,98474.0,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

```
I0120 17:35:46.590403       1 trace.go:116] Trace[1835007712]: ""Delete"" url:/api/v1/namespaces/admin/secrets/k8sconfig-sa-token-6c8hz,user-agent:kube-controller-manager/v1.17.9 (linux/amd64) kubernetes/59603c6/tokens-controller,client:1111::d15f (started: 2021-01-20 17:35:45.193170835 +0800 CST m=+196.151374014) (total time: 1.397188005s):
Trace[1835007712]: [1.397148638s] [1.397101146s] Object deleted from database
```

```
E0120 17:35:47.026101   70870 nestedpendingoperations.go:290] Operation for ""\""kubernetes.io/secret/ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8-k8sconfig-sa-token-6c8hz\"" (\""ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8\"")"" failed. No retries permitted until 2021-01-20 17:35:48.02606438 +0800 CST m=+183.395522570 (durationBeforeRetry 1s). Error: ""MountVolume.SetUp failed for volume \""k8sconfig-sa-token-6c8hz\"" (UniqueName: \""kubernetes.io/secret/ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8-k8sconfig-sa-token-6c8hz\"") pod \""op-containers-k8sconfig-1-jz7pb\"" (UID: \""ab1566ee-5d8a-4c24-825d-fc2ee7cc37f8\"") : secret \""k8sconfig-sa-token-6c8hz\"" not found""
E0120 17:37:55.849529   70870 secret.go:195] Couldn't get secret admin/k8sconfig-sa-token-6c8hz: secret ""k8sconfig-sa-token-6c8hz"" not found
```

I found in the code of `tokenController` that a secret may be deleted due to various reasons, such as failed  updating the secret reference in service account:

```
		// Try to add a reference to the token
		liveServiceAccount.Secrets = append(liveServiceAccount.Secrets, v1.ObjectReference{Name: secret.Name})
		if _, err := serviceAccounts.Update(context.TODO(), liveServiceAccount, metav1.UpdateOptions{}); err != nil {
			return err
		}

		addedReference = true
		return nil
	})

	if !addedReference {
		// we weren't able to use the token, try to clean it up.
		klog.V(2).Infof(""deleting secret %s/%s because reference couldn't be added (%v)"", secret.Namespace, secret.Name, err)
		deleteOpts := metav1.DeleteOptions{Preconditions: &metav1.Preconditions{UID: &createdToken.UID}}
		if err := e.client.CoreV1().Secrets(createdToken.Namespace).Delete(context.TODO(), createdToken.Name, deleteOpts); err != nil {
			klog.Error(err) // if we fail, just log it
		}
	}
``` 

But the are many scenes, kubeClient create a serviceaccount, and then it watches secrets until it find the first secret passes kube-apiserver's authentication(For example, the `SAControllerClientBuilder.Config(name string) (*restclient.Config, error)` method create a controller's sa and watch its secret to build a sa based controller client). In the tokenController's main goroutine, this secret was deleted after that, this case the newly build client failed with Unauthorized error.


**What you expected to happen**:

Once the secret of a sa is created, we cannot  restrict user use it before tokenController successfully updates the sa's secret reference. 

Is that possible to:

- make the secret failed to authenticate  before tokenController successfully updates the sa's secret reference?
- treat the creation of secret and the update of sa's secret reference as a transaction?
- do not delete the secret until tokenController update sa's secret reference successfully?

**How to reproduce it (as minimally and precisely as possible)**:

This may happens etcd works at a low IO disks or some other processes squeeze the disk IO.

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):  v1.19.4  
- Cloud provider or hardware configuration:
- OS (e.g: `cat /etc/os-release`): Centos 7
- Kernel (e.g. `uname -a`): 3.10.0-693.21.1.el7.x86_64 #1 SMP Sat Dec 12 11:46:01 CST 2020 x86_64 x86_64 x86_64 GNU/Linux
- Install tools:
- Network plugin and version (if this is a network-related bug):
- Others:
",authentication(For,https://github.com/kubernetes/kubernetes
28,95105.0,"Centos7, three servers are installed with k8s cluster, one master, other slave1 and slave2. The three nodes are installed with CEPH for data persistence.

the cephfs on master /mnt/cephfs directroy
master:6789,slave1:6789,slave2:6789:/ /mnt/cephfs ceph name=cephfs,secretfile=/etc/ceph/cephfskey,_netdev,noatime 0 0

Create deployment through the python API of kubernetes client, and mount three directories at the same time,
One is to dynamically hang to pod through PVC
Here's secret
apiVersion: v1
kind: Secret
metadata:
name: ceph-secret
namespace: espp-ns
type: ""kubernetes.io/rbd""
data:
key:

storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
name: ceph-espp
provisioner: kubernetes.io/rbd
parameters:
monitors: 10.197.1.11, 10.197.1.12, 10.197.1.13
adminId: admin
adminSecretName: ceph-secret
adminSecretNamespace: espp-ns
pool: rbd 
userId: admin
userSecretName: ceph-secret
reclaimPolicy: Retain

The other two directories are mounted through cephfs.
secret.yml
apiVersion: v1
kind: Secret
metadata:
name: cephfs-secret-fs
namespace: espp-ns
type: ""kubernetes.io/rbd""
data:
key: AQBJrUxfZiH6NxAAljeXDd/DBYBiHx64PJ+shQ==

**Problem**：All created pods are in the master node, not assigned to the other two nodes. At the same time, the created PVC and PV are also mounted on the master

below is on deployment example:
espp-ns-deployment-1-1-2-0-6fc6cd65c4-8lq9m 1/1 Running 0 13h 172.30.22.5 **master**

and the mount information:
172.18.101.10:6789,172.18.101.11:6789,172.18.101.12:6789:/studentenv/dataset_stuenv_2_1/ 1.2T 244M 1.2T 1% /var/lib/kubelet/pods/1d40454b-00fc-4847-8948-7b1d73e1bb98/volumes/kubernetes.iocephfs/espp-ns-deployment-1-1-2-0-pv-fs-stuenv-0-1
172.18.101.10:6789,172.18.101.11:6789,172.18.101.12:6789:/lessonplan/dataset_lp_1_1_19/ 1.2T 244M 1.2T 1% /var/lib/kubelet/pods/1d40454b-00fc-4847-8948-7b1d73e1bb98/volumes/kubernetes.iocephfs/espp-ns-deployment-1-1-2-0-pv-fs-stuenv-0-0
/dev/rbd0 20G 387M 20G 2% /var/lib/kubelet/plugins/kubernetes.io/rbd/mounts/rbd-image-kubernetes-dynamic-pvc-95ce6a24-e7fe-4456-b008-d53428be6352
overlay

thanks",adminSecretNamespace,https://github.com/kubernetes/kubernetes
29,95105.0,"Centos7, three servers are installed with k8s cluster, one master, other slave1 and slave2. The three nodes are installed with CEPH for data persistence.

the cephfs on master /mnt/cephfs directroy
master:6789,slave1:6789,slave2:6789:/ /mnt/cephfs ceph name=cephfs,secretfile=/etc/ceph/cephfskey,_netdev,noatime 0 0

Create deployment through the python API of kubernetes client, and mount three directories at the same time,
One is to dynamically hang to pod through PVC
Here's secret
apiVersion: v1
kind: Secret
metadata:
name: ceph-secret
namespace: espp-ns
type: ""kubernetes.io/rbd""
data:
key:

storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
name: ceph-espp
provisioner: kubernetes.io/rbd
parameters:
monitors: 10.197.1.11, 10.197.1.12, 10.197.1.13
adminId: admin
adminSecretName: ceph-secret
adminSecretNamespace: espp-ns
pool: rbd 
userId: admin
userSecretName: ceph-secret
reclaimPolicy: Retain

The other two directories are mounted through cephfs.
secret.yml
apiVersion: v1
kind: Secret
metadata:
name: cephfs-secret-fs
namespace: espp-ns
type: ""kubernetes.io/rbd""
data:
key: AQBJrUxfZiH6NxAAljeXDd/DBYBiHx64PJ+shQ==

**Problem**：All created pods are in the master node, not assigned to the other two nodes. At the same time, the created PVC and PV are also mounted on the master

below is on deployment example:
espp-ns-deployment-1-1-2-0-6fc6cd65c4-8lq9m 1/1 Running 0 13h 172.30.22.5 **master**

and the mount information:
172.18.101.10:6789,172.18.101.11:6789,172.18.101.12:6789:/studentenv/dataset_stuenv_2_1/ 1.2T 244M 1.2T 1% /var/lib/kubelet/pods/1d40454b-00fc-4847-8948-7b1d73e1bb98/volumes/kubernetes.iocephfs/espp-ns-deployment-1-1-2-0-pv-fs-stuenv-0-1
172.18.101.10:6789,172.18.101.11:6789,172.18.101.12:6789:/lessonplan/dataset_lp_1_1_19/ 1.2T 244M 1.2T 1% /var/lib/kubelet/pods/1d40454b-00fc-4847-8948-7b1d73e1bb98/volumes/kubernetes.iocephfs/espp-ns-deployment-1-1-2-0-pv-fs-stuenv-0-0
/dev/rbd0 20G 387M 20G 2% /var/lib/kubelet/plugins/kubernetes.io/rbd/mounts/rbd-image-kubernetes-dynamic-pvc-95ce6a24-e7fe-4456-b008-d53428be6352
overlay

thanks",cephfs-secret-fs,https://github.com/kubernetes/kubernetes
30,95105.0,"Centos7, three servers are installed with k8s cluster, one master, other slave1 and slave2. The three nodes are installed with CEPH for data persistence.

the cephfs on master /mnt/cephfs directroy
master:6789,slave1:6789,slave2:6789:/ /mnt/cephfs ceph name=cephfs,secretfile=/etc/ceph/cephfskey,_netdev,noatime 0 0

Create deployment through the python API of kubernetes client, and mount three directories at the same time,
One is to dynamically hang to pod through PVC
Here's secret
apiVersion: v1
kind: Secret
metadata:
name: ceph-secret
namespace: espp-ns
type: ""kubernetes.io/rbd""
data:
key:

storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
name: ceph-espp
provisioner: kubernetes.io/rbd
parameters:
monitors: 10.197.1.11, 10.197.1.12, 10.197.1.13
adminId: admin
adminSecretName: ceph-secret
adminSecretNamespace: espp-ns
pool: rbd 
userId: admin
userSecretName: ceph-secret
reclaimPolicy: Retain

The other two directories are mounted through cephfs.
secret.yml
apiVersion: v1
kind: Secret
metadata:
name: cephfs-secret-fs
namespace: espp-ns
type: ""kubernetes.io/rbd""
data:
key: AQBJrUxfZiH6NxAAljeXDd/DBYBiHx64PJ+shQ==

**Problem**：All created pods are in the master node, not assigned to the other two nodes. At the same time, the created PVC and PV are also mounted on the master

below is on deployment example:
espp-ns-deployment-1-1-2-0-6fc6cd65c4-8lq9m 1/1 Running 0 13h 172.30.22.5 **master**

and the mount information:
172.18.101.10:6789,172.18.101.11:6789,172.18.101.12:6789:/studentenv/dataset_stuenv_2_1/ 1.2T 244M 1.2T 1% /var/lib/kubelet/pods/1d40454b-00fc-4847-8948-7b1d73e1bb98/volumes/kubernetes.iocephfs/espp-ns-deployment-1-1-2-0-pv-fs-stuenv-0-1
172.18.101.10:6789,172.18.101.11:6789,172.18.101.12:6789:/lessonplan/dataset_lp_1_1_19/ 1.2T 244M 1.2T 1% /var/lib/kubelet/pods/1d40454b-00fc-4847-8948-7b1d73e1bb98/volumes/kubernetes.iocephfs/espp-ns-deployment-1-1-2-0-pv-fs-stuenv-0-0
/dev/rbd0 20G 387M 20G 2% /var/lib/kubelet/plugins/kubernetes.io/rbd/mounts/rbd-image-kubernetes-dynamic-pvc-95ce6a24-e7fe-4456-b008-d53428be6352
overlay

thanks","=,_netdev,noatime",https://github.com/kubernetes/kubernetes
31,95105.0,"Centos7, three servers are installed with k8s cluster, one master, other slave1 and slave2. The three nodes are installed with CEPH for data persistence.

the cephfs on master /mnt/cephfs directroy
master:6789,slave1:6789,slave2:6789:/ /mnt/cephfs ceph name=cephfs,secretfile=/etc/ceph/cephfskey,_netdev,noatime 0 0

Create deployment through the python API of kubernetes client, and mount three directories at the same time,
One is to dynamically hang to pod through PVC
Here's secret
apiVersion: v1
kind: Secret
metadata:
name: ceph-secret
namespace: espp-ns
type: ""kubernetes.io/rbd""
data:
key:

storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
name: ceph-espp
provisioner: kubernetes.io/rbd
parameters:
monitors: 10.197.1.11, 10.197.1.12, 10.197.1.13
adminId: admin
adminSecretName: ceph-secret
adminSecretNamespace: espp-ns
pool: rbd 
userId: admin
userSecretName: ceph-secret
reclaimPolicy: Retain

The other two directories are mounted through cephfs.
secret.yml
apiVersion: v1
kind: Secret
metadata:
name: cephfs-secret-fs
namespace: espp-ns
type: ""kubernetes.io/rbd""
data:
key: AQBJrUxfZiH6NxAAljeXDd/DBYBiHx64PJ+shQ==

**Problem**：All created pods are in the master node, not assigned to the other two nodes. At the same time, the created PVC and PV are also mounted on the master

below is on deployment example:
espp-ns-deployment-1-1-2-0-6fc6cd65c4-8lq9m 1/1 Running 0 13h 172.30.22.5 **master**

and the mount information:
172.18.101.10:6789,172.18.101.11:6789,172.18.101.12:6789:/studentenv/dataset_stuenv_2_1/ 1.2T 244M 1.2T 1% /var/lib/kubelet/pods/1d40454b-00fc-4847-8948-7b1d73e1bb98/volumes/kubernetes.iocephfs/espp-ns-deployment-1-1-2-0-pv-fs-stuenv-0-1
172.18.101.10:6789,172.18.101.11:6789,172.18.101.12:6789:/lessonplan/dataset_lp_1_1_19/ 1.2T 244M 1.2T 1% /var/lib/kubelet/pods/1d40454b-00fc-4847-8948-7b1d73e1bb98/volumes/kubernetes.iocephfs/espp-ns-deployment-1-1-2-0-pv-fs-stuenv-0-0
/dev/rbd0 20G 387M 20G 2% /var/lib/kubelet/plugins/kubernetes.io/rbd/mounts/rbd-image-kubernetes-dynamic-pvc-95ce6a24-e7fe-4456-b008-d53428be6352
overlay

thanks",AQBJrUxfZiH6NxAAljeXDd+shQ,https://github.com/kubernetes/kubernetes
32,95105.0,"Centos7, three servers are installed with k8s cluster, one master, other slave1 and slave2. The three nodes are installed with CEPH for data persistence.

the cephfs on master /mnt/cephfs directroy
master:6789,slave1:6789,slave2:6789:/ /mnt/cephfs ceph name=cephfs,secretfile=/etc/ceph/cephfskey,_netdev,noatime 0 0

Create deployment through the python API of kubernetes client, and mount three directories at the same time,
One is to dynamically hang to pod through PVC
Here's secret
apiVersion: v1
kind: Secret
metadata:
name: ceph-secret
namespace: espp-ns
type: ""kubernetes.io/rbd""
data:
key:

storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
name: ceph-espp
provisioner: kubernetes.io/rbd
parameters:
monitors: 10.197.1.11, 10.197.1.12, 10.197.1.13
adminId: admin
adminSecretName: ceph-secret
adminSecretNamespace: espp-ns
pool: rbd 
userId: admin
userSecretName: ceph-secret
reclaimPolicy: Retain

The other two directories are mounted through cephfs.
secret.yml
apiVersion: v1
kind: Secret
metadata:
name: cephfs-secret-fs
namespace: espp-ns
type: ""kubernetes.io/rbd""
data:
key: AQBJrUxfZiH6NxAAljeXDd/DBYBiHx64PJ+shQ==

**Problem**：All created pods are in the master node, not assigned to the other two nodes. At the same time, the created PVC and PV are also mounted on the master

below is on deployment example:
espp-ns-deployment-1-1-2-0-6fc6cd65c4-8lq9m 1/1 Running 0 13h 172.30.22.5 **master**

and the mount information:
172.18.101.10:6789,172.18.101.11:6789,172.18.101.12:6789:/studentenv/dataset_stuenv_2_1/ 1.2T 244M 1.2T 1% /var/lib/kubelet/pods/1d40454b-00fc-4847-8948-7b1d73e1bb98/volumes/kubernetes.iocephfs/espp-ns-deployment-1-1-2-0-pv-fs-stuenv-0-1
172.18.101.10:6789,172.18.101.11:6789,172.18.101.12:6789:/lessonplan/dataset_lp_1_1_19/ 1.2T 244M 1.2T 1% /var/lib/kubelet/pods/1d40454b-00fc-4847-8948-7b1d73e1bb98/volumes/kubernetes.iocephfs/espp-ns-deployment-1-1-2-0-pv-fs-stuenv-0-0
/dev/rbd0 20G 387M 20G 2% /var/lib/kubelet/plugins/kubernetes.io/rbd/mounts/rbd-image-kubernetes-dynamic-pvc-95ce6a24-e7fe-4456-b008-d53428be6352
overlay

thanks",AQBJrUxfZiH6NxAAljeXDd,https://github.com/kubernetes/kubernetes
33,91992.0,"**What happened**:

Hey everyone! Not sure why this is happening, I have not tried to set up HA k8s before this. Running normal kubeadm init works.... but I am trying to use HA Kubernetes and getting these following errors. Any pointers would be GREAT.

While installing HA kubernetes via command line with kubeadm, I am repeadetly blocked by kubelet message:

kubeadm init --config config.yaml --upload-certs

Contents of config.yaml:
```
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress:
  bindPort: 6444 #Changed this to 6444 per some documents I read through on GitHub issues... also tried with 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: 
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
controlPlaneEndpoint: ""LoadBalancerDNS:6443""
kubernetesVersion: v1.17.3
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
address: 0.0.0.0
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
cgroupDriver: systemd
cgroupsPerQOS: true
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
configMapAndSecretChangeDetectionStrategy: Watch
containerLogMaxFiles: 5
containerLogMaxSize: 10Mi
contentType: application/vnd.kubernetes.protobuf
cpuCFSQuota: true
cpuCFSQuotaPeriod: 100ms
cpuManagerPolicy: none
cpuManagerReconcilePeriod: 10s
enableControllerAttachDetach: true
enableDebuggingHandlers: true
enforceNodeAllocatable:
- pods
eventBurst: 10
eventRecordQPS: 5
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
evictionPressureTransitionPeriod: 5m0s
failSwapOn: false
fileCheckFrequency: 20s
hairpinMode: promiscuous-bridge
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 20s
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
imageMinimumGCAge: 2m0s
iptablesDropBit: 15
iptablesMasqueradeBit: 14
kind: KubeletConfiguration
kubeAPIBurst: 10
kubeAPIQPS: 5
makeIPTablesUtilChains: true
maxOpenFiles: 1000000
maxPods: 110
nodeLeaseDurationSeconds: 80 #changed from 40
nodeStatusReportFrequency: 1m0s
nodeStatusUpdateFrequency: 10s
oomScoreAdj: -999
podPidsLimit: -1
port: 10250
registryBurst: 10
registryPullQPS: 5
resolvConf: /etc/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 4m0s
serializeImagePulls: true
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 4h0m0s
syncFrequency: 1m0s
topologyManagerPolicy: none
volumeStatsAggPeriod: 1m0s
```
Failed to list *v1.Node: Get **https://load-balancer-dns:6443/api/v1/nodes?fieldSelector=metadata.name%3DCurrentMasterNodeName&limit=500&resourceVersion=0:** x509: certificate signed by unknown authority

This is the same for all:
Failed to list *v1beta1.CSIDriver
Failed to list *v1.Service
Failed to list *v1.Node
Failed to get status for pod ""etcd-NodeNameMaster

**For all of them, this is ending with x509: certificate signed by unknown authority**

**What you expected to happen**:

I expected Kubernetes to set up the first control plane node for me and use the F5 load balancer that I provided

**How to reproduce it (as minimally and precisely as possible)**:

Follow all docs here:
- https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/
My set up is for a stacked control plane

**Anything else we need to know?**:

Tried the following and set these params:
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward = 1

Driver for Docker and kubelet: **systemd**



**Environment**:
- Kubernetes version (use `kubectl version`):
kubeadm kubelet kubectl versions: v1.17.3
Docker version 19.03.11
- Cloud provider or hardware configuration:
Bare-metal with an F5 load balancer instead of HAproxy LB
- OS (e.g: `cat /etc/os-release`):
NAME=""Red Hat Enterprise Linux Server""
VERSION=""7.7 (Maipo)""
ID=""rhel""
ID_LIKE=""fedora""
VARIANT=""Server""
VARIANT_ID=""server""
VERSION_ID=""7.7""

- Kernel (e.g. `uname -a`):
Linux MasterNode 3.10.0-1062.18.1.el7.x86_64 #1 SMP Wed Feb 12 14:08:31 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
- Install tools:
yum
- Network plugin and version (if this is a network-related bug):
- Others:

```
Jun 10 11:19:30 MasterNode kubelet[3195]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-
Jun 10 11:19:30 MasterNode kubelet[3195]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964228    3195 server.go:416] Version: v1.17.3
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964703    3195 plugins.go:100] No cloud provider specified.
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964939    3195 server.go:821] Client rotation is on, will bootstrap in background
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.967369    3195 certificate_store.go:129] Loading cert/key pair from ""/var/lib/kubelet/pki/kubelet-client-current.pem"".
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054304    3195 server.go:641] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054653    3195 container_manager_linux.go:265] container manager verified user specified cgroup-root exists: []
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054687    3195 container_manager_linux.go:270] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: Con
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054789    3195 fake_topology_manager.go:29] [fake topologymanager] NewFakeManager
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054794    3195 container_manager_linux.go:305] Creating device plugin manager: true
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054826    3195 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider:  &{kubelet.sock /var/lib/kubelet/device-plugins/ map[] {0 0} <nil> {{
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054872    3195 state_mem.go:36] [cpumanager] initializing new in-memory state store
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054979    3195 state_mem.go:84] [cpumanager] updated default cpuset: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054985    3195 state_mem.go:92] [cpumanager] updated cpuset assignments: ""map[]""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054994    3195 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider:  &{{0 0} 0x6e9bc50 10000000000 0xc000ace6c0 <nil> <nil> <nil> <nil> m
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.055060    3195 kubelet.go:286] Adding pod path: /etc/kubernetes/manifests
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.055115    3195 kubelet.go:311] Watching apiserver
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.069097    3195 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.069444    3195 client.go:104] Start docker client with request timeout=4m0s
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.076542    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.077278    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.077500    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.079822    3195 docker_service.go:563] Hairpin mode set to ""promiscuous-bridge"" but kubenet is not enabled, falling back to ""hairpin-veth""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.079844    3195 docker_service.go:240] Hairpin mode set to ""hairpin-veth""
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.079937    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.083724    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.083778    3195 docker_service.go:255] Docker cri networking managed by cni
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.083836    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.093083    3195 docker_service.go:260] Docker Info: &{ID:CKKA:YUJL:2557:3IQE:7MRG:35J3:B2MN:GC3H:3WMM:FCV4:R2BU:TQW4 Containers:8 ContainersRunning:8 ContainersPaused:
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.093148    3195 docker_service.go:273] Setting cgroupDriver to systemd
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103239    3195 remote_runtime.go:59] parsed scheme: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103260    3195 remote_runtime.go:59] scheme """" not registered, fallback to default scheme
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103297    3195 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock 0  <nil>}] <nil>}
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103321    3195 clientconn.go:577] ClientConn switching balancer to ""pick_first""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103352    3195 remote_image.go:50] parsed scheme: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103358    3195 remote_image.go:50] scheme """" not registered, fallback to default scheme
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103366    3195 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock 0  <nil>}] <nil>}
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103370    3195 clientconn.go:577] ClientConn switching balancer to ""pick_first""
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.083448    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.086800    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.086872    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.090523    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.094073    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.094838    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.097327    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.102600    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.103424    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.108470    3195 aws_credentials.go:77] while getting AWS credentials NoCredentialProviders: no valid providers in chain. Deprecated.
Jun 10 11:19:34 MasterNode kubelet[3195]: For verbose messaging see aws.Config.CredentialsChainVerboseErrors
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.121933    3195 kuberuntime_manager.go:211] Container runtime docker initialized, version: 19.03.11, apiVersion: 1.40.0
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.135482    3195 server.go:1113] Started kubelet
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.135999    3195 kubelet.go:1302] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.136895    3195 fs_resource_analyzer.go:64] Starting FS ResourceAnalyzer
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.138046    3195 server.go:144] Starting to listen on 0.0.0.0:10250
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.140208    3195 server.go:384] Adding debug handlers to kubelet server.
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.151892    3195 volume_manager.go:265] Starting Kubelet Volume Manager
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.152428    3195 desired_state_of_world_populator.go:138] Desired state populator starts to run
```",streamingConnectionIdleTimeout,https://github.com/kubernetes/kubernetes
34,91992.0,"**What happened**:

Hey everyone! Not sure why this is happening, I have not tried to set up HA k8s before this. Running normal kubeadm init works.... but I am trying to use HA Kubernetes and getting these following errors. Any pointers would be GREAT.

While installing HA kubernetes via command line with kubeadm, I am repeadetly blocked by kubelet message:

kubeadm init --config config.yaml --upload-certs

Contents of config.yaml:
```
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress:
  bindPort: 6444 #Changed this to 6444 per some documents I read through on GitHub issues... also tried with 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: 
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
controlPlaneEndpoint: ""LoadBalancerDNS:6443""
kubernetesVersion: v1.17.3
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
address: 0.0.0.0
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
cgroupDriver: systemd
cgroupsPerQOS: true
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
configMapAndSecretChangeDetectionStrategy: Watch
containerLogMaxFiles: 5
containerLogMaxSize: 10Mi
contentType: application/vnd.kubernetes.protobuf
cpuCFSQuota: true
cpuCFSQuotaPeriod: 100ms
cpuManagerPolicy: none
cpuManagerReconcilePeriod: 10s
enableControllerAttachDetach: true
enableDebuggingHandlers: true
enforceNodeAllocatable:
- pods
eventBurst: 10
eventRecordQPS: 5
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
evictionPressureTransitionPeriod: 5m0s
failSwapOn: false
fileCheckFrequency: 20s
hairpinMode: promiscuous-bridge
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 20s
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
imageMinimumGCAge: 2m0s
iptablesDropBit: 15
iptablesMasqueradeBit: 14
kind: KubeletConfiguration
kubeAPIBurst: 10
kubeAPIQPS: 5
makeIPTablesUtilChains: true
maxOpenFiles: 1000000
maxPods: 110
nodeLeaseDurationSeconds: 80 #changed from 40
nodeStatusReportFrequency: 1m0s
nodeStatusUpdateFrequency: 10s
oomScoreAdj: -999
podPidsLimit: -1
port: 10250
registryBurst: 10
registryPullQPS: 5
resolvConf: /etc/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 4m0s
serializeImagePulls: true
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 4h0m0s
syncFrequency: 1m0s
topologyManagerPolicy: none
volumeStatsAggPeriod: 1m0s
```
Failed to list *v1.Node: Get **https://load-balancer-dns:6443/api/v1/nodes?fieldSelector=metadata.name%3DCurrentMasterNodeName&limit=500&resourceVersion=0:** x509: certificate signed by unknown authority

This is the same for all:
Failed to list *v1beta1.CSIDriver
Failed to list *v1.Service
Failed to list *v1.Node
Failed to get status for pod ""etcd-NodeNameMaster

**For all of them, this is ending with x509: certificate signed by unknown authority**

**What you expected to happen**:

I expected Kubernetes to set up the first control plane node for me and use the F5 load balancer that I provided

**How to reproduce it (as minimally and precisely as possible)**:

Follow all docs here:
- https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/
My set up is for a stacked control plane

**Anything else we need to know?**:

Tried the following and set these params:
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward = 1

Driver for Docker and kubelet: **systemd**



**Environment**:
- Kubernetes version (use `kubectl version`):
kubeadm kubelet kubectl versions: v1.17.3
Docker version 19.03.11
- Cloud provider or hardware configuration:
Bare-metal with an F5 load balancer instead of HAproxy LB
- OS (e.g: `cat /etc/os-release`):
NAME=""Red Hat Enterprise Linux Server""
VERSION=""7.7 (Maipo)""
ID=""rhel""
ID_LIKE=""fedora""
VARIANT=""Server""
VARIANT_ID=""server""
VERSION_ID=""7.7""

- Kernel (e.g. `uname -a`):
Linux MasterNode 3.10.0-1062.18.1.el7.x86_64 #1 SMP Wed Feb 12 14:08:31 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
- Install tools:
yum
- Network plugin and version (if this is a network-related bug):
- Others:

```
Jun 10 11:19:30 MasterNode kubelet[3195]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-
Jun 10 11:19:30 MasterNode kubelet[3195]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964228    3195 server.go:416] Version: v1.17.3
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964703    3195 plugins.go:100] No cloud provider specified.
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964939    3195 server.go:821] Client rotation is on, will bootstrap in background
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.967369    3195 certificate_store.go:129] Loading cert/key pair from ""/var/lib/kubelet/pki/kubelet-client-current.pem"".
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054304    3195 server.go:641] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054653    3195 container_manager_linux.go:265] container manager verified user specified cgroup-root exists: []
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054687    3195 container_manager_linux.go:270] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: Con
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054789    3195 fake_topology_manager.go:29] [fake topologymanager] NewFakeManager
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054794    3195 container_manager_linux.go:305] Creating device plugin manager: true
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054826    3195 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider:  &{kubelet.sock /var/lib/kubelet/device-plugins/ map[] {0 0} <nil> {{
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054872    3195 state_mem.go:36] [cpumanager] initializing new in-memory state store
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054979    3195 state_mem.go:84] [cpumanager] updated default cpuset: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054985    3195 state_mem.go:92] [cpumanager] updated cpuset assignments: ""map[]""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054994    3195 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider:  &{{0 0} 0x6e9bc50 10000000000 0xc000ace6c0 <nil> <nil> <nil> <nil> m
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.055060    3195 kubelet.go:286] Adding pod path: /etc/kubernetes/manifests
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.055115    3195 kubelet.go:311] Watching apiserver
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.069097    3195 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.069444    3195 client.go:104] Start docker client with request timeout=4m0s
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.076542    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.077278    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.077500    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.079822    3195 docker_service.go:563] Hairpin mode set to ""promiscuous-bridge"" but kubenet is not enabled, falling back to ""hairpin-veth""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.079844    3195 docker_service.go:240] Hairpin mode set to ""hairpin-veth""
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.079937    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.083724    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.083778    3195 docker_service.go:255] Docker cri networking managed by cni
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.083836    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.093083    3195 docker_service.go:260] Docker Info: &{ID:CKKA:YUJL:2557:3IQE:7MRG:35J3:B2MN:GC3H:3WMM:FCV4:R2BU:TQW4 Containers:8 ContainersRunning:8 ContainersPaused:
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.093148    3195 docker_service.go:273] Setting cgroupDriver to systemd
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103239    3195 remote_runtime.go:59] parsed scheme: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103260    3195 remote_runtime.go:59] scheme """" not registered, fallback to default scheme
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103297    3195 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock 0  <nil>}] <nil>}
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103321    3195 clientconn.go:577] ClientConn switching balancer to ""pick_first""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103352    3195 remote_image.go:50] parsed scheme: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103358    3195 remote_image.go:50] scheme """" not registered, fallback to default scheme
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103366    3195 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock 0  <nil>}] <nil>}
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103370    3195 clientconn.go:577] ClientConn switching balancer to ""pick_first""
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.083448    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.086800    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.086872    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.090523    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.094073    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.094838    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.097327    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.102600    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.103424    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.108470    3195 aws_credentials.go:77] while getting AWS credentials NoCredentialProviders: no valid providers in chain. Deprecated.
Jun 10 11:19:34 MasterNode kubelet[3195]: For verbose messaging see aws.Config.CredentialsChainVerboseErrors
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.121933    3195 kuberuntime_manager.go:211] Container runtime docker initialized, version: 19.03.11, apiVersion: 1.40.0
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.135482    3195 server.go:1113] Started kubelet
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.135999    3195 kubelet.go:1302] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.136895    3195 fs_resource_analyzer.go:64] Starting FS ResourceAnalyzer
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.138046    3195 server.go:144] Starting to listen on 0.0.0.0:10250
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.140208    3195 server.go:384] Adding debug handlers to kubelet server.
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.151892    3195 volume_manager.go:265] Starting Kubelet Volume Manager
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.152428    3195 desired_state_of_world_populator.go:138] Desired state populator starts to run
```",NoCredentialProviders,https://github.com/kubernetes/kubernetes
35,91992.0,"**What happened**:

Hey everyone! Not sure why this is happening, I have not tried to set up HA k8s before this. Running normal kubeadm init works.... but I am trying to use HA Kubernetes and getting these following errors. Any pointers would be GREAT.

While installing HA kubernetes via command line with kubeadm, I am repeadetly blocked by kubelet message:

kubeadm init --config config.yaml --upload-certs

Contents of config.yaml:
```
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress:
  bindPort: 6444 #Changed this to 6444 per some documents I read through on GitHub issues... also tried with 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: 
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
controlPlaneEndpoint: ""LoadBalancerDNS:6443""
kubernetesVersion: v1.17.3
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
address: 0.0.0.0
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
cgroupDriver: systemd
cgroupsPerQOS: true
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
configMapAndSecretChangeDetectionStrategy: Watch
containerLogMaxFiles: 5
containerLogMaxSize: 10Mi
contentType: application/vnd.kubernetes.protobuf
cpuCFSQuota: true
cpuCFSQuotaPeriod: 100ms
cpuManagerPolicy: none
cpuManagerReconcilePeriod: 10s
enableControllerAttachDetach: true
enableDebuggingHandlers: true
enforceNodeAllocatable:
- pods
eventBurst: 10
eventRecordQPS: 5
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
evictionPressureTransitionPeriod: 5m0s
failSwapOn: false
fileCheckFrequency: 20s
hairpinMode: promiscuous-bridge
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 20s
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
imageMinimumGCAge: 2m0s
iptablesDropBit: 15
iptablesMasqueradeBit: 14
kind: KubeletConfiguration
kubeAPIBurst: 10
kubeAPIQPS: 5
makeIPTablesUtilChains: true
maxOpenFiles: 1000000
maxPods: 110
nodeLeaseDurationSeconds: 80 #changed from 40
nodeStatusReportFrequency: 1m0s
nodeStatusUpdateFrequency: 10s
oomScoreAdj: -999
podPidsLimit: -1
port: 10250
registryBurst: 10
registryPullQPS: 5
resolvConf: /etc/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 4m0s
serializeImagePulls: true
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 4h0m0s
syncFrequency: 1m0s
topologyManagerPolicy: none
volumeStatsAggPeriod: 1m0s
```
Failed to list *v1.Node: Get **https://load-balancer-dns:6443/api/v1/nodes?fieldSelector=metadata.name%3DCurrentMasterNodeName&limit=500&resourceVersion=0:** x509: certificate signed by unknown authority

This is the same for all:
Failed to list *v1beta1.CSIDriver
Failed to list *v1.Service
Failed to list *v1.Node
Failed to get status for pod ""etcd-NodeNameMaster

**For all of them, this is ending with x509: certificate signed by unknown authority**

**What you expected to happen**:

I expected Kubernetes to set up the first control plane node for me and use the F5 load balancer that I provided

**How to reproduce it (as minimally and precisely as possible)**:

Follow all docs here:
- https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/
My set up is for a stacked control plane

**Anything else we need to know?**:

Tried the following and set these params:
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward = 1

Driver for Docker and kubelet: **systemd**



**Environment**:
- Kubernetes version (use `kubectl version`):
kubeadm kubelet kubectl versions: v1.17.3
Docker version 19.03.11
- Cloud provider or hardware configuration:
Bare-metal with an F5 load balancer instead of HAproxy LB
- OS (e.g: `cat /etc/os-release`):
NAME=""Red Hat Enterprise Linux Server""
VERSION=""7.7 (Maipo)""
ID=""rhel""
ID_LIKE=""fedora""
VARIANT=""Server""
VARIANT_ID=""server""
VERSION_ID=""7.7""

- Kernel (e.g. `uname -a`):
Linux MasterNode 3.10.0-1062.18.1.el7.x86_64 #1 SMP Wed Feb 12 14:08:31 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
- Install tools:
yum
- Network plugin and version (if this is a network-related bug):
- Others:

```
Jun 10 11:19:30 MasterNode kubelet[3195]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-
Jun 10 11:19:30 MasterNode kubelet[3195]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964228    3195 server.go:416] Version: v1.17.3
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964703    3195 plugins.go:100] No cloud provider specified.
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964939    3195 server.go:821] Client rotation is on, will bootstrap in background
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.967369    3195 certificate_store.go:129] Loading cert/key pair from ""/var/lib/kubelet/pki/kubelet-client-current.pem"".
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054304    3195 server.go:641] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054653    3195 container_manager_linux.go:265] container manager verified user specified cgroup-root exists: []
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054687    3195 container_manager_linux.go:270] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: Con
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054789    3195 fake_topology_manager.go:29] [fake topologymanager] NewFakeManager
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054794    3195 container_manager_linux.go:305] Creating device plugin manager: true
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054826    3195 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider:  &{kubelet.sock /var/lib/kubelet/device-plugins/ map[] {0 0} <nil> {{
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054872    3195 state_mem.go:36] [cpumanager] initializing new in-memory state store
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054979    3195 state_mem.go:84] [cpumanager] updated default cpuset: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054985    3195 state_mem.go:92] [cpumanager] updated cpuset assignments: ""map[]""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054994    3195 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider:  &{{0 0} 0x6e9bc50 10000000000 0xc000ace6c0 <nil> <nil> <nil> <nil> m
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.055060    3195 kubelet.go:286] Adding pod path: /etc/kubernetes/manifests
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.055115    3195 kubelet.go:311] Watching apiserver
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.069097    3195 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.069444    3195 client.go:104] Start docker client with request timeout=4m0s
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.076542    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.077278    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.077500    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.079822    3195 docker_service.go:563] Hairpin mode set to ""promiscuous-bridge"" but kubenet is not enabled, falling back to ""hairpin-veth""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.079844    3195 docker_service.go:240] Hairpin mode set to ""hairpin-veth""
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.079937    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.083724    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.083778    3195 docker_service.go:255] Docker cri networking managed by cni
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.083836    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.093083    3195 docker_service.go:260] Docker Info: &{ID:CKKA:YUJL:2557:3IQE:7MRG:35J3:B2MN:GC3H:3WMM:FCV4:R2BU:TQW4 Containers:8 ContainersRunning:8 ContainersPaused:
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.093148    3195 docker_service.go:273] Setting cgroupDriver to systemd
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103239    3195 remote_runtime.go:59] parsed scheme: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103260    3195 remote_runtime.go:59] scheme """" not registered, fallback to default scheme
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103297    3195 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock 0  <nil>}] <nil>}
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103321    3195 clientconn.go:577] ClientConn switching balancer to ""pick_first""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103352    3195 remote_image.go:50] parsed scheme: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103358    3195 remote_image.go:50] scheme """" not registered, fallback to default scheme
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103366    3195 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock 0  <nil>}] <nil>}
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103370    3195 clientconn.go:577] ClientConn switching balancer to ""pick_first""
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.083448    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.086800    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.086872    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.090523    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.094073    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.094838    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.097327    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.102600    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.103424    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.108470    3195 aws_credentials.go:77] while getting AWS credentials NoCredentialProviders: no valid providers in chain. Deprecated.
Jun 10 11:19:34 MasterNode kubelet[3195]: For verbose messaging see aws.Config.CredentialsChainVerboseErrors
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.121933    3195 kuberuntime_manager.go:211] Container runtime docker initialized, version: 19.03.11, apiVersion: 1.40.0
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.135482    3195 server.go:1113] Started kubelet
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.135999    3195 kubelet.go:1302] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.136895    3195 fs_resource_analyzer.go:64] Starting FS ResourceAnalyzer
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.138046    3195 server.go:144] Starting to listen on 0.0.0.0:10250
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.140208    3195 server.go:384] Adding debug handlers to kubelet server.
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.151892    3195 volume_manager.go:265] Starting Kubelet Volume Manager
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.152428    3195 desired_state_of_world_populator.go:138] Desired state populator starts to run
```",containerLogMaxFiles,https://github.com/kubernetes/kubernetes
36,91992.0,"**What happened**:

Hey everyone! Not sure why this is happening, I have not tried to set up HA k8s before this. Running normal kubeadm init works.... but I am trying to use HA Kubernetes and getting these following errors. Any pointers would be GREAT.

While installing HA kubernetes via command line with kubeadm, I am repeadetly blocked by kubelet message:

kubeadm init --config config.yaml --upload-certs

Contents of config.yaml:
```
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress:
  bindPort: 6444 #Changed this to 6444 per some documents I read through on GitHub issues... also tried with 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: 
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
controlPlaneEndpoint: ""LoadBalancerDNS:6443""
kubernetesVersion: v1.17.3
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
address: 0.0.0.0
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
cgroupDriver: systemd
cgroupsPerQOS: true
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
configMapAndSecretChangeDetectionStrategy: Watch
containerLogMaxFiles: 5
containerLogMaxSize: 10Mi
contentType: application/vnd.kubernetes.protobuf
cpuCFSQuota: true
cpuCFSQuotaPeriod: 100ms
cpuManagerPolicy: none
cpuManagerReconcilePeriod: 10s
enableControllerAttachDetach: true
enableDebuggingHandlers: true
enforceNodeAllocatable:
- pods
eventBurst: 10
eventRecordQPS: 5
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
evictionPressureTransitionPeriod: 5m0s
failSwapOn: false
fileCheckFrequency: 20s
hairpinMode: promiscuous-bridge
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 20s
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
imageMinimumGCAge: 2m0s
iptablesDropBit: 15
iptablesMasqueradeBit: 14
kind: KubeletConfiguration
kubeAPIBurst: 10
kubeAPIQPS: 5
makeIPTablesUtilChains: true
maxOpenFiles: 1000000
maxPods: 110
nodeLeaseDurationSeconds: 80 #changed from 40
nodeStatusReportFrequency: 1m0s
nodeStatusUpdateFrequency: 10s
oomScoreAdj: -999
podPidsLimit: -1
port: 10250
registryBurst: 10
registryPullQPS: 5
resolvConf: /etc/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 4m0s
serializeImagePulls: true
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 4h0m0s
syncFrequency: 1m0s
topologyManagerPolicy: none
volumeStatsAggPeriod: 1m0s
```
Failed to list *v1.Node: Get **https://load-balancer-dns:6443/api/v1/nodes?fieldSelector=metadata.name%3DCurrentMasterNodeName&limit=500&resourceVersion=0:** x509: certificate signed by unknown authority

This is the same for all:
Failed to list *v1beta1.CSIDriver
Failed to list *v1.Service
Failed to list *v1.Node
Failed to get status for pod ""etcd-NodeNameMaster

**For all of them, this is ending with x509: certificate signed by unknown authority**

**What you expected to happen**:

I expected Kubernetes to set up the first control plane node for me and use the F5 load balancer that I provided

**How to reproduce it (as minimally and precisely as possible)**:

Follow all docs here:
- https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/
My set up is for a stacked control plane

**Anything else we need to know?**:

Tried the following and set these params:
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward = 1

Driver for Docker and kubelet: **systemd**



**Environment**:
- Kubernetes version (use `kubectl version`):
kubeadm kubelet kubectl versions: v1.17.3
Docker version 19.03.11
- Cloud provider or hardware configuration:
Bare-metal with an F5 load balancer instead of HAproxy LB
- OS (e.g: `cat /etc/os-release`):
NAME=""Red Hat Enterprise Linux Server""
VERSION=""7.7 (Maipo)""
ID=""rhel""
ID_LIKE=""fedora""
VARIANT=""Server""
VARIANT_ID=""server""
VERSION_ID=""7.7""

- Kernel (e.g. `uname -a`):
Linux MasterNode 3.10.0-1062.18.1.el7.x86_64 #1 SMP Wed Feb 12 14:08:31 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
- Install tools:
yum
- Network plugin and version (if this is a network-related bug):
- Others:

```
Jun 10 11:19:30 MasterNode kubelet[3195]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-
Jun 10 11:19:30 MasterNode kubelet[3195]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964228    3195 server.go:416] Version: v1.17.3
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964703    3195 plugins.go:100] No cloud provider specified.
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964939    3195 server.go:821] Client rotation is on, will bootstrap in background
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.967369    3195 certificate_store.go:129] Loading cert/key pair from ""/var/lib/kubelet/pki/kubelet-client-current.pem"".
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054304    3195 server.go:641] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054653    3195 container_manager_linux.go:265] container manager verified user specified cgroup-root exists: []
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054687    3195 container_manager_linux.go:270] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: Con
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054789    3195 fake_topology_manager.go:29] [fake topologymanager] NewFakeManager
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054794    3195 container_manager_linux.go:305] Creating device plugin manager: true
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054826    3195 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider:  &{kubelet.sock /var/lib/kubelet/device-plugins/ map[] {0 0} <nil> {{
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054872    3195 state_mem.go:36] [cpumanager] initializing new in-memory state store
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054979    3195 state_mem.go:84] [cpumanager] updated default cpuset: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054985    3195 state_mem.go:92] [cpumanager] updated cpuset assignments: ""map[]""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054994    3195 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider:  &{{0 0} 0x6e9bc50 10000000000 0xc000ace6c0 <nil> <nil> <nil> <nil> m
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.055060    3195 kubelet.go:286] Adding pod path: /etc/kubernetes/manifests
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.055115    3195 kubelet.go:311] Watching apiserver
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.069097    3195 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.069444    3195 client.go:104] Start docker client with request timeout=4m0s
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.076542    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.077278    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.077500    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.079822    3195 docker_service.go:563] Hairpin mode set to ""promiscuous-bridge"" but kubenet is not enabled, falling back to ""hairpin-veth""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.079844    3195 docker_service.go:240] Hairpin mode set to ""hairpin-veth""
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.079937    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.083724    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.083778    3195 docker_service.go:255] Docker cri networking managed by cni
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.083836    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.093083    3195 docker_service.go:260] Docker Info: &{ID:CKKA:YUJL:2557:3IQE:7MRG:35J3:B2MN:GC3H:3WMM:FCV4:R2BU:TQW4 Containers:8 ContainersRunning:8 ContainersPaused:
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.093148    3195 docker_service.go:273] Setting cgroupDriver to systemd
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103239    3195 remote_runtime.go:59] parsed scheme: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103260    3195 remote_runtime.go:59] scheme """" not registered, fallback to default scheme
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103297    3195 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock 0  <nil>}] <nil>}
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103321    3195 clientconn.go:577] ClientConn switching balancer to ""pick_first""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103352    3195 remote_image.go:50] parsed scheme: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103358    3195 remote_image.go:50] scheme """" not registered, fallback to default scheme
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103366    3195 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock 0  <nil>}] <nil>}
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103370    3195 clientconn.go:577] ClientConn switching balancer to ""pick_first""
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.083448    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.086800    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.086872    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.090523    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.094073    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.094838    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.097327    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.102600    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.103424    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.108470    3195 aws_credentials.go:77] while getting AWS credentials NoCredentialProviders: no valid providers in chain. Deprecated.
Jun 10 11:19:34 MasterNode kubelet[3195]: For verbose messaging see aws.Config.CredentialsChainVerboseErrors
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.121933    3195 kuberuntime_manager.go:211] Container runtime docker initialized, version: 19.03.11, apiVersion: 1.40.0
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.135482    3195 server.go:1113] Started kubelet
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.135999    3195 kubelet.go:1302] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.136895    3195 fs_resource_analyzer.go:64] Starting FS ResourceAnalyzer
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.138046    3195 server.go:144] Starting to listen on 0.0.0.0:10250
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.140208    3195 server.go:384] Adding debug handlers to kubelet server.
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.151892    3195 volume_manager.go:265] Starting Kubelet Volume Manager
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.152428    3195 desired_state_of_world_populator.go:138] Desired state populator starts to run
```",kubeadm:default-node-token,https://github.com/kubernetes/kubernetes
37,91992.0,"**What happened**:

Hey everyone! Not sure why this is happening, I have not tried to set up HA k8s before this. Running normal kubeadm init works.... but I am trying to use HA Kubernetes and getting these following errors. Any pointers would be GREAT.

While installing HA kubernetes via command line with kubeadm, I am repeadetly blocked by kubelet message:

kubeadm init --config config.yaml --upload-certs

Contents of config.yaml:
```
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress:
  bindPort: 6444 #Changed this to 6444 per some documents I read through on GitHub issues... also tried with 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: 
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
controlPlaneEndpoint: ""LoadBalancerDNS:6443""
kubernetesVersion: v1.17.3
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
address: 0.0.0.0
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
cgroupDriver: systemd
cgroupsPerQOS: true
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
configMapAndSecretChangeDetectionStrategy: Watch
containerLogMaxFiles: 5
containerLogMaxSize: 10Mi
contentType: application/vnd.kubernetes.protobuf
cpuCFSQuota: true
cpuCFSQuotaPeriod: 100ms
cpuManagerPolicy: none
cpuManagerReconcilePeriod: 10s
enableControllerAttachDetach: true
enableDebuggingHandlers: true
enforceNodeAllocatable:
- pods
eventBurst: 10
eventRecordQPS: 5
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
evictionPressureTransitionPeriod: 5m0s
failSwapOn: false
fileCheckFrequency: 20s
hairpinMode: promiscuous-bridge
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 20s
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
imageMinimumGCAge: 2m0s
iptablesDropBit: 15
iptablesMasqueradeBit: 14
kind: KubeletConfiguration
kubeAPIBurst: 10
kubeAPIQPS: 5
makeIPTablesUtilChains: true
maxOpenFiles: 1000000
maxPods: 110
nodeLeaseDurationSeconds: 80 #changed from 40
nodeStatusReportFrequency: 1m0s
nodeStatusUpdateFrequency: 10s
oomScoreAdj: -999
podPidsLimit: -1
port: 10250
registryBurst: 10
registryPullQPS: 5
resolvConf: /etc/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 4m0s
serializeImagePulls: true
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 4h0m0s
syncFrequency: 1m0s
topologyManagerPolicy: none
volumeStatsAggPeriod: 1m0s
```
Failed to list *v1.Node: Get **https://load-balancer-dns:6443/api/v1/nodes?fieldSelector=metadata.name%3DCurrentMasterNodeName&limit=500&resourceVersion=0:** x509: certificate signed by unknown authority

This is the same for all:
Failed to list *v1beta1.CSIDriver
Failed to list *v1.Service
Failed to list *v1.Node
Failed to get status for pod ""etcd-NodeNameMaster

**For all of them, this is ending with x509: certificate signed by unknown authority**

**What you expected to happen**:

I expected Kubernetes to set up the first control plane node for me and use the F5 load balancer that I provided

**How to reproduce it (as minimally and precisely as possible)**:

Follow all docs here:
- https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/
My set up is for a stacked control plane

**Anything else we need to know?**:

Tried the following and set these params:
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward = 1

Driver for Docker and kubelet: **systemd**



**Environment**:
- Kubernetes version (use `kubectl version`):
kubeadm kubelet kubectl versions: v1.17.3
Docker version 19.03.11
- Cloud provider or hardware configuration:
Bare-metal with an F5 load balancer instead of HAproxy LB
- OS (e.g: `cat /etc/os-release`):
NAME=""Red Hat Enterprise Linux Server""
VERSION=""7.7 (Maipo)""
ID=""rhel""
ID_LIKE=""fedora""
VARIANT=""Server""
VARIANT_ID=""server""
VERSION_ID=""7.7""

- Kernel (e.g. `uname -a`):
Linux MasterNode 3.10.0-1062.18.1.el7.x86_64 #1 SMP Wed Feb 12 14:08:31 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
- Install tools:
yum
- Network plugin and version (if this is a network-related bug):
- Others:

```
Jun 10 11:19:30 MasterNode kubelet[3195]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-
Jun 10 11:19:30 MasterNode kubelet[3195]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964228    3195 server.go:416] Version: v1.17.3
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964703    3195 plugins.go:100] No cloud provider specified.
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.964939    3195 server.go:821] Client rotation is on, will bootstrap in background
Jun 10 11:19:30 MasterNode kubelet[3195]: I0610 11:19:30.967369    3195 certificate_store.go:129] Loading cert/key pair from ""/var/lib/kubelet/pki/kubelet-client-current.pem"".
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054304    3195 server.go:641] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054653    3195 container_manager_linux.go:265] container manager verified user specified cgroup-root exists: []
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054687    3195 container_manager_linux.go:270] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: Con
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054789    3195 fake_topology_manager.go:29] [fake topologymanager] NewFakeManager
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054794    3195 container_manager_linux.go:305] Creating device plugin manager: true
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054826    3195 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider:  &{kubelet.sock /var/lib/kubelet/device-plugins/ map[] {0 0} <nil> {{
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054872    3195 state_mem.go:36] [cpumanager] initializing new in-memory state store
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054979    3195 state_mem.go:84] [cpumanager] updated default cpuset: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054985    3195 state_mem.go:92] [cpumanager] updated cpuset assignments: ""map[]""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.054994    3195 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider:  &{{0 0} 0x6e9bc50 10000000000 0xc000ace6c0 <nil> <nil> <nil> <nil> m
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.055060    3195 kubelet.go:286] Adding pod path: /etc/kubernetes/manifests
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.055115    3195 kubelet.go:311] Watching apiserver
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.069097    3195 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.069444    3195 client.go:104] Start docker client with request timeout=4m0s
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.076542    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.077278    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:31 MasterNode kubelet[3195]: E0610 11:19:31.077500    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.079822    3195 docker_service.go:563] Hairpin mode set to ""promiscuous-bridge"" but kubenet is not enabled, falling back to ""hairpin-veth""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.079844    3195 docker_service.go:240] Hairpin mode set to ""hairpin-veth""
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.079937    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.083724    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.083778    3195 docker_service.go:255] Docker cri networking managed by cni
Jun 10 11:19:31 MasterNode kubelet[3195]: W0610 11:19:31.083836    3195 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.093083    3195 docker_service.go:260] Docker Info: &{ID:CKKA:YUJL:2557:3IQE:7MRG:35J3:B2MN:GC3H:3WMM:FCV4:R2BU:TQW4 Containers:8 ContainersRunning:8 ContainersPaused:
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.093148    3195 docker_service.go:273] Setting cgroupDriver to systemd
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103239    3195 remote_runtime.go:59] parsed scheme: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103260    3195 remote_runtime.go:59] scheme """" not registered, fallback to default scheme
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103297    3195 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock 0  <nil>}] <nil>}
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103321    3195 clientconn.go:577] ClientConn switching balancer to ""pick_first""
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103352    3195 remote_image.go:50] parsed scheme: """"
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103358    3195 remote_image.go:50] scheme """" not registered, fallback to default scheme
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103366    3195 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/run/dockershim.sock 0  <nil>}] <nil>}
Jun 10 11:19:31 MasterNode kubelet[3195]: I0610 11:19:31.103370    3195 clientconn.go:577] ClientConn switching balancer to ""pick_first""
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.083448    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.086800    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:32 MasterNode kubelet[3195]: E0610 11:19:32.086872    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.090523    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.094073    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:33 MasterNode kubelet[3195]: E0610 11:19:33.094838    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.097327    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:449: Failed to list *v1.Service: Get https://LoadBalancerIP:6443/api/v1/services?limit
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.102600    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/kubelet.go:458: Failed to list *v1.Node: Get https://LoadBalancerIP:6443/api/v1/nodes?fieldSelect
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.103424    3195 reflector.go:153] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:46: Failed to list *v1.Pod: Get https://LoadBalancerIP:6443/api/v1/pods?field
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.108470    3195 aws_credentials.go:77] while getting AWS credentials NoCredentialProviders: no valid providers in chain. Deprecated.
Jun 10 11:19:34 MasterNode kubelet[3195]: For verbose messaging see aws.Config.CredentialsChainVerboseErrors
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.121933    3195 kuberuntime_manager.go:211] Container runtime docker initialized, version: 19.03.11, apiVersion: 1.40.0
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.135482    3195 server.go:1113] Started kubelet
Jun 10 11:19:34 MasterNode kubelet[3195]: E0610 11:19:34.135999    3195 kubelet.go:1302] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.136895    3195 fs_resource_analyzer.go:64] Starting FS ResourceAnalyzer
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.138046    3195 server.go:144] Starting to listen on 0.0.0.0:10250
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.140208    3195 server.go:384] Adding debug handlers to kubelet server.
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.151892    3195 volume_manager.go:265] Starting Kubelet Volume Manager
Jun 10 11:19:34 MasterNode kubelet[3195]: I0610 11:19:34.152428    3195 desired_state_of_world_populator.go:138] Desired state populator starts to run
```",timeoutForControlPlane,https://github.com/kubernetes/kubernetes
38,90044.0,"I have configured keycloak for Kubernetes RBAC. 

- user having access to get pods

```
vagrant@haproxy:~/.kube$ kubectl auth can-i get pods --user=oidc
Warning: the server doesn't have a resource type 'pods'
yes
```

```
vagrant@haproxy:~/.kube$ kubectl get pods --user=oidc
error: You must be logged in to the server (Unauthorized)
```

my kubeconfig file for the user looks like below

```yaml
users:
- name: oidc
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - oidc-login
      - get-token
      - --oidc-issuer-url=https://test.example.com/auth/realms/kubernetes
      - --oidc-client-id=kubernetes
      - --oidc-client-secret=e479f74d-d9fd-415b-b1db-fd7946d3ad90
      - --username=test
      - --grant-type=authcode-keyboard
      command: kubectl
```

",e479f74d-d9fd-415b-b1db-fd7946d3ad90,https://github.com/kubernetes/kubernetes
39,90044.0,"I have configured keycloak for Kubernetes RBAC. 

- user having access to get pods

```
vagrant@haproxy:~/.kube$ kubectl auth can-i get pods --user=oidc
Warning: the server doesn't have a resource type 'pods'
yes
```

```
vagrant@haproxy:~/.kube$ kubectl get pods --user=oidc
error: You must be logged in to the server (Unauthorized)
```

my kubeconfig file for the user looks like below

```yaml
users:
- name: oidc
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - oidc-login
      - get-token
      - --oidc-issuer-url=https://test.example.com/auth/realms/kubernetes
      - --oidc-client-id=kubernetes
      - --oidc-client-secret=e479f74d-d9fd-415b-b1db-fd7946d3ad90
      - --username=test
      - --grant-type=authcode-keyboard
      command: kubectl
```

",oidc-client-id=kubernetes,https://github.com/kubernetes/kubernetes
40,90044.0,"I have configured keycloak for Kubernetes RBAC. 

- user having access to get pods

```
vagrant@haproxy:~/.kube$ kubectl auth can-i get pods --user=oidc
Warning: the server doesn't have a resource type 'pods'
yes
```

```
vagrant@haproxy:~/.kube$ kubectl get pods --user=oidc
error: You must be logged in to the server (Unauthorized)
```

my kubeconfig file for the user looks like below

```yaml
users:
- name: oidc
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - oidc-login
      - get-token
      - --oidc-issuer-url=https://test.example.com/auth/realms/kubernetes
      - --oidc-client-id=kubernetes
      - --oidc-client-secret=e479f74d-d9fd-415b-b1db-fd7946d3ad90
      - --username=test
      - --grant-type=authcode-keyboard
      command: kubectl
```

",b1db-fd7946d3ad90,https://github.com/kubernetes/kubernetes
41,89181.0,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

Pods with subPath are stuck on Terminating when deleting

**What you expected to happen**:

Pod succefully removed.

**How to reproduce it (as minimally and precisely as possible)**:

crate:

```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
  - image: docker.io/library/alpine
    name: test
    command: ['sleep', 'infinity']
    volumeMounts:
    - mountPath: /blabla/sss
      name: keys
      subPath: somekey
  volumes:
  - name: keys
    secret:
      secretName: test
---
apiVersion: v1
kind: Secret
metadata:
  name: test
data:
  somekey: R29vZCBhdHRlbXBsdCwgYnV0IG5vdCE=
  someotherkey: QXJlIHlvdSBraWRkaW5nIG1lPw==
```

remove the pod:

```
kubectl delete pod test
```

check the kubelet logs:

```
Mar 17 20:40:31 m1c4 kubelet[13866]: E0317 20:40:31.230957   13866 nestedpendingoperations.go:270] Operation for ""\""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"" (\""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"")"" failed. No retries permitted until 2020-03-17 20:42:33.230929328 +0100 CET m=+1192722.291304745 (durationBeforeRetry 2m2s). Error: ""error cleaning subPath mounts for volume \""keys\"" (UniqueName: \""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"") pod \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"" (UID: \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"") : error processing /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test: error cleaning subpath mount /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0\nOutput: umount: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: not mounted.\n""
Mar 17 20:42:33 m1c4 kubelet[13866]: I0317 20:42:33.234534   13866 reconciler.go:183] operationExecutor.UnmountVolume started for volume ""keys"" (UniqueName: ""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys"") pod ""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238"" (UID: ""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238"")
Mar 17 20:42:33 m1c4 kubelet[13866]: E0317 20:42:33.257819   13866 nestedpendingoperations.go:270] Operation for ""\""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"" (\""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"")"" failed. No retries permitted until 2020-03-17 20:44:35.257684981 +0100 CET m=+1192844.318060559 (durationBeforeRetry 2m2s). Error: ""error cleaning subPath mounts for volume \""keys\"" (UniqueName: \""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"") pod \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"" (UID: \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"") : error processing /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test: error cleaning subpath mount /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0\nOutput: umount: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: not mounted.\n""
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.0"", GitCommit:""70132b0f130acc0bed193d9ba59dd186f0e634cf"", GitTreeState:""clean"", BuildDate:""2019-12-07T21:20:10Z"", GoVersion:""go1.13.4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.0"", GitCommit:""70132b0f130acc0bed193d9ba59dd186f0e634cf"", GitTreeState:""clean"", BuildDate:""2019-12-07T21:12:17Z"", GoVersion:""go1.13.4"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration: Bare Metal
- OS (e.g: `cat /etc/os-release`):
```
NAME=""Ubuntu""
VERSION=""18.04.3 LTS (Bionic Beaver)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 18.04.3 LTS""
VERSION_ID=""18.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic
```
- Kernel (e.g. `uname -a`):
```
Linux m1c4 4.15.18-24-pve #1 SMP PVE 4.15.18-52 (Thu, 05 Dec 2019 10:14:17 +0100) x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools: kubeadm
- Network plugin and version (if this is a network-related bug):
- Others:
docker version: 19.3.5

docker is running with the next config:
```json
{
  ""exec-opts"": [""native.cgroupdriver=systemd""],
  ""iptables"": false,
  ""ip-forward"": false,
  ""bridge"": ""none"",
  ""log-driver"": ""journald"",
  ""storage-driver"": ""overlay2""
}
```

kubelet versions:
```
Kubernetes v1.17.1
Kubernetes v1.17.4
```

created file is empty and not mountpoint, path is not symlink:
```bash
# mountpoint /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 
/var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 is not a mountpoint

# ls -lah
-rw-r----- 1 root root 0 Mar 17 20:37 /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0

# readlink -f /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 
/var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0
```

Not sure, but maybe problem caused by overlayfs root:

```bash
# df -h /var/lib/kubelet/pods/
Filesystem               Size  Used Avail Use% Mounted on
/run/initramfs/ltsp/cow   16G  412M   16G   3% /
```",89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238,https://github.com/kubernetes/kubernetes
42,89181.0,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

Pods with subPath are stuck on Terminating when deleting

**What you expected to happen**:

Pod succefully removed.

**How to reproduce it (as minimally and precisely as possible)**:

crate:

```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
  - image: docker.io/library/alpine
    name: test
    command: ['sleep', 'infinity']
    volumeMounts:
    - mountPath: /blabla/sss
      name: keys
      subPath: somekey
  volumes:
  - name: keys
    secret:
      secretName: test
---
apiVersion: v1
kind: Secret
metadata:
  name: test
data:
  somekey: R29vZCBhdHRlbXBsdCwgYnV0IG5vdCE=
  someotherkey: QXJlIHlvdSBraWRkaW5nIG1lPw==
```

remove the pod:

```
kubectl delete pod test
```

check the kubelet logs:

```
Mar 17 20:40:31 m1c4 kubelet[13866]: E0317 20:40:31.230957   13866 nestedpendingoperations.go:270] Operation for ""\""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"" (\""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"")"" failed. No retries permitted until 2020-03-17 20:42:33.230929328 +0100 CET m=+1192722.291304745 (durationBeforeRetry 2m2s). Error: ""error cleaning subPath mounts for volume \""keys\"" (UniqueName: \""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"") pod \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"" (UID: \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"") : error processing /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test: error cleaning subpath mount /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0\nOutput: umount: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: not mounted.\n""
Mar 17 20:42:33 m1c4 kubelet[13866]: I0317 20:42:33.234534   13866 reconciler.go:183] operationExecutor.UnmountVolume started for volume ""keys"" (UniqueName: ""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys"") pod ""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238"" (UID: ""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238"")
Mar 17 20:42:33 m1c4 kubelet[13866]: E0317 20:42:33.257819   13866 nestedpendingoperations.go:270] Operation for ""\""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"" (\""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"")"" failed. No retries permitted until 2020-03-17 20:44:35.257684981 +0100 CET m=+1192844.318060559 (durationBeforeRetry 2m2s). Error: ""error cleaning subPath mounts for volume \""keys\"" (UniqueName: \""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"") pod \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"" (UID: \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"") : error processing /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test: error cleaning subpath mount /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0\nOutput: umount: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: not mounted.\n""
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.0"", GitCommit:""70132b0f130acc0bed193d9ba59dd186f0e634cf"", GitTreeState:""clean"", BuildDate:""2019-12-07T21:20:10Z"", GoVersion:""go1.13.4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.0"", GitCommit:""70132b0f130acc0bed193d9ba59dd186f0e634cf"", GitTreeState:""clean"", BuildDate:""2019-12-07T21:12:17Z"", GoVersion:""go1.13.4"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration: Bare Metal
- OS (e.g: `cat /etc/os-release`):
```
NAME=""Ubuntu""
VERSION=""18.04.3 LTS (Bionic Beaver)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 18.04.3 LTS""
VERSION_ID=""18.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic
```
- Kernel (e.g. `uname -a`):
```
Linux m1c4 4.15.18-24-pve #1 SMP PVE 4.15.18-52 (Thu, 05 Dec 2019 10:14:17 +0100) x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools: kubeadm
- Network plugin and version (if this is a network-related bug):
- Others:
docker version: 19.3.5

docker is running with the next config:
```json
{
  ""exec-opts"": [""native.cgroupdriver=systemd""],
  ""iptables"": false,
  ""ip-forward"": false,
  ""bridge"": ""none"",
  ""log-driver"": ""journald"",
  ""storage-driver"": ""overlay2""
}
```

kubelet versions:
```
Kubernetes v1.17.1
Kubernetes v1.17.4
```

created file is empty and not mountpoint, path is not symlink:
```bash
# mountpoint /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 
/var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 is not a mountpoint

# ls -lah
-rw-r----- 1 root root 0 Mar 17 20:37 /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0

# readlink -f /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 
/var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0
```

Not sure, but maybe problem caused by overlayfs root:

```bash
# df -h /var/lib/kubelet/pods/
Filesystem               Size  Used Avail Use% Mounted on
/run/initramfs/ltsp/cow   16G  412M   16G   3% /
```",QXJlIHlvdSBraWRkaW5nIG1lPw,https://github.com/kubernetes/kubernetes
43,89181.0,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

Pods with subPath are stuck on Terminating when deleting

**What you expected to happen**:

Pod succefully removed.

**How to reproduce it (as minimally and precisely as possible)**:

crate:

```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
  - image: docker.io/library/alpine
    name: test
    command: ['sleep', 'infinity']
    volumeMounts:
    - mountPath: /blabla/sss
      name: keys
      subPath: somekey
  volumes:
  - name: keys
    secret:
      secretName: test
---
apiVersion: v1
kind: Secret
metadata:
  name: test
data:
  somekey: R29vZCBhdHRlbXBsdCwgYnV0IG5vdCE=
  someotherkey: QXJlIHlvdSBraWRkaW5nIG1lPw==
```

remove the pod:

```
kubectl delete pod test
```

check the kubelet logs:

```
Mar 17 20:40:31 m1c4 kubelet[13866]: E0317 20:40:31.230957   13866 nestedpendingoperations.go:270] Operation for ""\""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"" (\""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"")"" failed. No retries permitted until 2020-03-17 20:42:33.230929328 +0100 CET m=+1192722.291304745 (durationBeforeRetry 2m2s). Error: ""error cleaning subPath mounts for volume \""keys\"" (UniqueName: \""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"") pod \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"" (UID: \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"") : error processing /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test: error cleaning subpath mount /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0\nOutput: umount: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: not mounted.\n""
Mar 17 20:42:33 m1c4 kubelet[13866]: I0317 20:42:33.234534   13866 reconciler.go:183] operationExecutor.UnmountVolume started for volume ""keys"" (UniqueName: ""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys"") pod ""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238"" (UID: ""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238"")
Mar 17 20:42:33 m1c4 kubelet[13866]: E0317 20:42:33.257819   13866 nestedpendingoperations.go:270] Operation for ""\""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"" (\""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"")"" failed. No retries permitted until 2020-03-17 20:44:35.257684981 +0100 CET m=+1192844.318060559 (durationBeforeRetry 2m2s). Error: ""error cleaning subPath mounts for volume \""keys\"" (UniqueName: \""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"") pod \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"" (UID: \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"") : error processing /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test: error cleaning subpath mount /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0\nOutput: umount: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: not mounted.\n""
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.0"", GitCommit:""70132b0f130acc0bed193d9ba59dd186f0e634cf"", GitTreeState:""clean"", BuildDate:""2019-12-07T21:20:10Z"", GoVersion:""go1.13.4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.0"", GitCommit:""70132b0f130acc0bed193d9ba59dd186f0e634cf"", GitTreeState:""clean"", BuildDate:""2019-12-07T21:12:17Z"", GoVersion:""go1.13.4"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration: Bare Metal
- OS (e.g: `cat /etc/os-release`):
```
NAME=""Ubuntu""
VERSION=""18.04.3 LTS (Bionic Beaver)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 18.04.3 LTS""
VERSION_ID=""18.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic
```
- Kernel (e.g. `uname -a`):
```
Linux m1c4 4.15.18-24-pve #1 SMP PVE 4.15.18-52 (Thu, 05 Dec 2019 10:14:17 +0100) x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools: kubeadm
- Network plugin and version (if this is a network-related bug):
- Others:
docker version: 19.3.5

docker is running with the next config:
```json
{
  ""exec-opts"": [""native.cgroupdriver=systemd""],
  ""iptables"": false,
  ""ip-forward"": false,
  ""bridge"": ""none"",
  ""log-driver"": ""journald"",
  ""storage-driver"": ""overlay2""
}
```

kubelet versions:
```
Kubernetes v1.17.1
Kubernetes v1.17.4
```

created file is empty and not mountpoint, path is not symlink:
```bash
# mountpoint /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 
/var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 is not a mountpoint

# ls -lah
-rw-r----- 1 root root 0 Mar 17 20:37 /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0

# readlink -f /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 
/var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0
```

Not sure, but maybe problem caused by overlayfs root:

```bash
# df -h /var/lib/kubelet/pods/
Filesystem               Size  Used Avail Use% Mounted on
/run/initramfs/ltsp/cow   16G  412M   16G   3% /
```",4cc6-89a0-2fd9a346e238,https://github.com/kubernetes/kubernetes
44,89181.0,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

Pods with subPath are stuck on Terminating when deleting

**What you expected to happen**:

Pod succefully removed.

**How to reproduce it (as minimally and precisely as possible)**:

crate:

```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
  - image: docker.io/library/alpine
    name: test
    command: ['sleep', 'infinity']
    volumeMounts:
    - mountPath: /blabla/sss
      name: keys
      subPath: somekey
  volumes:
  - name: keys
    secret:
      secretName: test
---
apiVersion: v1
kind: Secret
metadata:
  name: test
data:
  somekey: R29vZCBhdHRlbXBsdCwgYnV0IG5vdCE=
  someotherkey: QXJlIHlvdSBraWRkaW5nIG1lPw==
```

remove the pod:

```
kubectl delete pod test
```

check the kubelet logs:

```
Mar 17 20:40:31 m1c4 kubelet[13866]: E0317 20:40:31.230957   13866 nestedpendingoperations.go:270] Operation for ""\""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"" (\""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"")"" failed. No retries permitted until 2020-03-17 20:42:33.230929328 +0100 CET m=+1192722.291304745 (durationBeforeRetry 2m2s). Error: ""error cleaning subPath mounts for volume \""keys\"" (UniqueName: \""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"") pod \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"" (UID: \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"") : error processing /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test: error cleaning subpath mount /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0\nOutput: umount: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: not mounted.\n""
Mar 17 20:42:33 m1c4 kubelet[13866]: I0317 20:42:33.234534   13866 reconciler.go:183] operationExecutor.UnmountVolume started for volume ""keys"" (UniqueName: ""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys"") pod ""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238"" (UID: ""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238"")
Mar 17 20:42:33 m1c4 kubelet[13866]: E0317 20:42:33.257819   13866 nestedpendingoperations.go:270] Operation for ""\""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"" (\""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"")"" failed. No retries permitted until 2020-03-17 20:44:35.257684981 +0100 CET m=+1192844.318060559 (durationBeforeRetry 2m2s). Error: ""error cleaning subPath mounts for volume \""keys\"" (UniqueName: \""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"") pod \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"" (UID: \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"") : error processing /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test: error cleaning subpath mount /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0\nOutput: umount: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: not mounted.\n""
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.0"", GitCommit:""70132b0f130acc0bed193d9ba59dd186f0e634cf"", GitTreeState:""clean"", BuildDate:""2019-12-07T21:20:10Z"", GoVersion:""go1.13.4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.0"", GitCommit:""70132b0f130acc0bed193d9ba59dd186f0e634cf"", GitTreeState:""clean"", BuildDate:""2019-12-07T21:12:17Z"", GoVersion:""go1.13.4"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration: Bare Metal
- OS (e.g: `cat /etc/os-release`):
```
NAME=""Ubuntu""
VERSION=""18.04.3 LTS (Bionic Beaver)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 18.04.3 LTS""
VERSION_ID=""18.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic
```
- Kernel (e.g. `uname -a`):
```
Linux m1c4 4.15.18-24-pve #1 SMP PVE 4.15.18-52 (Thu, 05 Dec 2019 10:14:17 +0100) x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools: kubeadm
- Network plugin and version (if this is a network-related bug):
- Others:
docker version: 19.3.5

docker is running with the next config:
```json
{
  ""exec-opts"": [""native.cgroupdriver=systemd""],
  ""iptables"": false,
  ""ip-forward"": false,
  ""bridge"": ""none"",
  ""log-driver"": ""journald"",
  ""storage-driver"": ""overlay2""
}
```

kubelet versions:
```
Kubernetes v1.17.1
Kubernetes v1.17.4
```

created file is empty and not mountpoint, path is not symlink:
```bash
# mountpoint /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 
/var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 is not a mountpoint

# ls -lah
-rw-r----- 1 root root 0 Mar 17 20:37 /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0

# readlink -f /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 
/var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0
```

Not sure, but maybe problem caused by overlayfs root:

```bash
# df -h /var/lib/kubelet/pods/
Filesystem               Size  Used Avail Use% Mounted on
/run/initramfs/ltsp/cow   16G  412M   16G   3% /
```",R29vZCBhdHRlbXBsdCwgYnV0IG5vdCE,https://github.com/kubernetes/kubernetes
45,89181.0,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

Pods with subPath are stuck on Terminating when deleting

**What you expected to happen**:

Pod succefully removed.

**How to reproduce it (as minimally and precisely as possible)**:

crate:

```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
  - image: docker.io/library/alpine
    name: test
    command: ['sleep', 'infinity']
    volumeMounts:
    - mountPath: /blabla/sss
      name: keys
      subPath: somekey
  volumes:
  - name: keys
    secret:
      secretName: test
---
apiVersion: v1
kind: Secret
metadata:
  name: test
data:
  somekey: R29vZCBhdHRlbXBsdCwgYnV0IG5vdCE=
  someotherkey: QXJlIHlvdSBraWRkaW5nIG1lPw==
```

remove the pod:

```
kubectl delete pod test
```

check the kubelet logs:

```
Mar 17 20:40:31 m1c4 kubelet[13866]: E0317 20:40:31.230957   13866 nestedpendingoperations.go:270] Operation for ""\""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"" (\""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"")"" failed. No retries permitted until 2020-03-17 20:42:33.230929328 +0100 CET m=+1192722.291304745 (durationBeforeRetry 2m2s). Error: ""error cleaning subPath mounts for volume \""keys\"" (UniqueName: \""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"") pod \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"" (UID: \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"") : error processing /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test: error cleaning subpath mount /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0\nOutput: umount: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: not mounted.\n""
Mar 17 20:42:33 m1c4 kubelet[13866]: I0317 20:42:33.234534   13866 reconciler.go:183] operationExecutor.UnmountVolume started for volume ""keys"" (UniqueName: ""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys"") pod ""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238"" (UID: ""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238"")
Mar 17 20:42:33 m1c4 kubelet[13866]: E0317 20:42:33.257819   13866 nestedpendingoperations.go:270] Operation for ""\""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"" (\""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"")"" failed. No retries permitted until 2020-03-17 20:44:35.257684981 +0100 CET m=+1192844.318060559 (durationBeforeRetry 2m2s). Error: ""error cleaning subPath mounts for volume \""keys\"" (UniqueName: \""kubernetes.io/secret/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238-keys\"") pod \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"" (UID: \""89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238\"") : error processing /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test: error cleaning subpath mount /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0\nOutput: umount: /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0: not mounted.\n""
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.0"", GitCommit:""70132b0f130acc0bed193d9ba59dd186f0e634cf"", GitTreeState:""clean"", BuildDate:""2019-12-07T21:20:10Z"", GoVersion:""go1.13.4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.0"", GitCommit:""70132b0f130acc0bed193d9ba59dd186f0e634cf"", GitTreeState:""clean"", BuildDate:""2019-12-07T21:12:17Z"", GoVersion:""go1.13.4"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration: Bare Metal
- OS (e.g: `cat /etc/os-release`):
```
NAME=""Ubuntu""
VERSION=""18.04.3 LTS (Bionic Beaver)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 18.04.3 LTS""
VERSION_ID=""18.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic
```
- Kernel (e.g. `uname -a`):
```
Linux m1c4 4.15.18-24-pve #1 SMP PVE 4.15.18-52 (Thu, 05 Dec 2019 10:14:17 +0100) x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools: kubeadm
- Network plugin and version (if this is a network-related bug):
- Others:
docker version: 19.3.5

docker is running with the next config:
```json
{
  ""exec-opts"": [""native.cgroupdriver=systemd""],
  ""iptables"": false,
  ""ip-forward"": false,
  ""bridge"": ""none"",
  ""log-driver"": ""journald"",
  ""storage-driver"": ""overlay2""
}
```

kubelet versions:
```
Kubernetes v1.17.1
Kubernetes v1.17.4
```

created file is empty and not mountpoint, path is not symlink:
```bash
# mountpoint /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 
/var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 is not a mountpoint

# ls -lah
-rw-r----- 1 root root 0 Mar 17 20:37 /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0

# readlink -f /var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0 
/var/lib/kubelet/pods/89b3eaa9-0bd8-4cc6-89a0-2fd9a346e238/volume-subpaths/keys/test/0
```

Not sure, but maybe problem caused by overlayfs root:

```bash
# df -h /var/lib/kubelet/pods/
Filesystem               Size  Used Avail Use% Mounted on
/run/initramfs/ltsp/cow   16G  412M   16G   3% /
```",89a0-2fd9a346e238,https://github.com/kubernetes/kubernetes
46,88410.0,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:
I installed 4 new nodes on my cluster in same time I upgrade it to 1.16.7.
No problem for logs on old existing nodes.
And on this 4 new node I can't have logs with kubectl: 
```sh
 $ kubectl logs piddashboard-57b9d6bfbd-cbn9c
Error from server: Get https://10.194.26.119:10250/containerLogs/pid-dashboard/piddashboard-57b9d6bfbd-cbn9c/piddashboard: Service Unavailable
```

**What you expected to happen**:
Getting logs and no errors, from new nodes

**How to reproduce it (as minimally and precisely as possible)**:
I don't know if it's upgrade of cluster version 1.16.5 to 1.16.7

Or usage of proxy for the docker's daemon (I setup exactly like other working nodes)

**Anything else we need to know?**:
API logs:
```sh
...
E0221 14:48:32.630188       1 status.go:71] apiserver received an error that is not an metav1.Status: &url.Error{Op:""Get"", URL:""https://10.194.26.119:10250/containerLogs/pid-dashboard/piddashboard-57b9d6bfbd-cbn9c/piddashboard"", Err:(*errors.errorString)(0xc00b18a900)}
...
```

No problem from my PC:
```sh
$ export TOKEN=eyJhbGciOiJSUzI1NiIs...

$ curl --insecure --header ""Authorization: Bearer ${TOKEN}"" https://10.194.26.119:10250/containerLogs/pid-dashboard/piddashboard-57b9d6bfbd-cbn9c/piddashboard


Start command: java -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -Djava.security.egd=file:/dev/./urandom -jar /app.jar

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::        (v2.1.0.RELEASE)

2020-02-21 14:52:51.180  INFO 1 --- [           main] f.p.dashboard.PidDashboardApplicationKt  : Starting PidDashboardApplicationKt v0.0.1-SNAPSHOT on piddashboard-57b9d6bfbd-cbn9c with PID 1 (/app.jar started by root in /)
2020-02-21 14:52:51.187  INFO 1 --- [           main] f.p.dashboard.PidDashboardApplicationKt  : No active profile set, falling back to default profiles: default
2020-02-21 14:52:52.613  INFO 1 --- [           main] o.s.c.a.ConfigurationClassPostProcessor  : Cannot enhance @Configuration bean definition 'org.springframework.guice.annotation.ModuleRegistryConfiguration' since its singleton instanc
e has been created too early. The typical cause is a non-static @Bean method with a BeanDefinitionRegistryPostProcessor return type: Consider declaring such methods as 'static'.
2020-02-21 14:52:53.570  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
2020-02-21 14:52:53.594  INFO 1 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
...
```

**Environment**:
- Kubernetes version (use `kubectl version`):
```sh
Client Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.2"", GitCommit:""59603c6e503c87169aea6106f57b9f242f64df89"", GitTreeState:""clean"", BuildDate:""2020-01-18T23:30:10Z"", GoVersion:""go1.13.5"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""16"", GitVersion:""v1.16.3"", GitCommit:""b3cbbae08ec52a7fc73d334838e18d17e8512749"", GitTreeState:""clean"", BuildDate:""2019-11-13T11:13:49Z"", GoVersion:""go1.12.12"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration:
`On premise with kubespray`
- OS (e.g: `cat /etc/os-release`):
For all nodes:
```sh
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
```
- Kernel (e.g. `uname -a`):
For all nodes:
```sh
Linux forge-XX 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools:
`n/a`
- Network plugin and version (if this is a network-related bug):
`calico`
- Others:
Usage of proxy

Actual Cluster:
```sh
$ kubectl get nodes -o wide
NAME       STATUS   ROLES    AGE    VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME
forge-04   Ready    <none>   340d   v1.16.7   10.194.26.60    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-05   Ready    <none>   340d   v1.16.7   10.194.26.61    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-06   Ready    master   344d   v1.16.7   10.194.26.96    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-07   Ready    master   344d   v1.16.7   10.194.27.39    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-08   Ready    master   340d   v1.16.7   10.194.27.63    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-10   Ready    <none>   344d   v1.16.7   10.194.26.49    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-11   Ready    <none>   344d   v1.16.7   10.194.26.37    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-12   Ready    <none>   344d   v1.16.7   10.194.26.39    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
### LOGS KO since here
forge-13   Ready    <none>   26h    v1.16.7   10.194.26.59    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6 
forge-14   Ready    <none>   26h    v1.16.7   10.194.27.120   <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-15   Ready    <none>   26h    v1.16.7   10.194.26.213   <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-16   Ready    <none>   26h    v1.16.7   10.194.26.119   <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
```

There is no other visible bug, pod are scheduled on it, etc, etc.. just logs.

I thinks it's a config problem on masters but I don't know what...

",UnlockExperimentalVMOptions,https://github.com/kubernetes/kubernetes
47,88410.0,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:
I installed 4 new nodes on my cluster in same time I upgrade it to 1.16.7.
No problem for logs on old existing nodes.
And on this 4 new node I can't have logs with kubectl: 
```sh
 $ kubectl logs piddashboard-57b9d6bfbd-cbn9c
Error from server: Get https://10.194.26.119:10250/containerLogs/pid-dashboard/piddashboard-57b9d6bfbd-cbn9c/piddashboard: Service Unavailable
```

**What you expected to happen**:
Getting logs and no errors, from new nodes

**How to reproduce it (as minimally and precisely as possible)**:
I don't know if it's upgrade of cluster version 1.16.5 to 1.16.7

Or usage of proxy for the docker's daemon (I setup exactly like other working nodes)

**Anything else we need to know?**:
API logs:
```sh
...
E0221 14:48:32.630188       1 status.go:71] apiserver received an error that is not an metav1.Status: &url.Error{Op:""Get"", URL:""https://10.194.26.119:10250/containerLogs/pid-dashboard/piddashboard-57b9d6bfbd-cbn9c/piddashboard"", Err:(*errors.errorString)(0xc00b18a900)}
...
```

No problem from my PC:
```sh
$ export TOKEN=eyJhbGciOiJSUzI1NiIs...

$ curl --insecure --header ""Authorization: Bearer ${TOKEN}"" https://10.194.26.119:10250/containerLogs/pid-dashboard/piddashboard-57b9d6bfbd-cbn9c/piddashboard


Start command: java -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -Djava.security.egd=file:/dev/./urandom -jar /app.jar

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::        (v2.1.0.RELEASE)

2020-02-21 14:52:51.180  INFO 1 --- [           main] f.p.dashboard.PidDashboardApplicationKt  : Starting PidDashboardApplicationKt v0.0.1-SNAPSHOT on piddashboard-57b9d6bfbd-cbn9c with PID 1 (/app.jar started by root in /)
2020-02-21 14:52:51.187  INFO 1 --- [           main] f.p.dashboard.PidDashboardApplicationKt  : No active profile set, falling back to default profiles: default
2020-02-21 14:52:52.613  INFO 1 --- [           main] o.s.c.a.ConfigurationClassPostProcessor  : Cannot enhance @Configuration bean definition 'org.springframework.guice.annotation.ModuleRegistryConfiguration' since its singleton instanc
e has been created too early. The typical cause is a non-static @Bean method with a BeanDefinitionRegistryPostProcessor return type: Consider declaring such methods as 'static'.
2020-02-21 14:52:53.570  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
2020-02-21 14:52:53.594  INFO 1 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
...
```

**Environment**:
- Kubernetes version (use `kubectl version`):
```sh
Client Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.2"", GitCommit:""59603c6e503c87169aea6106f57b9f242f64df89"", GitTreeState:""clean"", BuildDate:""2020-01-18T23:30:10Z"", GoVersion:""go1.13.5"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""16"", GitVersion:""v1.16.3"", GitCommit:""b3cbbae08ec52a7fc73d334838e18d17e8512749"", GitTreeState:""clean"", BuildDate:""2019-11-13T11:13:49Z"", GoVersion:""go1.12.12"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration:
`On premise with kubespray`
- OS (e.g: `cat /etc/os-release`):
For all nodes:
```sh
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
```
- Kernel (e.g. `uname -a`):
For all nodes:
```sh
Linux forge-XX 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools:
`n/a`
- Network plugin and version (if this is a network-related bug):
`calico`
- Others:
Usage of proxy

Actual Cluster:
```sh
$ kubectl get nodes -o wide
NAME       STATUS   ROLES    AGE    VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME
forge-04   Ready    <none>   340d   v1.16.7   10.194.26.60    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-05   Ready    <none>   340d   v1.16.7   10.194.26.61    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-06   Ready    master   344d   v1.16.7   10.194.26.96    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-07   Ready    master   344d   v1.16.7   10.194.27.39    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-08   Ready    master   340d   v1.16.7   10.194.27.63    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-10   Ready    <none>   344d   v1.16.7   10.194.26.49    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-11   Ready    <none>   344d   v1.16.7   10.194.26.37    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-12   Ready    <none>   344d   v1.16.7   10.194.26.39    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
### LOGS KO since here
forge-13   Ready    <none>   26h    v1.16.7   10.194.26.59    <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6 
forge-14   Ready    <none>   26h    v1.16.7   10.194.27.120   <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-15   Ready    <none>   26h    v1.16.7   10.194.26.213   <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
forge-16   Ready    <none>   26h    v1.16.7   10.194.26.119   <none>        CentOS Linux 7 (Core)   3.10.0-1062.12.1.el7.x86_64   docker://19.3.6
```

There is no other visible bug, pod are scheduled on it, etc, etc.. just logs.

I thinks it's a config problem on masters but I don't know what...

",eyJhbGciOiJSUzI1NiIs,https://github.com/kubernetes/kubernetes
48,87229.0,"**What happened**:
```
[root@master01 kubernetes]# kubeadm init --config kubeadm.conf --upload-certs --certificate-key 5cf38ca270d75eb11fac6b3ef77b0e1ef7318d56a1edf3ad0687c72281c89aca
can not mix '--config' with arguments [certificate-key]
To see the stack trace of this error execute with --v=5 or higher
```
**What you expected to happen**:
i hope init success
**How to reproduce it (as minimally and precisely as possible)**:
```
kubeadm init --config kubeadm.conf --upload-certs --certificate-key 5cf38ca270d75eb11fac6b3ef77b0e1ef7318d56a1edf3ad0687c72281c89aca
```
**Anything else we need to know?**:

**Environment**:
[root@master01 kubernetes]# kubectl version --short
Client Version: v1.17.0
Unable to connect to the server: EOF
",5cf38ca270d75eb11fac6b3ef77b0e1ef7318d56a1edf3ad0687c72281c89aca,https://github.com/kubernetes/kubernetes
49,87229.0,"**What happened**:
```
[root@master01 kubernetes]# kubeadm init --config kubeadm.conf --upload-certs --certificate-key 5cf38ca270d75eb11fac6b3ef77b0e1ef7318d56a1edf3ad0687c72281c89aca
can not mix '--config' with arguments [certificate-key]
To see the stack trace of this error execute with --v=5 or higher
```
**What you expected to happen**:
i hope init success
**How to reproduce it (as minimally and precisely as possible)**:
```
kubeadm init --config kubeadm.conf --upload-certs --certificate-key 5cf38ca270d75eb11fac6b3ef77b0e1ef7318d56a1edf3ad0687c72281c89aca
```
**Anything else we need to know?**:

**Environment**:
[root@master01 kubernetes]# kubectl version --short
Client Version: v1.17.0
Unable to connect to the server: EOF
",5cf38ca270d75eb11fac6b3ef77b0e1ef7318d56a1edf3ad06,https://github.com/kubernetes/kubernetes
50,85578.0,"I am trying to create a new CRD with a password field as shown below

```
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: tests.abc.com
spec:
  group: abc.com
  versions:
    - name: v1
      served: true
      storage: true
  scope: Cluster
  names:
    plural: tests
    singular: test
    kind: Test
  validation:
    openAPIV3Schema:
      type: object
      properties:
        spec:
          type: object
          properties:
            user:
              type: string
            password:
              type: password
  additionalPrinterColumns:
    - name: User
      type: string
      description: The user who created this object
      JSONPath: .spec.user
    - name: Age
      type: date
      JSONPath: .metadata.creationTimestamp
```
When I try to create a new object

```
apiVersion: abc.com/v1
kind: Test
metadata:
  name: test
spec:
  user: john
  password: aGVsbG93b3JsZA==

```

**What happened**:

When I try to do `kubectl apply -f test.yaml`

I get the below error

```
The Test ""test"" is invalid: []: Invalid value: map[string]interface {}{""apiVersion"":""abc.com/v1"", ""kind"":""Test"", ""metadata"":map[string]interface {}{""annotations"":map[string]interface {}{""kubectl.kubernetes.io/last-applied-configuration"":""{\""apiVersion\"":\""abc.com/v1\"",\""kind\"":\""Test\"",\""metadata\"":{\""annotations\"":{},\""name\"":\""test\""},\""spec\"":{\""password\"":\""MWYyZDFlMmU2N2Rm\"",\""user\"":\""john\""}}\n""}, ""creationTimestamp"":""2019-11-24T01:24:24Z"", ""generation"":1, ""name"":""test"", ""uid"":""267b100d-0e59-11ea-8341-42010a80018e""}, ""spec"":map[string]interface {}{""password"":""aGVsbG93b3JsZA=="", ""user"":""john""}}: validation failure list:
spec.password in body must be of type password: ""string""
```

Am I missing something? My understanding was any string I provide should now be treated as a password and not exposed in kubectl get and describe commands but it returns me an error saying it should be on type password. Can someone pls clarify?

**Environment**:
- Kubernetes version (use `kubectl version`):
`Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.8"", GitCommit:""211047e9a1922595eaa3a1127ed365e9299a6c23"", GitTreeState:""clean"", BuildDate:""2019-10-15T12:11:03Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""14+"", GitVersion:""v1.14.8-gke.17"", GitCommit:""188432a69210ca32cafded81b4dd1c063720cac0"", GitTreeState:""clean"", BuildDate:""2019-11-13T20:47:11Z"", GoVersion:""go1.12.11b4"", Compiler:""gc"", Platform:""linux/amd64""}`
- Cloud provider or hardware configuration: GKE
- OS (e.g: `cat /etc/os-release`):

",additionalPrinterColumns,https://github.com/kubernetes/kubernetes
51,85578.0,"I am trying to create a new CRD with a password field as shown below

```
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: tests.abc.com
spec:
  group: abc.com
  versions:
    - name: v1
      served: true
      storage: true
  scope: Cluster
  names:
    plural: tests
    singular: test
    kind: Test
  validation:
    openAPIV3Schema:
      type: object
      properties:
        spec:
          type: object
          properties:
            user:
              type: string
            password:
              type: password
  additionalPrinterColumns:
    - name: User
      type: string
      description: The user who created this object
      JSONPath: .spec.user
    - name: Age
      type: date
      JSONPath: .metadata.creationTimestamp
```
When I try to create a new object

```
apiVersion: abc.com/v1
kind: Test
metadata:
  name: test
spec:
  user: john
  password: aGVsbG93b3JsZA==

```

**What happened**:

When I try to do `kubectl apply -f test.yaml`

I get the below error

```
The Test ""test"" is invalid: []: Invalid value: map[string]interface {}{""apiVersion"":""abc.com/v1"", ""kind"":""Test"", ""metadata"":map[string]interface {}{""annotations"":map[string]interface {}{""kubectl.kubernetes.io/last-applied-configuration"":""{\""apiVersion\"":\""abc.com/v1\"",\""kind\"":\""Test\"",\""metadata\"":{\""annotations\"":{},\""name\"":\""test\""},\""spec\"":{\""password\"":\""MWYyZDFlMmU2N2Rm\"",\""user\"":\""john\""}}\n""}, ""creationTimestamp"":""2019-11-24T01:24:24Z"", ""generation"":1, ""name"":""test"", ""uid"":""267b100d-0e59-11ea-8341-42010a80018e""}, ""spec"":map[string]interface {}{""password"":""aGVsbG93b3JsZA=="", ""user"":""john""}}: validation failure list:
spec.password in body must be of type password: ""string""
```

Am I missing something? My understanding was any string I provide should now be treated as a password and not exposed in kubectl get and describe commands but it returns me an error saying it should be on type password. Can someone pls clarify?

**Environment**:
- Kubernetes version (use `kubectl version`):
`Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.8"", GitCommit:""211047e9a1922595eaa3a1127ed365e9299a6c23"", GitTreeState:""clean"", BuildDate:""2019-10-15T12:11:03Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""14+"", GitVersion:""v1.14.8-gke.17"", GitCommit:""188432a69210ca32cafded81b4dd1c063720cac0"", GitTreeState:""clean"", BuildDate:""2019-11-13T20:47:11Z"", GoVersion:""go1.12.11b4"", Compiler:""gc"", Platform:""linux/amd64""}`
- Cloud provider or hardware configuration: GKE
- OS (e.g: `cat /etc/os-release`):

",user\:\john\}}\n,https://github.com/kubernetes/kubernetes
52,85578.0,"I am trying to create a new CRD with a password field as shown below

```
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: tests.abc.com
spec:
  group: abc.com
  versions:
    - name: v1
      served: true
      storage: true
  scope: Cluster
  names:
    plural: tests
    singular: test
    kind: Test
  validation:
    openAPIV3Schema:
      type: object
      properties:
        spec:
          type: object
          properties:
            user:
              type: string
            password:
              type: password
  additionalPrinterColumns:
    - name: User
      type: string
      description: The user who created this object
      JSONPath: .spec.user
    - name: Age
      type: date
      JSONPath: .metadata.creationTimestamp
```
When I try to create a new object

```
apiVersion: abc.com/v1
kind: Test
metadata:
  name: test
spec:
  user: john
  password: aGVsbG93b3JsZA==

```

**What happened**:

When I try to do `kubectl apply -f test.yaml`

I get the below error

```
The Test ""test"" is invalid: []: Invalid value: map[string]interface {}{""apiVersion"":""abc.com/v1"", ""kind"":""Test"", ""metadata"":map[string]interface {}{""annotations"":map[string]interface {}{""kubectl.kubernetes.io/last-applied-configuration"":""{\""apiVersion\"":\""abc.com/v1\"",\""kind\"":\""Test\"",\""metadata\"":{\""annotations\"":{},\""name\"":\""test\""},\""spec\"":{\""password\"":\""MWYyZDFlMmU2N2Rm\"",\""user\"":\""john\""}}\n""}, ""creationTimestamp"":""2019-11-24T01:24:24Z"", ""generation"":1, ""name"":""test"", ""uid"":""267b100d-0e59-11ea-8341-42010a80018e""}, ""spec"":map[string]interface {}{""password"":""aGVsbG93b3JsZA=="", ""user"":""john""}}: validation failure list:
spec.password in body must be of type password: ""string""
```

Am I missing something? My understanding was any string I provide should now be treated as a password and not exposed in kubectl get and describe commands but it returns me an error saying it should be on type password. Can someone pls clarify?

**Environment**:
- Kubernetes version (use `kubectl version`):
`Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.8"", GitCommit:""211047e9a1922595eaa3a1127ed365e9299a6c23"", GitTreeState:""clean"", BuildDate:""2019-10-15T12:11:03Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""14+"", GitVersion:""v1.14.8-gke.17"", GitCommit:""188432a69210ca32cafded81b4dd1c063720cac0"", GitTreeState:""clean"", BuildDate:""2019-11-13T20:47:11Z"", GoVersion:""go1.12.11b4"", Compiler:""gc"", Platform:""linux/amd64""}`
- Cloud provider or hardware configuration: GKE
- OS (e.g: `cat /etc/os-release`):

",MWYyZDFlMmU2N2Rm,https://github.com/kubernetes/kubernetes
53,85578.0,"I am trying to create a new CRD with a password field as shown below

```
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: tests.abc.com
spec:
  group: abc.com
  versions:
    - name: v1
      served: true
      storage: true
  scope: Cluster
  names:
    plural: tests
    singular: test
    kind: Test
  validation:
    openAPIV3Schema:
      type: object
      properties:
        spec:
          type: object
          properties:
            user:
              type: string
            password:
              type: password
  additionalPrinterColumns:
    - name: User
      type: string
      description: The user who created this object
      JSONPath: .spec.user
    - name: Age
      type: date
      JSONPath: .metadata.creationTimestamp
```
When I try to create a new object

```
apiVersion: abc.com/v1
kind: Test
metadata:
  name: test
spec:
  user: john
  password: aGVsbG93b3JsZA==

```

**What happened**:

When I try to do `kubectl apply -f test.yaml`

I get the below error

```
The Test ""test"" is invalid: []: Invalid value: map[string]interface {}{""apiVersion"":""abc.com/v1"", ""kind"":""Test"", ""metadata"":map[string]interface {}{""annotations"":map[string]interface {}{""kubectl.kubernetes.io/last-applied-configuration"":""{\""apiVersion\"":\""abc.com/v1\"",\""kind\"":\""Test\"",\""metadata\"":{\""annotations\"":{},\""name\"":\""test\""},\""spec\"":{\""password\"":\""MWYyZDFlMmU2N2Rm\"",\""user\"":\""john\""}}\n""}, ""creationTimestamp"":""2019-11-24T01:24:24Z"", ""generation"":1, ""name"":""test"", ""uid"":""267b100d-0e59-11ea-8341-42010a80018e""}, ""spec"":map[string]interface {}{""password"":""aGVsbG93b3JsZA=="", ""user"":""john""}}: validation failure list:
spec.password in body must be of type password: ""string""
```

Am I missing something? My understanding was any string I provide should now be treated as a password and not exposed in kubectl get and describe commands but it returns me an error saying it should be on type password. Can someone pls clarify?

**Environment**:
- Kubernetes version (use `kubectl version`):
`Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.8"", GitCommit:""211047e9a1922595eaa3a1127ed365e9299a6c23"", GitTreeState:""clean"", BuildDate:""2019-10-15T12:11:03Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""14+"", GitVersion:""v1.14.8-gke.17"", GitCommit:""188432a69210ca32cafded81b4dd1c063720cac0"", GitTreeState:""clean"", BuildDate:""2019-11-13T20:47:11Z"", GoVersion:""go1.12.11b4"", Compiler:""gc"", Platform:""linux/amd64""}`
- Cloud provider or hardware configuration: GKE
- OS (e.g: `cat /etc/os-release`):

",describe,https://github.com/kubernetes/kubernetes
54,85578.0,"I am trying to create a new CRD with a password field as shown below

```
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: tests.abc.com
spec:
  group: abc.com
  versions:
    - name: v1
      served: true
      storage: true
  scope: Cluster
  names:
    plural: tests
    singular: test
    kind: Test
  validation:
    openAPIV3Schema:
      type: object
      properties:
        spec:
          type: object
          properties:
            user:
              type: string
            password:
              type: password
  additionalPrinterColumns:
    - name: User
      type: string
      description: The user who created this object
      JSONPath: .spec.user
    - name: Age
      type: date
      JSONPath: .metadata.creationTimestamp
```
When I try to create a new object

```
apiVersion: abc.com/v1
kind: Test
metadata:
  name: test
spec:
  user: john
  password: aGVsbG93b3JsZA==

```

**What happened**:

When I try to do `kubectl apply -f test.yaml`

I get the below error

```
The Test ""test"" is invalid: []: Invalid value: map[string]interface {}{""apiVersion"":""abc.com/v1"", ""kind"":""Test"", ""metadata"":map[string]interface {}{""annotations"":map[string]interface {}{""kubectl.kubernetes.io/last-applied-configuration"":""{\""apiVersion\"":\""abc.com/v1\"",\""kind\"":\""Test\"",\""metadata\"":{\""annotations\"":{},\""name\"":\""test\""},\""spec\"":{\""password\"":\""MWYyZDFlMmU2N2Rm\"",\""user\"":\""john\""}}\n""}, ""creationTimestamp"":""2019-11-24T01:24:24Z"", ""generation"":1, ""name"":""test"", ""uid"":""267b100d-0e59-11ea-8341-42010a80018e""}, ""spec"":map[string]interface {}{""password"":""aGVsbG93b3JsZA=="", ""user"":""john""}}: validation failure list:
spec.password in body must be of type password: ""string""
```

Am I missing something? My understanding was any string I provide should now be treated as a password and not exposed in kubectl get and describe commands but it returns me an error saying it should be on type password. Can someone pls clarify?

**Environment**:
- Kubernetes version (use `kubectl version`):
`Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.8"", GitCommit:""211047e9a1922595eaa3a1127ed365e9299a6c23"", GitTreeState:""clean"", BuildDate:""2019-10-15T12:11:03Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""14+"", GitVersion:""v1.14.8-gke.17"", GitCommit:""188432a69210ca32cafded81b4dd1c063720cac0"", GitTreeState:""clean"", BuildDate:""2019-11-13T20:47:11Z"", GoVersion:""go1.12.11b4"", Compiler:""gc"", Platform:""linux/amd64""}`
- Cloud provider or hardware configuration: GKE
- OS (e.g: `cat /etc/os-release`):

",validation,https://github.com/kubernetes/kubernetes
55,85578.0,"I am trying to create a new CRD with a password field as shown below

```
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: tests.abc.com
spec:
  group: abc.com
  versions:
    - name: v1
      served: true
      storage: true
  scope: Cluster
  names:
    plural: tests
    singular: test
    kind: Test
  validation:
    openAPIV3Schema:
      type: object
      properties:
        spec:
          type: object
          properties:
            user:
              type: string
            password:
              type: password
  additionalPrinterColumns:
    - name: User
      type: string
      description: The user who created this object
      JSONPath: .spec.user
    - name: Age
      type: date
      JSONPath: .metadata.creationTimestamp
```
When I try to create a new object

```
apiVersion: abc.com/v1
kind: Test
metadata:
  name: test
spec:
  user: john
  password: aGVsbG93b3JsZA==

```

**What happened**:

When I try to do `kubectl apply -f test.yaml`

I get the below error

```
The Test ""test"" is invalid: []: Invalid value: map[string]interface {}{""apiVersion"":""abc.com/v1"", ""kind"":""Test"", ""metadata"":map[string]interface {}{""annotations"":map[string]interface {}{""kubectl.kubernetes.io/last-applied-configuration"":""{\""apiVersion\"":\""abc.com/v1\"",\""kind\"":\""Test\"",\""metadata\"":{\""annotations\"":{},\""name\"":\""test\""},\""spec\"":{\""password\"":\""MWYyZDFlMmU2N2Rm\"",\""user\"":\""john\""}}\n""}, ""creationTimestamp"":""2019-11-24T01:24:24Z"", ""generation"":1, ""name"":""test"", ""uid"":""267b100d-0e59-11ea-8341-42010a80018e""}, ""spec"":map[string]interface {}{""password"":""aGVsbG93b3JsZA=="", ""user"":""john""}}: validation failure list:
spec.password in body must be of type password: ""string""
```

Am I missing something? My understanding was any string I provide should now be treated as a password and not exposed in kubectl get and describe commands but it returns me an error saying it should be on type password. Can someone pls clarify?

**Environment**:
- Kubernetes version (use `kubectl version`):
`Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.8"", GitCommit:""211047e9a1922595eaa3a1127ed365e9299a6c23"", GitTreeState:""clean"", BuildDate:""2019-10-15T12:11:03Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""14+"", GitVersion:""v1.14.8-gke.17"", GitCommit:""188432a69210ca32cafded81b4dd1c063720cac0"", GitTreeState:""clean"", BuildDate:""2019-11-13T20:47:11Z"", GoVersion:""go1.12.11b4"", Compiler:""gc"", Platform:""linux/amd64""}`
- Cloud provider or hardware configuration: GKE
- OS (e.g: `cat /etc/os-release`):

",something,https://github.com/kubernetes/kubernetes
56,85578.0,"I am trying to create a new CRD with a password field as shown below

```
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: tests.abc.com
spec:
  group: abc.com
  versions:
    - name: v1
      served: true
      storage: true
  scope: Cluster
  names:
    plural: tests
    singular: test
    kind: Test
  validation:
    openAPIV3Schema:
      type: object
      properties:
        spec:
          type: object
          properties:
            user:
              type: string
            password:
              type: password
  additionalPrinterColumns:
    - name: User
      type: string
      description: The user who created this object
      JSONPath: .spec.user
    - name: Age
      type: date
      JSONPath: .metadata.creationTimestamp
```
When I try to create a new object

```
apiVersion: abc.com/v1
kind: Test
metadata:
  name: test
spec:
  user: john
  password: aGVsbG93b3JsZA==

```

**What happened**:

When I try to do `kubectl apply -f test.yaml`

I get the below error

```
The Test ""test"" is invalid: []: Invalid value: map[string]interface {}{""apiVersion"":""abc.com/v1"", ""kind"":""Test"", ""metadata"":map[string]interface {}{""annotations"":map[string]interface {}{""kubectl.kubernetes.io/last-applied-configuration"":""{\""apiVersion\"":\""abc.com/v1\"",\""kind\"":\""Test\"",\""metadata\"":{\""annotations\"":{},\""name\"":\""test\""},\""spec\"":{\""password\"":\""MWYyZDFlMmU2N2Rm\"",\""user\"":\""john\""}}\n""}, ""creationTimestamp"":""2019-11-24T01:24:24Z"", ""generation"":1, ""name"":""test"", ""uid"":""267b100d-0e59-11ea-8341-42010a80018e""}, ""spec"":map[string]interface {}{""password"":""aGVsbG93b3JsZA=="", ""user"":""john""}}: validation failure list:
spec.password in body must be of type password: ""string""
```

Am I missing something? My understanding was any string I provide should now be treated as a password and not exposed in kubectl get and describe commands but it returns me an error saying it should be on type password. Can someone pls clarify?

**Environment**:
- Kubernetes version (use `kubectl version`):
`Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.8"", GitCommit:""211047e9a1922595eaa3a1127ed365e9299a6c23"", GitTreeState:""clean"", BuildDate:""2019-10-15T12:11:03Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""14+"", GitVersion:""v1.14.8-gke.17"", GitCommit:""188432a69210ca32cafded81b4dd1c063720cac0"", GitTreeState:""clean"", BuildDate:""2019-11-13T20:47:11Z"", GoVersion:""go1.12.11b4"", Compiler:""gc"", Platform:""linux/amd64""}`
- Cloud provider or hardware configuration: GKE
- OS (e.g: `cat /etc/os-release`):

",apiVersion,https://github.com/kubernetes/kubernetes
57,85578.0,"I am trying to create a new CRD with a password field as shown below

```
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: tests.abc.com
spec:
  group: abc.com
  versions:
    - name: v1
      served: true
      storage: true
  scope: Cluster
  names:
    plural: tests
    singular: test
    kind: Test
  validation:
    openAPIV3Schema:
      type: object
      properties:
        spec:
          type: object
          properties:
            user:
              type: string
            password:
              type: password
  additionalPrinterColumns:
    - name: User
      type: string
      description: The user who created this object
      JSONPath: .spec.user
    - name: Age
      type: date
      JSONPath: .metadata.creationTimestamp
```
When I try to create a new object

```
apiVersion: abc.com/v1
kind: Test
metadata:
  name: test
spec:
  user: john
  password: aGVsbG93b3JsZA==

```

**What happened**:

When I try to do `kubectl apply -f test.yaml`

I get the below error

```
The Test ""test"" is invalid: []: Invalid value: map[string]interface {}{""apiVersion"":""abc.com/v1"", ""kind"":""Test"", ""metadata"":map[string]interface {}{""annotations"":map[string]interface {}{""kubectl.kubernetes.io/last-applied-configuration"":""{\""apiVersion\"":\""abc.com/v1\"",\""kind\"":\""Test\"",\""metadata\"":{\""annotations\"":{},\""name\"":\""test\""},\""spec\"":{\""password\"":\""MWYyZDFlMmU2N2Rm\"",\""user\"":\""john\""}}\n""}, ""creationTimestamp"":""2019-11-24T01:24:24Z"", ""generation"":1, ""name"":""test"", ""uid"":""267b100d-0e59-11ea-8341-42010a80018e""}, ""spec"":map[string]interface {}{""password"":""aGVsbG93b3JsZA=="", ""user"":""john""}}: validation failure list:
spec.password in body must be of type password: ""string""
```

Am I missing something? My understanding was any string I provide should now be treated as a password and not exposed in kubectl get and describe commands but it returns me an error saying it should be on type password. Can someone pls clarify?

**Environment**:
- Kubernetes version (use `kubectl version`):
`Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.8"", GitCommit:""211047e9a1922595eaa3a1127ed365e9299a6c23"", GitTreeState:""clean"", BuildDate:""2019-10-15T12:11:03Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""14+"", GitVersion:""v1.14.8-gke.17"", GitCommit:""188432a69210ca32cafded81b4dd1c063720cac0"", GitTreeState:""clean"", BuildDate:""2019-11-13T20:47:11Z"", GoVersion:""go1.12.11b4"", Compiler:""gc"", Platform:""linux/amd64""}`
- Cloud provider or hardware configuration: GKE
- OS (e.g: `cat /etc/os-release`):

",Environment,https://github.com/kubernetes/kubernetes
58,85578.0,"I am trying to create a new CRD with a password field as shown below

```
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: tests.abc.com
spec:
  group: abc.com
  versions:
    - name: v1
      served: true
      storage: true
  scope: Cluster
  names:
    plural: tests
    singular: test
    kind: Test
  validation:
    openAPIV3Schema:
      type: object
      properties:
        spec:
          type: object
          properties:
            user:
              type: string
            password:
              type: password
  additionalPrinterColumns:
    - name: User
      type: string
      description: The user who created this object
      JSONPath: .spec.user
    - name: Age
      type: date
      JSONPath: .metadata.creationTimestamp
```
When I try to create a new object

```
apiVersion: abc.com/v1
kind: Test
metadata:
  name: test
spec:
  user: john
  password: aGVsbG93b3JsZA==

```

**What happened**:

When I try to do `kubectl apply -f test.yaml`

I get the below error

```
The Test ""test"" is invalid: []: Invalid value: map[string]interface {}{""apiVersion"":""abc.com/v1"", ""kind"":""Test"", ""metadata"":map[string]interface {}{""annotations"":map[string]interface {}{""kubectl.kubernetes.io/last-applied-configuration"":""{\""apiVersion\"":\""abc.com/v1\"",\""kind\"":\""Test\"",\""metadata\"":{\""annotations\"":{},\""name\"":\""test\""},\""spec\"":{\""password\"":\""MWYyZDFlMmU2N2Rm\"",\""user\"":\""john\""}}\n""}, ""creationTimestamp"":""2019-11-24T01:24:24Z"", ""generation"":1, ""name"":""test"", ""uid"":""267b100d-0e59-11ea-8341-42010a80018e""}, ""spec"":map[string]interface {}{""password"":""aGVsbG93b3JsZA=="", ""user"":""john""}}: validation failure list:
spec.password in body must be of type password: ""string""
```

Am I missing something? My understanding was any string I provide should now be treated as a password and not exposed in kubectl get and describe commands but it returns me an error saying it should be on type password. Can someone pls clarify?

**Environment**:
- Kubernetes version (use `kubectl version`):
`Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.8"", GitCommit:""211047e9a1922595eaa3a1127ed365e9299a6c23"", GitTreeState:""clean"", BuildDate:""2019-10-15T12:11:03Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""14+"", GitVersion:""v1.14.8-gke.17"", GitCommit:""188432a69210ca32cafded81b4dd1c063720cac0"", GitTreeState:""clean"", BuildDate:""2019-11-13T20:47:11Z"", GoVersion:""go1.12.11b4"", Compiler:""gc"", Platform:""linux/amd64""}`
- Cloud provider or hardware configuration: GKE
- OS (e.g: `cat /etc/os-release`):

",happened,https://github.com/kubernetes/kubernetes
59,80582.0,"I can't create (join) a new control plane node to a cluster using a JoinConfiguration file (with ControlPlane informations in it).

**What happened**:
I'm using this JoinConfiguration:
```yaml
apiVersion: kubeadm.k8s.io/v1beta2
kind: JoinConfiguration
discovery:
  bootstrapToken:
    apiServerEndpoint: ""10.0.0.1:6443""
    token: ""3a08jv.c0izixklcxtmnze7""
    unsafeSkipCAVerification: true
nodeRegistration:
  kubeletExtraArgs:
    cloud-provider: ""external""
ControlPlane:
  localAPIEndpoint:
    advertiseAddress: ""10.0.0.2""
    bindPort: 6443
  certificateKey: ""e6a2eb8381237ab72a4fa94f30285ec12a9694d750b9785706a83bfcbbbd2204""
```
and try to join the cluster with the command
```shell
kubeadm join --config=join-config.yaml

[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Downloading configuration for the kubelet from the ""kubelet-config-1.15"" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file ""/var/lib/kubelet/config.yaml""
[kubelet-start] Writing kubelet environment file with flags to file ""/var/lib/kubelet/kubeadm-flags.env""
[kubelet-start] Activating the kubelet service
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

```
**What you expected to happen**:
I expected to have a new **control plane** node configured but kubeadm only configure a **worker** node (not a control plane node)

**Environment**:
- Kubernetes version (use `kubectl version`): v1.15.0
- Cloud provider or hardware configuration: external (openstack)
- OS (e.g: `cat /etc/os-release`): Container Linux by CoreOS 2023.4.0 (Rhyolite)
- Kernel (e.g. `uname -a`): 4.19.23-coreos-r1
- Install tools: kubeadm 1.15.0

@kubernetes/sig/cluster-lifecycle-bugs",e6a2eb8381237ab72a4fa94f30285ec12a9694d750b9785706a83bfcbbbd2204,https://github.com/kubernetes/kubernetes
60,80582.0,"I can't create (join) a new control plane node to a cluster using a JoinConfiguration file (with ControlPlane informations in it).

**What happened**:
I'm using this JoinConfiguration:
```yaml
apiVersion: kubeadm.k8s.io/v1beta2
kind: JoinConfiguration
discovery:
  bootstrapToken:
    apiServerEndpoint: ""10.0.0.1:6443""
    token: ""3a08jv.c0izixklcxtmnze7""
    unsafeSkipCAVerification: true
nodeRegistration:
  kubeletExtraArgs:
    cloud-provider: ""external""
ControlPlane:
  localAPIEndpoint:
    advertiseAddress: ""10.0.0.2""
    bindPort: 6443
  certificateKey: ""e6a2eb8381237ab72a4fa94f30285ec12a9694d750b9785706a83bfcbbbd2204""
```
and try to join the cluster with the command
```shell
kubeadm join --config=join-config.yaml

[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Downloading configuration for the kubelet from the ""kubelet-config-1.15"" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file ""/var/lib/kubelet/config.yaml""
[kubelet-start] Writing kubelet environment file with flags to file ""/var/lib/kubelet/kubeadm-flags.env""
[kubelet-start] Activating the kubelet service
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

```
**What you expected to happen**:
I expected to have a new **control plane** node configured but kubeadm only configure a **worker** node (not a control plane node)

**Environment**:
- Kubernetes version (use `kubectl version`): v1.15.0
- Cloud provider or hardware configuration: external (openstack)
- OS (e.g: `cat /etc/os-release`): Container Linux by CoreOS 2023.4.0 (Rhyolite)
- Kernel (e.g. `uname -a`): 4.19.23-coreos-r1
- Install tools: kubeadm 1.15.0

@kubernetes/sig/cluster-lifecycle-bugs",nodeRegistration,https://github.com/kubernetes/kubernetes
61,80582.0,"I can't create (join) a new control plane node to a cluster using a JoinConfiguration file (with ControlPlane informations in it).

**What happened**:
I'm using this JoinConfiguration:
```yaml
apiVersion: kubeadm.k8s.io/v1beta2
kind: JoinConfiguration
discovery:
  bootstrapToken:
    apiServerEndpoint: ""10.0.0.1:6443""
    token: ""3a08jv.c0izixklcxtmnze7""
    unsafeSkipCAVerification: true
nodeRegistration:
  kubeletExtraArgs:
    cloud-provider: ""external""
ControlPlane:
  localAPIEndpoint:
    advertiseAddress: ""10.0.0.2""
    bindPort: 6443
  certificateKey: ""e6a2eb8381237ab72a4fa94f30285ec12a9694d750b9785706a83bfcbbbd2204""
```
and try to join the cluster with the command
```shell
kubeadm join --config=join-config.yaml

[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Downloading configuration for the kubelet from the ""kubelet-config-1.15"" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file ""/var/lib/kubelet/config.yaml""
[kubelet-start] Writing kubelet environment file with flags to file ""/var/lib/kubelet/kubeadm-flags.env""
[kubelet-start] Activating the kubelet service
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

```
**What you expected to happen**:
I expected to have a new **control plane** node configured but kubeadm only configure a **worker** node (not a control plane node)

**Environment**:
- Kubernetes version (use `kubectl version`): v1.15.0
- Cloud provider or hardware configuration: external (openstack)
- OS (e.g: `cat /etc/os-release`): Container Linux by CoreOS 2023.4.0 (Rhyolite)
- Kernel (e.g. `uname -a`): 4.19.23-coreos-r1
- Install tools: kubeadm 1.15.0

@kubernetes/sig/cluster-lifecycle-bugs",apiServerEndpoint,https://github.com/kubernetes/kubernetes
62,76667.0,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

```
kubectl get secrets personal-secret  -o yaml
Error from server: illegal base64 data at input byte 3
```

this is the output most of the times. It sometimes gives the correct output also.

**What you expected to happen**:
```
kubectl get secrets personal-secret  -o yaml
apiVersion: v1
data:
  password: c3Vwc2VyLXN0cm9uZy1wYXNz
  username: bXVuYWk=
kind: Secret
metadata:
  creationTimestamp: 2019-04-16T15:07:24Z
  name: personal-secret
  namespace: default
  resourceVersion: ""74250""
  selfLink: /api/v1/namespaces/default/secrets/personal-secret
  uid: 57288509-6059-11e9-affd-021b36e1a948
type: Opaque
```

**How to reproduce it (as minimally and precisely as possible)**:

For encryption of etcd I followed [docs](https://v1-12.docs.kubernetes.io/docs/tasks/administer-cluster/encrypt-data/)

Encryption Config with aescbc
```
EtcdEncryptionKey=$(head -c 32 /dev/urandom | base64)
```
```cat >> /etc/kubernetes/etcdEncryption/etcdEncryptionConfig.conf << EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
    - secrets
    providers:
    - aescbc:
        keys:
        - name: key1
          secret: ${EtcdEncryptionKey}
    - identity: {}
```

Create a secret
```
kubectl create secret generic personal-secret --from-literal=username=munai --from-literal=password=supser-strong-pass
```
Retrive the secret
```
kubectl get secrets personal-secret  -o yaml
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```./kubectl version
Client Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:24:45Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:14:49Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration:
AWS - EC2
- OS (e.g: `cat /etc/os-release`):
```
cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
```
- Kernel (e.g. `uname -a`):
```uname -a
Linux ip-10-13-0-207.us-west-2.compute.internal 3.10.0-957.10.1.el7.x86_64 #1 SMP Mon Mar 18 15:06:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools:
kubeadm
- Others:
",EtcdEncryptionKey,https://github.com/kubernetes/kubernetes
63,76667.0,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

```
kubectl get secrets personal-secret  -o yaml
Error from server: illegal base64 data at input byte 3
```

this is the output most of the times. It sometimes gives the correct output also.

**What you expected to happen**:
```
kubectl get secrets personal-secret  -o yaml
apiVersion: v1
data:
  password: c3Vwc2VyLXN0cm9uZy1wYXNz
  username: bXVuYWk=
kind: Secret
metadata:
  creationTimestamp: 2019-04-16T15:07:24Z
  name: personal-secret
  namespace: default
  resourceVersion: ""74250""
  selfLink: /api/v1/namespaces/default/secrets/personal-secret
  uid: 57288509-6059-11e9-affd-021b36e1a948
type: Opaque
```

**How to reproduce it (as minimally and precisely as possible)**:

For encryption of etcd I followed [docs](https://v1-12.docs.kubernetes.io/docs/tasks/administer-cluster/encrypt-data/)

Encryption Config with aescbc
```
EtcdEncryptionKey=$(head -c 32 /dev/urandom | base64)
```
```cat >> /etc/kubernetes/etcdEncryption/etcdEncryptionConfig.conf << EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
    - secrets
    providers:
    - aescbc:
        keys:
        - name: key1
          secret: ${EtcdEncryptionKey}
    - identity: {}
```

Create a secret
```
kubectl create secret generic personal-secret --from-literal=username=munai --from-literal=password=supser-strong-pass
```
Retrive the secret
```
kubectl get secrets personal-secret  -o yaml
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```./kubectl version
Client Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:24:45Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:14:49Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration:
AWS - EC2
- OS (e.g: `cat /etc/os-release`):
```
cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
```
- Kernel (e.g. `uname -a`):
```uname -a
Linux ip-10-13-0-207.us-west-2.compute.internal 3.10.0-957.10.1.el7.x86_64 #1 SMP Mon Mar 18 15:06:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools:
kubeadm
- Others:
",c3Vwc2VyLXN0cm9uZy1wYXNz,https://github.com/kubernetes/kubernetes
64,76667.0,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

```
kubectl get secrets personal-secret  -o yaml
Error from server: illegal base64 data at input byte 3
```

this is the output most of the times. It sometimes gives the correct output also.

**What you expected to happen**:
```
kubectl get secrets personal-secret  -o yaml
apiVersion: v1
data:
  password: c3Vwc2VyLXN0cm9uZy1wYXNz
  username: bXVuYWk=
kind: Secret
metadata:
  creationTimestamp: 2019-04-16T15:07:24Z
  name: personal-secret
  namespace: default
  resourceVersion: ""74250""
  selfLink: /api/v1/namespaces/default/secrets/personal-secret
  uid: 57288509-6059-11e9-affd-021b36e1a948
type: Opaque
```

**How to reproduce it (as minimally and precisely as possible)**:

For encryption of etcd I followed [docs](https://v1-12.docs.kubernetes.io/docs/tasks/administer-cluster/encrypt-data/)

Encryption Config with aescbc
```
EtcdEncryptionKey=$(head -c 32 /dev/urandom | base64)
```
```cat >> /etc/kubernetes/etcdEncryption/etcdEncryptionConfig.conf << EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
    - secrets
    providers:
    - aescbc:
        keys:
        - name: key1
          secret: ${EtcdEncryptionKey}
    - identity: {}
```

Create a secret
```
kubectl create secret generic personal-secret --from-literal=username=munai --from-literal=password=supser-strong-pass
```
Retrive the secret
```
kubectl get secrets personal-secret  -o yaml
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```./kubectl version
Client Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:24:45Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:14:49Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration:
AWS - EC2
- OS (e.g: `cat /etc/os-release`):
```
cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
```
- Kernel (e.g. `uname -a`):
```uname -a
Linux ip-10-13-0-207.us-west-2.compute.internal 3.10.0-957.10.1.el7.x86_64 #1 SMP Mon Mar 18 15:06:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools:
kubeadm
- Others:
",-04-16T15:07:24Z,https://github.com/kubernetes/kubernetes
65,76667.0,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

```
kubectl get secrets personal-secret  -o yaml
Error from server: illegal base64 data at input byte 3
```

this is the output most of the times. It sometimes gives the correct output also.

**What you expected to happen**:
```
kubectl get secrets personal-secret  -o yaml
apiVersion: v1
data:
  password: c3Vwc2VyLXN0cm9uZy1wYXNz
  username: bXVuYWk=
kind: Secret
metadata:
  creationTimestamp: 2019-04-16T15:07:24Z
  name: personal-secret
  namespace: default
  resourceVersion: ""74250""
  selfLink: /api/v1/namespaces/default/secrets/personal-secret
  uid: 57288509-6059-11e9-affd-021b36e1a948
type: Opaque
```

**How to reproduce it (as minimally and precisely as possible)**:

For encryption of etcd I followed [docs](https://v1-12.docs.kubernetes.io/docs/tasks/administer-cluster/encrypt-data/)

Encryption Config with aescbc
```
EtcdEncryptionKey=$(head -c 32 /dev/urandom | base64)
```
```cat >> /etc/kubernetes/etcdEncryption/etcdEncryptionConfig.conf << EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
    - secrets
    providers:
    - aescbc:
        keys:
        - name: key1
          secret: ${EtcdEncryptionKey}
    - identity: {}
```

Create a secret
```
kubectl create secret generic personal-secret --from-literal=username=munai --from-literal=password=supser-strong-pass
```
Retrive the secret
```
kubectl get secrets personal-secret  -o yaml
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```./kubectl version
Client Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:24:45Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:14:49Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration:
AWS - EC2
- OS (e.g: `cat /etc/os-release`):
```
cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
```
- Kernel (e.g. `uname -a`):
```uname -a
Linux ip-10-13-0-207.us-west-2.compute.internal 3.10.0-957.10.1.el7.x86_64 #1 SMP Mon Mar 18 15:06:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools:
kubeadm
- Others:
",supser-strong-pass,https://github.com/kubernetes/kubernetes
66,76667.0,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

```
kubectl get secrets personal-secret  -o yaml
Error from server: illegal base64 data at input byte 3
```

this is the output most of the times. It sometimes gives the correct output also.

**What you expected to happen**:
```
kubectl get secrets personal-secret  -o yaml
apiVersion: v1
data:
  password: c3Vwc2VyLXN0cm9uZy1wYXNz
  username: bXVuYWk=
kind: Secret
metadata:
  creationTimestamp: 2019-04-16T15:07:24Z
  name: personal-secret
  namespace: default
  resourceVersion: ""74250""
  selfLink: /api/v1/namespaces/default/secrets/personal-secret
  uid: 57288509-6059-11e9-affd-021b36e1a948
type: Opaque
```

**How to reproduce it (as minimally and precisely as possible)**:

For encryption of etcd I followed [docs](https://v1-12.docs.kubernetes.io/docs/tasks/administer-cluster/encrypt-data/)

Encryption Config with aescbc
```
EtcdEncryptionKey=$(head -c 32 /dev/urandom | base64)
```
```cat >> /etc/kubernetes/etcdEncryption/etcdEncryptionConfig.conf << EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
    - secrets
    providers:
    - aescbc:
        keys:
        - name: key1
          secret: ${EtcdEncryptionKey}
    - identity: {}
```

Create a secret
```
kubectl create secret generic personal-secret --from-literal=username=munai --from-literal=password=supser-strong-pass
```
Retrive the secret
```
kubectl get secrets personal-secret  -o yaml
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```./kubectl version
Client Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:24:45Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:14:49Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration:
AWS - EC2
- OS (e.g: `cat /etc/os-release`):
```
cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
```
- Kernel (e.g. `uname -a`):
```uname -a
Linux ip-10-13-0-207.us-west-2.compute.internal 3.10.0-957.10.1.el7.x86_64 #1 SMP Mon Mar 18 15:06:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools:
kubeadm
- Others:
",literal=username=munai,https://github.com/kubernetes/kubernetes
67,76667.0,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->


**What happened**:

```
kubectl get secrets personal-secret  -o yaml
Error from server: illegal base64 data at input byte 3
```

this is the output most of the times. It sometimes gives the correct output also.

**What you expected to happen**:
```
kubectl get secrets personal-secret  -o yaml
apiVersion: v1
data:
  password: c3Vwc2VyLXN0cm9uZy1wYXNz
  username: bXVuYWk=
kind: Secret
metadata:
  creationTimestamp: 2019-04-16T15:07:24Z
  name: personal-secret
  namespace: default
  resourceVersion: ""74250""
  selfLink: /api/v1/namespaces/default/secrets/personal-secret
  uid: 57288509-6059-11e9-affd-021b36e1a948
type: Opaque
```

**How to reproduce it (as minimally and precisely as possible)**:

For encryption of etcd I followed [docs](https://v1-12.docs.kubernetes.io/docs/tasks/administer-cluster/encrypt-data/)

Encryption Config with aescbc
```
EtcdEncryptionKey=$(head -c 32 /dev/urandom | base64)
```
```cat >> /etc/kubernetes/etcdEncryption/etcdEncryptionConfig.conf << EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
    - secrets
    providers:
    - aescbc:
        keys:
        - name: key1
          secret: ${EtcdEncryptionKey}
    - identity: {}
```

Create a secret
```
kubectl create secret generic personal-secret --from-literal=username=munai --from-literal=password=supser-strong-pass
```
Retrive the secret
```
kubectl get secrets personal-secret  -o yaml
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
```./kubectl version
Client Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:24:45Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.5"", GitCommit:""51dd616cdd25d6ee22c83a858773b607328a18ec"", GitTreeState:""clean"", BuildDate:""2019-01-16T18:14:49Z"", GoVersion:""go1.10.7"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration:
AWS - EC2
- OS (e.g: `cat /etc/os-release`):
```
cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""
```
- Kernel (e.g. `uname -a`):
```uname -a
Linux ip-10-13-0-207.us-west-2.compute.internal 3.10.0-957.10.1.el7.x86_64 #1 SMP Mon Mar 18 15:06:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
```
- Install tools:
kubeadm
- Others:
",bXVuYWk=,https://github.com/kubernetes/kubernetes
68,76010.0,"I set kubernetes cluster by kubeadm on my local ubuntu server. I tried to encrypt etcd by the same way as https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ , then it breaks `/registry/secrets/default/default-token-xxxxx`

here is my encryption configuration file.

```yaml
    kind: EncryptionConfiguration
    apiVersion: apiserver.config.k8s.io/v1
    resources:
      - resources:
        - secrets
        providers:
        - aescbc:
            keys:
            - name: key1
              secret: F3rax7e5B/gGbAR8LLLLbPAp+R4W8KGXKjXfhTp8bb0=
            - name: key2
              secret: yuat0/ntgNh8d4NJwkWWSCdi7/WCYJ7AGfjTSU1XKHE=
```

and after created it, I configured my kube-apiserver.yaml like below.

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=xx.x.xxx.xxx
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/path/to/my/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/path/to/my/etcd/ca.crt
    - --etcd-certfile=/path/to/my/apiserver-etcd-client.crt
    - --etcd-keyfile=/path/to/my/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --encryption-provider-config=/path/to/enc/encryption_conf.yaml
    - --insecure-port=0
    - --kubelet-client-certificate=/path/to/myapiserver-kubelet-client.crt
    - --kubelet-client-key=/path/to/my/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/path/to/my/front-proxy-client.crt
    - --proxy-client-key-file=/path/to/my/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/path/to/my/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-key-file=/path/to/my/sa.pub
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/path/to/my/apiserver.crt
    - --tls-private-key-file=/path/to/my/apiserver.key
    image: k8s.gcr.io/kube-apiserver:v1.14.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: xx.x.xxx.xxx
        path: /healthz
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 15
      timeoutSeconds: 15
    name: kube-apiserver
    resources:
      requests:
        cpu: 250m
    volumeMounts:
    - mountPath: /path/to/enc
      name: encryption-conf
      readOnly: true
  ...
  hostNetwork: true
  priorityClassName: system-cluster-critical
  volumes:
  - hostPath:
      path: /path/to/enc
      type: DirectoryOrCreate
    name: encryption-conf
 ...
status: {}
```

after restarting apiserver, `unable to transform key ""/registry/secrets/default/default-token-xxx"": no matching prefix found` appeared.

**environment**

- Ubuntu Server 18.04
- build cluster by using kubeadm(v1.14.0)
- `kubectl version`: v1.14.0
- cni:  Calico(v3.5)

/kind bug",R4W8KGXKjXfhTp8bb0,https://github.com/kubernetes/kubernetes
69,76010.0,"I set kubernetes cluster by kubeadm on my local ubuntu server. I tried to encrypt etcd by the same way as https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ , then it breaks `/registry/secrets/default/default-token-xxxxx`

here is my encryption configuration file.

```yaml
    kind: EncryptionConfiguration
    apiVersion: apiserver.config.k8s.io/v1
    resources:
      - resources:
        - secrets
        providers:
        - aescbc:
            keys:
            - name: key1
              secret: F3rax7e5B/gGbAR8LLLLbPAp+R4W8KGXKjXfhTp8bb0=
            - name: key2
              secret: yuat0/ntgNh8d4NJwkWWSCdi7/WCYJ7AGfjTSU1XKHE=
```

and after created it, I configured my kube-apiserver.yaml like below.

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=xx.x.xxx.xxx
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/path/to/my/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/path/to/my/etcd/ca.crt
    - --etcd-certfile=/path/to/my/apiserver-etcd-client.crt
    - --etcd-keyfile=/path/to/my/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --encryption-provider-config=/path/to/enc/encryption_conf.yaml
    - --insecure-port=0
    - --kubelet-client-certificate=/path/to/myapiserver-kubelet-client.crt
    - --kubelet-client-key=/path/to/my/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/path/to/my/front-proxy-client.crt
    - --proxy-client-key-file=/path/to/my/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/path/to/my/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-key-file=/path/to/my/sa.pub
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/path/to/my/apiserver.crt
    - --tls-private-key-file=/path/to/my/apiserver.key
    image: k8s.gcr.io/kube-apiserver:v1.14.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: xx.x.xxx.xxx
        path: /healthz
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 15
      timeoutSeconds: 15
    name: kube-apiserver
    resources:
      requests:
        cpu: 250m
    volumeMounts:
    - mountPath: /path/to/enc
      name: encryption-conf
      readOnly: true
  ...
  hostNetwork: true
  priorityClassName: system-cluster-critical
  volumes:
  - hostPath:
      path: /path/to/enc
      type: DirectoryOrCreate
    name: encryption-conf
 ...
status: {}
```

after restarting apiserver, `unable to transform key ""/registry/secrets/default/default-token-xxx"": no matching prefix found` appeared.

**environment**

- Ubuntu Server 18.04
- build cluster by using kubeadm(v1.14.0)
- `kubectl version`: v1.14.0
- cni:  Calico(v3.5)

/kind bug","types=InternalIP,ExternalIP,Hostname",https://github.com/kubernetes/kubernetes
70,76010.0,"I set kubernetes cluster by kubeadm on my local ubuntu server. I tried to encrypt etcd by the same way as https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ , then it breaks `/registry/secrets/default/default-token-xxxxx`

here is my encryption configuration file.

```yaml
    kind: EncryptionConfiguration
    apiVersion: apiserver.config.k8s.io/v1
    resources:
      - resources:
        - secrets
        providers:
        - aescbc:
            keys:
            - name: key1
              secret: F3rax7e5B/gGbAR8LLLLbPAp+R4W8KGXKjXfhTp8bb0=
            - name: key2
              secret: yuat0/ntgNh8d4NJwkWWSCdi7/WCYJ7AGfjTSU1XKHE=
```

and after created it, I configured my kube-apiserver.yaml like below.

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=xx.x.xxx.xxx
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/path/to/my/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/path/to/my/etcd/ca.crt
    - --etcd-certfile=/path/to/my/apiserver-etcd-client.crt
    - --etcd-keyfile=/path/to/my/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --encryption-provider-config=/path/to/enc/encryption_conf.yaml
    - --insecure-port=0
    - --kubelet-client-certificate=/path/to/myapiserver-kubelet-client.crt
    - --kubelet-client-key=/path/to/my/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/path/to/my/front-proxy-client.crt
    - --proxy-client-key-file=/path/to/my/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/path/to/my/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-key-file=/path/to/my/sa.pub
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/path/to/my/apiserver.crt
    - --tls-private-key-file=/path/to/my/apiserver.key
    image: k8s.gcr.io/kube-apiserver:v1.14.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: xx.x.xxx.xxx
        path: /healthz
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 15
      timeoutSeconds: 15
    name: kube-apiserver
    resources:
      requests:
        cpu: 250m
    volumeMounts:
    - mountPath: /path/to/enc
      name: encryption-conf
      readOnly: true
  ...
  hostNetwork: true
  priorityClassName: system-cluster-critical
  volumes:
  - hostPath:
      path: /path/to/enc
      type: DirectoryOrCreate
    name: encryption-conf
 ...
status: {}
```

after restarting apiserver, `unable to transform key ""/registry/secrets/default/default-token-xxx"": no matching prefix found` appeared.

**environment**

- Ubuntu Server 18.04
- build cluster by using kubeadm(v1.14.0)
- `kubectl version`: v1.14.0
- cni:  Calico(v3.5)

/kind bug",cluster-ip-range,https://github.com/kubernetes/kubernetes
71,76010.0,"I set kubernetes cluster by kubeadm on my local ubuntu server. I tried to encrypt etcd by the same way as https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ , then it breaks `/registry/secrets/default/default-token-xxxxx`

here is my encryption configuration file.

```yaml
    kind: EncryptionConfiguration
    apiVersion: apiserver.config.k8s.io/v1
    resources:
      - resources:
        - secrets
        providers:
        - aescbc:
            keys:
            - name: key1
              secret: F3rax7e5B/gGbAR8LLLLbPAp+R4W8KGXKjXfhTp8bb0=
            - name: key2
              secret: yuat0/ntgNh8d4NJwkWWSCdi7/WCYJ7AGfjTSU1XKHE=
```

and after created it, I configured my kube-apiserver.yaml like below.

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=xx.x.xxx.xxx
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/path/to/my/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/path/to/my/etcd/ca.crt
    - --etcd-certfile=/path/to/my/apiserver-etcd-client.crt
    - --etcd-keyfile=/path/to/my/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --encryption-provider-config=/path/to/enc/encryption_conf.yaml
    - --insecure-port=0
    - --kubelet-client-certificate=/path/to/myapiserver-kubelet-client.crt
    - --kubelet-client-key=/path/to/my/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/path/to/my/front-proxy-client.crt
    - --proxy-client-key-file=/path/to/my/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/path/to/my/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-key-file=/path/to/my/sa.pub
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/path/to/my/apiserver.crt
    - --tls-private-key-file=/path/to/my/apiserver.key
    image: k8s.gcr.io/kube-apiserver:v1.14.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: xx.x.xxx.xxx
        path: /healthz
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 15
      timeoutSeconds: 15
    name: kube-apiserver
    resources:
      requests:
        cpu: 250m
    volumeMounts:
    - mountPath: /path/to/enc
      name: encryption-conf
      readOnly: true
  ...
  hostNetwork: true
  priorityClassName: system-cluster-critical
  volumes:
  - hostPath:
      path: /path/to/enc
      type: DirectoryOrCreate
    name: encryption-conf
 ...
status: {}
```

after restarting apiserver, `unable to transform key ""/registry/secrets/default/default-token-xxx"": no matching prefix found` appeared.

**environment**

- Ubuntu Server 18.04
- build cluster by using kubeadm(v1.14.0)
- `kubectl version`: v1.14.0
- cni:  Calico(v3.5)

/kind bug",encryption-provider-config,https://github.com/kubernetes/kubernetes
72,76010.0,"I set kubernetes cluster by kubeadm on my local ubuntu server. I tried to encrypt etcd by the same way as https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ , then it breaks `/registry/secrets/default/default-token-xxxxx`

here is my encryption configuration file.

```yaml
    kind: EncryptionConfiguration
    apiVersion: apiserver.config.k8s.io/v1
    resources:
      - resources:
        - secrets
        providers:
        - aescbc:
            keys:
            - name: key1
              secret: F3rax7e5B/gGbAR8LLLLbPAp+R4W8KGXKjXfhTp8bb0=
            - name: key2
              secret: yuat0/ntgNh8d4NJwkWWSCdi7/WCYJ7AGfjTSU1XKHE=
```

and after created it, I configured my kube-apiserver.yaml like below.

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=xx.x.xxx.xxx
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/path/to/my/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/path/to/my/etcd/ca.crt
    - --etcd-certfile=/path/to/my/apiserver-etcd-client.crt
    - --etcd-keyfile=/path/to/my/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --encryption-provider-config=/path/to/enc/encryption_conf.yaml
    - --insecure-port=0
    - --kubelet-client-certificate=/path/to/myapiserver-kubelet-client.crt
    - --kubelet-client-key=/path/to/my/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/path/to/my/front-proxy-client.crt
    - --proxy-client-key-file=/path/to/my/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/path/to/my/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-key-file=/path/to/my/sa.pub
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/path/to/my/apiserver.crt
    - --tls-private-key-file=/path/to/my/apiserver.key
    image: k8s.gcr.io/kube-apiserver:v1.14.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: xx.x.xxx.xxx
        path: /healthz
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 15
      timeoutSeconds: 15
    name: kube-apiserver
    resources:
      requests:
        cpu: 250m
    volumeMounts:
    - mountPath: /path/to/enc
      name: encryption-conf
      readOnly: true
  ...
  hostNetwork: true
  priorityClassName: system-cluster-critical
  volumes:
  - hostPath:
      path: /path/to/enc
      type: DirectoryOrCreate
    name: encryption-conf
 ...
status: {}
```

after restarting apiserver, `unable to transform key ""/registry/secrets/default/default-token-xxx"": no matching prefix found` appeared.

**environment**

- Ubuntu Server 18.04
- build cluster by using kubeadm(v1.14.0)
- `kubectl version`: v1.14.0
- cni:  Calico(v3.5)

/kind bug",names=front-proxy-client,https://github.com/kubernetes/kubernetes
73,75633.0,"**What type of PR is this?**

/kind feature

**What this PR does / why we need it**:

Implemented --output option for 'kubeadm token list' to
show token info in parseable form.

The output includes all information needed for 'kubeadm join'
command. This should help other tools to better integrate with
kubeadm.

Example output:

```
root@kind-control-plane:/# kubeadm token list --output json
{
  ""publicKeyPins"": [
    ""sha256:5f50bc74bd59d2ad0063e7aa639a0c1161dbde0101cb499b33e9881f37b60d77""
  ],
  ""controlPlaneHostPort"": ""172.17.0.2:6443"",
  ""tokens"": [
    {
      ""token"": ""uvxdac.fq35fuyue3kd4gda"",
      ""expires"": ""2019-03-30T09:54:53Z"",
      ""usages"": [
        ""authentication"",
        ""signing""
      ],
      ""groups"": [
        ""system:bootstrappers:kubeadm:default-node-token""
      ]
    },
    {
      ""token"": ""ywmdha.ruymv2h6n9i9mdlj"",
      ""expires"": ""2019-03-30T09:54:51Z"",
      ""usages"": [
        ""authentication"",
        ""signing""
      ],
      ""groups"": [
        ""system:bootstrappers:kubeadm:default-node-token""
      ]
    }
  ]
}

root@kind-control-plane:/# kubeadm token list --output yaml
controlPlaneHostPort: 172.17.0.2:6443
publicKeyPins:
- sha256:5f50bc74bd59d2ad0063e7aa639a0c1161dbde0101cb499b33e9881f37b60d77
tokens:
- expires: ""2019-03-30T09:54:53Z""
  groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: uvxdac.fq35fuyue3kd4gda
  usages:
  - authentication
  - signing
- expires: ""2019-03-30T09:54:51Z""
  groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: ywmdha.ruymv2h6n9i9mdlj
  usages:
  - authentication
  - signing

```

**Does this PR introduce a user-facing change?**:
<!--
If no, just write ""NONE"" in the release-note block below.
If yes, a release note is required:
Enter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string ""action required"".
-->
```release-note
kubeadm: support optional YAML/JSON output when listing bootstrap tokens
```",-03-30T09:54:53Z,https://github.com/kubernetes/kubernetes
74,75633.0,"**What type of PR is this?**

/kind feature

**What this PR does / why we need it**:

Implemented --output option for 'kubeadm token list' to
show token info in parseable form.

The output includes all information needed for 'kubeadm join'
command. This should help other tools to better integrate with
kubeadm.

Example output:

```
root@kind-control-plane:/# kubeadm token list --output json
{
  ""publicKeyPins"": [
    ""sha256:5f50bc74bd59d2ad0063e7aa639a0c1161dbde0101cb499b33e9881f37b60d77""
  ],
  ""controlPlaneHostPort"": ""172.17.0.2:6443"",
  ""tokens"": [
    {
      ""token"": ""uvxdac.fq35fuyue3kd4gda"",
      ""expires"": ""2019-03-30T09:54:53Z"",
      ""usages"": [
        ""authentication"",
        ""signing""
      ],
      ""groups"": [
        ""system:bootstrappers:kubeadm:default-node-token""
      ]
    },
    {
      ""token"": ""ywmdha.ruymv2h6n9i9mdlj"",
      ""expires"": ""2019-03-30T09:54:51Z"",
      ""usages"": [
        ""authentication"",
        ""signing""
      ],
      ""groups"": [
        ""system:bootstrappers:kubeadm:default-node-token""
      ]
    }
  ]
}

root@kind-control-plane:/# kubeadm token list --output yaml
controlPlaneHostPort: 172.17.0.2:6443
publicKeyPins:
- sha256:5f50bc74bd59d2ad0063e7aa639a0c1161dbde0101cb499b33e9881f37b60d77
tokens:
- expires: ""2019-03-30T09:54:53Z""
  groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: uvxdac.fq35fuyue3kd4gda
  usages:
  - authentication
  - signing
- expires: ""2019-03-30T09:54:51Z""
  groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: ywmdha.ruymv2h6n9i9mdlj
  usages:
  - authentication
  - signing

```

**Does this PR introduce a user-facing change?**:
<!--
If no, just write ""NONE"" in the release-note block below.
If yes, a release note is required:
Enter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string ""action required"".
-->
```release-note
kubeadm: support optional YAML/JSON output when listing bootstrap tokens
```",controlPlaneHostPort,https://github.com/kubernetes/kubernetes
75,75633.0,"**What type of PR is this?**

/kind feature

**What this PR does / why we need it**:

Implemented --output option for 'kubeadm token list' to
show token info in parseable form.

The output includes all information needed for 'kubeadm join'
command. This should help other tools to better integrate with
kubeadm.

Example output:

```
root@kind-control-plane:/# kubeadm token list --output json
{
  ""publicKeyPins"": [
    ""sha256:5f50bc74bd59d2ad0063e7aa639a0c1161dbde0101cb499b33e9881f37b60d77""
  ],
  ""controlPlaneHostPort"": ""172.17.0.2:6443"",
  ""tokens"": [
    {
      ""token"": ""uvxdac.fq35fuyue3kd4gda"",
      ""expires"": ""2019-03-30T09:54:53Z"",
      ""usages"": [
        ""authentication"",
        ""signing""
      ],
      ""groups"": [
        ""system:bootstrappers:kubeadm:default-node-token""
      ]
    },
    {
      ""token"": ""ywmdha.ruymv2h6n9i9mdlj"",
      ""expires"": ""2019-03-30T09:54:51Z"",
      ""usages"": [
        ""authentication"",
        ""signing""
      ],
      ""groups"": [
        ""system:bootstrappers:kubeadm:default-node-token""
      ]
    }
  ]
}

root@kind-control-plane:/# kubeadm token list --output yaml
controlPlaneHostPort: 172.17.0.2:6443
publicKeyPins:
- sha256:5f50bc74bd59d2ad0063e7aa639a0c1161dbde0101cb499b33e9881f37b60d77
tokens:
- expires: ""2019-03-30T09:54:53Z""
  groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: uvxdac.fq35fuyue3kd4gda
  usages:
  - authentication
  - signing
- expires: ""2019-03-30T09:54:51Z""
  groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: ywmdha.ruymv2h6n9i9mdlj
  usages:
  - authentication
  - signing

```

**Does this PR introduce a user-facing change?**:
<!--
If no, just write ""NONE"" in the release-note block below.
If yes, a release note is required:
Enter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string ""action required"".
-->
```release-note
kubeadm: support optional YAML/JSON output when listing bootstrap tokens
```",-03-30T09:54:51Z,https://github.com/kubernetes/kubernetes
76,75633.0,"**What type of PR is this?**

/kind feature

**What this PR does / why we need it**:

Implemented --output option for 'kubeadm token list' to
show token info in parseable form.

The output includes all information needed for 'kubeadm join'
command. This should help other tools to better integrate with
kubeadm.

Example output:

```
root@kind-control-plane:/# kubeadm token list --output json
{
  ""publicKeyPins"": [
    ""sha256:5f50bc74bd59d2ad0063e7aa639a0c1161dbde0101cb499b33e9881f37b60d77""
  ],
  ""controlPlaneHostPort"": ""172.17.0.2:6443"",
  ""tokens"": [
    {
      ""token"": ""uvxdac.fq35fuyue3kd4gda"",
      ""expires"": ""2019-03-30T09:54:53Z"",
      ""usages"": [
        ""authentication"",
        ""signing""
      ],
      ""groups"": [
        ""system:bootstrappers:kubeadm:default-node-token""
      ]
    },
    {
      ""token"": ""ywmdha.ruymv2h6n9i9mdlj"",
      ""expires"": ""2019-03-30T09:54:51Z"",
      ""usages"": [
        ""authentication"",
        ""signing""
      ],
      ""groups"": [
        ""system:bootstrappers:kubeadm:default-node-token""
      ]
    }
  ]
}

root@kind-control-plane:/# kubeadm token list --output yaml
controlPlaneHostPort: 172.17.0.2:6443
publicKeyPins:
- sha256:5f50bc74bd59d2ad0063e7aa639a0c1161dbde0101cb499b33e9881f37b60d77
tokens:
- expires: ""2019-03-30T09:54:53Z""
  groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: uvxdac.fq35fuyue3kd4gda
  usages:
  - authentication
  - signing
- expires: ""2019-03-30T09:54:51Z""
  groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: ywmdha.ruymv2h6n9i9mdlj
  usages:
  - authentication
  - signing

```

**Does this PR introduce a user-facing change?**:
<!--
If no, just write ""NONE"" in the release-note block below.
If yes, a release note is required:
Enter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string ""action required"".
-->
```release-note
kubeadm: support optional YAML/JSON output when listing bootstrap tokens
```",kind-control-plane,https://github.com/kubernetes/kubernetes
77,75626.0,"Didn't find the right answer after trying Google

After running the cluster with kubeadm for a while, the node node often fails to log in remotely due to the system hang.

Later, the default resource reservation for kubelet was found to be none. Now I want to make a kubelet configuration update for an already running cluster.

The following is the configuration of my initial cluster.
```yaml
apiVersion: kubeadm.k8s.io/v1beta1
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.1.166
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s-master-1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: 1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: ""10.244.0.0/16""
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
pod-infra-container-image: registry.aliyuncs.com/google_containers/pause:3.1
maxPods: 90
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
featureGates:
  SupportIPVSProxyMode: true
mode: ipvs
```

After I updated the configuration.Got the following configuration
```yaml
apiVersion: kubeadm.k8s.io/v1beta1
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.1.166
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s-master-1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: 1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: ""10.244.0.0/16""
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
#pod-infra-container-image: registry.aliyuncs.com/google_containers/pause:3.1
#podInfraContainerImage: registry.aliyuncs.com/google_containers/pause:3.1
maxPods: 80
evictionHard:
  memory.available: ""1024Mi""
  nodefs.available: ""10%""
kubeReserved:
  cpu: ""500m""
  memory: ""1Gi""
systemReserved:
  cpu: ""1""
  memory: ""1Gi""
  ephemeral-storage: ""10Gi""
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
featureGates:
  SupportIPVSProxyMode: true
mode: ipvs
```

Then executed the cluster configuration update command
```bash
kubeadm config upload from-file --config kubeadm-kubelet-update.yaml
```


Viewing the cluster configuration has not changed
```bash
$ kubeadm config view
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: 10.244.0.0/16
  serviceSubnet: 10.96.0.0/12
scheduler: {}
```

```bash
$ kubectl get -n kube-system  configmaps kubelet-config-1.13 -o yaml
apiVersion: v1
data:
  kubelet: |
    address: 0.0.0.0
    apiVersion: kubelet.config.k8s.io/v1beta1
    authentication:
      anonymous:
        enabled: false
      webhook:
        cacheTTL: 2m0s
        enabled: true
      x509:
        clientCAFile: /etc/kubernetes/pki/ca.crt
    authorization:
      mode: Webhook
      webhook:
        cacheAuthorizedTTL: 5m0s
        cacheUnauthorizedTTL: 30s
    cgroupDriver: cgroupfs
    cgroupsPerQOS: true
    clusterDNS:
    - 10.96.0.10
    clusterDomain: cluster.local
    configMapAndSecretChangeDetectionStrategy: Watch
    containerLogMaxFiles: 5
    containerLogMaxSize: 10Mi
    contentType: application/vnd.kubernetes.protobuf
    cpuCFSQuota: true
    cpuCFSQuotaPeriod: 100ms
    cpuManagerPolicy: none
    cpuManagerReconcilePeriod: 10s
    enableControllerAttachDetach: true
    enableDebuggingHandlers: true
    enforceNodeAllocatable:
    - pods
    eventBurst: 10
    eventRecordQPS: 5
    evictionHard:
      imagefs.available: 15%
      memory.available: 100Mi
      nodefs.available: 10%
      nodefs.inodesFree: 5%
    evictionPressureTransitionPeriod: 5m0s
    failSwapOn: true
    fileCheckFrequency: 20s
    hairpinMode: promiscuous-bridge
    healthzBindAddress: 127.0.0.1
    healthzPort: 10248
    httpCheckFrequency: 20s
    imageGCHighThresholdPercent: 85
    imageGCLowThresholdPercent: 80
    imageMinimumGCAge: 2m0s
    iptablesDropBit: 15
    iptablesMasqueradeBit: 14
    kind: KubeletConfiguration
    kubeAPIBurst: 10
    kubeAPIQPS: 5
    makeIPTablesUtilChains: true
    maxOpenFiles: 1000000
    maxPods: 90
    nodeLeaseDurationSeconds: 40
    nodeStatusReportFrequency: 1m0s
    nodeStatusUpdateFrequency: 10s
    oomScoreAdj: -999
    podPidsLimit: -1
    port: 10250
    registryBurst: 10
    registryPullQPS: 5
    resolvConf: /etc/resolv.conf
    rotateCertificates: true
    runtimeRequestTimeout: 2m0s
    serializeImagePulls: true
    staticPodPath: /etc/kubernetes/manifests
    streamingConnectionIdleTimeout: 4h0m0s
    syncFrequency: 1m0s
    volumeStatsAggPeriod: 1m0s
kind: ConfigMap
metadata:
  creationTimestamp: ""2019-02-18T09:45:25Z""
  name: kubelet-config-1.13
  namespace: kube-system
  resourceVersion: ""153""
  selfLink: /api/v1/namespaces/kube-system/configmaps/kubelet-config-1.13
  uid: eb08bbd9-3361-11e9-8888-000c299021ee
```

How can I use kubeadm to update the configuration of a kubernetes cluster that is already running?


",streamingConnectionIdleTimeout,https://github.com/kubernetes/kubernetes
78,75626.0,"Didn't find the right answer after trying Google

After running the cluster with kubeadm for a while, the node node often fails to log in remotely due to the system hang.

Later, the default resource reservation for kubelet was found to be none. Now I want to make a kubelet configuration update for an already running cluster.

The following is the configuration of my initial cluster.
```yaml
apiVersion: kubeadm.k8s.io/v1beta1
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.1.166
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s-master-1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: 1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: ""10.244.0.0/16""
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
pod-infra-container-image: registry.aliyuncs.com/google_containers/pause:3.1
maxPods: 90
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
featureGates:
  SupportIPVSProxyMode: true
mode: ipvs
```

After I updated the configuration.Got the following configuration
```yaml
apiVersion: kubeadm.k8s.io/v1beta1
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.1.166
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s-master-1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: 1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: ""10.244.0.0/16""
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
#pod-infra-container-image: registry.aliyuncs.com/google_containers/pause:3.1
#podInfraContainerImage: registry.aliyuncs.com/google_containers/pause:3.1
maxPods: 80
evictionHard:
  memory.available: ""1024Mi""
  nodefs.available: ""10%""
kubeReserved:
  cpu: ""500m""
  memory: ""1Gi""
systemReserved:
  cpu: ""1""
  memory: ""1Gi""
  ephemeral-storage: ""10Gi""
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
featureGates:
  SupportIPVSProxyMode: true
mode: ipvs
```

Then executed the cluster configuration update command
```bash
kubeadm config upload from-file --config kubeadm-kubelet-update.yaml
```


Viewing the cluster configuration has not changed
```bash
$ kubeadm config view
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: 10.244.0.0/16
  serviceSubnet: 10.96.0.0/12
scheduler: {}
```

```bash
$ kubectl get -n kube-system  configmaps kubelet-config-1.13 -o yaml
apiVersion: v1
data:
  kubelet: |
    address: 0.0.0.0
    apiVersion: kubelet.config.k8s.io/v1beta1
    authentication:
      anonymous:
        enabled: false
      webhook:
        cacheTTL: 2m0s
        enabled: true
      x509:
        clientCAFile: /etc/kubernetes/pki/ca.crt
    authorization:
      mode: Webhook
      webhook:
        cacheAuthorizedTTL: 5m0s
        cacheUnauthorizedTTL: 30s
    cgroupDriver: cgroupfs
    cgroupsPerQOS: true
    clusterDNS:
    - 10.96.0.10
    clusterDomain: cluster.local
    configMapAndSecretChangeDetectionStrategy: Watch
    containerLogMaxFiles: 5
    containerLogMaxSize: 10Mi
    contentType: application/vnd.kubernetes.protobuf
    cpuCFSQuota: true
    cpuCFSQuotaPeriod: 100ms
    cpuManagerPolicy: none
    cpuManagerReconcilePeriod: 10s
    enableControllerAttachDetach: true
    enableDebuggingHandlers: true
    enforceNodeAllocatable:
    - pods
    eventBurst: 10
    eventRecordQPS: 5
    evictionHard:
      imagefs.available: 15%
      memory.available: 100Mi
      nodefs.available: 10%
      nodefs.inodesFree: 5%
    evictionPressureTransitionPeriod: 5m0s
    failSwapOn: true
    fileCheckFrequency: 20s
    hairpinMode: promiscuous-bridge
    healthzBindAddress: 127.0.0.1
    healthzPort: 10248
    httpCheckFrequency: 20s
    imageGCHighThresholdPercent: 85
    imageGCLowThresholdPercent: 80
    imageMinimumGCAge: 2m0s
    iptablesDropBit: 15
    iptablesMasqueradeBit: 14
    kind: KubeletConfiguration
    kubeAPIBurst: 10
    kubeAPIQPS: 5
    makeIPTablesUtilChains: true
    maxOpenFiles: 1000000
    maxPods: 90
    nodeLeaseDurationSeconds: 40
    nodeStatusReportFrequency: 1m0s
    nodeStatusUpdateFrequency: 10s
    oomScoreAdj: -999
    podPidsLimit: -1
    port: 10250
    registryBurst: 10
    registryPullQPS: 5
    resolvConf: /etc/resolv.conf
    rotateCertificates: true
    runtimeRequestTimeout: 2m0s
    serializeImagePulls: true
    staticPodPath: /etc/kubernetes/manifests
    streamingConnectionIdleTimeout: 4h0m0s
    syncFrequency: 1m0s
    volumeStatsAggPeriod: 1m0s
kind: ConfigMap
metadata:
  creationTimestamp: ""2019-02-18T09:45:25Z""
  name: kubelet-config-1.13
  namespace: kube-system
  resourceVersion: ""153""
  selfLink: /api/v1/namespaces/kube-system/configmaps/kubelet-config-1.13
  uid: eb08bbd9-3361-11e9-8888-000c299021ee
```

How can I use kubeadm to update the configuration of a kubernetes cluster that is already running?


",containerLogMaxFiles,https://github.com/kubernetes/kubernetes
79,75626.0,"Didn't find the right answer after trying Google

After running the cluster with kubeadm for a while, the node node often fails to log in remotely due to the system hang.

Later, the default resource reservation for kubelet was found to be none. Now I want to make a kubelet configuration update for an already running cluster.

The following is the configuration of my initial cluster.
```yaml
apiVersion: kubeadm.k8s.io/v1beta1
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.1.166
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s-master-1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: 1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: ""10.244.0.0/16""
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
pod-infra-container-image: registry.aliyuncs.com/google_containers/pause:3.1
maxPods: 90
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
featureGates:
  SupportIPVSProxyMode: true
mode: ipvs
```

After I updated the configuration.Got the following configuration
```yaml
apiVersion: kubeadm.k8s.io/v1beta1
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.1.166
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s-master-1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: 1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: ""10.244.0.0/16""
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
#pod-infra-container-image: registry.aliyuncs.com/google_containers/pause:3.1
#podInfraContainerImage: registry.aliyuncs.com/google_containers/pause:3.1
maxPods: 80
evictionHard:
  memory.available: ""1024Mi""
  nodefs.available: ""10%""
kubeReserved:
  cpu: ""500m""
  memory: ""1Gi""
systemReserved:
  cpu: ""1""
  memory: ""1Gi""
  ephemeral-storage: ""10Gi""
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
featureGates:
  SupportIPVSProxyMode: true
mode: ipvs
```

Then executed the cluster configuration update command
```bash
kubeadm config upload from-file --config kubeadm-kubelet-update.yaml
```


Viewing the cluster configuration has not changed
```bash
$ kubeadm config view
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: 10.244.0.0/16
  serviceSubnet: 10.96.0.0/12
scheduler: {}
```

```bash
$ kubectl get -n kube-system  configmaps kubelet-config-1.13 -o yaml
apiVersion: v1
data:
  kubelet: |
    address: 0.0.0.0
    apiVersion: kubelet.config.k8s.io/v1beta1
    authentication:
      anonymous:
        enabled: false
      webhook:
        cacheTTL: 2m0s
        enabled: true
      x509:
        clientCAFile: /etc/kubernetes/pki/ca.crt
    authorization:
      mode: Webhook
      webhook:
        cacheAuthorizedTTL: 5m0s
        cacheUnauthorizedTTL: 30s
    cgroupDriver: cgroupfs
    cgroupsPerQOS: true
    clusterDNS:
    - 10.96.0.10
    clusterDomain: cluster.local
    configMapAndSecretChangeDetectionStrategy: Watch
    containerLogMaxFiles: 5
    containerLogMaxSize: 10Mi
    contentType: application/vnd.kubernetes.protobuf
    cpuCFSQuota: true
    cpuCFSQuotaPeriod: 100ms
    cpuManagerPolicy: none
    cpuManagerReconcilePeriod: 10s
    enableControllerAttachDetach: true
    enableDebuggingHandlers: true
    enforceNodeAllocatable:
    - pods
    eventBurst: 10
    eventRecordQPS: 5
    evictionHard:
      imagefs.available: 15%
      memory.available: 100Mi
      nodefs.available: 10%
      nodefs.inodesFree: 5%
    evictionPressureTransitionPeriod: 5m0s
    failSwapOn: true
    fileCheckFrequency: 20s
    hairpinMode: promiscuous-bridge
    healthzBindAddress: 127.0.0.1
    healthzPort: 10248
    httpCheckFrequency: 20s
    imageGCHighThresholdPercent: 85
    imageGCLowThresholdPercent: 80
    imageMinimumGCAge: 2m0s
    iptablesDropBit: 15
    iptablesMasqueradeBit: 14
    kind: KubeletConfiguration
    kubeAPIBurst: 10
    kubeAPIQPS: 5
    makeIPTablesUtilChains: true
    maxOpenFiles: 1000000
    maxPods: 90
    nodeLeaseDurationSeconds: 40
    nodeStatusReportFrequency: 1m0s
    nodeStatusUpdateFrequency: 10s
    oomScoreAdj: -999
    podPidsLimit: -1
    port: 10250
    registryBurst: 10
    registryPullQPS: 5
    resolvConf: /etc/resolv.conf
    rotateCertificates: true
    runtimeRequestTimeout: 2m0s
    serializeImagePulls: true
    staticPodPath: /etc/kubernetes/manifests
    streamingConnectionIdleTimeout: 4h0m0s
    syncFrequency: 1m0s
    volumeStatsAggPeriod: 1m0s
kind: ConfigMap
metadata:
  creationTimestamp: ""2019-02-18T09:45:25Z""
  name: kubelet-config-1.13
  namespace: kube-system
  resourceVersion: ""153""
  selfLink: /api/v1/namespaces/kube-system/configmaps/kubelet-config-1.13
  uid: eb08bbd9-3361-11e9-8888-000c299021ee
```

How can I use kubeadm to update the configuration of a kubernetes cluster that is already running?


",kubeadm:default-node-token,https://github.com/kubernetes/kubernetes
80,75626.0,"Didn't find the right answer after trying Google

After running the cluster with kubeadm for a while, the node node often fails to log in remotely due to the system hang.

Later, the default resource reservation for kubelet was found to be none. Now I want to make a kubelet configuration update for an already running cluster.

The following is the configuration of my initial cluster.
```yaml
apiVersion: kubeadm.k8s.io/v1beta1
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.1.166
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s-master-1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: 1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: ""10.244.0.0/16""
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
pod-infra-container-image: registry.aliyuncs.com/google_containers/pause:3.1
maxPods: 90
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
featureGates:
  SupportIPVSProxyMode: true
mode: ipvs
```

After I updated the configuration.Got the following configuration
```yaml
apiVersion: kubeadm.k8s.io/v1beta1
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.1.166
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s-master-1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: 1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: ""10.244.0.0/16""
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
#pod-infra-container-image: registry.aliyuncs.com/google_containers/pause:3.1
#podInfraContainerImage: registry.aliyuncs.com/google_containers/pause:3.1
maxPods: 80
evictionHard:
  memory.available: ""1024Mi""
  nodefs.available: ""10%""
kubeReserved:
  cpu: ""500m""
  memory: ""1Gi""
systemReserved:
  cpu: ""1""
  memory: ""1Gi""
  ephemeral-storage: ""10Gi""
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
featureGates:
  SupportIPVSProxyMode: true
mode: ipvs
```

Then executed the cluster configuration update command
```bash
kubeadm config upload from-file --config kubeadm-kubelet-update.yaml
```


Viewing the cluster configuration has not changed
```bash
$ kubeadm config view
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: """"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.13.3
networking:
  dnsDomain: cluster.local
  podSubnet: 10.244.0.0/16
  serviceSubnet: 10.96.0.0/12
scheduler: {}
```

```bash
$ kubectl get -n kube-system  configmaps kubelet-config-1.13 -o yaml
apiVersion: v1
data:
  kubelet: |
    address: 0.0.0.0
    apiVersion: kubelet.config.k8s.io/v1beta1
    authentication:
      anonymous:
        enabled: false
      webhook:
        cacheTTL: 2m0s
        enabled: true
      x509:
        clientCAFile: /etc/kubernetes/pki/ca.crt
    authorization:
      mode: Webhook
      webhook:
        cacheAuthorizedTTL: 5m0s
        cacheUnauthorizedTTL: 30s
    cgroupDriver: cgroupfs
    cgroupsPerQOS: true
    clusterDNS:
    - 10.96.0.10
    clusterDomain: cluster.local
    configMapAndSecretChangeDetectionStrategy: Watch
    containerLogMaxFiles: 5
    containerLogMaxSize: 10Mi
    contentType: application/vnd.kubernetes.protobuf
    cpuCFSQuota: true
    cpuCFSQuotaPeriod: 100ms
    cpuManagerPolicy: none
    cpuManagerReconcilePeriod: 10s
    enableControllerAttachDetach: true
    enableDebuggingHandlers: true
    enforceNodeAllocatable:
    - pods
    eventBurst: 10
    eventRecordQPS: 5
    evictionHard:
      imagefs.available: 15%
      memory.available: 100Mi
      nodefs.available: 10%
      nodefs.inodesFree: 5%
    evictionPressureTransitionPeriod: 5m0s
    failSwapOn: true
    fileCheckFrequency: 20s
    hairpinMode: promiscuous-bridge
    healthzBindAddress: 127.0.0.1
    healthzPort: 10248
    httpCheckFrequency: 20s
    imageGCHighThresholdPercent: 85
    imageGCLowThresholdPercent: 80
    imageMinimumGCAge: 2m0s
    iptablesDropBit: 15
    iptablesMasqueradeBit: 14
    kind: KubeletConfiguration
    kubeAPIBurst: 10
    kubeAPIQPS: 5
    makeIPTablesUtilChains: true
    maxOpenFiles: 1000000
    maxPods: 90
    nodeLeaseDurationSeconds: 40
    nodeStatusReportFrequency: 1m0s
    nodeStatusUpdateFrequency: 10s
    oomScoreAdj: -999
    podPidsLimit: -1
    port: 10250
    registryBurst: 10
    registryPullQPS: 5
    resolvConf: /etc/resolv.conf
    rotateCertificates: true
    runtimeRequestTimeout: 2m0s
    serializeImagePulls: true
    staticPodPath: /etc/kubernetes/manifests
    streamingConnectionIdleTimeout: 4h0m0s
    syncFrequency: 1m0s
    volumeStatsAggPeriod: 1m0s
kind: ConfigMap
metadata:
  creationTimestamp: ""2019-02-18T09:45:25Z""
  name: kubelet-config-1.13
  namespace: kube-system
  resourceVersion: ""153""
  selfLink: /api/v1/namespaces/kube-system/configmaps/kubelet-config-1.13
  uid: eb08bbd9-3361-11e9-8888-000c299021ee
```

How can I use kubeadm to update the configuration of a kubernetes cluster that is already running?


",timeoutForControlPlane,https://github.com/kubernetes/kubernetes
81,69850.0,"Microsoft.AspNetCore.Session.SessionMiddleware[7]
      Error unprotecting the session cookie.
System.Security.Cryptography.CryptographicException: The key {d13b9581-1fee-45c2-ad1b-e89680402540} was not found in the key ring.
   at Microsoft.AspNetCore.DataProtection.KeyManagement.KeyRingBasedDataProtector.UnprotectCore(Byte[] protectedData, Boolean allowOperationsOnRevokedKeys, UnprotectStatus& status)
   at Microsoft.AspNetCore.DataProtection.KeyManagement.KeyRingBasedDataProtector.DangerousUnprotect(Byte[] protectedData, Boolean ignoreRevocationErrors, Boolean& requiresMigration, Boolean& wasRevoked)
   at Microsoft.AspNetCore.DataProtection.KeyManagement.KeyRingBasedDataProtector.Unprotect(Byte[] protectedData)
   at Microsoft.AspNetCore.Session.CookieProtection.Unprotect(IDataProtector protector, String protectedText, ILogger logger)",d13b9581-1fee-45c2-ad1b-e89680402540,https://github.com/kubernetes/kubernetes
82,69850.0,"Microsoft.AspNetCore.Session.SessionMiddleware[7]
      Error unprotecting the session cookie.
System.Security.Cryptography.CryptographicException: The key {d13b9581-1fee-45c2-ad1b-e89680402540} was not found in the key ring.
   at Microsoft.AspNetCore.DataProtection.KeyManagement.KeyRingBasedDataProtector.UnprotectCore(Byte[] protectedData, Boolean allowOperationsOnRevokedKeys, UnprotectStatus& status)
   at Microsoft.AspNetCore.DataProtection.KeyManagement.KeyRingBasedDataProtector.DangerousUnprotect(Byte[] protectedData, Boolean ignoreRevocationErrors, Boolean& requiresMigration, Boolean& wasRevoked)
   at Microsoft.AspNetCore.DataProtection.KeyManagement.KeyRingBasedDataProtector.Unprotect(Byte[] protectedData)
   at Microsoft.AspNetCore.Session.CookieProtection.Unprotect(IDataProtector protector, String protectedText, ILogger logger)",ad1b-e89680402540,https://github.com/kubernetes/kubernetes
83,69221.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If the matter is security related, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>
> /kind bug
> /kind feature


**What happened**:
Tried to join node to kube.

**What you expected to happen**:
Join cluster.

**How to reproduce it (as minimally and precisely as possible)**:
kubeadm join 172.17.15.14:6443 --token n31b8f.yquu66k6az5ktlo7 --discovery-token-ca-cert-hash sha256:9ee1d6eed7eba52cf3f425b2d0757fcf10148b5261...........

**Anything else we need to know?**:
[discovery] Trying to connect to API Server ""172.17.15.14:6443""
[discovery] Created cluster-info discovery client, requesting info from ""https://172.17.15.14:6443""
[discovery] Requesting info from ""https://172.17.15.14:6443"" again to validate TLS against the pinned public key
[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server ""172.17.15.14:6443""
[discovery] Successfully established connection with API Server ""172.17.15.14:6443""
[kubelet] Downloading configuration for the kubelet from the ""kubelet-config-1.12"" ConfigMap in the kube-system namespace
configmaps ""kubelet-config-1.12"" is forbidden: User ""system:bootstrap:n31b8f"" cannot get configmaps in the namespace ""kube-system""


**Environment**:
Bare metal

- Kubernetes version (use `kubectl version`):
1.12

- Cloud provider or hardware configuration:
NA

- OS (e.g. from /etc/os-release):
RHEL 7.5

- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",9ee1d6eed7eba52cf3f425b2d0757fcf10148b5261,https://github.com/kubernetes/kubernetes
84,68334.0,"**What this PR does / why we need it**:
fix kubeadm bug
**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #68333 
```
Is this a BUG REPORT or FEATURE REQUEST?:
/kind bug

What happened:
When I use kubeadm to create a cluster, the specified parameter ""listen-client-urls"" does not work.
kubeadm init --config=kube-config.yaml fail with connection refuse for get http://127.0.0.1:2379
I think the element 'listen-client-urls' is unrecognized if the customized value is not equal '127.0.0.1'
Here is my kube-config.yaml

apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
apiServerCertSANs:    
- 192.168.192.128
- kube-dev
api:
  advertiseAddress: 192.168.192.128
  bindPort: 6440
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 48h0m0s
  usages:
  - signing
  - authentication
etcd:
  local:
    extraArgs:
      listen-client-urls: https://192.168.192.128:2379
      advertise-client-urls: https://192.168.192.128:2379
      listen-peer-urls: https://192.168.192.128:2380
      initial-advertise-peer-urls: https://192.168.192.128:2380
      initial-cluster: kube-dev=https://192.168.192.128:2380
      initial-cluster-state: new
    serverCertSANs:
      - kube-dev
      - 192.168.192.128
    peerCertSANs:
      - kube-dev
      - 192.168.192.128
ServerExtraArgs:
  endpoint-reconciler-type: lease

networking:
  podSubnet: 192.168.0.0/16  
kubernetesVersion: v1.11.2 
featureGates:  
   CoreDNS: true

No matter how I set the ""listen-client-urls"" parameter in the kube-config.yaml ,
it is awlays be ""https://127.0.0.1:2379""

What you expected to happen:
The parameter ""listen-client-urls"" can work!
How to reproduce it (as minimally and precisely as possible):

Anything else we need to know?:

Environment:

Kubernetes version (use kubectl version): v1.11.2
Cloud provider or hardware configuration:
OS (e.g. from /etc/os-release): CentOS Linux release 7.5.1804 (Core)
Kernel (e.g. uname -a): Linux kube-dev 3.10.0-862.el7.x86_64 #1 SMP Fri Apr 20 16:44:24 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
Install tools:
Others:
```
**Special notes for your reviewer**:

**Release note**:
```
kubeadm: if defined, pass the local etcd value of `listen-client-urls` as `etcd-servers` argument to kube-apiserver.
```
",listen-client-urls,https://github.com/kubernetes/kubernetes
85,68334.0,"**What this PR does / why we need it**:
fix kubeadm bug
**Which issue(s) this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close the issue(s) when PR gets merged)*:
Fixes #68333 
```
Is this a BUG REPORT or FEATURE REQUEST?:
/kind bug

What happened:
When I use kubeadm to create a cluster, the specified parameter ""listen-client-urls"" does not work.
kubeadm init --config=kube-config.yaml fail with connection refuse for get http://127.0.0.1:2379
I think the element 'listen-client-urls' is unrecognized if the customized value is not equal '127.0.0.1'
Here is my kube-config.yaml

apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
apiServerCertSANs:    
- 192.168.192.128
- kube-dev
api:
  advertiseAddress: 192.168.192.128
  bindPort: 6440
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 48h0m0s
  usages:
  - signing
  - authentication
etcd:
  local:
    extraArgs:
      listen-client-urls: https://192.168.192.128:2379
      advertise-client-urls: https://192.168.192.128:2379
      listen-peer-urls: https://192.168.192.128:2380
      initial-advertise-peer-urls: https://192.168.192.128:2380
      initial-cluster: kube-dev=https://192.168.192.128:2380
      initial-cluster-state: new
    serverCertSANs:
      - kube-dev
      - 192.168.192.128
    peerCertSANs:
      - kube-dev
      - 192.168.192.128
ServerExtraArgs:
  endpoint-reconciler-type: lease

networking:
  podSubnet: 192.168.0.0/16  
kubernetesVersion: v1.11.2 
featureGates:  
   CoreDNS: true

No matter how I set the ""listen-client-urls"" parameter in the kube-config.yaml ,
it is awlays be ""https://127.0.0.1:2379""

What you expected to happen:
The parameter ""listen-client-urls"" can work!
How to reproduce it (as minimally and precisely as possible):

Anything else we need to know?:

Environment:

Kubernetes version (use kubectl version): v1.11.2
Cloud provider or hardware configuration:
OS (e.g. from /etc/os-release): CentOS Linux release 7.5.1804 (Core)
Kernel (e.g. uname -a): Linux kube-dev 3.10.0-862.el7.x86_64 #1 SMP Fri Apr 20 16:44:24 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
Install tools:
Others:
```
**Special notes for your reviewer**:

**Release note**:
```
kubeadm: if defined, pass the local etcd value of `listen-client-urls` as `etcd-servers` argument to kube-apiserver.
```
",kubeadm:default-node-token,https://github.com/kubernetes/kubernetes
86,65695.0,"# Pod restart multiple times

I have a question...

- Kubernetes version: 1.10.4

### Question 1:

- Environment variables: 
- key: POD_XXX_KEY value: helloworld

my application start code:

```go
if key := os.Getenv(""POD_XXX_KEY""); key == """" {
	panic(""POD_XXX_KEY not found."")
}
fmt.println(""biubiuibu..."")
```

so.. Then pod at start
0 start: POD_XXX_KEY is null, panic POD_XXX_KEY not found.
1 start: POD_XXX_KEY is null, panic POD_XXX_KEY not found.
2 start: POD_XXX_KEY is 282ecd65-222f-e22c1aa8986b

deployment yaml:

```
env:
- name: POD_XXX_KEY
  valueFrom:
    configMapKeyRef:
      name: myservice
      key: hellokey
```

configmap yaml:

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: myservice
data:
  hellokey: 282ecd65-222f-e22c1aa8986b
```

No problem at 1.10.0 But 1.10.4 has the question.

What is the reason?

![](https://ofbudvg4c.qnssl.com/images/k8s/WechatIMG2274.jpeg)",282ecd65-222f-e22c1aa8986b,https://github.com/kubernetes/kubernetes
87,65695.0,"# Pod restart multiple times

I have a question...

- Kubernetes version: 1.10.4

### Question 1:

- Environment variables: 
- key: POD_XXX_KEY value: helloworld

my application start code:

```go
if key := os.Getenv(""POD_XXX_KEY""); key == """" {
	panic(""POD_XXX_KEY not found."")
}
fmt.println(""biubiuibu..."")
```

so.. Then pod at start
0 start: POD_XXX_KEY is null, panic POD_XXX_KEY not found.
1 start: POD_XXX_KEY is null, panic POD_XXX_KEY not found.
2 start: POD_XXX_KEY is 282ecd65-222f-e22c1aa8986b

deployment yaml:

```
env:
- name: POD_XXX_KEY
  valueFrom:
    configMapKeyRef:
      name: myservice
      key: hellokey
```

configmap yaml:

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: myservice
data:
  hellokey: 282ecd65-222f-e22c1aa8986b
```

No problem at 1.10.0 But 1.10.4 has the question.

What is the reason?

![](https://ofbudvg4c.qnssl.com/images/k8s/WechatIMG2274.jpeg)",panic(POD_XXX_KEY,https://github.com/kubernetes/kubernetes
88,65695.0,"# Pod restart multiple times

I have a question...

- Kubernetes version: 1.10.4

### Question 1:

- Environment variables: 
- key: POD_XXX_KEY value: helloworld

my application start code:

```go
if key := os.Getenv(""POD_XXX_KEY""); key == """" {
	panic(""POD_XXX_KEY not found."")
}
fmt.println(""biubiuibu..."")
```

so.. Then pod at start
0 start: POD_XXX_KEY is null, panic POD_XXX_KEY not found.
1 start: POD_XXX_KEY is null, panic POD_XXX_KEY not found.
2 start: POD_XXX_KEY is 282ecd65-222f-e22c1aa8986b

deployment yaml:

```
env:
- name: POD_XXX_KEY
  valueFrom:
    configMapKeyRef:
      name: myservice
      key: hellokey
```

configmap yaml:

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: myservice
data:
  hellokey: 282ecd65-222f-e22c1aa8986b
```

No problem at 1.10.0 But 1.10.4 has the question.

What is the reason?

![](https://ofbudvg4c.qnssl.com/images/k8s/WechatIMG2274.jpeg)",222f-e22c1aa8986b,https://github.com/kubernetes/kubernetes
89,64802.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If the matter is security related, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>
/kind bug
> /kind feature


**What happened**:
In low memory, crucial pods are candidates for eviction.

**What you expected to happen**:
Evict only non-critical pods

**How to reproduce it (as minimally and precisely as possible)**:
1. minikube start  --extra-config=kubelet.eviction-hard=""memory.available<500Mi""
2. create a pod (I used [memory-demo](https://github.com/kubernetes/website/blob/master/content/en/docs/tasks/configure-pod-container/memory-request-limit.yaml))
3. see a critical pod will be evicted (in my case, kube-apiserver-minikube_kube-system)

**Anything else we need to know?**:

`minikube logs` snippet:
```
Jun 06 00:34:08 minikube kubelet[3594]: I0606 00:34:08.130867    3594 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume ""default-token-bvkxc"" (UniqueName: ""kubernetes.io/secret/52e5e2af-6921-11e8-8ba7-080027da9d03-default-token-bvkxc"") pod ""memory-demo-787bcd6d76-nw7rz"" (UID: ""52e5e2af-6921-11e8-8ba7-080027da9d03"")
Jun 06 00:34:19 minikube kubelet[3594]: W0606 00:34:19.515478    3594 eviction_manager.go:343] eviction manager: attempting to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515701    3594 eviction_manager.go:357] eviction manager: must evict pod(s) to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515869    3594 eviction_manager.go:375] eviction manager: pods ranked for eviction: kube-apiserver-minikube_kube-system(9e218749db575502dc2153f1dcfcd8f5), kube-controller-manager-minikube_kube-system(af5fba67ac3471c82c00451a6f0db9a5), storage-provisioner_kube-system(1a69c6d7-6921-11e8-8ba7-080027da9d03), etcd-minikube_kube-system(8904a21094f73743ab3c7734da699963), kube-proxy-988jf_kube-system(18edc583-6921-11e8-8ba7-080027da9d03), kube-scheduler-minikube_kube-system(31cf0ccbee286239d451edb6fb511513), kubernetes-dashboard-5498ccf677-vgcw2_kube-system(19af187b-6921-11e8-8ba7-080027da9d03), influxdb-grafana-9sxtq_kube-system(19c5427a-6921-11e8-8ba7-080027da9d03), heapster-z8wfw_kube-system(19ba41de-6921-11e8-8ba7-080027da9d03), kube-addon-manager-minikube_kube-system(3afaf06535cc3b85be93c31632b765da), memory-demo-787bcd6d76-nw7rz_default(52e5e2af-6921-11e8-8ba7-080027da9d03), kube-dns-86f4d74b45-bczhr_kube-system(18e7ed0d-6921-11e8-8ba7-080027da9d03)
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550634    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to watch *v1.Service: Get https://192.168.99.100:8443/api/v1/services?resourceVersion=367&timeoutSeconds=385&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550700    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:460: Failed to watch *v1.Node: Get https://192.168.99.100:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=606&timeoutSeconds=362&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
```

May relate to other similar issues (ex. https://github.com/kubernetes/kubernetes/issues/53659 https://github.com/kubernetes/kubernetes/issues/63005)

**Environment**:
- Kubernetes version (use `kubectl version`): v1.10.0
- Cloud provider or hardware configuration: minikube v0.27.0
",19af187b-6921-11e8-8ba7-080027da9d03,https://github.com/kubernetes/kubernetes
90,64802.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If the matter is security related, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>
/kind bug
> /kind feature


**What happened**:
In low memory, crucial pods are candidates for eviction.

**What you expected to happen**:
Evict only non-critical pods

**How to reproduce it (as minimally and precisely as possible)**:
1. minikube start  --extra-config=kubelet.eviction-hard=""memory.available<500Mi""
2. create a pod (I used [memory-demo](https://github.com/kubernetes/website/blob/master/content/en/docs/tasks/configure-pod-container/memory-request-limit.yaml))
3. see a critical pod will be evicted (in my case, kube-apiserver-minikube_kube-system)

**Anything else we need to know?**:

`minikube logs` snippet:
```
Jun 06 00:34:08 minikube kubelet[3594]: I0606 00:34:08.130867    3594 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume ""default-token-bvkxc"" (UniqueName: ""kubernetes.io/secret/52e5e2af-6921-11e8-8ba7-080027da9d03-default-token-bvkxc"") pod ""memory-demo-787bcd6d76-nw7rz"" (UID: ""52e5e2af-6921-11e8-8ba7-080027da9d03"")
Jun 06 00:34:19 minikube kubelet[3594]: W0606 00:34:19.515478    3594 eviction_manager.go:343] eviction manager: attempting to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515701    3594 eviction_manager.go:357] eviction manager: must evict pod(s) to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515869    3594 eviction_manager.go:375] eviction manager: pods ranked for eviction: kube-apiserver-minikube_kube-system(9e218749db575502dc2153f1dcfcd8f5), kube-controller-manager-minikube_kube-system(af5fba67ac3471c82c00451a6f0db9a5), storage-provisioner_kube-system(1a69c6d7-6921-11e8-8ba7-080027da9d03), etcd-minikube_kube-system(8904a21094f73743ab3c7734da699963), kube-proxy-988jf_kube-system(18edc583-6921-11e8-8ba7-080027da9d03), kube-scheduler-minikube_kube-system(31cf0ccbee286239d451edb6fb511513), kubernetes-dashboard-5498ccf677-vgcw2_kube-system(19af187b-6921-11e8-8ba7-080027da9d03), influxdb-grafana-9sxtq_kube-system(19c5427a-6921-11e8-8ba7-080027da9d03), heapster-z8wfw_kube-system(19ba41de-6921-11e8-8ba7-080027da9d03), kube-addon-manager-minikube_kube-system(3afaf06535cc3b85be93c31632b765da), memory-demo-787bcd6d76-nw7rz_default(52e5e2af-6921-11e8-8ba7-080027da9d03), kube-dns-86f4d74b45-bczhr_kube-system(18e7ed0d-6921-11e8-8ba7-080027da9d03)
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550634    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to watch *v1.Service: Get https://192.168.99.100:8443/api/v1/services?resourceVersion=367&timeoutSeconds=385&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550700    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:460: Failed to watch *v1.Node: Get https://192.168.99.100:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=606&timeoutSeconds=362&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
```

May relate to other similar issues (ex. https://github.com/kubernetes/kubernetes/issues/53659 https://github.com/kubernetes/kubernetes/issues/63005)

**Environment**:
- Kubernetes version (use `kubectl version`): v1.10.0
- Cloud provider or hardware configuration: minikube v0.27.0
",18e7ed0d-6921-11e8-8ba7-080027da9d03,https://github.com/kubernetes/kubernetes
91,64802.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If the matter is security related, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>
/kind bug
> /kind feature


**What happened**:
In low memory, crucial pods are candidates for eviction.

**What you expected to happen**:
Evict only non-critical pods

**How to reproduce it (as minimally and precisely as possible)**:
1. minikube start  --extra-config=kubelet.eviction-hard=""memory.available<500Mi""
2. create a pod (I used [memory-demo](https://github.com/kubernetes/website/blob/master/content/en/docs/tasks/configure-pod-container/memory-request-limit.yaml))
3. see a critical pod will be evicted (in my case, kube-apiserver-minikube_kube-system)

**Anything else we need to know?**:

`minikube logs` snippet:
```
Jun 06 00:34:08 minikube kubelet[3594]: I0606 00:34:08.130867    3594 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume ""default-token-bvkxc"" (UniqueName: ""kubernetes.io/secret/52e5e2af-6921-11e8-8ba7-080027da9d03-default-token-bvkxc"") pod ""memory-demo-787bcd6d76-nw7rz"" (UID: ""52e5e2af-6921-11e8-8ba7-080027da9d03"")
Jun 06 00:34:19 minikube kubelet[3594]: W0606 00:34:19.515478    3594 eviction_manager.go:343] eviction manager: attempting to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515701    3594 eviction_manager.go:357] eviction manager: must evict pod(s) to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515869    3594 eviction_manager.go:375] eviction manager: pods ranked for eviction: kube-apiserver-minikube_kube-system(9e218749db575502dc2153f1dcfcd8f5), kube-controller-manager-minikube_kube-system(af5fba67ac3471c82c00451a6f0db9a5), storage-provisioner_kube-system(1a69c6d7-6921-11e8-8ba7-080027da9d03), etcd-minikube_kube-system(8904a21094f73743ab3c7734da699963), kube-proxy-988jf_kube-system(18edc583-6921-11e8-8ba7-080027da9d03), kube-scheduler-minikube_kube-system(31cf0ccbee286239d451edb6fb511513), kubernetes-dashboard-5498ccf677-vgcw2_kube-system(19af187b-6921-11e8-8ba7-080027da9d03), influxdb-grafana-9sxtq_kube-system(19c5427a-6921-11e8-8ba7-080027da9d03), heapster-z8wfw_kube-system(19ba41de-6921-11e8-8ba7-080027da9d03), kube-addon-manager-minikube_kube-system(3afaf06535cc3b85be93c31632b765da), memory-demo-787bcd6d76-nw7rz_default(52e5e2af-6921-11e8-8ba7-080027da9d03), kube-dns-86f4d74b45-bczhr_kube-system(18e7ed0d-6921-11e8-8ba7-080027da9d03)
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550634    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to watch *v1.Service: Get https://192.168.99.100:8443/api/v1/services?resourceVersion=367&timeoutSeconds=385&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550700    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:460: Failed to watch *v1.Node: Get https://192.168.99.100:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=606&timeoutSeconds=362&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
```

May relate to other similar issues (ex. https://github.com/kubernetes/kubernetes/issues/53659 https://github.com/kubernetes/kubernetes/issues/63005)

**Environment**:
- Kubernetes version (use `kubectl version`): v1.10.0
- Cloud provider or hardware configuration: minikube v0.27.0
",19ba41de-6921-11e8-8ba7-080027da9d03,https://github.com/kubernetes/kubernetes
92,64802.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If the matter is security related, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>
/kind bug
> /kind feature


**What happened**:
In low memory, crucial pods are candidates for eviction.

**What you expected to happen**:
Evict only non-critical pods

**How to reproduce it (as minimally and precisely as possible)**:
1. minikube start  --extra-config=kubelet.eviction-hard=""memory.available<500Mi""
2. create a pod (I used [memory-demo](https://github.com/kubernetes/website/blob/master/content/en/docs/tasks/configure-pod-container/memory-request-limit.yaml))
3. see a critical pod will be evicted (in my case, kube-apiserver-minikube_kube-system)

**Anything else we need to know?**:

`minikube logs` snippet:
```
Jun 06 00:34:08 minikube kubelet[3594]: I0606 00:34:08.130867    3594 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume ""default-token-bvkxc"" (UniqueName: ""kubernetes.io/secret/52e5e2af-6921-11e8-8ba7-080027da9d03-default-token-bvkxc"") pod ""memory-demo-787bcd6d76-nw7rz"" (UID: ""52e5e2af-6921-11e8-8ba7-080027da9d03"")
Jun 06 00:34:19 minikube kubelet[3594]: W0606 00:34:19.515478    3594 eviction_manager.go:343] eviction manager: attempting to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515701    3594 eviction_manager.go:357] eviction manager: must evict pod(s) to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515869    3594 eviction_manager.go:375] eviction manager: pods ranked for eviction: kube-apiserver-minikube_kube-system(9e218749db575502dc2153f1dcfcd8f5), kube-controller-manager-minikube_kube-system(af5fba67ac3471c82c00451a6f0db9a5), storage-provisioner_kube-system(1a69c6d7-6921-11e8-8ba7-080027da9d03), etcd-minikube_kube-system(8904a21094f73743ab3c7734da699963), kube-proxy-988jf_kube-system(18edc583-6921-11e8-8ba7-080027da9d03), kube-scheduler-minikube_kube-system(31cf0ccbee286239d451edb6fb511513), kubernetes-dashboard-5498ccf677-vgcw2_kube-system(19af187b-6921-11e8-8ba7-080027da9d03), influxdb-grafana-9sxtq_kube-system(19c5427a-6921-11e8-8ba7-080027da9d03), heapster-z8wfw_kube-system(19ba41de-6921-11e8-8ba7-080027da9d03), kube-addon-manager-minikube_kube-system(3afaf06535cc3b85be93c31632b765da), memory-demo-787bcd6d76-nw7rz_default(52e5e2af-6921-11e8-8ba7-080027da9d03), kube-dns-86f4d74b45-bczhr_kube-system(18e7ed0d-6921-11e8-8ba7-080027da9d03)
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550634    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to watch *v1.Service: Get https://192.168.99.100:8443/api/v1/services?resourceVersion=367&timeoutSeconds=385&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550700    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:460: Failed to watch *v1.Node: Get https://192.168.99.100:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=606&timeoutSeconds=362&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
```

May relate to other similar issues (ex. https://github.com/kubernetes/kubernetes/issues/53659 https://github.com/kubernetes/kubernetes/issues/63005)

**Environment**:
- Kubernetes version (use `kubectl version`): v1.10.0
- Cloud provider or hardware configuration: minikube v0.27.0
",52e5e2af-6921-11e8-8ba7-080027da9d03,https://github.com/kubernetes/kubernetes
93,64802.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If the matter is security related, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>
/kind bug
> /kind feature


**What happened**:
In low memory, crucial pods are candidates for eviction.

**What you expected to happen**:
Evict only non-critical pods

**How to reproduce it (as minimally and precisely as possible)**:
1. minikube start  --extra-config=kubelet.eviction-hard=""memory.available<500Mi""
2. create a pod (I used [memory-demo](https://github.com/kubernetes/website/blob/master/content/en/docs/tasks/configure-pod-container/memory-request-limit.yaml))
3. see a critical pod will be evicted (in my case, kube-apiserver-minikube_kube-system)

**Anything else we need to know?**:

`minikube logs` snippet:
```
Jun 06 00:34:08 minikube kubelet[3594]: I0606 00:34:08.130867    3594 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume ""default-token-bvkxc"" (UniqueName: ""kubernetes.io/secret/52e5e2af-6921-11e8-8ba7-080027da9d03-default-token-bvkxc"") pod ""memory-demo-787bcd6d76-nw7rz"" (UID: ""52e5e2af-6921-11e8-8ba7-080027da9d03"")
Jun 06 00:34:19 minikube kubelet[3594]: W0606 00:34:19.515478    3594 eviction_manager.go:343] eviction manager: attempting to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515701    3594 eviction_manager.go:357] eviction manager: must evict pod(s) to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515869    3594 eviction_manager.go:375] eviction manager: pods ranked for eviction: kube-apiserver-minikube_kube-system(9e218749db575502dc2153f1dcfcd8f5), kube-controller-manager-minikube_kube-system(af5fba67ac3471c82c00451a6f0db9a5), storage-provisioner_kube-system(1a69c6d7-6921-11e8-8ba7-080027da9d03), etcd-minikube_kube-system(8904a21094f73743ab3c7734da699963), kube-proxy-988jf_kube-system(18edc583-6921-11e8-8ba7-080027da9d03), kube-scheduler-minikube_kube-system(31cf0ccbee286239d451edb6fb511513), kubernetes-dashboard-5498ccf677-vgcw2_kube-system(19af187b-6921-11e8-8ba7-080027da9d03), influxdb-grafana-9sxtq_kube-system(19c5427a-6921-11e8-8ba7-080027da9d03), heapster-z8wfw_kube-system(19ba41de-6921-11e8-8ba7-080027da9d03), kube-addon-manager-minikube_kube-system(3afaf06535cc3b85be93c31632b765da), memory-demo-787bcd6d76-nw7rz_default(52e5e2af-6921-11e8-8ba7-080027da9d03), kube-dns-86f4d74b45-bczhr_kube-system(18e7ed0d-6921-11e8-8ba7-080027da9d03)
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550634    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to watch *v1.Service: Get https://192.168.99.100:8443/api/v1/services?resourceVersion=367&timeoutSeconds=385&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550700    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:460: Failed to watch *v1.Node: Get https://192.168.99.100:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=606&timeoutSeconds=362&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
```

May relate to other similar issues (ex. https://github.com/kubernetes/kubernetes/issues/53659 https://github.com/kubernetes/kubernetes/issues/63005)

**Environment**:
- Kubernetes version (use `kubectl version`): v1.10.0
- Cloud provider or hardware configuration: minikube v0.27.0
",18edc583-6921-11e8-8ba7-080027da9d03,https://github.com/kubernetes/kubernetes
94,64802.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If the matter is security related, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>
/kind bug
> /kind feature


**What happened**:
In low memory, crucial pods are candidates for eviction.

**What you expected to happen**:
Evict only non-critical pods

**How to reproduce it (as minimally and precisely as possible)**:
1. minikube start  --extra-config=kubelet.eviction-hard=""memory.available<500Mi""
2. create a pod (I used [memory-demo](https://github.com/kubernetes/website/blob/master/content/en/docs/tasks/configure-pod-container/memory-request-limit.yaml))
3. see a critical pod will be evicted (in my case, kube-apiserver-minikube_kube-system)

**Anything else we need to know?**:

`minikube logs` snippet:
```
Jun 06 00:34:08 minikube kubelet[3594]: I0606 00:34:08.130867    3594 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume ""default-token-bvkxc"" (UniqueName: ""kubernetes.io/secret/52e5e2af-6921-11e8-8ba7-080027da9d03-default-token-bvkxc"") pod ""memory-demo-787bcd6d76-nw7rz"" (UID: ""52e5e2af-6921-11e8-8ba7-080027da9d03"")
Jun 06 00:34:19 minikube kubelet[3594]: W0606 00:34:19.515478    3594 eviction_manager.go:343] eviction manager: attempting to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515701    3594 eviction_manager.go:357] eviction manager: must evict pod(s) to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515869    3594 eviction_manager.go:375] eviction manager: pods ranked for eviction: kube-apiserver-minikube_kube-system(9e218749db575502dc2153f1dcfcd8f5), kube-controller-manager-minikube_kube-system(af5fba67ac3471c82c00451a6f0db9a5), storage-provisioner_kube-system(1a69c6d7-6921-11e8-8ba7-080027da9d03), etcd-minikube_kube-system(8904a21094f73743ab3c7734da699963), kube-proxy-988jf_kube-system(18edc583-6921-11e8-8ba7-080027da9d03), kube-scheduler-minikube_kube-system(31cf0ccbee286239d451edb6fb511513), kubernetes-dashboard-5498ccf677-vgcw2_kube-system(19af187b-6921-11e8-8ba7-080027da9d03), influxdb-grafana-9sxtq_kube-system(19c5427a-6921-11e8-8ba7-080027da9d03), heapster-z8wfw_kube-system(19ba41de-6921-11e8-8ba7-080027da9d03), kube-addon-manager-minikube_kube-system(3afaf06535cc3b85be93c31632b765da), memory-demo-787bcd6d76-nw7rz_default(52e5e2af-6921-11e8-8ba7-080027da9d03), kube-dns-86f4d74b45-bczhr_kube-system(18e7ed0d-6921-11e8-8ba7-080027da9d03)
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550634    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to watch *v1.Service: Get https://192.168.99.100:8443/api/v1/services?resourceVersion=367&timeoutSeconds=385&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550700    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:460: Failed to watch *v1.Node: Get https://192.168.99.100:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=606&timeoutSeconds=362&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
```

May relate to other similar issues (ex. https://github.com/kubernetes/kubernetes/issues/53659 https://github.com/kubernetes/kubernetes/issues/63005)

**Environment**:
- Kubernetes version (use `kubectl version`): v1.10.0
- Cloud provider or hardware configuration: minikube v0.27.0
",1a69c6d7-6921-11e8-8ba7-080027da9d03,https://github.com/kubernetes/kubernetes
95,64802.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If the matter is security related, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>
/kind bug
> /kind feature


**What happened**:
In low memory, crucial pods are candidates for eviction.

**What you expected to happen**:
Evict only non-critical pods

**How to reproduce it (as minimally and precisely as possible)**:
1. minikube start  --extra-config=kubelet.eviction-hard=""memory.available<500Mi""
2. create a pod (I used [memory-demo](https://github.com/kubernetes/website/blob/master/content/en/docs/tasks/configure-pod-container/memory-request-limit.yaml))
3. see a critical pod will be evicted (in my case, kube-apiserver-minikube_kube-system)

**Anything else we need to know?**:

`minikube logs` snippet:
```
Jun 06 00:34:08 minikube kubelet[3594]: I0606 00:34:08.130867    3594 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume ""default-token-bvkxc"" (UniqueName: ""kubernetes.io/secret/52e5e2af-6921-11e8-8ba7-080027da9d03-default-token-bvkxc"") pod ""memory-demo-787bcd6d76-nw7rz"" (UID: ""52e5e2af-6921-11e8-8ba7-080027da9d03"")
Jun 06 00:34:19 minikube kubelet[3594]: W0606 00:34:19.515478    3594 eviction_manager.go:343] eviction manager: attempting to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515701    3594 eviction_manager.go:357] eviction manager: must evict pod(s) to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515869    3594 eviction_manager.go:375] eviction manager: pods ranked for eviction: kube-apiserver-minikube_kube-system(9e218749db575502dc2153f1dcfcd8f5), kube-controller-manager-minikube_kube-system(af5fba67ac3471c82c00451a6f0db9a5), storage-provisioner_kube-system(1a69c6d7-6921-11e8-8ba7-080027da9d03), etcd-minikube_kube-system(8904a21094f73743ab3c7734da699963), kube-proxy-988jf_kube-system(18edc583-6921-11e8-8ba7-080027da9d03), kube-scheduler-minikube_kube-system(31cf0ccbee286239d451edb6fb511513), kubernetes-dashboard-5498ccf677-vgcw2_kube-system(19af187b-6921-11e8-8ba7-080027da9d03), influxdb-grafana-9sxtq_kube-system(19c5427a-6921-11e8-8ba7-080027da9d03), heapster-z8wfw_kube-system(19ba41de-6921-11e8-8ba7-080027da9d03), kube-addon-manager-minikube_kube-system(3afaf06535cc3b85be93c31632b765da), memory-demo-787bcd6d76-nw7rz_default(52e5e2af-6921-11e8-8ba7-080027da9d03), kube-dns-86f4d74b45-bczhr_kube-system(18e7ed0d-6921-11e8-8ba7-080027da9d03)
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550634    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to watch *v1.Service: Get https://192.168.99.100:8443/api/v1/services?resourceVersion=367&timeoutSeconds=385&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550700    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:460: Failed to watch *v1.Node: Get https://192.168.99.100:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=606&timeoutSeconds=362&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
```

May relate to other similar issues (ex. https://github.com/kubernetes/kubernetes/issues/53659 https://github.com/kubernetes/kubernetes/issues/63005)

**Environment**:
- Kubernetes version (use `kubectl version`): v1.10.0
- Cloud provider or hardware configuration: minikube v0.27.0
",19c5427a-6921-11e8-8ba7-080027da9d03,https://github.com/kubernetes/kubernetes
96,64802.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If the matter is security related, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>
/kind bug
> /kind feature


**What happened**:
In low memory, crucial pods are candidates for eviction.

**What you expected to happen**:
Evict only non-critical pods

**How to reproduce it (as minimally and precisely as possible)**:
1. minikube start  --extra-config=kubelet.eviction-hard=""memory.available<500Mi""
2. create a pod (I used [memory-demo](https://github.com/kubernetes/website/blob/master/content/en/docs/tasks/configure-pod-container/memory-request-limit.yaml))
3. see a critical pod will be evicted (in my case, kube-apiserver-minikube_kube-system)

**Anything else we need to know?**:

`minikube logs` snippet:
```
Jun 06 00:34:08 minikube kubelet[3594]: I0606 00:34:08.130867    3594 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume ""default-token-bvkxc"" (UniqueName: ""kubernetes.io/secret/52e5e2af-6921-11e8-8ba7-080027da9d03-default-token-bvkxc"") pod ""memory-demo-787bcd6d76-nw7rz"" (UID: ""52e5e2af-6921-11e8-8ba7-080027da9d03"")
Jun 06 00:34:19 minikube kubelet[3594]: W0606 00:34:19.515478    3594 eviction_manager.go:343] eviction manager: attempting to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515701    3594 eviction_manager.go:357] eviction manager: must evict pod(s) to reclaim memory
Jun 06 00:34:19 minikube kubelet[3594]: I0606 00:34:19.515869    3594 eviction_manager.go:375] eviction manager: pods ranked for eviction: kube-apiserver-minikube_kube-system(9e218749db575502dc2153f1dcfcd8f5), kube-controller-manager-minikube_kube-system(af5fba67ac3471c82c00451a6f0db9a5), storage-provisioner_kube-system(1a69c6d7-6921-11e8-8ba7-080027da9d03), etcd-minikube_kube-system(8904a21094f73743ab3c7734da699963), kube-proxy-988jf_kube-system(18edc583-6921-11e8-8ba7-080027da9d03), kube-scheduler-minikube_kube-system(31cf0ccbee286239d451edb6fb511513), kubernetes-dashboard-5498ccf677-vgcw2_kube-system(19af187b-6921-11e8-8ba7-080027da9d03), influxdb-grafana-9sxtq_kube-system(19c5427a-6921-11e8-8ba7-080027da9d03), heapster-z8wfw_kube-system(19ba41de-6921-11e8-8ba7-080027da9d03), kube-addon-manager-minikube_kube-system(3afaf06535cc3b85be93c31632b765da), memory-demo-787bcd6d76-nw7rz_default(52e5e2af-6921-11e8-8ba7-080027da9d03), kube-dns-86f4d74b45-bczhr_kube-system(18e7ed0d-6921-11e8-8ba7-080027da9d03)
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550634    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to watch *v1.Service: Get https://192.168.99.100:8443/api/v1/services?resourceVersion=367&timeoutSeconds=385&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
Jun 06 00:34:19 minikube kubelet[3594]: E0606 00:34:19.550700    3594 reflector.go:322] k8s.io/kubernetes/pkg/kubelet/kubelet.go:460: Failed to watch *v1.Node: Get https://192.168.99.100:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=606&timeoutSeconds=362&watch=true: dial tcp 192.168.99.100:8443: getsockopt: connection refused
```

May relate to other similar issues (ex. https://github.com/kubernetes/kubernetes/issues/53659 https://github.com/kubernetes/kubernetes/issues/63005)

**Environment**:
- Kubernetes version (use `kubectl version`): v1.10.0
- Cloud provider or hardware configuration: minikube v0.27.0
",787bcd6d76-nw7rz,https://github.com/kubernetes/kubernetes
97,64492.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>
/kind bug
> /kind feature


**What happened**:
When get resource with invalid token, the error info kubectl outputs is wrong.
For example:
`kubectl get pod --token=eyJhbGciOiJSUzI1NiIsIm`

Apparently, the token here is invalid. And the kubectl will output error info like
`error: the server doesn't have a resource type ""pod""`
It would mislead user.

**How to reproduce it (as minimally and precisely as possible)**:


**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
- Cloud provider or hardware configuration:
- OS (e.g. from /etc/os-release):
- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",eyJhbGciOiJSUzI1NiIsIm,https://github.com/kubernetes/kubernetes
98,63435.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",#NAME?,https://github.com/kubernetes/kubernetes
99,63435.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",token=bearer_token,https://github.com/kubernetes/kubernetes
100,63435.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",certificate=~=true,https://github.com/kubernetes/kubernetes
101,63435.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",auth-provider=oidc,https://github.com/kubernetes/kubernetes
102,63435.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",username=basic_user,https://github.com/kubernetes/kubernetes
103,63435.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",certificate=path,https://github.com/kubernetes/kubernetes
104,63435.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",client-certificate,https://github.com/kubernetes/kubernetes
105,63435.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",provider=provider_name,https://github.com/kubernetes/kubernetes
106,63435.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",auth-provider=gcp,https://github.com/kubernetes/kubernetes
107,63435.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",mutually,https://github.com/kubernetes/kubernetes
108,63435.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",certificate,https://github.com/kubernetes/kubernetes
109,63435.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",kubeconfig,https://github.com/kubernetes/kubernetes
110,63435.0,"<!-- This form is for bug reports and feature requests ONLY!

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).

If this may be security issue, please disclose it privately via https://kubernetes.io/security/.
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line:
>

/kind bug

> /kind feature


**What happened**:

```
root@gyliu-ubuntu-1:~/multicluster# kubectl config set-credentials --help
Sets a user entry in kubeconfig

Specifying a name that already exists will merge new fields on top of existing values.

  Client-certificate flags:
  --client-certificate=certfile --client-key=keyfile

  Bearer token flags:
    --token=bearer_token

  Basic auth flags:
    --username=basic_user --password=basic_password

Bearer token and basic auth are mutually exclusive.

Examples:
  # Set only the ""client-key"" field on the ""cluster-admin""
  # entry, without touching other values:
  kubectl config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the ""cluster-admin"" entry with additional args
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo
--auth-provider-arg=client-secret=bar

  # Remove the ""client-secret"" config value for the OpenID Connect auth provider for the ""cluster-admin"" entry
  kubectl config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

Options:
      --auth-provider='': Auth provider for the user entry in kubeconfig
      --auth-provider-arg=[]: 'key=value' arguments for the auth provider
      --client-certificate='': Path to client-certificate file for the user entry in kubeconfig
      --client-key='': Path to client-key file for the user entry in kubeconfig
      --embed-certs=false: Embed client cert/key for the user entry in kubeconfig
      --password='': password for the user entry in kubeconfig
      --token='': token for the user entry in kubeconfig
      --username='': username for the user entry in kubeconfig

Usage:
  kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile]
[--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name]
[--auth-provider-arg=key=value] [options]

Use ""kubectl options"" for a list of global command-line options (applies to all commands).
```

From above, we can see I have to use file name for certificate and key.
```
--client-certificate='': Path to client-certificate file for the user entry in kubeconfig
--client-key='': Path to client-key file for the user entry in kubeconfig
```

If I want to use content string of the certificate and key, I have to update `$HOME/.kube/config` manually by changing `client-certificate` to `client-certificate-data`, and change `client-key` to `client-key-data`.

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://9.111.255.21:8001
  name: myistio.icp
contexts:
- context:
    cluster: myistio.icp
    namespace: default
    user: admin
  name: myistio.icp-context
current-context: myistio.icp-context
kind: Config
preferences: {}
users:
- name: admin
  user:
    as-user-extra: {}
    client-certificate-data: <xxxx>
    client-key-data: <xxxx>
```

/sig security",provider,https://github.com/kubernetes/kubernetes
111,62950.0,"I'm trying to add a clusters to federation using kubefed join : 
this is the federation kube config file : 
```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://k8s-apiserver.cluster.local:8443
  name: kubernetes
- cluster:
    certificate-authority-data: REDACTED
    server: https://172.16.1.4:32471
  name: federation
- cluster:
    insecure-skip-tls-verify: true
    server: https://139.54.130.49:32046
  name: kubernetes-s1
contexts:
- context:
    cluster: kubernetes
    namespace: default
    user: kubectl
  name: default-context
- context:
    cluster: federation
    user: federation
  name: federation
- context:
    cluster: kubernetes
    namespace: kube-system
    user: kubectl
  name: kube-system-context
- context:
    cluster: kubernetes-s1
    namespace: default
    user: kubernetes-admins1
  name: kubernetes-admin-s1
current-context: federation
kind: Config
preferences: {}
users:
- name: federation
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
    token: e7506989-42eb-11e8-bf70-fa163eb593a3
- name: federation-basic-auth
  user:
    password: e7506937-42eb-11e8-bf70-fa163eb593a3
    username: admin
- name: kubectl
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
- name: kubernetes-admins1
  user:
    token: eyJhbGciOiJSUz............
```

i run this command : kubefed join site-1 --host-cluster-context=default-context --cluster-context=kubernetes-admin-s1 --insecure-skip-tls-verify=true, **the cluster is created but with offline status , is not reacheable ;** 
I'm using token bearer to reach the api server of the target cluster; 
where am I going wrong ? 


/sig multicluster
/sig federation ",e7506989-42eb-11e8-bf70-fa163eb593a3,https://github.com/kubernetes/kubernetes
112,62950.0,"I'm trying to add a clusters to federation using kubefed join : 
this is the federation kube config file : 
```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://k8s-apiserver.cluster.local:8443
  name: kubernetes
- cluster:
    certificate-authority-data: REDACTED
    server: https://172.16.1.4:32471
  name: federation
- cluster:
    insecure-skip-tls-verify: true
    server: https://139.54.130.49:32046
  name: kubernetes-s1
contexts:
- context:
    cluster: kubernetes
    namespace: default
    user: kubectl
  name: default-context
- context:
    cluster: federation
    user: federation
  name: federation
- context:
    cluster: kubernetes
    namespace: kube-system
    user: kubectl
  name: kube-system-context
- context:
    cluster: kubernetes-s1
    namespace: default
    user: kubernetes-admins1
  name: kubernetes-admin-s1
current-context: federation
kind: Config
preferences: {}
users:
- name: federation
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
    token: e7506989-42eb-11e8-bf70-fa163eb593a3
- name: federation-basic-auth
  user:
    password: e7506937-42eb-11e8-bf70-fa163eb593a3
    username: admin
- name: kubectl
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
- name: kubernetes-admins1
  user:
    token: eyJhbGciOiJSUz............
```

i run this command : kubefed join site-1 --host-cluster-context=default-context --cluster-context=kubernetes-admin-s1 --insecure-skip-tls-verify=true, **the cluster is created but with offline status , is not reacheable ;** 
I'm using token bearer to reach the api server of the target cluster; 
where am I going wrong ? 


/sig multicluster
/sig federation ",e7506937-42eb-11e8-bf70-fa163eb593a3,https://github.com/kubernetes/kubernetes
113,62950.0,"I'm trying to add a clusters to federation using kubefed join : 
this is the federation kube config file : 
```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://k8s-apiserver.cluster.local:8443
  name: kubernetes
- cluster:
    certificate-authority-data: REDACTED
    server: https://172.16.1.4:32471
  name: federation
- cluster:
    insecure-skip-tls-verify: true
    server: https://139.54.130.49:32046
  name: kubernetes-s1
contexts:
- context:
    cluster: kubernetes
    namespace: default
    user: kubectl
  name: default-context
- context:
    cluster: federation
    user: federation
  name: federation
- context:
    cluster: kubernetes
    namespace: kube-system
    user: kubectl
  name: kube-system-context
- context:
    cluster: kubernetes-s1
    namespace: default
    user: kubernetes-admins1
  name: kubernetes-admin-s1
current-context: federation
kind: Config
preferences: {}
users:
- name: federation
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
    token: e7506989-42eb-11e8-bf70-fa163eb593a3
- name: federation-basic-auth
  user:
    password: e7506937-42eb-11e8-bf70-fa163eb593a3
    username: admin
- name: kubectl
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
- name: kubernetes-admins1
  user:
    token: eyJhbGciOiJSUz............
```

i run this command : kubefed join site-1 --host-cluster-context=default-context --cluster-context=kubernetes-admin-s1 --insecure-skip-tls-verify=true, **the cluster is created but with offline status , is not reacheable ;** 
I'm using token bearer to reach the api server of the target cluster; 
where am I going wrong ? 


/sig multicluster
/sig federation ",42eb-11e8-bf70-fa163eb593a3,https://github.com/kubernetes/kubernetes
114,62950.0,"I'm trying to add a clusters to federation using kubefed join : 
this is the federation kube config file : 
```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://k8s-apiserver.cluster.local:8443
  name: kubernetes
- cluster:
    certificate-authority-data: REDACTED
    server: https://172.16.1.4:32471
  name: federation
- cluster:
    insecure-skip-tls-verify: true
    server: https://139.54.130.49:32046
  name: kubernetes-s1
contexts:
- context:
    cluster: kubernetes
    namespace: default
    user: kubectl
  name: default-context
- context:
    cluster: federation
    user: federation
  name: federation
- context:
    cluster: kubernetes
    namespace: kube-system
    user: kubectl
  name: kube-system-context
- context:
    cluster: kubernetes-s1
    namespace: default
    user: kubernetes-admins1
  name: kubernetes-admin-s1
current-context: federation
kind: Config
preferences: {}
users:
- name: federation
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
    token: e7506989-42eb-11e8-bf70-fa163eb593a3
- name: federation-basic-auth
  user:
    password: e7506937-42eb-11e8-bf70-fa163eb593a3
    username: admin
- name: kubectl
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
- name: kubernetes-admins1
  user:
    token: eyJhbGciOiJSUz............
```

i run this command : kubefed join site-1 --host-cluster-context=default-context --cluster-context=kubernetes-admin-s1 --insecure-skip-tls-verify=true, **the cluster is created but with offline status , is not reacheable ;** 
I'm using token bearer to reach the api server of the target cluster; 
where am I going wrong ? 


/sig multicluster
/sig federation ",bf70-fa163eb593a3,https://github.com/kubernetes/kubernetes
115,62950.0,"I'm trying to add a clusters to federation using kubefed join : 
this is the federation kube config file : 
```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://k8s-apiserver.cluster.local:8443
  name: kubernetes
- cluster:
    certificate-authority-data: REDACTED
    server: https://172.16.1.4:32471
  name: federation
- cluster:
    insecure-skip-tls-verify: true
    server: https://139.54.130.49:32046
  name: kubernetes-s1
contexts:
- context:
    cluster: kubernetes
    namespace: default
    user: kubectl
  name: default-context
- context:
    cluster: federation
    user: federation
  name: federation
- context:
    cluster: kubernetes
    namespace: kube-system
    user: kubectl
  name: kube-system-context
- context:
    cluster: kubernetes-s1
    namespace: default
    user: kubernetes-admins1
  name: kubernetes-admin-s1
current-context: federation
kind: Config
preferences: {}
users:
- name: federation
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
    token: e7506989-42eb-11e8-bf70-fa163eb593a3
- name: federation-basic-auth
  user:
    password: e7506937-42eb-11e8-bf70-fa163eb593a3
    username: admin
- name: kubectl
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
- name: kubernetes-admins1
  user:
    token: eyJhbGciOiJSUz............
```

i run this command : kubefed join site-1 --host-cluster-context=default-context --cluster-context=kubernetes-admin-s1 --insecure-skip-tls-verify=true, **the cluster is created but with offline status , is not reacheable ;** 
I'm using token bearer to reach the api server of the target cluster; 
where am I going wrong ? 


/sig multicluster
/sig federation ",kubernetes-admins1,https://github.com/kubernetes/kubernetes
116,62950.0,"I'm trying to add a clusters to federation using kubefed join : 
this is the federation kube config file : 
```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://k8s-apiserver.cluster.local:8443
  name: kubernetes
- cluster:
    certificate-authority-data: REDACTED
    server: https://172.16.1.4:32471
  name: federation
- cluster:
    insecure-skip-tls-verify: true
    server: https://139.54.130.49:32046
  name: kubernetes-s1
contexts:
- context:
    cluster: kubernetes
    namespace: default
    user: kubectl
  name: default-context
- context:
    cluster: federation
    user: federation
  name: federation
- context:
    cluster: kubernetes
    namespace: kube-system
    user: kubectl
  name: kube-system-context
- context:
    cluster: kubernetes-s1
    namespace: default
    user: kubernetes-admins1
  name: kubernetes-admin-s1
current-context: federation
kind: Config
preferences: {}
users:
- name: federation
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
    token: e7506989-42eb-11e8-bf70-fa163eb593a3
- name: federation-basic-auth
  user:
    password: e7506937-42eb-11e8-bf70-fa163eb593a3
    username: admin
- name: kubectl
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
- name: kubernetes-admins1
  user:
    token: eyJhbGciOiJSUz............
```

i run this command : kubefed join site-1 --host-cluster-context=default-context --cluster-context=kubernetes-admin-s1 --insecure-skip-tls-verify=true, **the cluster is created but with offline status , is not reacheable ;** 
I'm using token bearer to reach the api server of the target cluster; 
where am I going wrong ? 


/sig multicluster
/sig federation ",fa163eb593a3,https://github.com/kubernetes/kubernetes
117,61532.0,"I install ceph luminous, and create rbd pool. When I configure kubernetes to use ceph by storageclass, the pvc is pending. I have install ceph-common and modprobe rbd on kubernetes node（version:v1.8.3）.
The config of ceph-secret and ceph-storageclass is like this:
apiVersion: v1
kind: Secret
metadata:
name: ceph-secret
namespace: default
type: ""kubernetes.io/rbd""
data:
key: QVFBckViSmFuVDdvSXhBQXgwNXFNZGVNMzhSM012dG1ISzFqMWc9PQ==

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
name: ceph-iop
provisioner: kubernetes.io/rbd
parameters:
monitors: 10.110.25.71:6789,10.110.25.72:6789,10.110.25.73:6789
adminId: admin
adminSecretName: ceph-secret
adminSecretNamespace: default
pool: rbd
userId: admin
userSecretName: ceph-secret
imageFormat: ""1""

I found some error in kube-control-manager log is like this:
2018-03-22 07:30:32.039740 7f6e50267700 0 -- 10.110.22.81:0/1000234 >> 10.110.25.73:6789/0 pipe(0x7f6e3c001c30 sd=3 :39474 s=1 pgs=0 cs=0 l=1 c=0x7f6e3c005e10).connect protocol feature mismatch, my fffffffffff < peer 4010ff8eea4fffb missing 401000000000000
2018-03-22 07:30:33.532998 7f6e568b8780 0 monclient(hunting): authenticate timed out after 300
2018-03-22 07:30:33.554389 7f6e568b8780 0 librados: client.admin authentication error (110) Connection timed out
rbd: couldn't connect to the cluster!
I0322 07:30:33.818456 1 pv_controller.go:1317] failed to provision volume for claim ""default/pvc"" with StorageClass ""ceph-iop"": failed to create rbd image: exit status 1, command output: 2018-03-22 07:25:33.379161 7f6e568b8780 -1 did not load config file, using default settings.",ceph-storageclass,https://github.com/kubernetes/kubernetes
118,61532.0,"I install ceph luminous, and create rbd pool. When I configure kubernetes to use ceph by storageclass, the pvc is pending. I have install ceph-common and modprobe rbd on kubernetes node（version:v1.8.3）.
The config of ceph-secret and ceph-storageclass is like this:
apiVersion: v1
kind: Secret
metadata:
name: ceph-secret
namespace: default
type: ""kubernetes.io/rbd""
data:
key: QVFBckViSmFuVDdvSXhBQXgwNXFNZGVNMzhSM012dG1ISzFqMWc9PQ==

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
name: ceph-iop
provisioner: kubernetes.io/rbd
parameters:
monitors: 10.110.25.71:6789,10.110.25.72:6789,10.110.25.73:6789
adminId: admin
adminSecretName: ceph-secret
adminSecretNamespace: default
pool: rbd
userId: admin
userSecretName: ceph-secret
imageFormat: ""1""

I found some error in kube-control-manager log is like this:
2018-03-22 07:30:32.039740 7f6e50267700 0 -- 10.110.22.81:0/1000234 >> 10.110.25.73:6789/0 pipe(0x7f6e3c001c30 sd=3 :39474 s=1 pgs=0 cs=0 l=1 c=0x7f6e3c005e10).connect protocol feature mismatch, my fffffffffff < peer 4010ff8eea4fffb missing 401000000000000
2018-03-22 07:30:33.532998 7f6e568b8780 0 monclient(hunting): authenticate timed out after 300
2018-03-22 07:30:33.554389 7f6e568b8780 0 librados: client.admin authentication error (110) Connection timed out
rbd: couldn't connect to the cluster!
I0322 07:30:33.818456 1 pv_controller.go:1317] failed to provision volume for claim ""default/pvc"" with StorageClass ""ceph-iop"": failed to create rbd image: exit status 1, command output: 2018-03-22 07:25:33.379161 7f6e568b8780 -1 did not load config file, using default settings.",adminSecretNamespace,https://github.com/kubernetes/kubernetes
119,61532.0,"I install ceph luminous, and create rbd pool. When I configure kubernetes to use ceph by storageclass, the pvc is pending. I have install ceph-common and modprobe rbd on kubernetes node（version:v1.8.3）.
The config of ceph-secret and ceph-storageclass is like this:
apiVersion: v1
kind: Secret
metadata:
name: ceph-secret
namespace: default
type: ""kubernetes.io/rbd""
data:
key: QVFBckViSmFuVDdvSXhBQXgwNXFNZGVNMzhSM012dG1ISzFqMWc9PQ==

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
name: ceph-iop
provisioner: kubernetes.io/rbd
parameters:
monitors: 10.110.25.71:6789,10.110.25.72:6789,10.110.25.73:6789
adminId: admin
adminSecretName: ceph-secret
adminSecretNamespace: default
pool: rbd
userId: admin
userSecretName: ceph-secret
imageFormat: ""1""

I found some error in kube-control-manager log is like this:
2018-03-22 07:30:32.039740 7f6e50267700 0 -- 10.110.22.81:0/1000234 >> 10.110.25.73:6789/0 pipe(0x7f6e3c001c30 sd=3 :39474 s=1 pgs=0 cs=0 l=1 c=0x7f6e3c005e10).connect protocol feature mismatch, my fffffffffff < peer 4010ff8eea4fffb missing 401000000000000
2018-03-22 07:30:33.532998 7f6e568b8780 0 monclient(hunting): authenticate timed out after 300
2018-03-22 07:30:33.554389 7f6e568b8780 0 librados: client.admin authentication error (110) Connection timed out
rbd: couldn't connect to the cluster!
I0322 07:30:33.818456 1 pv_controller.go:1317] failed to provision volume for claim ""default/pvc"" with StorageClass ""ceph-iop"": failed to create rbd image: exit status 1, command output: 2018-03-22 07:25:33.379161 7f6e568b8780 -1 did not load config file, using default settings.",QVFBckViSmFuVDdvSXhBQXgwNXFNZGVNMzhSM012dG1ISzFqMWc9PQ,https://github.com/kubernetes/kubernetes
120,61532.0,"I install ceph luminous, and create rbd pool. When I configure kubernetes to use ceph by storageclass, the pvc is pending. I have install ceph-common and modprobe rbd on kubernetes node（version:v1.8.3）.
The config of ceph-secret and ceph-storageclass is like this:
apiVersion: v1
kind: Secret
metadata:
name: ceph-secret
namespace: default
type: ""kubernetes.io/rbd""
data:
key: QVFBckViSmFuVDdvSXhBQXgwNXFNZGVNMzhSM012dG1ISzFqMWc9PQ==

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
name: ceph-iop
provisioner: kubernetes.io/rbd
parameters:
monitors: 10.110.25.71:6789,10.110.25.72:6789,10.110.25.73:6789
adminId: admin
adminSecretName: ceph-secret
adminSecretNamespace: default
pool: rbd
userId: admin
userSecretName: ceph-secret
imageFormat: ""1""

I found some error in kube-control-manager log is like this:
2018-03-22 07:30:32.039740 7f6e50267700 0 -- 10.110.22.81:0/1000234 >> 10.110.25.73:6789/0 pipe(0x7f6e3c001c30 sd=3 :39474 s=1 pgs=0 cs=0 l=1 c=0x7f6e3c005e10).connect protocol feature mismatch, my fffffffffff < peer 4010ff8eea4fffb missing 401000000000000
2018-03-22 07:30:33.532998 7f6e568b8780 0 monclient(hunting): authenticate timed out after 300
2018-03-22 07:30:33.554389 7f6e568b8780 0 librados: client.admin authentication error (110) Connection timed out
rbd: couldn't connect to the cluster!
I0322 07:30:33.818456 1 pv_controller.go:1317] failed to provision volume for claim ""default/pvc"" with StorageClass ""ceph-iop"": failed to create rbd image: exit status 1, command output: 2018-03-22 07:25:33.379161 7f6e568b8780 -1 did not load config file, using default settings.",QVFBckViSmFuVDdvSXhBQXgwNXFNZGVNMzhSM012dG1ISzFqMW,https://github.com/kubernetes/kubernetes
121,61140.0,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG REPORT
> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
After upgrading the master to 1.9.3-gke.0 in my GKE cluster, the kubectl replare freezes when I perform the command below:

kubectl create secret -n ""test"" \
       docker-registry test-registry \
       --docker-server = ""registry.devops.teste.com"" \
       --docker-username = ""test"" \
       --docker-password = ""pKu3fzFfxW2xV2ygm-A1"" \
       --docker-email = ""test@teste.com""
       -o yaml --dry-run | kubectl replace -n ""test"" --force -f -

If the resource does not exist it creates without problems, but if the resource exists after deleting the previous one it hangs.

**What you expected to happen**:
I hope the secret is recreated

**How to reproduce it (as minimally and precisely as possible)**:

execute the command 2x

kubectl create secret -n ""teste"" \
      docker-registry teste-registry \
      --docker-server=""registry.devops.teste.com"" \
      --docker-username=""teste"" \
      --docker-password=""pKu3fzFfxW2xV2ygm-A1"" \
      --docker-email=""teste@teste.com"" \
      -o yaml --dry-run | kubectl replace -n ""teste"" --force -f -

**Anything else we need to know?**:
This started after Master's upgra for version
1.9.3-gke.0

**Environment**:
- Kubernetes version (use `kubectl version`): 
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.0"", GitCommit:""925c127ec6b946659ad0fd596fa959be43f0cc05"", GitTreeState:""clean"", BuildDate:""2017-12-15T21:07:38Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9+"", GitVersion:""v1.9.3-gke.0"", GitCommit:""a7b719f7d3463eb5431cf8a3caf5d485827b4210"", GitTreeState:""clean"", BuildDate:""2018-02-16T18:26:01Z"", GoVersion:""go1.9.2b4"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
n1-standard-4 (4 vCPUs, 15 GB memory)

- OS (e.g. from /etc/os-release):
Container-Optimized OS (cos)

- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",docker-email=teste,https://github.com/kubernetes/kubernetes
122,61140.0,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG REPORT
> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
After upgrading the master to 1.9.3-gke.0 in my GKE cluster, the kubectl replare freezes when I perform the command below:

kubectl create secret -n ""test"" \
       docker-registry test-registry \
       --docker-server = ""registry.devops.teste.com"" \
       --docker-username = ""test"" \
       --docker-password = ""pKu3fzFfxW2xV2ygm-A1"" \
       --docker-email = ""test@teste.com""
       -o yaml --dry-run | kubectl replace -n ""test"" --force -f -

If the resource does not exist it creates without problems, but if the resource exists after deleting the previous one it hangs.

**What you expected to happen**:
I hope the secret is recreated

**How to reproduce it (as minimally and precisely as possible)**:

execute the command 2x

kubectl create secret -n ""teste"" \
      docker-registry teste-registry \
      --docker-server=""registry.devops.teste.com"" \
      --docker-username=""teste"" \
      --docker-password=""pKu3fzFfxW2xV2ygm-A1"" \
      --docker-email=""teste@teste.com"" \
      -o yaml --dry-run | kubectl replace -n ""teste"" --force -f -

**Anything else we need to know?**:
This started after Master's upgra for version
1.9.3-gke.0

**Environment**:
- Kubernetes version (use `kubectl version`): 
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.0"", GitCommit:""925c127ec6b946659ad0fd596fa959be43f0cc05"", GitTreeState:""clean"", BuildDate:""2017-12-15T21:07:38Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9+"", GitVersion:""v1.9.3-gke.0"", GitCommit:""a7b719f7d3463eb5431cf8a3caf5d485827b4210"", GitTreeState:""clean"", BuildDate:""2018-02-16T18:26:01Z"", GoVersion:""go1.9.2b4"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
n1-standard-4 (4 vCPUs, 15 GB memory)

- OS (e.g. from /etc/os-release):
Container-Optimized OS (cos)

- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",pKu3fzFfxW2xV2ygm-A1,https://github.com/kubernetes/kubernetes
123,61140.0,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG REPORT
> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
After upgrading the master to 1.9.3-gke.0 in my GKE cluster, the kubectl replare freezes when I perform the command below:

kubectl create secret -n ""test"" \
       docker-registry test-registry \
       --docker-server = ""registry.devops.teste.com"" \
       --docker-username = ""test"" \
       --docker-password = ""pKu3fzFfxW2xV2ygm-A1"" \
       --docker-email = ""test@teste.com""
       -o yaml --dry-run | kubectl replace -n ""test"" --force -f -

If the resource does not exist it creates without problems, but if the resource exists after deleting the previous one it hangs.

**What you expected to happen**:
I hope the secret is recreated

**How to reproduce it (as minimally and precisely as possible)**:

execute the command 2x

kubectl create secret -n ""teste"" \
      docker-registry teste-registry \
      --docker-server=""registry.devops.teste.com"" \
      --docker-username=""teste"" \
      --docker-password=""pKu3fzFfxW2xV2ygm-A1"" \
      --docker-email=""teste@teste.com"" \
      -o yaml --dry-run | kubectl replace -n ""teste"" --force -f -

**Anything else we need to know?**:
This started after Master's upgra for version
1.9.3-gke.0

**Environment**:
- Kubernetes version (use `kubectl version`): 
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.0"", GitCommit:""925c127ec6b946659ad0fd596fa959be43f0cc05"", GitTreeState:""clean"", BuildDate:""2017-12-15T21:07:38Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9+"", GitVersion:""v1.9.3-gke.0"", GitCommit:""a7b719f7d3463eb5431cf8a3caf5d485827b4210"", GitTreeState:""clean"", BuildDate:""2018-02-16T18:26:01Z"", GoVersion:""go1.9.2b4"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
n1-standard-4 (4 vCPUs, 15 GB memory)

- OS (e.g. from /etc/os-release):
Container-Optimized OS (cos)

- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",pKu3fzFfxW2xV2ygm,https://github.com/kubernetes/kubernetes
124,61140.0,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG REPORT
> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:
After upgrading the master to 1.9.3-gke.0 in my GKE cluster, the kubectl replare freezes when I perform the command below:

kubectl create secret -n ""test"" \
       docker-registry test-registry \
       --docker-server = ""registry.devops.teste.com"" \
       --docker-username = ""test"" \
       --docker-password = ""pKu3fzFfxW2xV2ygm-A1"" \
       --docker-email = ""test@teste.com""
       -o yaml --dry-run | kubectl replace -n ""test"" --force -f -

If the resource does not exist it creates without problems, but if the resource exists after deleting the previous one it hangs.

**What you expected to happen**:
I hope the secret is recreated

**How to reproduce it (as minimally and precisely as possible)**:

execute the command 2x

kubectl create secret -n ""teste"" \
      docker-registry teste-registry \
      --docker-server=""registry.devops.teste.com"" \
      --docker-username=""teste"" \
      --docker-password=""pKu3fzFfxW2xV2ygm-A1"" \
      --docker-email=""teste@teste.com"" \
      -o yaml --dry-run | kubectl replace -n ""teste"" --force -f -

**Anything else we need to know?**:
This started after Master's upgra for version
1.9.3-gke.0

**Environment**:
- Kubernetes version (use `kubectl version`): 
Client Version: version.Info{Major:""1"", Minor:""9"", GitVersion:""v1.9.0"", GitCommit:""925c127ec6b946659ad0fd596fa959be43f0cc05"", GitTreeState:""clean"", BuildDate:""2017-12-15T21:07:38Z"", GoVersion:""go1.9.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""9+"", GitVersion:""v1.9.3-gke.0"", GitCommit:""a7b719f7d3463eb5431cf8a3caf5d485827b4210"", GitTreeState:""clean"", BuildDate:""2018-02-16T18:26:01Z"", GoVersion:""go1.9.2b4"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
n1-standard-4 (4 vCPUs, 15 GB memory)

- OS (e.g. from /etc/os-release):
Container-Optimized OS (cos)

- Kernel (e.g. `uname -a`):
- Install tools:
- Others:
",email=teste@,https://github.com/kubernetes/kubernetes
125,60410.0,"> /kind bug

> /sig cluster-lifecycle

## Problem describe
kubeadm.cfg
```
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
cloudProvider: external
imageRepository: registry.cn-hangzhou.aliyuncs.com/acs
featureGates:
  selfHosting: true
networking:
  dnsDomain: cluster.local
  serviceSubnet: 172.19.0.0/20
  podSubnet: 172.16.0.0/16
apiServerCertSANs:
  - 192.168.110.109
  - 127.0.0.1
token: cdca7c.3e8035f5959f0da6
nodeName: cn-hongkong.i-j6cal2nyrssnrcz2mju7
kubernetesVersion: v1.9.3
```

kubeadm init --config kubeadm.cfg

```
[self-hosted] Creating self-hosted control plane.
[apiclient] Found 0 Pods for label selector k8s-app=self-hosted-kube-apiserver
error creating self hosted control plane: timed out waiting for the condition
```

**error creating self hosted control plane: timed out waiting for the condition** Error occured.

```
kubectl get no -o yaml 

  spec:
    externalID: cn-hongkong.i-j6cal2nyrssnrcz2mju7
    podCIDR: 172.16.0.0/24
    providerID: cn-hongkong.i-j6cal2nyrssnrcz2mju7
    taints:
    - effect: NoSchedule
      key: node.cloudprovider.kubernetes.io/uninitialized
      value: ""true""
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
```

## Fix 

kubelet will taint node with ```node.cloudprovider.kubernetes.io/uninitialized``` on node initializing. kubeadm should take care of this taint by adding a toleration on control panel pod when bootstrap a cluster.

A PR to fix this will be submitted soon. 
",kubernetesVersion,https://github.com/kubernetes/kubernetes
126,55850.0,"/sig api-machinery

**What happened**:

i  want to get kubernetes  data in etcd v3 via etcd clientV3 , but however, the value of the key is unreadable ,like this 

```
key:""/registry/pods/default/tserver-79f7f9fbd6-hw4pb"" create_revision:2712131 mod_revision:2712145 version:4 value:""k8s\000\n\t\n\002v1\022\003Pod\022\361\014\n\211\004\n\030tserver-79f7f9fbd6-hw4pb\022\023tserver-79f7f9fbd6-\032\007default\""\000*$7238544b-ca7e-11e7-8e50-5820b101504c2\0008\000B\010\010\300\214\264\320\005\020\000Z\016\n\003app\022\007tserverZ\037\n\021pod-template-hash\022\n3593959682b\205\002\n\030kubernetes.io/created-by\022\350\001{\""kind\"":\""SerializedReference\"",\""apiVersion\"":\""v1\"",\""reference\"":{\""kind\"":\""ReplicaSet\"",\""namespace\"":\""default\"",\""name\"":\""tserver-79f7f9fbd6\"",\""uid\"":\""722d7d9d-ca7e-11e7-8e50-5820b101504c\"",\""apiVersion\"":\""extensions\"",\""resourceVersion\"":\""2712127\""}}\nj^\n\nReplicaSet\032\022tserver-79f7f9fbd6\""$722d7d9d-ca7e-11e7-8e50-5820b101504c*\022extensions/v1beta10\0018\001z\000\022\335\004\n$\n\013test-volume\022\025\n\023\n\017/data/hostPath/\022\000\n1\n\023default-token-cq7gh\022\0322\030\n\023default-token-cq7gh\030\244\003\022\220\002\n\007tserver\022Pregistry.cn-shanghai.aliyuncs.com/xxtest/linlintest_server:20171108_1011_582*\0002\016\n\000\020\000\030\204N\""\003TCP*\000B\000J!\n\013test-volume\020\000\032\016/data/myvolume\""\000JH\n\023default-token-cq7gh\020\001\032-/var/run/secrets/kubernetes.io/serviceaccount\""\000j\024/dev/termination-logr\014IfNotPresent\200\001\000\210\001\000\220\001\000\242\001\004File\032\006Always \0362\014ClusterFirstB\007defaultJ\007defaultR\017centos-minion-2X\000`\000h\000r\000z\020\n\016linlinregistry\202\001\000\212\001\000\232\001\021default-scheduler\262\001;\n!node.alpha.kubernetes.io/notReady\022\006Exists\032\000\""\tNoExecute(\254\002\262\001>\n$node.alpha.kubernetes.io/unreachable\022\006Exists\032\000\""\tNoExecute(\254\002\302\001\000\032\202\004\n\007Running\022#\n\013Initialized\022\004True\032\000\""\010\010\315\214\264\320\005\020\000*\0002\000\022\035\n\005Ready\022\004True\032\000\""\010\010\317\214\264\320\005\020\000*\0002\000\022$\n\014PodScheduled\022\004True\032\000\""\010\010\300\214\264\320\005\020\000*\0002\000\032\000\""\000*\017192.168.200.1472\n10.50.2.22:\010\010\315\214\264\320\005\020\000B\325\002\n\007tserver\022\014\022\n\n\010\010\316\214\264\320\005\020\000\032\000 \001(\0002Pregistry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server:20171108_1011_582:\230\001docker-pullable://registry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server@sha256:ddca47c2019bd664d9851cd0230251598f4f07d5189d57ad1c77eec2ff07da09BIdocker://6aa5baa96a48617cdd47230f08c9754ef035897d44354c2b3606842630eb660cJ\nBestEffort\032\000\""\000""
```


and  i notice that the value  been Serialized 
so i try to import k8s runtime api to decode the value , but an error occured

 ## no kind ""Pod"" is registered for version ""v1""




here are my code:

BTW:  tserver is my k8s service created by yaml file
```
package main
import (
	""context""
	""fmt""
	""github.com/coreos/etcd/clientv3""
	""k8s.io/apimachinery/pkg/runtime""
	""k8s.io/apimachinery/pkg/runtime/serializer""
	""os""
	
)

func main() {

//other code

resp, err := cli.Get(ctx, ""/registry/pods/default/tserver"", clientv3.WithPrefix(), clientv3.WithSort(clientv3.SortByKey, clientv3.SortDescend))

var Scheme = runtime.NewScheme()
var Codecs = serializer.NewCodecFactory(Scheme)
decoder := Codecs.UniversalDeserializer()

	for _, kv := range resp.Kvs {

		obj, gvk, err := decoder.Decode(kv.Value, nil, nil)
		if err != nil {

			fmt.Fprintf(os.Stderr, ""WARN: unable to decode key [%s]: %v\n"", kv.Key, err)
			fmt.Println(gvk)
			continue
		}

		fmt.Println(gvk)

	}

}
```

but when  i run this code , it make some error

```
[root@VM-201-26 etcd]# /usr/bin/go run k8sEtcdGet.go 
WARN: unable to decode key [/registry/pods/default/tserver-79f7f9fbd6-hw4pb]: no kind ""Pod"" is registered for version ""v1""
```


if there any solution ? i just what string/readable  pod/service data from etcd V3 

thanks alot 


**Environment**: Centos  7.2.X
- Kubernetes version (use `kubectl version`): v1.8.0
go version:  go1.9.1 linux/amd64
@k8s-mirror-api-machinery-api-reviews ",722d7d9d-ca7e-11e7-8e50-5820b101504c,https://github.com/kubernetes/kubernetes
127,55850.0,"/sig api-machinery

**What happened**:

i  want to get kubernetes  data in etcd v3 via etcd clientV3 , but however, the value of the key is unreadable ,like this 

```
key:""/registry/pods/default/tserver-79f7f9fbd6-hw4pb"" create_revision:2712131 mod_revision:2712145 version:4 value:""k8s\000\n\t\n\002v1\022\003Pod\022\361\014\n\211\004\n\030tserver-79f7f9fbd6-hw4pb\022\023tserver-79f7f9fbd6-\032\007default\""\000*$7238544b-ca7e-11e7-8e50-5820b101504c2\0008\000B\010\010\300\214\264\320\005\020\000Z\016\n\003app\022\007tserverZ\037\n\021pod-template-hash\022\n3593959682b\205\002\n\030kubernetes.io/created-by\022\350\001{\""kind\"":\""SerializedReference\"",\""apiVersion\"":\""v1\"",\""reference\"":{\""kind\"":\""ReplicaSet\"",\""namespace\"":\""default\"",\""name\"":\""tserver-79f7f9fbd6\"",\""uid\"":\""722d7d9d-ca7e-11e7-8e50-5820b101504c\"",\""apiVersion\"":\""extensions\"",\""resourceVersion\"":\""2712127\""}}\nj^\n\nReplicaSet\032\022tserver-79f7f9fbd6\""$722d7d9d-ca7e-11e7-8e50-5820b101504c*\022extensions/v1beta10\0018\001z\000\022\335\004\n$\n\013test-volume\022\025\n\023\n\017/data/hostPath/\022\000\n1\n\023default-token-cq7gh\022\0322\030\n\023default-token-cq7gh\030\244\003\022\220\002\n\007tserver\022Pregistry.cn-shanghai.aliyuncs.com/xxtest/linlintest_server:20171108_1011_582*\0002\016\n\000\020\000\030\204N\""\003TCP*\000B\000J!\n\013test-volume\020\000\032\016/data/myvolume\""\000JH\n\023default-token-cq7gh\020\001\032-/var/run/secrets/kubernetes.io/serviceaccount\""\000j\024/dev/termination-logr\014IfNotPresent\200\001\000\210\001\000\220\001\000\242\001\004File\032\006Always \0362\014ClusterFirstB\007defaultJ\007defaultR\017centos-minion-2X\000`\000h\000r\000z\020\n\016linlinregistry\202\001\000\212\001\000\232\001\021default-scheduler\262\001;\n!node.alpha.kubernetes.io/notReady\022\006Exists\032\000\""\tNoExecute(\254\002\262\001>\n$node.alpha.kubernetes.io/unreachable\022\006Exists\032\000\""\tNoExecute(\254\002\302\001\000\032\202\004\n\007Running\022#\n\013Initialized\022\004True\032\000\""\010\010\315\214\264\320\005\020\000*\0002\000\022\035\n\005Ready\022\004True\032\000\""\010\010\317\214\264\320\005\020\000*\0002\000\022$\n\014PodScheduled\022\004True\032\000\""\010\010\300\214\264\320\005\020\000*\0002\000\032\000\""\000*\017192.168.200.1472\n10.50.2.22:\010\010\315\214\264\320\005\020\000B\325\002\n\007tserver\022\014\022\n\n\010\010\316\214\264\320\005\020\000\032\000 \001(\0002Pregistry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server:20171108_1011_582:\230\001docker-pullable://registry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server@sha256:ddca47c2019bd664d9851cd0230251598f4f07d5189d57ad1c77eec2ff07da09BIdocker://6aa5baa96a48617cdd47230f08c9754ef035897d44354c2b3606842630eb660cJ\nBestEffort\032\000\""\000""
```


and  i notice that the value  been Serialized 
so i try to import k8s runtime api to decode the value , but an error occured

 ## no kind ""Pod"" is registered for version ""v1""




here are my code:

BTW:  tserver is my k8s service created by yaml file
```
package main
import (
	""context""
	""fmt""
	""github.com/coreos/etcd/clientv3""
	""k8s.io/apimachinery/pkg/runtime""
	""k8s.io/apimachinery/pkg/runtime/serializer""
	""os""
	
)

func main() {

//other code

resp, err := cli.Get(ctx, ""/registry/pods/default/tserver"", clientv3.WithPrefix(), clientv3.WithSort(clientv3.SortByKey, clientv3.SortDescend))

var Scheme = runtime.NewScheme()
var Codecs = serializer.NewCodecFactory(Scheme)
decoder := Codecs.UniversalDeserializer()

	for _, kv := range resp.Kvs {

		obj, gvk, err := decoder.Decode(kv.Value, nil, nil)
		if err != nil {

			fmt.Fprintf(os.Stderr, ""WARN: unable to decode key [%s]: %v\n"", kv.Key, err)
			fmt.Println(gvk)
			continue
		}

		fmt.Println(gvk)

	}

}
```

but when  i run this code , it make some error

```
[root@VM-201-26 etcd]# /usr/bin/go run k8sEtcdGet.go 
WARN: unable to decode key [/registry/pods/default/tserver-79f7f9fbd6-hw4pb]: no kind ""Pod"" is registered for version ""v1""
```


if there any solution ? i just what string/readable  pod/service data from etcd V3 

thanks alot 


**Environment**: Centos  7.2.X
- Kubernetes version (use `kubectl version`): v1.8.0
go version:  go1.9.1 linux/amd64
@k8s-mirror-api-machinery-api-reviews ",cq7gh\030\244\003\022\220\002\n\007tserver\-*\0002\016\n\000\020,https://github.com/kubernetes/kubernetes
128,55850.0,"/sig api-machinery

**What happened**:

i  want to get kubernetes  data in etcd v3 via etcd clientV3 , but however, the value of the key is unreadable ,like this 

```
key:""/registry/pods/default/tserver-79f7f9fbd6-hw4pb"" create_revision:2712131 mod_revision:2712145 version:4 value:""k8s\000\n\t\n\002v1\022\003Pod\022\361\014\n\211\004\n\030tserver-79f7f9fbd6-hw4pb\022\023tserver-79f7f9fbd6-\032\007default\""\000*$7238544b-ca7e-11e7-8e50-5820b101504c2\0008\000B\010\010\300\214\264\320\005\020\000Z\016\n\003app\022\007tserverZ\037\n\021pod-template-hash\022\n3593959682b\205\002\n\030kubernetes.io/created-by\022\350\001{\""kind\"":\""SerializedReference\"",\""apiVersion\"":\""v1\"",\""reference\"":{\""kind\"":\""ReplicaSet\"",\""namespace\"":\""default\"",\""name\"":\""tserver-79f7f9fbd6\"",\""uid\"":\""722d7d9d-ca7e-11e7-8e50-5820b101504c\"",\""apiVersion\"":\""extensions\"",\""resourceVersion\"":\""2712127\""}}\nj^\n\nReplicaSet\032\022tserver-79f7f9fbd6\""$722d7d9d-ca7e-11e7-8e50-5820b101504c*\022extensions/v1beta10\0018\001z\000\022\335\004\n$\n\013test-volume\022\025\n\023\n\017/data/hostPath/\022\000\n1\n\023default-token-cq7gh\022\0322\030\n\023default-token-cq7gh\030\244\003\022\220\002\n\007tserver\022Pregistry.cn-shanghai.aliyuncs.com/xxtest/linlintest_server:20171108_1011_582*\0002\016\n\000\020\000\030\204N\""\003TCP*\000B\000J!\n\013test-volume\020\000\032\016/data/myvolume\""\000JH\n\023default-token-cq7gh\020\001\032-/var/run/secrets/kubernetes.io/serviceaccount\""\000j\024/dev/termination-logr\014IfNotPresent\200\001\000\210\001\000\220\001\000\242\001\004File\032\006Always \0362\014ClusterFirstB\007defaultJ\007defaultR\017centos-minion-2X\000`\000h\000r\000z\020\n\016linlinregistry\202\001\000\212\001\000\232\001\021default-scheduler\262\001;\n!node.alpha.kubernetes.io/notReady\022\006Exists\032\000\""\tNoExecute(\254\002\262\001>\n$node.alpha.kubernetes.io/unreachable\022\006Exists\032\000\""\tNoExecute(\254\002\302\001\000\032\202\004\n\007Running\022#\n\013Initialized\022\004True\032\000\""\010\010\315\214\264\320\005\020\000*\0002\000\022\035\n\005Ready\022\004True\032\000\""\010\010\317\214\264\320\005\020\000*\0002\000\022$\n\014PodScheduled\022\004True\032\000\""\010\010\300\214\264\320\005\020\000*\0002\000\032\000\""\000*\017192.168.200.1472\n10.50.2.22:\010\010\315\214\264\320\005\020\000B\325\002\n\007tserver\022\014\022\n\n\010\010\316\214\264\320\005\020\000\032\000 \001(\0002Pregistry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server:20171108_1011_582:\230\001docker-pullable://registry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server@sha256:ddca47c2019bd664d9851cd0230251598f4f07d5189d57ad1c77eec2ff07da09BIdocker://6aa5baa96a48617cdd47230f08c9754ef035897d44354c2b3606842630eb660cJ\nBestEffort\032\000\""\000""
```


and  i notice that the value  been Serialized 
so i try to import k8s runtime api to decode the value , but an error occured

 ## no kind ""Pod"" is registered for version ""v1""




here are my code:

BTW:  tserver is my k8s service created by yaml file
```
package main
import (
	""context""
	""fmt""
	""github.com/coreos/etcd/clientv3""
	""k8s.io/apimachinery/pkg/runtime""
	""k8s.io/apimachinery/pkg/runtime/serializer""
	""os""
	
)

func main() {

//other code

resp, err := cli.Get(ctx, ""/registry/pods/default/tserver"", clientv3.WithPrefix(), clientv3.WithSort(clientv3.SortByKey, clientv3.SortDescend))

var Scheme = runtime.NewScheme()
var Codecs = serializer.NewCodecFactory(Scheme)
decoder := Codecs.UniversalDeserializer()

	for _, kv := range resp.Kvs {

		obj, gvk, err := decoder.Decode(kv.Value, nil, nil)
		if err != nil {

			fmt.Fprintf(os.Stderr, ""WARN: unable to decode key [%s]: %v\n"", kv.Key, err)
			fmt.Println(gvk)
			continue
		}

		fmt.Println(gvk)

	}

}
```

but when  i run this code , it make some error

```
[root@VM-201-26 etcd]# /usr/bin/go run k8sEtcdGet.go 
WARN: unable to decode key [/registry/pods/default/tserver-79f7f9fbd6-hw4pb]: no kind ""Pod"" is registered for version ""v1""
```


if there any solution ? i just what string/readable  pod/service data from etcd V3 

thanks alot 


**Environment**: Centos  7.2.X
- Kubernetes version (use `kubectl version`): v1.8.0
go version:  go1.9.1 linux/amd64
@k8s-mirror-api-machinery-api-reviews ",000\n\t\n\002v1\022\003Pod\022\361\014\n\211\004\n\030tserver-,https://github.com/kubernetes/kubernetes
129,55850.0,"/sig api-machinery

**What happened**:

i  want to get kubernetes  data in etcd v3 via etcd clientV3 , but however, the value of the key is unreadable ,like this 

```
key:""/registry/pods/default/tserver-79f7f9fbd6-hw4pb"" create_revision:2712131 mod_revision:2712145 version:4 value:""k8s\000\n\t\n\002v1\022\003Pod\022\361\014\n\211\004\n\030tserver-79f7f9fbd6-hw4pb\022\023tserver-79f7f9fbd6-\032\007default\""\000*$7238544b-ca7e-11e7-8e50-5820b101504c2\0008\000B\010\010\300\214\264\320\005\020\000Z\016\n\003app\022\007tserverZ\037\n\021pod-template-hash\022\n3593959682b\205\002\n\030kubernetes.io/created-by\022\350\001{\""kind\"":\""SerializedReference\"",\""apiVersion\"":\""v1\"",\""reference\"":{\""kind\"":\""ReplicaSet\"",\""namespace\"":\""default\"",\""name\"":\""tserver-79f7f9fbd6\"",\""uid\"":\""722d7d9d-ca7e-11e7-8e50-5820b101504c\"",\""apiVersion\"":\""extensions\"",\""resourceVersion\"":\""2712127\""}}\nj^\n\nReplicaSet\032\022tserver-79f7f9fbd6\""$722d7d9d-ca7e-11e7-8e50-5820b101504c*\022extensions/v1beta10\0018\001z\000\022\335\004\n$\n\013test-volume\022\025\n\023\n\017/data/hostPath/\022\000\n1\n\023default-token-cq7gh\022\0322\030\n\023default-token-cq7gh\030\244\003\022\220\002\n\007tserver\022Pregistry.cn-shanghai.aliyuncs.com/xxtest/linlintest_server:20171108_1011_582*\0002\016\n\000\020\000\030\204N\""\003TCP*\000B\000J!\n\013test-volume\020\000\032\016/data/myvolume\""\000JH\n\023default-token-cq7gh\020\001\032-/var/run/secrets/kubernetes.io/serviceaccount\""\000j\024/dev/termination-logr\014IfNotPresent\200\001\000\210\001\000\220\001\000\242\001\004File\032\006Always \0362\014ClusterFirstB\007defaultJ\007defaultR\017centos-minion-2X\000`\000h\000r\000z\020\n\016linlinregistry\202\001\000\212\001\000\232\001\021default-scheduler\262\001;\n!node.alpha.kubernetes.io/notReady\022\006Exists\032\000\""\tNoExecute(\254\002\262\001>\n$node.alpha.kubernetes.io/unreachable\022\006Exists\032\000\""\tNoExecute(\254\002\302\001\000\032\202\004\n\007Running\022#\n\013Initialized\022\004True\032\000\""\010\010\315\214\264\320\005\020\000*\0002\000\022\035\n\005Ready\022\004True\032\000\""\010\010\317\214\264\320\005\020\000*\0002\000\022$\n\014PodScheduled\022\004True\032\000\""\010\010\300\214\264\320\005\020\000*\0002\000\032\000\""\000*\017192.168.200.1472\n10.50.2.22:\010\010\315\214\264\320\005\020\000B\325\002\n\007tserver\022\014\022\n\n\010\010\316\214\264\320\005\020\000\032\000 \001(\0002Pregistry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server:20171108_1011_582:\230\001docker-pullable://registry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server@sha256:ddca47c2019bd664d9851cd0230251598f4f07d5189d57ad1c77eec2ff07da09BIdocker://6aa5baa96a48617cdd47230f08c9754ef035897d44354c2b3606842630eb660cJ\nBestEffort\032\000\""\000""
```


and  i notice that the value  been Serialized 
so i try to import k8s runtime api to decode the value , but an error occured

 ## no kind ""Pod"" is registered for version ""v1""




here are my code:

BTW:  tserver is my k8s service created by yaml file
```
package main
import (
	""context""
	""fmt""
	""github.com/coreos/etcd/clientv3""
	""k8s.io/apimachinery/pkg/runtime""
	""k8s.io/apimachinery/pkg/runtime/serializer""
	""os""
	
)

func main() {

//other code

resp, err := cli.Get(ctx, ""/registry/pods/default/tserver"", clientv3.WithPrefix(), clientv3.WithSort(clientv3.SortByKey, clientv3.SortDescend))

var Scheme = runtime.NewScheme()
var Codecs = serializer.NewCodecFactory(Scheme)
decoder := Codecs.UniversalDeserializer()

	for _, kv := range resp.Kvs {

		obj, gvk, err := decoder.Decode(kv.Value, nil, nil)
		if err != nil {

			fmt.Fprintf(os.Stderr, ""WARN: unable to decode key [%s]: %v\n"", kv.Key, err)
			fmt.Println(gvk)
			continue
		}

		fmt.Println(gvk)

	}

}
```

but when  i run this code , it make some error

```
[root@VM-201-26 etcd]# /usr/bin/go run k8sEtcdGet.go 
WARN: unable to decode key [/registry/pods/default/tserver-79f7f9fbd6-hw4pb]: no kind ""Pod"" is registered for version ""v1""
```


if there any solution ? i just what string/readable  pod/service data from etcd V3 

thanks alot 


**Environment**: Centos  7.2.X
- Kubernetes version (use `kubectl version`): v1.8.0
go version:  go1.9.1 linux/amd64
@k8s-mirror-api-machinery-api-reviews ",014IfNotPresent\200\001\000\210\001\000\220\001\000\242\001\,https://github.com/kubernetes/kubernetes
130,55850.0,"/sig api-machinery

**What happened**:

i  want to get kubernetes  data in etcd v3 via etcd clientV3 , but however, the value of the key is unreadable ,like this 

```
key:""/registry/pods/default/tserver-79f7f9fbd6-hw4pb"" create_revision:2712131 mod_revision:2712145 version:4 value:""k8s\000\n\t\n\002v1\022\003Pod\022\361\014\n\211\004\n\030tserver-79f7f9fbd6-hw4pb\022\023tserver-79f7f9fbd6-\032\007default\""\000*$7238544b-ca7e-11e7-8e50-5820b101504c2\0008\000B\010\010\300\214\264\320\005\020\000Z\016\n\003app\022\007tserverZ\037\n\021pod-template-hash\022\n3593959682b\205\002\n\030kubernetes.io/created-by\022\350\001{\""kind\"":\""SerializedReference\"",\""apiVersion\"":\""v1\"",\""reference\"":{\""kind\"":\""ReplicaSet\"",\""namespace\"":\""default\"",\""name\"":\""tserver-79f7f9fbd6\"",\""uid\"":\""722d7d9d-ca7e-11e7-8e50-5820b101504c\"",\""apiVersion\"":\""extensions\"",\""resourceVersion\"":\""2712127\""}}\nj^\n\nReplicaSet\032\022tserver-79f7f9fbd6\""$722d7d9d-ca7e-11e7-8e50-5820b101504c*\022extensions/v1beta10\0018\001z\000\022\335\004\n$\n\013test-volume\022\025\n\023\n\017/data/hostPath/\022\000\n1\n\023default-token-cq7gh\022\0322\030\n\023default-token-cq7gh\030\244\003\022\220\002\n\007tserver\022Pregistry.cn-shanghai.aliyuncs.com/xxtest/linlintest_server:20171108_1011_582*\0002\016\n\000\020\000\030\204N\""\003TCP*\000B\000J!\n\013test-volume\020\000\032\016/data/myvolume\""\000JH\n\023default-token-cq7gh\020\001\032-/var/run/secrets/kubernetes.io/serviceaccount\""\000j\024/dev/termination-logr\014IfNotPresent\200\001\000\210\001\000\220\001\000\242\001\004File\032\006Always \0362\014ClusterFirstB\007defaultJ\007defaultR\017centos-minion-2X\000`\000h\000r\000z\020\n\016linlinregistry\202\001\000\212\001\000\232\001\021default-scheduler\262\001;\n!node.alpha.kubernetes.io/notReady\022\006Exists\032\000\""\tNoExecute(\254\002\262\001>\n$node.alpha.kubernetes.io/unreachable\022\006Exists\032\000\""\tNoExecute(\254\002\302\001\000\032\202\004\n\007Running\022#\n\013Initialized\022\004True\032\000\""\010\010\315\214\264\320\005\020\000*\0002\000\022\035\n\005Ready\022\004True\032\000\""\010\010\317\214\264\320\005\020\000*\0002\000\022$\n\014PodScheduled\022\004True\032\000\""\010\010\300\214\264\320\005\020\000*\0002\000\032\000\""\000*\017192.168.200.1472\n10.50.2.22:\010\010\315\214\264\320\005\020\000B\325\002\n\007tserver\022\014\022\n\n\010\010\316\214\264\320\005\020\000\032\000 \001(\0002Pregistry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server:20171108_1011_582:\230\001docker-pullable://registry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server@sha256:ddca47c2019bd664d9851cd0230251598f4f07d5189d57ad1c77eec2ff07da09BIdocker://6aa5baa96a48617cdd47230f08c9754ef035897d44354c2b3606842630eb660cJ\nBestEffort\032\000\""\000""
```


and  i notice that the value  been Serialized 
so i try to import k8s runtime api to decode the value , but an error occured

 ## no kind ""Pod"" is registered for version ""v1""




here are my code:

BTW:  tserver is my k8s service created by yaml file
```
package main
import (
	""context""
	""fmt""
	""github.com/coreos/etcd/clientv3""
	""k8s.io/apimachinery/pkg/runtime""
	""k8s.io/apimachinery/pkg/runtime/serializer""
	""os""
	
)

func main() {

//other code

resp, err := cli.Get(ctx, ""/registry/pods/default/tserver"", clientv3.WithPrefix(), clientv3.WithSort(clientv3.SortByKey, clientv3.SortDescend))

var Scheme = runtime.NewScheme()
var Codecs = serializer.NewCodecFactory(Scheme)
decoder := Codecs.UniversalDeserializer()

	for _, kv := range resp.Kvs {

		obj, gvk, err := decoder.Decode(kv.Value, nil, nil)
		if err != nil {

			fmt.Fprintf(os.Stderr, ""WARN: unable to decode key [%s]: %v\n"", kv.Key, err)
			fmt.Println(gvk)
			continue
		}

		fmt.Println(gvk)

	}

}
```

but when  i run this code , it make some error

```
[root@VM-201-26 etcd]# /usr/bin/go run k8sEtcdGet.go 
WARN: unable to decode key [/registry/pods/default/tserver-79f7f9fbd6-hw4pb]: no kind ""Pod"" is registered for version ""v1""
```


if there any solution ? i just what string/readable  pod/service data from etcd V3 

thanks alot 


**Environment**: Centos  7.2.X
- Kubernetes version (use `kubectl version`): v1.8.0
go version:  go1.9.1 linux/amd64
@k8s-mirror-api-machinery-api-reviews ",\\010\010\300\214\264\320\,https://github.com/kubernetes/kubernetes
131,55850.0,"/sig api-machinery

**What happened**:

i  want to get kubernetes  data in etcd v3 via etcd clientV3 , but however, the value of the key is unreadable ,like this 

```
key:""/registry/pods/default/tserver-79f7f9fbd6-hw4pb"" create_revision:2712131 mod_revision:2712145 version:4 value:""k8s\000\n\t\n\002v1\022\003Pod\022\361\014\n\211\004\n\030tserver-79f7f9fbd6-hw4pb\022\023tserver-79f7f9fbd6-\032\007default\""\000*$7238544b-ca7e-11e7-8e50-5820b101504c2\0008\000B\010\010\300\214\264\320\005\020\000Z\016\n\003app\022\007tserverZ\037\n\021pod-template-hash\022\n3593959682b\205\002\n\030kubernetes.io/created-by\022\350\001{\""kind\"":\""SerializedReference\"",\""apiVersion\"":\""v1\"",\""reference\"":{\""kind\"":\""ReplicaSet\"",\""namespace\"":\""default\"",\""name\"":\""tserver-79f7f9fbd6\"",\""uid\"":\""722d7d9d-ca7e-11e7-8e50-5820b101504c\"",\""apiVersion\"":\""extensions\"",\""resourceVersion\"":\""2712127\""}}\nj^\n\nReplicaSet\032\022tserver-79f7f9fbd6\""$722d7d9d-ca7e-11e7-8e50-5820b101504c*\022extensions/v1beta10\0018\001z\000\022\335\004\n$\n\013test-volume\022\025\n\023\n\017/data/hostPath/\022\000\n1\n\023default-token-cq7gh\022\0322\030\n\023default-token-cq7gh\030\244\003\022\220\002\n\007tserver\022Pregistry.cn-shanghai.aliyuncs.com/xxtest/linlintest_server:20171108_1011_582*\0002\016\n\000\020\000\030\204N\""\003TCP*\000B\000J!\n\013test-volume\020\000\032\016/data/myvolume\""\000JH\n\023default-token-cq7gh\020\001\032-/var/run/secrets/kubernetes.io/serviceaccount\""\000j\024/dev/termination-logr\014IfNotPresent\200\001\000\210\001\000\220\001\000\242\001\004File\032\006Always \0362\014ClusterFirstB\007defaultJ\007defaultR\017centos-minion-2X\000`\000h\000r\000z\020\n\016linlinregistry\202\001\000\212\001\000\232\001\021default-scheduler\262\001;\n!node.alpha.kubernetes.io/notReady\022\006Exists\032\000\""\tNoExecute(\254\002\262\001>\n$node.alpha.kubernetes.io/unreachable\022\006Exists\032\000\""\tNoExecute(\254\002\302\001\000\032\202\004\n\007Running\022#\n\013Initialized\022\004True\032\000\""\010\010\315\214\264\320\005\020\000*\0002\000\022\035\n\005Ready\022\004True\032\000\""\010\010\317\214\264\320\005\020\000*\0002\000\022$\n\014PodScheduled\022\004True\032\000\""\010\010\300\214\264\320\005\020\000*\0002\000\032\000\""\000*\017192.168.200.1472\n10.50.2.22:\010\010\315\214\264\320\005\020\000B\325\002\n\007tserver\022\014\022\n\n\010\010\316\214\264\320\005\020\000\032\000 \001(\0002Pregistry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server:20171108_1011_582:\230\001docker-pullable://registry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server@sha256:ddca47c2019bd664d9851cd0230251598f4f07d5189d57ad1c77eec2ff07da09BIdocker://6aa5baa96a48617cdd47230f08c9754ef035897d44354c2b3606842630eb660cJ\nBestEffort\032\000\""\000""
```


and  i notice that the value  been Serialized 
so i try to import k8s runtime api to decode the value , but an error occured

 ## no kind ""Pod"" is registered for version ""v1""




here are my code:

BTW:  tserver is my k8s service created by yaml file
```
package main
import (
	""context""
	""fmt""
	""github.com/coreos/etcd/clientv3""
	""k8s.io/apimachinery/pkg/runtime""
	""k8s.io/apimachinery/pkg/runtime/serializer""
	""os""
	
)

func main() {

//other code

resp, err := cli.Get(ctx, ""/registry/pods/default/tserver"", clientv3.WithPrefix(), clientv3.WithSort(clientv3.SortByKey, clientv3.SortDescend))

var Scheme = runtime.NewScheme()
var Codecs = serializer.NewCodecFactory(Scheme)
decoder := Codecs.UniversalDeserializer()

	for _, kv := range resp.Kvs {

		obj, gvk, err := decoder.Decode(kv.Value, nil, nil)
		if err != nil {

			fmt.Fprintf(os.Stderr, ""WARN: unable to decode key [%s]: %v\n"", kv.Key, err)
			fmt.Println(gvk)
			continue
		}

		fmt.Println(gvk)

	}

}
```

but when  i run this code , it make some error

```
[root@VM-201-26 etcd]# /usr/bin/go run k8sEtcdGet.go 
WARN: unable to decode key [/registry/pods/default/tserver-79f7f9fbd6-hw4pb]: no kind ""Pod"" is registered for version ""v1""
```


if there any solution ? i just what string/readable  pod/service data from etcd V3 

thanks alot 


**Environment**: Centos  7.2.X
- Kubernetes version (use `kubectl version`): v1.8.0
go version:  go1.9.1 linux/amd64
@k8s-mirror-api-machinery-api-reviews ",\\010\010\315\214\264\320\,https://github.com/kubernetes/kubernetes
132,55850.0,"/sig api-machinery

**What happened**:

i  want to get kubernetes  data in etcd v3 via etcd clientV3 , but however, the value of the key is unreadable ,like this 

```
key:""/registry/pods/default/tserver-79f7f9fbd6-hw4pb"" create_revision:2712131 mod_revision:2712145 version:4 value:""k8s\000\n\t\n\002v1\022\003Pod\022\361\014\n\211\004\n\030tserver-79f7f9fbd6-hw4pb\022\023tserver-79f7f9fbd6-\032\007default\""\000*$7238544b-ca7e-11e7-8e50-5820b101504c2\0008\000B\010\010\300\214\264\320\005\020\000Z\016\n\003app\022\007tserverZ\037\n\021pod-template-hash\022\n3593959682b\205\002\n\030kubernetes.io/created-by\022\350\001{\""kind\"":\""SerializedReference\"",\""apiVersion\"":\""v1\"",\""reference\"":{\""kind\"":\""ReplicaSet\"",\""namespace\"":\""default\"",\""name\"":\""tserver-79f7f9fbd6\"",\""uid\"":\""722d7d9d-ca7e-11e7-8e50-5820b101504c\"",\""apiVersion\"":\""extensions\"",\""resourceVersion\"":\""2712127\""}}\nj^\n\nReplicaSet\032\022tserver-79f7f9fbd6\""$722d7d9d-ca7e-11e7-8e50-5820b101504c*\022extensions/v1beta10\0018\001z\000\022\335\004\n$\n\013test-volume\022\025\n\023\n\017/data/hostPath/\022\000\n1\n\023default-token-cq7gh\022\0322\030\n\023default-token-cq7gh\030\244\003\022\220\002\n\007tserver\022Pregistry.cn-shanghai.aliyuncs.com/xxtest/linlintest_server:20171108_1011_582*\0002\016\n\000\020\000\030\204N\""\003TCP*\000B\000J!\n\013test-volume\020\000\032\016/data/myvolume\""\000JH\n\023default-token-cq7gh\020\001\032-/var/run/secrets/kubernetes.io/serviceaccount\""\000j\024/dev/termination-logr\014IfNotPresent\200\001\000\210\001\000\220\001\000\242\001\004File\032\006Always \0362\014ClusterFirstB\007defaultJ\007defaultR\017centos-minion-2X\000`\000h\000r\000z\020\n\016linlinregistry\202\001\000\212\001\000\232\001\021default-scheduler\262\001;\n!node.alpha.kubernetes.io/notReady\022\006Exists\032\000\""\tNoExecute(\254\002\262\001>\n$node.alpha.kubernetes.io/unreachable\022\006Exists\032\000\""\tNoExecute(\254\002\302\001\000\032\202\004\n\007Running\022#\n\013Initialized\022\004True\032\000\""\010\010\315\214\264\320\005\020\000*\0002\000\022\035\n\005Ready\022\004True\032\000\""\010\010\317\214\264\320\005\020\000*\0002\000\022$\n\014PodScheduled\022\004True\032\000\""\010\010\300\214\264\320\005\020\000*\0002\000\032\000\""\000*\017192.168.200.1472\n10.50.2.22:\010\010\315\214\264\320\005\020\000B\325\002\n\007tserver\022\014\022\n\n\010\010\316\214\264\320\005\020\000\032\000 \001(\0002Pregistry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server:20171108_1011_582:\230\001docker-pullable://registry.cn-shanghai.aliyuncs.com/boyaa-test/linlintest_server@sha256:ddca47c2019bd664d9851cd0230251598f4f07d5189d57ad1c77eec2ff07da09BIdocker://6aa5baa96a48617cdd47230f08c9754ef035897d44354c2b3606842630eb660cJ\nBestEffort\032\000\""\000""
```


and  i notice that the value  been Serialized 
so i try to import k8s runtime api to decode the value , but an error occured

 ## no kind ""Pod"" is registered for version ""v1""




here are my code:

BTW:  tserver is my k8s service created by yaml file
```
package main
import (
	""context""
	""fmt""
	""github.com/coreos/etcd/clientv3""
	""k8s.io/apimachinery/pkg/runtime""
	""k8s.io/apimachinery/pkg/runtime/serializer""
	""os""
	
)

func main() {

//other code

resp, err := cli.Get(ctx, ""/registry/pods/default/tserver"", clientv3.WithPrefix(), clientv3.WithSort(clientv3.SortByKey, clientv3.SortDescend))

var Scheme = runtime.NewScheme()
var Codecs = serializer.NewCodecFactory(Scheme)
decoder := Codecs.UniversalDeserializer()

	for _, kv := range resp.Kvs {

		obj, gvk, err := decoder.Decode(kv.Value, nil, nil)
		if err != nil {

			fmt.Fprintf(os.Stderr, ""WARN: unable to decode key [%s]: %v\n"", kv.Key, err)
			fmt.Println(gvk)
			continue
		}

		fmt.Println(gvk)

	}

}
```

but when  i run this code , it make some error

```
[root@VM-201-26 etcd]# /usr/bin/go run k8sEtcdGet.go 
WARN: unable to decode key [/registry/pods/default/tserver-79f7f9fbd6-hw4pb]: no kind ""Pod"" is registered for version ""v1""
```


if there any solution ? i just what string/readable  pod/service data from etcd V3 

thanks alot 


**Environment**: Centos  7.2.X
- Kubernetes version (use `kubectl version`): v1.8.0
go version:  go1.9.1 linux/amd64
@k8s-mirror-api-machinery-api-reviews ",\\010\010\317\214\264\320\,https://github.com/kubernetes/kubernetes
133,54941.0,"/kind bug
/sig storage

## **What happened**:
**logs of controller-manager**
W1101 14:05:30.134407       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134459       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134507       7 rbd_util.go:364] failed to create rbd image, output
E1101 14:05:30.134529       7 rbd.go:323] rbd: create volume failed, err: failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:
I1101 14:05:30.134553       7 pv_controller.go:1331] failed to provision volume for claim ""default/claim5"" with StorageClass ""fast"": failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:

## What you expected to happen:
rbd executed successfully to create a volume on the ceph pool 'kube'

## How to reproduce it (as minimally and precisely as possible):
**ceph config**
ceph admin key = AQAMZ/hZheYqAxAAURdTDYs4GPc378Vp0PLeqA==
ceph kube key = AQDcffhZ1i3HMxAA5oB+SuIbzSKc+9FDc5tKeQ==
ceph --cluster ceph auth get-or-create client.kube mon 'allow r' osd 'allow rwx pool=kube'

ceph-secrets for users (admin, kube) with base64 encoded key
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: QVFBTVovaFpoZVlxQXhBQVVSZFREWXM0R1BjMzc4VnAwUExlcUE9PQ==
---
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-kube
data:
  key: QVFEY2ZmaFoxaTNITXhBQTVvQitTdUlielNLYys5RkRjNXRLZVE9PQ==

**Storageclass 'fast'**
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 1:6789,2:6789,3:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: ""kube-system""
  pool: kube
  userId: kube
  userSecretName: ceph-secret-kube

**PersistentStorageClaim**
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim5
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
  storageClassName: fast

**Executing 'rbd' inside of controller-manager works:**
$ kubectl exec -ti kube-controller-manager-st14d02ls-ztda11350501 -n kube-system rbd
   
2017-11-01 14:10:26.664691 7febc9f05780 -1 did not load config file, using default settings.
rbd: you must specify a command.
**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T07:00:21Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.5+coreos.0"", GitCommit:""070d238cd2ec359928548e486a9171b498573181"", GitTreeState:""clean"", BuildDate:""2017-08-31T21:28:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
. 3 masters / api servers

- OS (e.g. from /etc/os-release):
10x centos 7.3 nodes

- Kernel (e.g. `uname -a`):
4.9.44, custom kernel build.

- Install tools:
Kubespray

- Others:
docker 1.13
hyperkube 1.7.5
ceph version 0.94.10 (b1e0532418e4631af01acbc0cedd426f1905f4af
",b1e0532418e4631af01acbc0cedd426f1905f4af,https://github.com/kubernetes/kubernetes
134,54941.0,"/kind bug
/sig storage

## **What happened**:
**logs of controller-manager**
W1101 14:05:30.134407       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134459       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134507       7 rbd_util.go:364] failed to create rbd image, output
E1101 14:05:30.134529       7 rbd.go:323] rbd: create volume failed, err: failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:
I1101 14:05:30.134553       7 pv_controller.go:1331] failed to provision volume for claim ""default/claim5"" with StorageClass ""fast"": failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:

## What you expected to happen:
rbd executed successfully to create a volume on the ceph pool 'kube'

## How to reproduce it (as minimally and precisely as possible):
**ceph config**
ceph admin key = AQAMZ/hZheYqAxAAURdTDYs4GPc378Vp0PLeqA==
ceph kube key = AQDcffhZ1i3HMxAA5oB+SuIbzSKc+9FDc5tKeQ==
ceph --cluster ceph auth get-or-create client.kube mon 'allow r' osd 'allow rwx pool=kube'

ceph-secrets for users (admin, kube) with base64 encoded key
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: QVFBTVovaFpoZVlxQXhBQVVSZFREWXM0R1BjMzc4VnAwUExlcUE9PQ==
---
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-kube
data:
  key: QVFEY2ZmaFoxaTNITXhBQTVvQitTdUlielNLYys5RkRjNXRLZVE9PQ==

**Storageclass 'fast'**
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 1:6789,2:6789,3:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: ""kube-system""
  pool: kube
  userId: kube
  userSecretName: ceph-secret-kube

**PersistentStorageClaim**
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim5
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
  storageClassName: fast

**Executing 'rbd' inside of controller-manager works:**
$ kubectl exec -ti kube-controller-manager-st14d02ls-ztda11350501 -n kube-system rbd
   
2017-11-01 14:10:26.664691 7febc9f05780 -1 did not load config file, using default settings.
rbd: you must specify a command.
**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T07:00:21Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.5+coreos.0"", GitCommit:""070d238cd2ec359928548e486a9171b498573181"", GitTreeState:""clean"", BuildDate:""2017-08-31T21:28:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
. 3 masters / api servers

- OS (e.g. from /etc/os-release):
10x centos 7.3 nodes

- Kernel (e.g. `uname -a`):
4.9.44, custom kernel build.

- Install tools:
Kubespray

- Others:
docker 1.13
hyperkube 1.7.5
ceph version 0.94.10 (b1e0532418e4631af01acbc0cedd426f1905f4af
",QVFEY2ZmaFoxaTNITXhBQTVvQitTdUlielNLYys5RkRjNXRLZVE9PQ,https://github.com/kubernetes/kubernetes
135,54941.0,"/kind bug
/sig storage

## **What happened**:
**logs of controller-manager**
W1101 14:05:30.134407       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134459       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134507       7 rbd_util.go:364] failed to create rbd image, output
E1101 14:05:30.134529       7 rbd.go:323] rbd: create volume failed, err: failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:
I1101 14:05:30.134553       7 pv_controller.go:1331] failed to provision volume for claim ""default/claim5"" with StorageClass ""fast"": failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:

## What you expected to happen:
rbd executed successfully to create a volume on the ceph pool 'kube'

## How to reproduce it (as minimally and precisely as possible):
**ceph config**
ceph admin key = AQAMZ/hZheYqAxAAURdTDYs4GPc378Vp0PLeqA==
ceph kube key = AQDcffhZ1i3HMxAA5oB+SuIbzSKc+9FDc5tKeQ==
ceph --cluster ceph auth get-or-create client.kube mon 'allow r' osd 'allow rwx pool=kube'

ceph-secrets for users (admin, kube) with base64 encoded key
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: QVFBTVovaFpoZVlxQXhBQVVSZFREWXM0R1BjMzc4VnAwUExlcUE9PQ==
---
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-kube
data:
  key: QVFEY2ZmaFoxaTNITXhBQTVvQitTdUlielNLYys5RkRjNXRLZVE9PQ==

**Storageclass 'fast'**
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 1:6789,2:6789,3:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: ""kube-system""
  pool: kube
  userId: kube
  userSecretName: ceph-secret-kube

**PersistentStorageClaim**
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim5
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
  storageClassName: fast

**Executing 'rbd' inside of controller-manager works:**
$ kubectl exec -ti kube-controller-manager-st14d02ls-ztda11350501 -n kube-system rbd
   
2017-11-01 14:10:26.664691 7febc9f05780 -1 did not load config file, using default settings.
rbd: you must specify a command.
**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T07:00:21Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.5+coreos.0"", GitCommit:""070d238cd2ec359928548e486a9171b498573181"", GitTreeState:""clean"", BuildDate:""2017-08-31T21:28:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
. 3 masters / api servers

- OS (e.g. from /etc/os-release):
10x centos 7.3 nodes

- Kernel (e.g. `uname -a`):
4.9.44, custom kernel build.

- Install tools:
Kubespray

- Others:
docker 1.13
hyperkube 1.7.5
ceph version 0.94.10 (b1e0532418e4631af01acbc0cedd426f1905f4af
",adminSecretNamespace,https://github.com/kubernetes/kubernetes
136,54941.0,"/kind bug
/sig storage

## **What happened**:
**logs of controller-manager**
W1101 14:05:30.134407       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134459       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134507       7 rbd_util.go:364] failed to create rbd image, output
E1101 14:05:30.134529       7 rbd.go:323] rbd: create volume failed, err: failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:
I1101 14:05:30.134553       7 pv_controller.go:1331] failed to provision volume for claim ""default/claim5"" with StorageClass ""fast"": failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:

## What you expected to happen:
rbd executed successfully to create a volume on the ceph pool 'kube'

## How to reproduce it (as minimally and precisely as possible):
**ceph config**
ceph admin key = AQAMZ/hZheYqAxAAURdTDYs4GPc378Vp0PLeqA==
ceph kube key = AQDcffhZ1i3HMxAA5oB+SuIbzSKc+9FDc5tKeQ==
ceph --cluster ceph auth get-or-create client.kube mon 'allow r' osd 'allow rwx pool=kube'

ceph-secrets for users (admin, kube) with base64 encoded key
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: QVFBTVovaFpoZVlxQXhBQVVSZFREWXM0R1BjMzc4VnAwUExlcUE9PQ==
---
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-kube
data:
  key: QVFEY2ZmaFoxaTNITXhBQTVvQitTdUlielNLYys5RkRjNXRLZVE9PQ==

**Storageclass 'fast'**
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 1:6789,2:6789,3:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: ""kube-system""
  pool: kube
  userId: kube
  userSecretName: ceph-secret-kube

**PersistentStorageClaim**
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim5
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
  storageClassName: fast

**Executing 'rbd' inside of controller-manager works:**
$ kubectl exec -ti kube-controller-manager-st14d02ls-ztda11350501 -n kube-system rbd
   
2017-11-01 14:10:26.664691 7febc9f05780 -1 did not load config file, using default settings.
rbd: you must specify a command.
**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T07:00:21Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.5+coreos.0"", GitCommit:""070d238cd2ec359928548e486a9171b498573181"", GitTreeState:""clean"", BuildDate:""2017-08-31T21:28:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
. 3 masters / api servers

- OS (e.g. from /etc/os-release):
10x centos 7.3 nodes

- Kernel (e.g. `uname -a`):
4.9.44, custom kernel build.

- Install tools:
Kubespray

- Others:
docker 1.13
hyperkube 1.7.5
ceph version 0.94.10 (b1e0532418e4631af01acbc0cedd426f1905f4af
",AQDcffhZ1i3HMxAA5oB+SuIbzSKc+9FDc5tKeQ,https://github.com/kubernetes/kubernetes
137,54941.0,"/kind bug
/sig storage

## **What happened**:
**logs of controller-manager**
W1101 14:05:30.134407       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134459       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134507       7 rbd_util.go:364] failed to create rbd image, output
E1101 14:05:30.134529       7 rbd.go:323] rbd: create volume failed, err: failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:
I1101 14:05:30.134553       7 pv_controller.go:1331] failed to provision volume for claim ""default/claim5"" with StorageClass ""fast"": failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:

## What you expected to happen:
rbd executed successfully to create a volume on the ceph pool 'kube'

## How to reproduce it (as minimally and precisely as possible):
**ceph config**
ceph admin key = AQAMZ/hZheYqAxAAURdTDYs4GPc378Vp0PLeqA==
ceph kube key = AQDcffhZ1i3HMxAA5oB+SuIbzSKc+9FDc5tKeQ==
ceph --cluster ceph auth get-or-create client.kube mon 'allow r' osd 'allow rwx pool=kube'

ceph-secrets for users (admin, kube) with base64 encoded key
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: QVFBTVovaFpoZVlxQXhBQVVSZFREWXM0R1BjMzc4VnAwUExlcUE9PQ==
---
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-kube
data:
  key: QVFEY2ZmaFoxaTNITXhBQTVvQitTdUlielNLYys5RkRjNXRLZVE9PQ==

**Storageclass 'fast'**
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 1:6789,2:6789,3:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: ""kube-system""
  pool: kube
  userId: kube
  userSecretName: ceph-secret-kube

**PersistentStorageClaim**
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim5
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
  storageClassName: fast

**Executing 'rbd' inside of controller-manager works:**
$ kubectl exec -ti kube-controller-manager-st14d02ls-ztda11350501 -n kube-system rbd
   
2017-11-01 14:10:26.664691 7febc9f05780 -1 did not load config file, using default settings.
rbd: you must specify a command.
**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T07:00:21Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.5+coreos.0"", GitCommit:""070d238cd2ec359928548e486a9171b498573181"", GitTreeState:""clean"", BuildDate:""2017-08-31T21:28:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
. 3 masters / api servers

- OS (e.g. from /etc/os-release):
10x centos 7.3 nodes

- Kernel (e.g. `uname -a`):
4.9.44, custom kernel build.

- Install tools:
Kubespray

- Others:
docker 1.13
hyperkube 1.7.5
ceph version 0.94.10 (b1e0532418e4631af01acbc0cedd426f1905f4af
",PersistentStorageClaim,https://github.com/kubernetes/kubernetes
138,54941.0,"/kind bug
/sig storage

## **What happened**:
**logs of controller-manager**
W1101 14:05:30.134407       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134459       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134507       7 rbd_util.go:364] failed to create rbd image, output
E1101 14:05:30.134529       7 rbd.go:323] rbd: create volume failed, err: failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:
I1101 14:05:30.134553       7 pv_controller.go:1331] failed to provision volume for claim ""default/claim5"" with StorageClass ""fast"": failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:

## What you expected to happen:
rbd executed successfully to create a volume on the ceph pool 'kube'

## How to reproduce it (as minimally and precisely as possible):
**ceph config**
ceph admin key = AQAMZ/hZheYqAxAAURdTDYs4GPc378Vp0PLeqA==
ceph kube key = AQDcffhZ1i3HMxAA5oB+SuIbzSKc+9FDc5tKeQ==
ceph --cluster ceph auth get-or-create client.kube mon 'allow r' osd 'allow rwx pool=kube'

ceph-secrets for users (admin, kube) with base64 encoded key
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: QVFBTVovaFpoZVlxQXhBQVVSZFREWXM0R1BjMzc4VnAwUExlcUE9PQ==
---
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-kube
data:
  key: QVFEY2ZmaFoxaTNITXhBQTVvQitTdUlielNLYys5RkRjNXRLZVE9PQ==

**Storageclass 'fast'**
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 1:6789,2:6789,3:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: ""kube-system""
  pool: kube
  userId: kube
  userSecretName: ceph-secret-kube

**PersistentStorageClaim**
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim5
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
  storageClassName: fast

**Executing 'rbd' inside of controller-manager works:**
$ kubectl exec -ti kube-controller-manager-st14d02ls-ztda11350501 -n kube-system rbd
   
2017-11-01 14:10:26.664691 7febc9f05780 -1 did not load config file, using default settings.
rbd: you must specify a command.
**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T07:00:21Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.5+coreos.0"", GitCommit:""070d238cd2ec359928548e486a9171b498573181"", GitTreeState:""clean"", BuildDate:""2017-08-31T21:28:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
. 3 masters / api servers

- OS (e.g. from /etc/os-release):
10x centos 7.3 nodes

- Kernel (e.g. `uname -a`):
4.9.44, custom kernel build.

- Install tools:
Kubespray

- Others:
docker 1.13
hyperkube 1.7.5
ceph version 0.94.10 (b1e0532418e4631af01acbc0cedd426f1905f4af
",QVFBTVovaFpoZVlxQXhBQVVSZFREWXM0R1BjMzc4VnAwUExlcUE9PQ,https://github.com/kubernetes/kubernetes
139,54941.0,"/kind bug
/sig storage

## **What happened**:
**logs of controller-manager**
W1101 14:05:30.134407       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134459       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134507       7 rbd_util.go:364] failed to create rbd image, output
E1101 14:05:30.134529       7 rbd.go:323] rbd: create volume failed, err: failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:
I1101 14:05:30.134553       7 pv_controller.go:1331] failed to provision volume for claim ""default/claim5"" with StorageClass ""fast"": failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:

## What you expected to happen:
rbd executed successfully to create a volume on the ceph pool 'kube'

## How to reproduce it (as minimally and precisely as possible):
**ceph config**
ceph admin key = AQAMZ/hZheYqAxAAURdTDYs4GPc378Vp0PLeqA==
ceph kube key = AQDcffhZ1i3HMxAA5oB+SuIbzSKc+9FDc5tKeQ==
ceph --cluster ceph auth get-or-create client.kube mon 'allow r' osd 'allow rwx pool=kube'

ceph-secrets for users (admin, kube) with base64 encoded key
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: QVFBTVovaFpoZVlxQXhBQVVSZFREWXM0R1BjMzc4VnAwUExlcUE9PQ==
---
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-kube
data:
  key: QVFEY2ZmaFoxaTNITXhBQTVvQitTdUlielNLYys5RkRjNXRLZVE9PQ==

**Storageclass 'fast'**
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 1:6789,2:6789,3:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: ""kube-system""
  pool: kube
  userId: kube
  userSecretName: ceph-secret-kube

**PersistentStorageClaim**
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim5
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
  storageClassName: fast

**Executing 'rbd' inside of controller-manager works:**
$ kubectl exec -ti kube-controller-manager-st14d02ls-ztda11350501 -n kube-system rbd
   
2017-11-01 14:10:26.664691 7febc9f05780 -1 did not load config file, using default settings.
rbd: you must specify a command.
**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T07:00:21Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.5+coreos.0"", GitCommit:""070d238cd2ec359928548e486a9171b498573181"", GitTreeState:""clean"", BuildDate:""2017-08-31T21:28:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
. 3 masters / api servers

- OS (e.g. from /etc/os-release):
10x centos 7.3 nodes

- Kernel (e.g. `uname -a`):
4.9.44, custom kernel build.

- Install tools:
Kubespray

- Others:
docker 1.13
hyperkube 1.7.5
ceph version 0.94.10 (b1e0532418e4631af01acbc0cedd426f1905f4af
",ceph-secret-kube,https://github.com/kubernetes/kubernetes
140,54941.0,"/kind bug
/sig storage

## **What happened**:
**logs of controller-manager**
W1101 14:05:30.134407       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134459       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134507       7 rbd_util.go:364] failed to create rbd image, output
E1101 14:05:30.134529       7 rbd.go:323] rbd: create volume failed, err: failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:
I1101 14:05:30.134553       7 pv_controller.go:1331] failed to provision volume for claim ""default/claim5"" with StorageClass ""fast"": failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:

## What you expected to happen:
rbd executed successfully to create a volume on the ceph pool 'kube'

## How to reproduce it (as minimally and precisely as possible):
**ceph config**
ceph admin key = AQAMZ/hZheYqAxAAURdTDYs4GPc378Vp0PLeqA==
ceph kube key = AQDcffhZ1i3HMxAA5oB+SuIbzSKc+9FDc5tKeQ==
ceph --cluster ceph auth get-or-create client.kube mon 'allow r' osd 'allow rwx pool=kube'

ceph-secrets for users (admin, kube) with base64 encoded key
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: QVFBTVovaFpoZVlxQXhBQVVSZFREWXM0R1BjMzc4VnAwUExlcUE9PQ==
---
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-kube
data:
  key: QVFEY2ZmaFoxaTNITXhBQTVvQitTdUlielNLYys5RkRjNXRLZVE9PQ==

**Storageclass 'fast'**
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 1:6789,2:6789,3:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: ""kube-system""
  pool: kube
  userId: kube
  userSecretName: ceph-secret-kube

**PersistentStorageClaim**
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim5
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
  storageClassName: fast

**Executing 'rbd' inside of controller-manager works:**
$ kubectl exec -ti kube-controller-manager-st14d02ls-ztda11350501 -n kube-system rbd
   
2017-11-01 14:10:26.664691 7febc9f05780 -1 did not load config file, using default settings.
rbd: you must specify a command.
**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T07:00:21Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.5+coreos.0"", GitCommit:""070d238cd2ec359928548e486a9171b498573181"", GitTreeState:""clean"", BuildDate:""2017-08-31T21:28:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
. 3 masters / api servers

- OS (e.g. from /etc/os-release):
10x centos 7.3 nodes

- Kernel (e.g. `uname -a`):
4.9.44, custom kernel build.

- Install tools:
Kubespray

- Others:
docker 1.13
hyperkube 1.7.5
ceph version 0.94.10 (b1e0532418e4631af01acbc0cedd426f1905f4af
",QVFEY2ZmaFoxaTNITXhBQTVvQitTdUlielNLYys5RkRjNXRLZV,https://github.com/kubernetes/kubernetes
141,54941.0,"/kind bug
/sig storage

## **What happened**:
**logs of controller-manager**
W1101 14:05:30.134407       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134459       7 rbd_util.go:364] failed to create rbd image, output
W1101 14:05:30.134507       7 rbd_util.go:364] failed to create rbd image, output
E1101 14:05:30.134529       7 rbd.go:323] rbd: create volume failed, err: failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:
I1101 14:05:30.134553       7 pv_controller.go:1331] failed to provision volume for claim ""default/claim5"" with StorageClass ""fast"": failed to create rbd image: fork/exec /usr/bin/rbd: invalid argument, command output:

## What you expected to happen:
rbd executed successfully to create a volume on the ceph pool 'kube'

## How to reproduce it (as minimally and precisely as possible):
**ceph config**
ceph admin key = AQAMZ/hZheYqAxAAURdTDYs4GPc378Vp0PLeqA==
ceph kube key = AQDcffhZ1i3HMxAA5oB+SuIbzSKc+9FDc5tKeQ==
ceph --cluster ceph auth get-or-create client.kube mon 'allow r' osd 'allow rwx pool=kube'

ceph-secrets for users (admin, kube) with base64 encoded key
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: QVFBTVovaFpoZVlxQXhBQVVSZFREWXM0R1BjMzc4VnAwUExlcUE9PQ==
---
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-kube
data:
  key: QVFEY2ZmaFoxaTNITXhBQTVvQitTdUlielNLYys5RkRjNXRLZVE9PQ==

**Storageclass 'fast'**
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 1:6789,2:6789,3:6789
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: ""kube-system""
  pool: kube
  userId: kube
  userSecretName: ceph-secret-kube

**PersistentStorageClaim**
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim5
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
  storageClassName: fast

**Executing 'rbd' inside of controller-manager works:**
$ kubectl exec -ti kube-controller-manager-st14d02ls-ztda11350501 -n kube-system rbd
   
2017-11-01 14:10:26.664691 7febc9f05780 -1 did not load config file, using default settings.
rbd: you must specify a command.
**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T07:00:21Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.5+coreos.0"", GitCommit:""070d238cd2ec359928548e486a9171b498573181"", GitTreeState:""clean"", BuildDate:""2017-08-31T21:28:39Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
. 3 masters / api servers

- OS (e.g. from /etc/os-release):
10x centos 7.3 nodes

- Kernel (e.g. `uname -a`):
4.9.44, custom kernel build.

- Install tools:
Kubespray

- Others:
docker 1.13
hyperkube 1.7.5
ceph version 0.94.10 (b1e0532418e4631af01acbc0cedd426f1905f4af
",QVFBTVovaFpoZVlxQXhBQVVSZFREWXM0R1BjMzc4VnAwUExlcU,https://github.com/kubernetes/kubernetes
142,53853.0,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug

**What happened**:
`kubeadm init` timed out before kubelet finished downloading and initializing.

**What you expected to happen**:
`kubeadm init` would wait for the kubelet to completely initialize (or fail) before exiting

**How to reproduce it (as minimally and precisely as possible)**:
(A prerequisite is probably a bad (or manually restricted) internet connection, I'm testing on ~8-10Mb/s)

You just need to run `kubeadm init` and wait for the timeout to elapse.
Eventually the `kubelet` service will download and set itself up, but not until after `kubeadm` has given up.

```
Oct 13 00:48:02 core-01 kubelet-wrapper[1268]: gpg key fingerprint is: BFF3 13CD AA56 0B16 A898  7B8F 72AB F5F6 799D 33BC
Oct 13 00:48:02 core-01 kubelet-wrapper[1268]:         Quay.io ACI Converter (ACI conversion signing key) <support@quay.io>
Oct 13 00:48:02 core-01 kubelet-wrapper[1268]: Trusting ""https://quay.io/aci-signing-key"" for prefix ""quay.io/coreos/hyperkube"" without fingerprint review.
Oct 13 00:48:02 core-01 kubelet-wrapper[1268]: Added key for prefix ""quay.io/coreos/hyperkube"" at ""/etc/rkt/trustedkeys/prefix.d/quay.io/coreos/hyperkube/bff313cdaa560b16a8987b8f72abf5f6799d33bc""
Oct 13 00:48:04 core-01 kubelet-wrapper[1268]: Downloading signature:  0 B/473 B
Oct 13 00:48:04 core-01 kubelet-wrapper[1268]: Downloading signature:  473 B/473 B
Oct 13 00:48:10 core-01 kubelet-wrapper[1268]: Downloading ACI:  0 B/158 MB
Oct 13 00:48:10 core-01 kubelet-wrapper[1268]: Downloading ACI:  8.95 KB/158 MB
Oct 13 00:48:12 core-01 kubelet-wrapper[1268]: Downloading ACI:  470 KB/158 MB
Oct 13 00:48:13 core-01 kubelet-wrapper[1268]: Downloading ACI:  1.06 MB/158 MB
...
Oct 13 00:52:05 core-01 kubelet-wrapper[1268]: Downloading ACI:  156 MB/158 MB
Oct 13 00:52:06 core-01 kubelet-wrapper[1268]: Downloading ACI:  157 MB/158 MB
Oct 13 00:52:07 core-01 kubelet-wrapper[1268]: Downloading ACI:  158 MB/158 MB
Oct 13 00:52:08 core-01 kubelet-wrapper[1268]: Downloading ACI:  158 MB/158 MB
Oct 13 00:52:09 core-01 kubelet-wrapper[1268]: image: signature verified:
Oct 13 00:52:09 core-01 kubelet-wrapper[1268]:   Quay.io ACI Converter (ACI conversion signing key) <support@quay.io>
Oct 13 00:52:32 core-01 kubelet-wrapper[1268]: Flag --require-kubeconfig has been deprecated, You no longer need to use --require-kubeconfig. This will be removed in a future version. Providing --kubeconfig enables API server mode, omitting --kubeconfig enables standalone mode unless --require-kubeconfig=true is also set. In the latter case, the legacy default kubeconfig path will be used until --require-kubeconfig is removed.
```

```
core@core-01 ~ $ systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled)
   Active: active (running) since Fri 2017-10-13 00:47:56 UTC; 23min ago
     Docs: http://kubernetes.io/docs/
 Main PID: 1268 (kubelet)
    Tasks: 16 (limit: 32768)
   Memory: 253.5M
      CPU: 52.096s
   CGroup: /system.slice/kubelet.service
           └─1268 /kubelet --kubeconfig=/etc/kubernetes/kubelet.conf --require-kubeconfig=true --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cn

core@core-01 ~ $ kubectl get all --all-namespaces
NAMESPACE     NAME                                 READY     STATUS             RESTARTS   AGE
kube-system   po/etcd-core-01                      1/1       Running            0          17m
kube-system   po/kube-apiserver-core-01            1/1       Running            0          17m
kube-system   po/kube-controller-manager-core-01   1/1       Running   0          18m
kube-system   po/kube-scheduler-core-01            1/1       Running            0          18m

NAMESPACE   NAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
default     svc/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   18m
```

```
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[init] Using Kubernetes version: v1.8.1
[init] Using Authorization modes: [Node RBAC]
[preflight] Running pre-flight checks
[preflight] WARNING: docker version is greater than the most recently validated version. Docker version: 17.06.2-ce. Max validated version: 17.03
[preflight] WARNING: hostname ""core-01"" could not be reached
[preflight] WARNING: hostname ""core-01"" lookup core-01 on 10.0.2.3:53: no such host
[preflight] WARNING: socat not found in system path
[kubeadm] WARNING: starting in 1.8, tokens expire after 24 hours by default (if you require a non-expiring token use --token-ttl 0)
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [core-01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.17.8.101]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Valid certificates and keys now exist in ""/etc/kubernetes/pki""
[kubeconfig] Wrote KubeConfig file to disk: ""admin.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""kubelet.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""controller-manager.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""scheduler.conf""
[controlplane] Wrote Static Pod manifest for component kube-apiserver to ""/etc/kubernetes/manifests/kube-apiserver.yaml""
[controlplane] Wrote Static Pod manifest for component kube-controller-manager to ""/etc/kubernetes/manifests/kube-controller-manager.yaml""
[controlplane] Wrote Static Pod manifest for component kube-scheduler to ""/etc/kubernetes/manifests/kube-scheduler.yaml""
[etcd] Wrote Static Pod manifest for a local etcd instance to ""/etc/kubernetes/manifests/etcd.yaml""
[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory ""/etc/kubernetes/manifests""
[init] This often takes around a minute; or longer if the control plane images have to be pulled.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz' failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.
...
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz' failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.

Unfortunately, an error has occurred:
        timed out waiting for the condition

This error is likely caused by that:
        - The kubelet is not running
        - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)
        - There is no internet connection; so the kubelet can't pull the following control plane images:
                - gcr.io/google_containers/kube-apiserver-amd64:v1.8.1
                - gcr.io/google_containers/kube-controller-manager-amd64:v1.8.1
                - gcr.io/google_containers/kube-scheduler-amd64:v1.8.1

You can troubleshoot this for example with the following commands if you're on a systemd-powered system:
        - 'systemctl status kubelet'
        - 'journalctl -xeu kubelet'
couldn't initialize a Kubernetes cluster
```

**Anything else we need to know?**:
Would it be possible to just allow the timeout to be configurable?
I noticed it was mentioned in the pull request for the timing out behaviour (#51369)

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.1"", GitCommit:""f38e43b221d08850172a9a4ea785a86a3ffa3b3a"", GitTreeState:""clean"", BuildDate:""2017-10-11T23:27:35Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd
64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.1"", GitCommit:""f38e43b221d08850172a9a4ea785a86a3ffa3b3a"", GitTreeState:""clean"", BuildDate:""2017-10-11T23:16:41Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd
64""}
```

- Cloud provider or hardware configuration**:
Locally, within a Vagrant VM (CoreOS, Vagrant version 1.9.7).
2GB RAM, 1-core of a 3GHz CPU, Australian internet connection

- OS (e.g. from /etc/os-release):
```
NAME=""Container Linux by CoreOS""
ID=coreos
VERSION=1548.0.0
VERSION_ID=1548.0.0
BUILD_ID=2017-09-27-0012
PRETTY_NAME=""Container Linux by CoreOS 1548.0.0 (Ladybug)""
ANSI_COLOR=""38;5;75""
HOME_URL=""https://coreos.com/""
BUG_REPORT_URL=""https://issues.coreos.com""
COREOS_BOARD=""amd64-usr""
```

- Kernel (e.g. `uname -a`):
```
Linux core-01 4.13.3-coreos-r1 #1 SMP Tue Sep 26 23:51:59 UTC 2017 x86_64 Intel(R) Core(TM) i7-5700HQ CPU @ 2.70GHz GenuineIntel GNU/Linux
```",kubelet-wrapper[1268,https://github.com/kubernetes/kubernetes
143,53853.0,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:
/kind bug

**What happened**:
`kubeadm init` timed out before kubelet finished downloading and initializing.

**What you expected to happen**:
`kubeadm init` would wait for the kubelet to completely initialize (or fail) before exiting

**How to reproduce it (as minimally and precisely as possible)**:
(A prerequisite is probably a bad (or manually restricted) internet connection, I'm testing on ~8-10Mb/s)

You just need to run `kubeadm init` and wait for the timeout to elapse.
Eventually the `kubelet` service will download and set itself up, but not until after `kubeadm` has given up.

```
Oct 13 00:48:02 core-01 kubelet-wrapper[1268]: gpg key fingerprint is: BFF3 13CD AA56 0B16 A898  7B8F 72AB F5F6 799D 33BC
Oct 13 00:48:02 core-01 kubelet-wrapper[1268]:         Quay.io ACI Converter (ACI conversion signing key) <support@quay.io>
Oct 13 00:48:02 core-01 kubelet-wrapper[1268]: Trusting ""https://quay.io/aci-signing-key"" for prefix ""quay.io/coreos/hyperkube"" without fingerprint review.
Oct 13 00:48:02 core-01 kubelet-wrapper[1268]: Added key for prefix ""quay.io/coreos/hyperkube"" at ""/etc/rkt/trustedkeys/prefix.d/quay.io/coreos/hyperkube/bff313cdaa560b16a8987b8f72abf5f6799d33bc""
Oct 13 00:48:04 core-01 kubelet-wrapper[1268]: Downloading signature:  0 B/473 B
Oct 13 00:48:04 core-01 kubelet-wrapper[1268]: Downloading signature:  473 B/473 B
Oct 13 00:48:10 core-01 kubelet-wrapper[1268]: Downloading ACI:  0 B/158 MB
Oct 13 00:48:10 core-01 kubelet-wrapper[1268]: Downloading ACI:  8.95 KB/158 MB
Oct 13 00:48:12 core-01 kubelet-wrapper[1268]: Downloading ACI:  470 KB/158 MB
Oct 13 00:48:13 core-01 kubelet-wrapper[1268]: Downloading ACI:  1.06 MB/158 MB
...
Oct 13 00:52:05 core-01 kubelet-wrapper[1268]: Downloading ACI:  156 MB/158 MB
Oct 13 00:52:06 core-01 kubelet-wrapper[1268]: Downloading ACI:  157 MB/158 MB
Oct 13 00:52:07 core-01 kubelet-wrapper[1268]: Downloading ACI:  158 MB/158 MB
Oct 13 00:52:08 core-01 kubelet-wrapper[1268]: Downloading ACI:  158 MB/158 MB
Oct 13 00:52:09 core-01 kubelet-wrapper[1268]: image: signature verified:
Oct 13 00:52:09 core-01 kubelet-wrapper[1268]:   Quay.io ACI Converter (ACI conversion signing key) <support@quay.io>
Oct 13 00:52:32 core-01 kubelet-wrapper[1268]: Flag --require-kubeconfig has been deprecated, You no longer need to use --require-kubeconfig. This will be removed in a future version. Providing --kubeconfig enables API server mode, omitting --kubeconfig enables standalone mode unless --require-kubeconfig=true is also set. In the latter case, the legacy default kubeconfig path will be used until --require-kubeconfig is removed.
```

```
core@core-01 ~ $ systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled)
   Active: active (running) since Fri 2017-10-13 00:47:56 UTC; 23min ago
     Docs: http://kubernetes.io/docs/
 Main PID: 1268 (kubelet)
    Tasks: 16 (limit: 32768)
   Memory: 253.5M
      CPU: 52.096s
   CGroup: /system.slice/kubelet.service
           └─1268 /kubelet --kubeconfig=/etc/kubernetes/kubelet.conf --require-kubeconfig=true --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cn

core@core-01 ~ $ kubectl get all --all-namespaces
NAMESPACE     NAME                                 READY     STATUS             RESTARTS   AGE
kube-system   po/etcd-core-01                      1/1       Running            0          17m
kube-system   po/kube-apiserver-core-01            1/1       Running            0          17m
kube-system   po/kube-controller-manager-core-01   1/1       Running   0          18m
kube-system   po/kube-scheduler-core-01            1/1       Running            0          18m

NAMESPACE   NAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
default     svc/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   18m
```

```
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[init] Using Kubernetes version: v1.8.1
[init] Using Authorization modes: [Node RBAC]
[preflight] Running pre-flight checks
[preflight] WARNING: docker version is greater than the most recently validated version. Docker version: 17.06.2-ce. Max validated version: 17.03
[preflight] WARNING: hostname ""core-01"" could not be reached
[preflight] WARNING: hostname ""core-01"" lookup core-01 on 10.0.2.3:53: no such host
[preflight] WARNING: socat not found in system path
[kubeadm] WARNING: starting in 1.8, tokens expire after 24 hours by default (if you require a non-expiring token use --token-ttl 0)
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [core-01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.17.8.101]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Valid certificates and keys now exist in ""/etc/kubernetes/pki""
[kubeconfig] Wrote KubeConfig file to disk: ""admin.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""kubelet.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""controller-manager.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""scheduler.conf""
[controlplane] Wrote Static Pod manifest for component kube-apiserver to ""/etc/kubernetes/manifests/kube-apiserver.yaml""
[controlplane] Wrote Static Pod manifest for component kube-controller-manager to ""/etc/kubernetes/manifests/kube-controller-manager.yaml""
[controlplane] Wrote Static Pod manifest for component kube-scheduler to ""/etc/kubernetes/manifests/kube-scheduler.yaml""
[etcd] Wrote Static Pod manifest for a local etcd instance to ""/etc/kubernetes/manifests/etcd.yaml""
[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory ""/etc/kubernetes/manifests""
[init] This often takes around a minute; or longer if the control plane images have to be pulled.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz' failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.
...
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz' failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.

Unfortunately, an error has occurred:
        timed out waiting for the condition

This error is likely caused by that:
        - The kubelet is not running
        - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)
        - There is no internet connection; so the kubelet can't pull the following control plane images:
                - gcr.io/google_containers/kube-apiserver-amd64:v1.8.1
                - gcr.io/google_containers/kube-controller-manager-amd64:v1.8.1
                - gcr.io/google_containers/kube-scheduler-amd64:v1.8.1

You can troubleshoot this for example with the following commands if you're on a systemd-powered system:
        - 'systemctl status kubelet'
        - 'journalctl -xeu kubelet'
couldn't initialize a Kubernetes cluster
```

**Anything else we need to know?**:
Would it be possible to just allow the timeout to be configurable?
I noticed it was mentioned in the pull request for the timing out behaviour (#51369)

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.1"", GitCommit:""f38e43b221d08850172a9a4ea785a86a3ffa3b3a"", GitTreeState:""clean"", BuildDate:""2017-10-11T23:27:35Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd
64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.1"", GitCommit:""f38e43b221d08850172a9a4ea785a86a3ffa3b3a"", GitTreeState:""clean"", BuildDate:""2017-10-11T23:16:41Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd
64""}
```

- Cloud provider or hardware configuration**:
Locally, within a Vagrant VM (CoreOS, Vagrant version 1.9.7).
2GB RAM, 1-core of a 3GHz CPU, Australian internet connection

- OS (e.g. from /etc/os-release):
```
NAME=""Container Linux by CoreOS""
ID=coreos
VERSION=1548.0.0
VERSION_ID=1548.0.0
BUILD_ID=2017-09-27-0012
PRETTY_NAME=""Container Linux by CoreOS 1548.0.0 (Ladybug)""
ANSI_COLOR=""38;5;75""
HOME_URL=""https://coreos.com/""
BUG_REPORT_URL=""https://issues.coreos.com""
COREOS_BOARD=""amd64-usr""
```

- Kernel (e.g. `uname -a`):
```
Linux core-01 4.13.3-coreos-r1 #1 SMP Tue Sep 26 23:51:59 UTC 2017 x86_64 Intel(R) Core(TM) i7-5700HQ CPU @ 2.70GHz GenuineIntel GNU/Linux
```",front-proxy-client,https://github.com/kubernetes/kubernetes
144,53373.0,"**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG REPORT

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:

Pod cannot be terminated because local volume with token cannot be unmounted.

**What you expected to happen**:

Pod is terminated correctly.

**How to reproduce it (as minimally and precisely as possible)**:

```
# kubectl describe pod kube-proxy-gngh8 -n kube-system
[...]
    Mounts:
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-79dz0 (ro)
[...]
```
```
# kubectl delete pod kube-proxy-gngh8 -n kube-system
```
kubelet log:
```
E1003 08:16:40.821333   11479 nestedpendingoperations.go:264] Operation for ""\""kubernetes.io/secret/df09b119-a772-11e7-82ce-5254009ef6db-default-token-79dz0\"" (\""df09b119-a772-11e7-82ce-5254009ef6db\"")"" failed. No retries permitted until 2017-10-03 08:18:42.821312669 +0000 UTC (durationBeforeRetry 2m2s). Error: UnmountVolume.TearDown failed for volume ""default-token-79dz0"" (UniqueName: ""kubernetes.io/secret/df09b119-a772-11e7-82ce-5254009ef6db-default-token-79dz0"") pod ""df09b119-a772-11e7-82ce-5254009ef6db"" (UID: ""df09b119-a772-11e7-82ce-5254009ef6db"") : remove /var/lib/kubelet/pods/df09b119-a772-11e7-82ce-5254009ef6db/volumes/kubernetes.io~secret/default-token-79dz0: device or resource busy
```
More kubelet logs:
https://gist.github.com/r0bj/2c7b4809a0a75c99f91589746ae95da9

Pod `kube-proxy-gngh8` stays forever in `Terminating` state.

**Anything else we need to know?**:
The same setup with k8s 1.7.x works fine.

kubelet config:
```
/usr/bin/kubelet \
    --register-with-taints=node-role.kubernetes.io/master="""":NoSchedule \
    --pod-manifest-path=/etc/kubernetes/manifests \
    --node-labels=k8s-role=k8s-controller,node-role.kubernetes.io/master="""" \
    --allow-privileged=true \
    --cloud-provider= \
    --cluster-dns=10.223.0.10 \
    --cluster-domain=cluster.local \
    --container-runtime=docker \
    --docker=unix:///var/run/docker.sock \
    --network-plugin=cni \
    --require-kubeconfig=true \
    --kubeconfig=/var/lib/kubelet/kubeconfig \
    --serialize-image-pulls=false \
    --tls-cert-file=/var/lib/kubernetes/kubelet-cert.pem \
    --tls-private-key-file=/var/lib/kubernetes/kubelet-key.pem \
    --eviction-soft=""memory.available<200Mi,nodefs.available<12%,nodefs.inodesFree<7%,imagefs.available<12%,imagefs.inodesFree<12%"" \
    --eviction-soft-grace-period=""memory.available=5m,nodefs.available=5m,nodefs.inodesFree=5m,imagefs.available=5m,imagefs.inodesFree=5m"" \
    --eviction-max-pod-grace-period=60 \
    --eviction-hard=""memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<10%,imagefs.inodesFree<10%"" \
    --v=2
```


**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""6e937839ac04a38cac63e6a7a306c5d035fe7b0a"", GitTreeState:""clean"", BuildDate:""2017-09-28T22:57:57Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""6e937839ac04a38cac63e6a7a306c5d035fe7b0a"", GitTreeState:""clean"", BuildDate:""2017-09-28T22:46:41Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration**:
bare matal
- OS (e.g. from /etc/os-release):
Ubuntu 14.04 LTS
- Kernel (e.g. `uname -a`):
Linux dev-k8s-controller-p1 4.4.0-53-generic #74~14.04.1-Ubuntu SMP Fri Dec 2 03:43:31 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
- Others:
calico version: v2.6.1
docker: 1.12.6
",df09b119-a772-11e7-82ce-5254009ef6db,https://github.com/kubernetes/kubernetes
145,53373.0,"**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG REPORT

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:

Pod cannot be terminated because local volume with token cannot be unmounted.

**What you expected to happen**:

Pod is terminated correctly.

**How to reproduce it (as minimally and precisely as possible)**:

```
# kubectl describe pod kube-proxy-gngh8 -n kube-system
[...]
    Mounts:
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-79dz0 (ro)
[...]
```
```
# kubectl delete pod kube-proxy-gngh8 -n kube-system
```
kubelet log:
```
E1003 08:16:40.821333   11479 nestedpendingoperations.go:264] Operation for ""\""kubernetes.io/secret/df09b119-a772-11e7-82ce-5254009ef6db-default-token-79dz0\"" (\""df09b119-a772-11e7-82ce-5254009ef6db\"")"" failed. No retries permitted until 2017-10-03 08:18:42.821312669 +0000 UTC (durationBeforeRetry 2m2s). Error: UnmountVolume.TearDown failed for volume ""default-token-79dz0"" (UniqueName: ""kubernetes.io/secret/df09b119-a772-11e7-82ce-5254009ef6db-default-token-79dz0"") pod ""df09b119-a772-11e7-82ce-5254009ef6db"" (UID: ""df09b119-a772-11e7-82ce-5254009ef6db"") : remove /var/lib/kubelet/pods/df09b119-a772-11e7-82ce-5254009ef6db/volumes/kubernetes.io~secret/default-token-79dz0: device or resource busy
```
More kubelet logs:
https://gist.github.com/r0bj/2c7b4809a0a75c99f91589746ae95da9

Pod `kube-proxy-gngh8` stays forever in `Terminating` state.

**Anything else we need to know?**:
The same setup with k8s 1.7.x works fine.

kubelet config:
```
/usr/bin/kubelet \
    --register-with-taints=node-role.kubernetes.io/master="""":NoSchedule \
    --pod-manifest-path=/etc/kubernetes/manifests \
    --node-labels=k8s-role=k8s-controller,node-role.kubernetes.io/master="""" \
    --allow-privileged=true \
    --cloud-provider= \
    --cluster-dns=10.223.0.10 \
    --cluster-domain=cluster.local \
    --container-runtime=docker \
    --docker=unix:///var/run/docker.sock \
    --network-plugin=cni \
    --require-kubeconfig=true \
    --kubeconfig=/var/lib/kubelet/kubeconfig \
    --serialize-image-pulls=false \
    --tls-cert-file=/var/lib/kubernetes/kubelet-cert.pem \
    --tls-private-key-file=/var/lib/kubernetes/kubelet-key.pem \
    --eviction-soft=""memory.available<200Mi,nodefs.available<12%,nodefs.inodesFree<7%,imagefs.available<12%,imagefs.inodesFree<12%"" \
    --eviction-soft-grace-period=""memory.available=5m,nodefs.available=5m,nodefs.inodesFree=5m,imagefs.available=5m,imagefs.inodesFree=5m"" \
    --eviction-max-pod-grace-period=60 \
    --eviction-hard=""memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<10%,imagefs.inodesFree<10%"" \
    --v=2
```


**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""6e937839ac04a38cac63e6a7a306c5d035fe7b0a"", GitTreeState:""clean"", BuildDate:""2017-09-28T22:57:57Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""6e937839ac04a38cac63e6a7a306c5d035fe7b0a"", GitTreeState:""clean"", BuildDate:""2017-09-28T22:46:41Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration**:
bare matal
- OS (e.g. from /etc/os-release):
Ubuntu 14.04 LTS
- Kernel (e.g. `uname -a`):
Linux dev-k8s-controller-p1 4.4.0-53-generic #74~14.04.1-Ubuntu SMP Fri Dec 2 03:43:31 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
- Others:
calico version: v2.6.1
docker: 1.12.6
",11e7-82ce-5254009ef6db,https://github.com/kubernetes/kubernetes
146,53373.0,"**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG REPORT

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:

Pod cannot be terminated because local volume with token cannot be unmounted.

**What you expected to happen**:

Pod is terminated correctly.

**How to reproduce it (as minimally and precisely as possible)**:

```
# kubectl describe pod kube-proxy-gngh8 -n kube-system
[...]
    Mounts:
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-79dz0 (ro)
[...]
```
```
# kubectl delete pod kube-proxy-gngh8 -n kube-system
```
kubelet log:
```
E1003 08:16:40.821333   11479 nestedpendingoperations.go:264] Operation for ""\""kubernetes.io/secret/df09b119-a772-11e7-82ce-5254009ef6db-default-token-79dz0\"" (\""df09b119-a772-11e7-82ce-5254009ef6db\"")"" failed. No retries permitted until 2017-10-03 08:18:42.821312669 +0000 UTC (durationBeforeRetry 2m2s). Error: UnmountVolume.TearDown failed for volume ""default-token-79dz0"" (UniqueName: ""kubernetes.io/secret/df09b119-a772-11e7-82ce-5254009ef6db-default-token-79dz0"") pod ""df09b119-a772-11e7-82ce-5254009ef6db"" (UID: ""df09b119-a772-11e7-82ce-5254009ef6db"") : remove /var/lib/kubelet/pods/df09b119-a772-11e7-82ce-5254009ef6db/volumes/kubernetes.io~secret/default-token-79dz0: device or resource busy
```
More kubelet logs:
https://gist.github.com/r0bj/2c7b4809a0a75c99f91589746ae95da9

Pod `kube-proxy-gngh8` stays forever in `Terminating` state.

**Anything else we need to know?**:
The same setup with k8s 1.7.x works fine.

kubelet config:
```
/usr/bin/kubelet \
    --register-with-taints=node-role.kubernetes.io/master="""":NoSchedule \
    --pod-manifest-path=/etc/kubernetes/manifests \
    --node-labels=k8s-role=k8s-controller,node-role.kubernetes.io/master="""" \
    --allow-privileged=true \
    --cloud-provider= \
    --cluster-dns=10.223.0.10 \
    --cluster-domain=cluster.local \
    --container-runtime=docker \
    --docker=unix:///var/run/docker.sock \
    --network-plugin=cni \
    --require-kubeconfig=true \
    --kubeconfig=/var/lib/kubelet/kubeconfig \
    --serialize-image-pulls=false \
    --tls-cert-file=/var/lib/kubernetes/kubelet-cert.pem \
    --tls-private-key-file=/var/lib/kubernetes/kubelet-key.pem \
    --eviction-soft=""memory.available<200Mi,nodefs.available<12%,nodefs.inodesFree<7%,imagefs.available<12%,imagefs.inodesFree<12%"" \
    --eviction-soft-grace-period=""memory.available=5m,nodefs.available=5m,nodefs.inodesFree=5m,imagefs.available=5m,imagefs.inodesFree=5m"" \
    --eviction-max-pod-grace-period=60 \
    --eviction-hard=""memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<10%,imagefs.inodesFree<10%"" \
    --v=2
```


**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""6e937839ac04a38cac63e6a7a306c5d035fe7b0a"", GitTreeState:""clean"", BuildDate:""2017-09-28T22:57:57Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""6e937839ac04a38cac63e6a7a306c5d035fe7b0a"", GitTreeState:""clean"", BuildDate:""2017-09-28T22:46:41Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration**:
bare matal
- OS (e.g. from /etc/os-release):
Ubuntu 14.04 LTS
- Kernel (e.g. `uname -a`):
Linux dev-k8s-controller-p1 4.4.0-53-generic #74~14.04.1-Ubuntu SMP Fri Dec 2 03:43:31 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
- Others:
calico version: v2.6.1
docker: 1.12.6
","12%,<7%,<12%,<12",https://github.com/kubernetes/kubernetes
147,53373.0,"**Is this a BUG REPORT or FEATURE REQUEST?**:
BUG REPORT

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**:

Pod cannot be terminated because local volume with token cannot be unmounted.

**What you expected to happen**:

Pod is terminated correctly.

**How to reproduce it (as minimally and precisely as possible)**:

```
# kubectl describe pod kube-proxy-gngh8 -n kube-system
[...]
    Mounts:
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-79dz0 (ro)
[...]
```
```
# kubectl delete pod kube-proxy-gngh8 -n kube-system
```
kubelet log:
```
E1003 08:16:40.821333   11479 nestedpendingoperations.go:264] Operation for ""\""kubernetes.io/secret/df09b119-a772-11e7-82ce-5254009ef6db-default-token-79dz0\"" (\""df09b119-a772-11e7-82ce-5254009ef6db\"")"" failed. No retries permitted until 2017-10-03 08:18:42.821312669 +0000 UTC (durationBeforeRetry 2m2s). Error: UnmountVolume.TearDown failed for volume ""default-token-79dz0"" (UniqueName: ""kubernetes.io/secret/df09b119-a772-11e7-82ce-5254009ef6db-default-token-79dz0"") pod ""df09b119-a772-11e7-82ce-5254009ef6db"" (UID: ""df09b119-a772-11e7-82ce-5254009ef6db"") : remove /var/lib/kubelet/pods/df09b119-a772-11e7-82ce-5254009ef6db/volumes/kubernetes.io~secret/default-token-79dz0: device or resource busy
```
More kubelet logs:
https://gist.github.com/r0bj/2c7b4809a0a75c99f91589746ae95da9

Pod `kube-proxy-gngh8` stays forever in `Terminating` state.

**Anything else we need to know?**:
The same setup with k8s 1.7.x works fine.

kubelet config:
```
/usr/bin/kubelet \
    --register-with-taints=node-role.kubernetes.io/master="""":NoSchedule \
    --pod-manifest-path=/etc/kubernetes/manifests \
    --node-labels=k8s-role=k8s-controller,node-role.kubernetes.io/master="""" \
    --allow-privileged=true \
    --cloud-provider= \
    --cluster-dns=10.223.0.10 \
    --cluster-domain=cluster.local \
    --container-runtime=docker \
    --docker=unix:///var/run/docker.sock \
    --network-plugin=cni \
    --require-kubeconfig=true \
    --kubeconfig=/var/lib/kubelet/kubeconfig \
    --serialize-image-pulls=false \
    --tls-cert-file=/var/lib/kubernetes/kubelet-cert.pem \
    --tls-private-key-file=/var/lib/kubernetes/kubelet-key.pem \
    --eviction-soft=""memory.available<200Mi,nodefs.available<12%,nodefs.inodesFree<7%,imagefs.available<12%,imagefs.inodesFree<12%"" \
    --eviction-soft-grace-period=""memory.available=5m,nodefs.available=5m,nodefs.inodesFree=5m,imagefs.available=5m,imagefs.inodesFree=5m"" \
    --eviction-max-pod-grace-period=60 \
    --eviction-hard=""memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<10%,imagefs.inodesFree<10%"" \
    --v=2
```


**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""6e937839ac04a38cac63e6a7a306c5d035fe7b0a"", GitTreeState:""clean"", BuildDate:""2017-09-28T22:57:57Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""8"", GitVersion:""v1.8.0"", GitCommit:""6e937839ac04a38cac63e6a7a306c5d035fe7b0a"", GitTreeState:""clean"", BuildDate:""2017-09-28T22:46:41Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration**:
bare matal
- OS (e.g. from /etc/os-release):
Ubuntu 14.04 LTS
- Kernel (e.g. `uname -a`):
Linux dev-k8s-controller-p1 4.4.0-53-generic #74~14.04.1-Ubuntu SMP Fri Dec 2 03:43:31 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
- Others:
calico version: v2.6.1
docker: 1.12.6
",kube-proxy-gngh8,https://github.com/kubernetes/kubernetes
148,52022.0,"
**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug

**What happened**:

Apiserver proxy removes leading slash from path that is send to destination service or pod, effectively breaking applications.

Example tshark output:

Hypertext Transfer Protocol
    GET test HTTP/1.1\r\n
        [Expert Info (Chat/Sequence): GET test HTTP/1.1\r\n]
            [GET test HTTP/1.1\r\n]
            [Severity level: Chat]
            [Group: Sequence]
        Request Method: GET
        Request URI: test
        Request Version: HTTP/1.1
    Host: localhost:8080\r\n
    User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.101 Safari/537.36\r\n
    Accept-Encoding: gzip, deflate, br\r\n
    Accept-Language: en-US,en;q=0.8,uk;q=0.6,ru;q=0.4\r\n
    Cache-Control: no-cache\r\n
    Connection: Upgrade\r\n
    Origin: http://localhost:8080\r\n
    Pragma: no-cache\r\n
    Sec-Websocket-Extensions: permessage-deflate; client_max_window_bits\r\n
    Sec-Websocket-Key: rtBOKvTHtRAbTpz9vhyAng==\r\n
    Sec-Websocket-Version: 13\r\n
    Upgrade: websocket\r\n
    X-Forwarded-For: 127.0.0.1\r\n
    \r\n
    [Full request URI: **http://localhost:8080test**]
    [HTTP request 1/1]


**What you expected to happen**:

Proxied URLs should begin with slash.
Example:
`GET /test HTTP/1.1\r\n`


**How to reproduce it (as minimally and precisely as possible)**:

Use this guideline to construct proxy URL: https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#discovering-builtin-services

This request will not work (404 not found):
`curl http://localhost:8080/api/v1/namespaces/default/services/testservice:http/proxy/test`
backend result (wrong): GET test HTTP/1.1\r\n

Request with double slash will work:
`curl http://localhost:8080/api/v1/namespaces/default/services/testservice:http/proxy//test`
backend result (correct): GET /test HTTP/1.1\r\n

Also old-style proxy URL format works too:
`curl http://localhost:8080/api/v1/proxy/namespaces/default/services/testservice:http/test`
backend result (correct): GET /test HTTP/1.1\r\n

**Anything else we need to know?**:
First observed with 1.7 kubernetes release

**Environment**:
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""8+"", GitVersion:""v1.8.0-alpha.3"", GitCommit:""6a4203eb4b5f59ac7a602e0c83023d15d991fd58"", GitTreeState:""clean"", BuildDate:""2017-08-23T22:58:52Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7+"", GitVersion:""v1.7.5-dirty"", GitCommit:""17d7182a7ccbb167074be7a87f0a68bd00d58d97"", GitTreeState:""dirty"", BuildDate:""2017-09-04T14:44:32Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration**:
Bare metal
- OS (e.g. from /etc/os-release):
CoreOS 1409.6.0
- Kernel (e.g. `uname -a`):
4.11.9-coreos
- Install tools:
- Others:
",Websocket-Version,https://github.com/kubernetes/kubernetes
149,50888.0,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug


**What happened**:
I was successfully able to `kubefed init` a federation control plane on a GKE k8s cluster, but upon attempting to join the cluster the following error occurs:

```
...
I0816 10:46:38.538848    5942 join.go:538] Creating service account in joining cluster
I0816 10:46:38.539061    5942 request.go:991] Request Body: {""kind"":""ServiceAccount"",""apiVersion"":""v1"",""metadata"":{""name"":""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"",""namespace"":""federation-system"",""creationTimestamp"":null,""annotations"":{""federation.alpha.kubernetes.io/cluster-name"":""gce-us-west1"",""federation.alpha.kubernetes.io/federation-name"":""federation""}}}
I0816 10:46:38.539134    5942 round_trippers.go:386] curl -k -v -XPOST  -H ""Accept: application/json, */*"" -H ""Content-Type: application/json"" -H ""User-Agent: kubefed/v1.7.3 (linux/amd64) kubernetes/2c2fe6e"" -H ""Authorization: Basic ABCDEFGHIJKLMNOPQRSTUVWXYZabcd=="" https://35.197.74.218/api/v1/namespaces/federation-system/serviceaccounts
I0816 10:46:38.576175    5942 round_trippers.go:405] POST https://35.197.74.218/api/v1/namespaces/federation-system/serviceaccounts 422 Unprocessable Entity in 37 milliseconds
I0816 10:46:38.576211    5942 round_trippers.go:411] Response Headers:
I0816 10:46:38.576225    5942 round_trippers.go:414]     Date: Wed, 16 Aug 2017 17:46:38 GMT
I0816 10:46:38.576236    5942 round_trippers.go:414]     Content-Type: application/json
I0816 10:46:38.576245    5942 round_trippers.go:414]     Content-Length: 1038
I0816 10:46:38.576515    5942 request.go:991] Response Body: {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""ServiceAccount \""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1\"" is invalid: metadata.name: Invalid value: \""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1\"": a DNS-1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')"",""reason"":""Invalid"",""details"":{""name"":""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"",""kind"":""ServiceAccount"",""causes"":[{""reason"":""FieldValueInvalid"",""message"":""Invalid value: \""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1\"": a DNS-1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')"",""field"":""metadata.name""}]},""code"":422}
I0816 10:46:38.576711    5942 join.go:541] Error creating service account in joining cluster: ServiceAccount ""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"" is invalid: metadata.name: Invalid value: ""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"": a DNS-1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')
I0816 10:46:38.576736    5942 join.go:234] Could not create cluster credentials secret: ServiceAccount ""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"" is invalid: metadata.name: Invalid value: ""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"": a DNS-1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')
```
This appears to be caused by the context name `gke_<project>_us-west1-b_gce-us-west1` which is using the default context name for a GKE cluster that is automatically imported to the kubeconfig file by `gcloud`.

If I update the kubeconfig context to get rid of the `_` to something like `gce-us-west1`, I can get past the above error but then subsequently hit the following error:

```
I0817 18:12:26.855195   14103 join.go:538] Creating service account in joining cluster
I0817 18:12:26.855329   14103 request.go:991] Request Body: {""kind"":""ServiceAccount"",""apiVersion"":""v1"",""metadata"":{""name"":""gce-us-west1-gce-us-west1"",""namespace"":""federation-system"",""creationTimestamp"":null,""annotations"":{""federation.alpha.kubernetes.io/cluster-name"":""gce-us-west1"",""federation.alpha.kubernetes.io/federation-name"":""federation""}}}
I0817 18:12:26.855376   14103 round_trippers.go:386] curl -k -v -XPOST  -H ""Accept: application/json, */*"" -H ""Content-Type: application/json"" -H ""User-Agent: kubefed/v1.7.3 (linux/amd64) kubernetes/2c2fe6e"" -H ""Authorization: Basic ABCDEFGHIJKLMNOPQRSTUVWXYZabcd=="" https://35.197.74.218/api/v1/namespaces/federation-system/serviceaccounts
I0817 18:12:26.885490   14103 round_trippers.go:405] POST https://35.197.74.218/api/v1/namespaces/federation-system/serviceaccounts 201 Created in 30 milliseconds
I0817 18:12:26.885536   14103 round_trippers.go:411] Response Headers:
I0817 18:12:26.885550   14103 round_trippers.go:414]     Content-Length: 470
I0817 18:12:26.885560   14103 round_trippers.go:414]     Date: Fri, 18 Aug 2017 01:12:26 GMT
I0817 18:12:26.885570   14103 round_trippers.go:414]     Content-Type: application/json
I0817 18:12:26.886617   14103 request.go:991] Response Body: {""kind"":""ServiceAccount"",""apiVersion"":""v1"",""metadata"":{""name"":""gce-us-west1-gce-us-west1"",""namespace"":""federation-system"",""selfLink"":""/api/v1/namespaces/federation-system/serviceaccounts/gce-us-west1-gce-us-west1"",""uid"":""4c7d5fb9-83b2-11e7-bda1-42010a8a01d8"",""resourceVersion"":""220285"",""creationTimestamp"":""2017-08-18T01:12:26Z"",""annotations"":{""federation.alpha.kubernetes.io/cluster-name"":""gce-us-west1"",""federation.alpha.kubernetes.io/federation-name"":""federation""}}}
I0817 18:12:26.886800   14103 join.go:544] Created service account in joining cluster
I0817 18:12:26.886817   14103 join.go:546] Creating role binding for service account in joining cluster
I0817 18:12:26.887024   14103 request.go:991] Request Body: {""kind"":""ClusterRole"",""apiVersion"":""rbac.authorization.k8s.io/v1beta1"",""metadata"":{""name"":""federation-controller-manager:federation-gce-us-west1-gce-us-west1"",""namespace"":""federation-system"",""creationTimestamp"":null,""annotations"":{""federation.alpha.kubernetes.io/cluster-name"":""gce-us-west1"",""federation.alpha.kubernetes.io/federation-name"":""federation""}},""rules"":[{""verbs"":[""*""],""apiGroups"":[""*""],""resources"":[""*""]},{""verbs"":[""get""],""nonResourceURLs"":[""/healthz""]}]}
I0817 18:12:26.887093   14103 round_trippers.go:386] curl -k -v -XPOST  -H ""Accept: application/json, */*"" -H ""Content-Type: application/json"" -H ""User-Agent: kubefed/v1.7.3 (linux/amd64) kubernetes/2c2fe6e"" -H ""Authorization: Basic ABCDEFGHIJKLMNOPQRSTUVWXYZabcd=="" https://35.197.74.218/apis/rbac.authorization.k8s.io/v1beta1/clusterroles
I0817 18:12:26.917114   14103 round_trippers.go:405] POST https://35.197.74.218/apis/rbac.authorization.k8s.io/v1beta1/clusterroles 409 Conflict in 29 milliseconds
I0817 18:12:26.917149   14103 round_trippers.go:411] Response Headers:
I0817 18:12:26.917161   14103 round_trippers.go:414]     Content-Length: 388
I0817 18:12:26.917167   14103 round_trippers.go:414]     Date: Fri, 18 Aug 2017 01:12:26 GMT
I0817 18:12:26.917173   14103 round_trippers.go:414]     Content-Type: application/json
I0817 18:12:26.917214   14103 request.go:991] Response Body: {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""clusterroles.rbac.authorization.k8s.io \""federation-controller-manager:federation-gce-us-west1-gce-us-west1\"" already exists"",""reason"":""AlreadyExists"",""details"":{""name"":""federation-controller-manager:federation-gce-us-west1-gce-us-west1"",""group"":""rbac.authorization.k8s.io"",""kind"":""clusterroles""},""code"":409}
I0817 18:12:26.917356   14103 join.go:630] Could not create role for service account in joining cluster: clusterroles.rbac.authorization.k8s.io ""federation-controller-manager:federation-gce-us-west1-gce-us-west1"" already exists
I0817 18:12:26.917375   14103 join.go:549] Error creating role binding for service account in joining cluster: clusterroles.rbac.authorization.k8s.io ""federation-controller-manager:federation-gce-us-west1-gce-us-west1"" already exists
I0817 18:12:26.917390   14103 join.go:234] Could not create cluster credentials secret: clusterroles.rbac.authorization.k8s.io ""federation-controller-manager:federation-gce-us-west1-gce-us-west1"" already exists
I0817 18:12:26.917446   14103 helpers.go:207] server response object: [{
  ""metadata"": {},
  ""status"": ""Failure"",
  ""message"": ""clusterroles.rbac.authorization.k8s.io \""federation-controller-manager:federation-gce-us-west1-gce-us-west1\"" already exists"",
  ""reason"": ""AlreadyExists"",
  ""details"": {
    ""name"": ""federation-controller-manager:federation-gce-us-west1-gce-us-west1"",
    ""group"": ""rbac.authorization.k8s.io"",
    ""kind"": ""clusterroles""
  },
  ""code"": 409
}]
F0817 18:12:26.917481   14103 helpers.go:120] Error from server (AlreadyExists): clusterroles.rbac.authorization.k8s.io ""federation-controller-manager:federation-gce-us-west1-gce-us-west1"" already exists
```

**What you expected to happen**:
Properly be able to `kubefed join` the cluster into the federation.

**How to reproduce it (as minimally and precisely as possible)**:
```
1. gcloud container clusters create --cluster-version=1.7.3 <cluster_name> --zone=<zone> --scopes ""cloud-platform,storage-ro,logging-write,monitoring-write,service-control,service-management,https://www.googleapis.com/auth/ndev.clouddns.readwrite""
2. kubefed init federation --host-cluster-context=<cluster_context> --dns-provider='google-clouddns' --dns-zone-name=<dns_zone_name> --controllermanager-arg-overrides=""--v=10"" --apiserver-arg-overrides=""--v=10"" --v=10
3. kubefed join <cluster_name> --host-cluster-context=<cluster_context> --cluster-context=<cluster_context> --v=10
```

**Environment**:
- Kubernetes version (use `kubectl version`):
```
→ kubefed version
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T07:00:21Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T06:43:48Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration**: GKE

cc: @kubernetes/sig-federation-bugs",west1-gke__us-west1-b_gce-us-west1,https://github.com/kubernetes/kubernetes
150,50888.0,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

/kind bug


**What happened**:
I was successfully able to `kubefed init` a federation control plane on a GKE k8s cluster, but upon attempting to join the cluster the following error occurs:

```
...
I0816 10:46:38.538848    5942 join.go:538] Creating service account in joining cluster
I0816 10:46:38.539061    5942 request.go:991] Request Body: {""kind"":""ServiceAccount"",""apiVersion"":""v1"",""metadata"":{""name"":""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"",""namespace"":""federation-system"",""creationTimestamp"":null,""annotations"":{""federation.alpha.kubernetes.io/cluster-name"":""gce-us-west1"",""federation.alpha.kubernetes.io/federation-name"":""federation""}}}
I0816 10:46:38.539134    5942 round_trippers.go:386] curl -k -v -XPOST  -H ""Accept: application/json, */*"" -H ""Content-Type: application/json"" -H ""User-Agent: kubefed/v1.7.3 (linux/amd64) kubernetes/2c2fe6e"" -H ""Authorization: Basic ABCDEFGHIJKLMNOPQRSTUVWXYZabcd=="" https://35.197.74.218/api/v1/namespaces/federation-system/serviceaccounts
I0816 10:46:38.576175    5942 round_trippers.go:405] POST https://35.197.74.218/api/v1/namespaces/federation-system/serviceaccounts 422 Unprocessable Entity in 37 milliseconds
I0816 10:46:38.576211    5942 round_trippers.go:411] Response Headers:
I0816 10:46:38.576225    5942 round_trippers.go:414]     Date: Wed, 16 Aug 2017 17:46:38 GMT
I0816 10:46:38.576236    5942 round_trippers.go:414]     Content-Type: application/json
I0816 10:46:38.576245    5942 round_trippers.go:414]     Content-Length: 1038
I0816 10:46:38.576515    5942 request.go:991] Response Body: {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""ServiceAccount \""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1\"" is invalid: metadata.name: Invalid value: \""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1\"": a DNS-1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')"",""reason"":""Invalid"",""details"":{""name"":""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"",""kind"":""ServiceAccount"",""causes"":[{""reason"":""FieldValueInvalid"",""message"":""Invalid value: \""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1\"": a DNS-1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')"",""field"":""metadata.name""}]},""code"":422}
I0816 10:46:38.576711    5942 join.go:541] Error creating service account in joining cluster: ServiceAccount ""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"" is invalid: metadata.name: Invalid value: ""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"": a DNS-1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')
I0816 10:46:38.576736    5942 join.go:234] Could not create cluster credentials secret: ServiceAccount ""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"" is invalid: metadata.name: Invalid value: ""gce-us-west1-gke_<project>_us-west1-b_gce-us-west1"": a DNS-1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')
```
This appears to be caused by the context name `gke_<project>_us-west1-b_gce-us-west1` which is using the default context name for a GKE cluster that is automatically imported to the kubeconfig file by `gcloud`.

If I update the kubeconfig context to get rid of the `_` to something like `gce-us-west1`, I can get past the above error but then subsequently hit the following error:

```
I0817 18:12:26.855195   14103 join.go:538] Creating service account in joining cluster
I0817 18:12:26.855329   14103 request.go:991] Request Body: {""kind"":""ServiceAccount"",""apiVersion"":""v1"",""metadata"":{""name"":""gce-us-west1-gce-us-west1"",""namespace"":""federation-system"",""creationTimestamp"":null,""annotations"":{""federation.alpha.kubernetes.io/cluster-name"":""gce-us-west1"",""federation.alpha.kubernetes.io/federation-name"":""federation""}}}
I0817 18:12:26.855376   14103 round_trippers.go:386] curl -k -v -XPOST  -H ""Accept: application/json, */*"" -H ""Content-Type: application/json"" -H ""User-Agent: kubefed/v1.7.3 (linux/amd64) kubernetes/2c2fe6e"" -H ""Authorization: Basic ABCDEFGHIJKLMNOPQRSTUVWXYZabcd=="" https://35.197.74.218/api/v1/namespaces/federation-system/serviceaccounts
I0817 18:12:26.885490   14103 round_trippers.go:405] POST https://35.197.74.218/api/v1/namespaces/federation-system/serviceaccounts 201 Created in 30 milliseconds
I0817 18:12:26.885536   14103 round_trippers.go:411] Response Headers:
I0817 18:12:26.885550   14103 round_trippers.go:414]     Content-Length: 470
I0817 18:12:26.885560   14103 round_trippers.go:414]     Date: Fri, 18 Aug 2017 01:12:26 GMT
I0817 18:12:26.885570   14103 round_trippers.go:414]     Content-Type: application/json
I0817 18:12:26.886617   14103 request.go:991] Response Body: {""kind"":""ServiceAccount"",""apiVersion"":""v1"",""metadata"":{""name"":""gce-us-west1-gce-us-west1"",""namespace"":""federation-system"",""selfLink"":""/api/v1/namespaces/federation-system/serviceaccounts/gce-us-west1-gce-us-west1"",""uid"":""4c7d5fb9-83b2-11e7-bda1-42010a8a01d8"",""resourceVersion"":""220285"",""creationTimestamp"":""2017-08-18T01:12:26Z"",""annotations"":{""federation.alpha.kubernetes.io/cluster-name"":""gce-us-west1"",""federation.alpha.kubernetes.io/federation-name"":""federation""}}}
I0817 18:12:26.886800   14103 join.go:544] Created service account in joining cluster
I0817 18:12:26.886817   14103 join.go:546] Creating role binding for service account in joining cluster
I0817 18:12:26.887024   14103 request.go:991] Request Body: {""kind"":""ClusterRole"",""apiVersion"":""rbac.authorization.k8s.io/v1beta1"",""metadata"":{""name"":""federation-controller-manager:federation-gce-us-west1-gce-us-west1"",""namespace"":""federation-system"",""creationTimestamp"":null,""annotations"":{""federation.alpha.kubernetes.io/cluster-name"":""gce-us-west1"",""federation.alpha.kubernetes.io/federation-name"":""federation""}},""rules"":[{""verbs"":[""*""],""apiGroups"":[""*""],""resources"":[""*""]},{""verbs"":[""get""],""nonResourceURLs"":[""/healthz""]}]}
I0817 18:12:26.887093   14103 round_trippers.go:386] curl -k -v -XPOST  -H ""Accept: application/json, */*"" -H ""Content-Type: application/json"" -H ""User-Agent: kubefed/v1.7.3 (linux/amd64) kubernetes/2c2fe6e"" -H ""Authorization: Basic ABCDEFGHIJKLMNOPQRSTUVWXYZabcd=="" https://35.197.74.218/apis/rbac.authorization.k8s.io/v1beta1/clusterroles
I0817 18:12:26.917114   14103 round_trippers.go:405] POST https://35.197.74.218/apis/rbac.authorization.k8s.io/v1beta1/clusterroles 409 Conflict in 29 milliseconds
I0817 18:12:26.917149   14103 round_trippers.go:411] Response Headers:
I0817 18:12:26.917161   14103 round_trippers.go:414]     Content-Length: 388
I0817 18:12:26.917167   14103 round_trippers.go:414]     Date: Fri, 18 Aug 2017 01:12:26 GMT
I0817 18:12:26.917173   14103 round_trippers.go:414]     Content-Type: application/json
I0817 18:12:26.917214   14103 request.go:991] Response Body: {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""clusterroles.rbac.authorization.k8s.io \""federation-controller-manager:federation-gce-us-west1-gce-us-west1\"" already exists"",""reason"":""AlreadyExists"",""details"":{""name"":""federation-controller-manager:federation-gce-us-west1-gce-us-west1"",""group"":""rbac.authorization.k8s.io"",""kind"":""clusterroles""},""code"":409}
I0817 18:12:26.917356   14103 join.go:630] Could not create role for service account in joining cluster: clusterroles.rbac.authorization.k8s.io ""federation-controller-manager:federation-gce-us-west1-gce-us-west1"" already exists
I0817 18:12:26.917375   14103 join.go:549] Error creating role binding for service account in joining cluster: clusterroles.rbac.authorization.k8s.io ""federation-controller-manager:federation-gce-us-west1-gce-us-west1"" already exists
I0817 18:12:26.917390   14103 join.go:234] Could not create cluster credentials secret: clusterroles.rbac.authorization.k8s.io ""federation-controller-manager:federation-gce-us-west1-gce-us-west1"" already exists
I0817 18:12:26.917446   14103 helpers.go:207] server response object: [{
  ""metadata"": {},
  ""status"": ""Failure"",
  ""message"": ""clusterroles.rbac.authorization.k8s.io \""federation-controller-manager:federation-gce-us-west1-gce-us-west1\"" already exists"",
  ""reason"": ""AlreadyExists"",
  ""details"": {
    ""name"": ""federation-controller-manager:federation-gce-us-west1-gce-us-west1"",
    ""group"": ""rbac.authorization.k8s.io"",
    ""kind"": ""clusterroles""
  },
  ""code"": 409
}]
F0817 18:12:26.917481   14103 helpers.go:120] Error from server (AlreadyExists): clusterroles.rbac.authorization.k8s.io ""federation-controller-manager:federation-gce-us-west1-gce-us-west1"" already exists
```

**What you expected to happen**:
Properly be able to `kubefed join` the cluster into the federation.

**How to reproduce it (as minimally and precisely as possible)**:
```
1. gcloud container clusters create --cluster-version=1.7.3 <cluster_name> --zone=<zone> --scopes ""cloud-platform,storage-ro,logging-write,monitoring-write,service-control,service-management,https://www.googleapis.com/auth/ndev.clouddns.readwrite""
2. kubefed init federation --host-cluster-context=<cluster_context> --dns-provider='google-clouddns' --dns-zone-name=<dns_zone_name> --controllermanager-arg-overrides=""--v=10"" --apiserver-arg-overrides=""--v=10"" --v=10
3. kubefed join <cluster_name> --host-cluster-context=<cluster_context> --cluster-context=<cluster_context> --v=10
```

**Environment**:
- Kubernetes version (use `kubectl version`):
```
→ kubefed version
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T07:00:21Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.3"", GitCommit:""2c2fe6e8278a5db2d15a013987b53968c743f2a1"", GitTreeState:""clean"", BuildDate:""2017-08-03T06:43:48Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration**: GKE

cc: @kubernetes/sig-federation-bugs",manager:federation-gce-us-west1-gce-us-west1,https://github.com/kubernetes/kubernetes
151,50857.0,"/kind bug
/sig api-machinery

kubernetes is sending a broken response when watching logs over HTTP 1.1. It sends a chunked response without sending the the size of the chunk first.

[Relevant RFC for reference.](https://tools.ietf.org/html/rfc7230#section-4.1)

Example:
```
curl --http1.1 -kv 'https://192.168.99.100:8443/api/v1/namespaces/default/pods/nginx-1423793266-mq53r/log?follow=true' -H ""Authorization: Bearer $TOKEN""
*   Trying 192.168.99.100...
* TCP_NODELAY set
* Connected to 192.168.99.100 (192.168.99.100) port 8443 (#0)
* Initializing NSS with certpath: sql:/etc/pki/nssdb
* skipping SSL peer certificate verification
* NSS: client certificate not found (nickname not specified)
* ALPN, server accepted to use http/1.1
* SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
* Server certificate:
* 	subject: CN=minikube,O=system:masters
* 	start date: Aug 17 17:04:24 2017 GMT
* 	expire date: Aug 17 17:04:24 2018 GMT
* 	common name: minikube
* 	issuer: CN=minikubeCA
> GET /api/v1/namespaces/default/pods/nginx-1423793266-mq53r/log?follow=true HTTP/1.1
> Host: 192.168.99.100:8443
> User-Agent: curl/7.53.1
> Accept: */*
> Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tZjl2bHAiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjI1MDRlOWFkLTgzNmUtMTFlNy05MWIwLTA4MDAyN2MzY2I1NyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.QFajWXR2ZFg72giS_kuudL0Lz5yPGZ_sQdaYRrwAyOjfVwcPWqjnFOwyqCesKrqMsdQkAWLNYiCb1h4SxpBW9HPjymAisqcO3Dq8azL9zMpFHwbZNtboUrSz5XuWtun3zOgbG3JhK-KZTuD2yP241Aaf0QAFZ80k-5ZT4chELQ-gQzCJco_8kv1trhCmveKk0aozMApKYQOJPpN0Z-cPCHEOxUsEMk2mtJEd2IvaFl6QerHKA-guUPuYMkbWAM_GMZh_D5IGcsAj9x9gJb7d5EvbMx4qYW_hW0SGbDIcwxdjAY4_Nv0xrOmYWIh789RcmM2ikCH5gOpesF8hhdUWjg
> 
< HTTP/1.1 200 OK
< Content-Type: text/plain
< Date: Thu, 17 Aug 2017 17:45:03 GMT
< Transfer-Encoding: chunked
< 
172.17.0.1 - - [17/Aug/2017:17:43:54 +0000] ""GET / HTTP/1.1"" 200 612 ""-"" ""curl/7.53.1"" ""-""
```

**Environment**:
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.0+695f48a16f"", GitCommit:""d2e5420"", GitTreeState:""clean"", BuildDate:""2017-08-10T18:57:36Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.0"", GitCommit:""d3ada0119e776222f11ec7945e6d860061339aad"", GitTreeState:""clean"", BuildDate:""2017-07-26T00:12:31Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}
```",KZTuD2yP241Aaf0QAFZ80k-5ZT4chELQ-gQzCJco_8kv1trhCmveKk0aozMApKYQOJPpN0Z-cPCHEOxUsEMk2mtJEd2IvaFl6QerHKA-guUPuYMkbWAM_GMZh_D5IGcsAj9x9gJb7d5EvbMx4qYW_hW0SGbDIcwxdjAY4_Nv0xrOmYWIh789RcmM2ikCH5gOpesF8hhdUWjg,https://github.com/kubernetes/kubernetes
152,50432.0,"**Is this a BUG REPORT or FEATURE REQUEST?**: BUG REPORT


**What happened**:
I have configured oidc with kubernetes. But when i try `kubectl --user=spnzip@gmail.com get nodes` i get `error from server (Forbidden): User ""system:anonymous"" cannot list nodes at the cluster scope. (get nodes)
`

**What you expected to happen**:
The command should authenticate user and display the nodes 

**How to reproduce it (as minimally and precisely as possible)**:
1. Install single node kubernetes cluster with kubeadm
2. Obtain client secrets with the k8s-oidc-helper tool
k8s-oidc-helper -c <path where user's client id and secret is stored)
3. copy paste the code generated to ~/.kube/config file
```
- name: spnzig@gmail.com
  user:
    auth-provider:
      config:
        client-id: xxxxxxxxxx-xxxxxxx.apps.googleusercontent.com
        client-secret: xxxxxxxxxxxxxxxxxxxxxxx
        id-token: eyJhbGciOiJSUzI1NiIsImtpZCI6IjczMTdkOTM1MWQ1Y .... njZaIWlMaAFEfIrGPy-49TQ
        idp-issuer-url: https://accounts.google.com
        refresh-token: 1/3XPbQPD...g8PuNPs
      name: oidc
```

4. Set the context for the new user in ~/.kube/config file
```
- context:
    cluster: kubernetes
    user: spnzig@gmail.com
  name: spnzig@kubernetes
```

5. Switch context from admin to new user
`kubectl config use-context spnzig@kubernetes `
 
6. Run kubectl --user=spnzig@gmail.com get nodes to see the error 
`kubectl --user=spnzig@gmail.com get nodes`

**Anything else we need to know?**:
I have installed kubernetes on Ubuntu 16.04LTS.

**Environment**:
- Kubernetes version (use `kubectl version`): v1.7.3
- Cloud provider or hardware configuration**: hardware 
- OS (e.g. from /etc/os-release): Ubuntu 16.04
- Install tools: kubeadm, kubernetes, k8s-oidc-helper, go

",eyJhbGciOiJSUzI1NiIsImtpZCI6IjczMTdkOTM1MWQ1Y,https://github.com/kubernetes/kubernetes
153,50432.0,"**Is this a BUG REPORT or FEATURE REQUEST?**: BUG REPORT


**What happened**:
I have configured oidc with kubernetes. But when i try `kubectl --user=spnzip@gmail.com get nodes` i get `error from server (Forbidden): User ""system:anonymous"" cannot list nodes at the cluster scope. (get nodes)
`

**What you expected to happen**:
The command should authenticate user and display the nodes 

**How to reproduce it (as minimally and precisely as possible)**:
1. Install single node kubernetes cluster with kubeadm
2. Obtain client secrets with the k8s-oidc-helper tool
k8s-oidc-helper -c <path where user's client id and secret is stored)
3. copy paste the code generated to ~/.kube/config file
```
- name: spnzig@gmail.com
  user:
    auth-provider:
      config:
        client-id: xxxxxxxxxx-xxxxxxx.apps.googleusercontent.com
        client-secret: xxxxxxxxxxxxxxxxxxxxxxx
        id-token: eyJhbGciOiJSUzI1NiIsImtpZCI6IjczMTdkOTM1MWQ1Y .... njZaIWlMaAFEfIrGPy-49TQ
        idp-issuer-url: https://accounts.google.com
        refresh-token: 1/3XPbQPD...g8PuNPs
      name: oidc
```

4. Set the context for the new user in ~/.kube/config file
```
- context:
    cluster: kubernetes
    user: spnzig@gmail.com
  name: spnzig@kubernetes
```

5. Switch context from admin to new user
`kubectl config use-context spnzig@kubernetes `
 
6. Run kubectl --user=spnzig@gmail.com get nodes to see the error 
`kubectl --user=spnzig@gmail.com get nodes`

**Anything else we need to know?**:
I have installed kubernetes on Ubuntu 16.04LTS.

**Environment**:
- Kubernetes version (use `kubectl version`): v1.7.3
- Cloud provider or hardware configuration**: hardware 
- OS (e.g. from /etc/os-release): Ubuntu 16.04
- Install tools: kubeadm, kubernetes, k8s-oidc-helper, go

",xxxxxxxxxxxxxxxxxxxxxxx,https://github.com/kubernetes/kubernetes
154,49415.0,"<!-- This form is for bug reports and feature requests ONLY! 

If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
-->

**Is this a BUG REPORT or FEATURE REQUEST?**:

> Uncomment only one, leave it on its own line: 
>
/kind bug
> /kind feature


**What happened**: Running `kubeadm init --config=<path>` fails.

**What you expected to happen**: `kubeadm init --config=<path>` configures based on the provided file.

**How to reproduce it (as minimally and precisely as possible)**:

Create a file `/tmp/kubeadm.yaml` with the below content:

```
---
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
token: m52zte.m52ztet6s1jknlpg
cloudProvider: aws
kubernetesVersion: v1.7.1
apiServerCertSANs:
- plombardi.kubernaut.io
```

Run `kubeadm init --config=/tmp/kubeadm.yaml`

Get error: 

```
kubeadm init --config /tmp/kubeadm.yaml
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[init] Using Kubernetes version: v1.7.1
[init] Using Authorization modes: [Node RBAC]
[init] WARNING: For cloudprovider integrations to work --cloud-provider must be set for all kubelets in the cluster.
	(/etc/systemd/system/kubelet.service.d/10-kubeadm.conf should be edited for this purpose)
[preflight] Running pre-flight checks
[preflight] WARNING: docker version is greater than the most recently validated version. Docker version: 17.06.0-ce. Max validated version: 1.12
can not mix '--config' with other arguments

```

**Anything else we need to know?**:

**Environment**:
- Kubernetes version (use `kubectl version`): NA
- Cloud provider or hardware configuration**: AWS
- OS (e.g. from /etc/os-release): CentOS 7.3 1703_01
- Kernel (e.g. `uname -a`): Linux ip-10-10-0-216.ec2.internal 3.10.0-514.26.2.el7.x86_64 #1 SMP Tue Jul 4 15:04:05 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

- Install tools:
- Others: Kubeadm Version = `kubeadm version: &version.Info{Major:""1"", Minor:""7"", GitVersion:""v1.7.1"", GitCommit:""1dc5c66f5dd61da08412a74221ecc79208c2165b"", GitTreeState:""clean"", BuildDate:""2017-07-14T01:48:01Z"", GoVersion:""go1.8.3"", Compiler:""gc"", Platform:""linux/amd64""}`
",kubernetesVersion,https://github.com/kubernetes/kubernetes
155,46460.0,"## Additions

Allows providing a configuration file (using flag `--experimental-encryption-provider-config`) to use the existing AEAD transformer (with multiple keys) by composing mutable transformer, prefix transformer (for parsing providerId), another prefix transformer (for parsing keyId), and AES-GCM transformers (one for each key). Multiple providers can be configured using the configuration file.

Example configuration:
```
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
    - namespaces
    providers:
    - aes:
        keys:
        - name: key1
          secret: c2vjcmv0iglzihnly3vyzq==
        - name: key2
          secret: dghpcybpcybwyxnzd29yza==
    - identity: {}
```

Need for configuration discussed in:
#41939
[Encryption](https://github.com/destijl/community/blob/3418b4e4c6358f5dc747a37b90a97bc792f159ee/contributors/design-proposals/encryption.md)

**Pathway of a read/write request**:
1. MutableTransformer
2. PrefixTransformer reads the provider-id, and passes the request further if that matches.
3. PrefixTransformer reads the key-id, and passes the request further if that matches.
4. GCMTransformer tries decrypting and authenticating the cipher text in case of reads. Similarly for writes.

## Caveats
1. To keep the command line parameter parsing independent of the individual transformer's configuration, we need to convert the configuration to an `interface{}` and manually parse it in the transformer. Suggestions on better ways to do this are welcome.

2. Flags `--encryption-provider` and `--encrypt-resource` (both mentioned in [this document](https://github.com/destijl/community/blob/3418b4e4c6358f5dc747a37b90a97bc792f159ee/contributors/design-proposals/encryption.md) ) are not supported in this because they do not allow more than one provider, and the current format for the configuration file possibly supersedes their functionality.

3. Currently, it can be tested by adding `--experimental-encryption-provider-config=config.yml` to `hack/local-up-cluster.sh` on line 511, and placing the above configuration in `config.yml` in the root project directory.

Previous discussion on these changes:
https://github.com/sakshamsharma/kubernetes/pull/1

@jcbsmpsn @destijl @smarterclayton

## TODO
1. Investigate if we need to store keys on disk (per [encryption.md](https://github.com/destijl/community/blob/3418b4e4c6358f5dc747a37b90a97bc792f159ee/contributors/design-proposals/encryption.md#option-1-simple-list-of-keys-on-disk))
2. Look at [alpha flag conventions](https://github.com/kubernetes/kubernetes/blob/master/pkg/features/kube_features.go)
3. Need to reserve `k8s:enc` prefix formally for encrypted data. Else find a better way to detect transformed data.",c2vjcmv0iglzihnly3vyzq,https://github.com/kubernetes/kubernetes
156,46460.0,"## Additions

Allows providing a configuration file (using flag `--experimental-encryption-provider-config`) to use the existing AEAD transformer (with multiple keys) by composing mutable transformer, prefix transformer (for parsing providerId), another prefix transformer (for parsing keyId), and AES-GCM transformers (one for each key). Multiple providers can be configured using the configuration file.

Example configuration:
```
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
    - namespaces
    providers:
    - aes:
        keys:
        - name: key1
          secret: c2vjcmv0iglzihnly3vyzq==
        - name: key2
          secret: dghpcybpcybwyxnzd29yza==
    - identity: {}
```

Need for configuration discussed in:
#41939
[Encryption](https://github.com/destijl/community/blob/3418b4e4c6358f5dc747a37b90a97bc792f159ee/contributors/design-proposals/encryption.md)

**Pathway of a read/write request**:
1. MutableTransformer
2. PrefixTransformer reads the provider-id, and passes the request further if that matches.
3. PrefixTransformer reads the key-id, and passes the request further if that matches.
4. GCMTransformer tries decrypting and authenticating the cipher text in case of reads. Similarly for writes.

## Caveats
1. To keep the command line parameter parsing independent of the individual transformer's configuration, we need to convert the configuration to an `interface{}` and manually parse it in the transformer. Suggestions on better ways to do this are welcome.

2. Flags `--encryption-provider` and `--encrypt-resource` (both mentioned in [this document](https://github.com/destijl/community/blob/3418b4e4c6358f5dc747a37b90a97bc792f159ee/contributors/design-proposals/encryption.md) ) are not supported in this because they do not allow more than one provider, and the current format for the configuration file possibly supersedes their functionality.

3. Currently, it can be tested by adding `--experimental-encryption-provider-config=config.yml` to `hack/local-up-cluster.sh` on line 511, and placing the above configuration in `config.yml` in the root project directory.

Previous discussion on these changes:
https://github.com/sakshamsharma/kubernetes/pull/1

@jcbsmpsn @destijl @smarterclayton

## TODO
1. Investigate if we need to store keys on disk (per [encryption.md](https://github.com/destijl/community/blob/3418b4e4c6358f5dc747a37b90a97bc792f159ee/contributors/design-proposals/encryption.md#option-1-simple-list-of-keys-on-disk))
2. Look at [alpha flag conventions](https://github.com/kubernetes/kubernetes/blob/master/pkg/features/kube_features.go)
3. Need to reserve `k8s:enc` prefix formally for encrypted data. Else find a better way to detect transformed data.",dghpcybpcybwyxnzd29yza,https://github.com/kubernetes/kubernetes
157,43435.0,"**Kubernetes version** (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""6+"", GitVersion:""v1.6.0-beta.1-dirty"", GitCommit:""23cded36d1d20a538f97e0da05c1d2b62a6be700"", GitTreeState:""dirty"", BuildDate:""2017-03-20T09:11:01Z"", GoVersion:""go1.7"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""6+"", GitVersion:""v1.6.0-beta.3"", GitCommit:""0cd5ed469508e6dfc807ee6681561c845828917e"", GitTreeState:""clean"", BuildDate:""2017-03-11T04:09:08Z"", GoVersion:""go1.7.5"", Compiler:""gc"", Platform:""linux/amd64""}

**What happened**:
We are creating TPRs inside the pod using incluster authentication. Previously (version 1.5.4 and before that) they were successfully created, but using trunk kubernetes i am getting Forbidden error with following logs:

```
I0321 07:50:52.754466       1 request.go:558] Request Body: ""{\""kind\"":\""ThirdPartyResource\"",\""apiVersion\"":\""extensions/v1beta1\"",\""metadata\"":{\""name\"":\""ip-node.ipcontroller.ext\"",\""creationTimestamp\"":null},\""versions\"":[{\""name\"":\""v1\""}]}\n""
I0321 07:50:52.754539       1 round_trippers.go:299] curl -k -v -XPOST  -H ""Accept: application/json, */*"" -H ""Content-Type: application/json"" -H ""User-Agent: ipmanager/v0.0.0 (linux/amd64) kubernetes/$Format"" -H ""Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tcnIyMzMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImMyZGIyNGFjLTBkN2EtMTFlNy1iZjZkLTAyNDIwYWMwMDAwMiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.DGDxUqIpcDhdgqvfRDoA3QP3wj5d94Z6mZ1Vs0-hlV5_652E6-TIMfwQu2yjPGlokgOzox4zUVzswCiVipLyg-ZDWNlaL3qrQLY5h0_iXcB3elO1XLzBXxhRhahKX9V34ECSjk1ztgeGkWpdnFBFCaVcCO8RzaqVyzcsHQGj_WMwoGVON_6c9a2e1X5mRGI-vI2V9gXilm_1Ct92Nx84zOo6tT7xz1iUXJh9WF6s1QQNYGQmr4QOQkeZXGvcwaCezfg3V_Aj7YXHTfMDZDXfYpKNBXE1TCG4wlrbXKNWZZuPsWxhKUvf86DEYUktUVpoqTmybSiyZlUGPWQ18Gzz5w"" https://10.96.0.1:443/apis/extensions/v1beta1/thirdpartyresources
I0321 07:50:52.754976       1 round_trippers.go:318] POST https://10.96.0.1:443/apis/extensions/v1beta1/thirdpartyresources 403 Forbidden in 0 milliseconds
I0321 07:50:52.754986       1 round_trippers.go:324] Response Headers:
I0321 07:50:52.754990       1 round_trippers.go:327]     Content-Type: text/plain
I0321 07:50:52.754995       1 round_trippers.go:327]     X-Content-Type-Options: nosniff
I0321 07:50:52.755000       1 round_trippers.go:327]     Content-Length: 111
I0321 07:50:52.755002       1 round_trippers.go:327]     Date: Tue, 21 Mar 2017 07:50:52 GMT
I0321 07:50:52.755017       1 request.go:896] Response Body: User ""system:serviceaccount:default:default"" cannot create thirdpartyresources.extensions at the cluster scope.
I0321 07:50:52.755024       1 request.go:986] Response Body: ""User \""system:serviceaccount:default:default\"" cannot create thirdpartyresources.extensions at the cluster scope.""
```

**What you expected to happen**:
Resources will be created

**How to reproduce it** (as minimally and precisely as possible):
Create any TPR using incluster auth

",hlV5_652E6-TIMfwQu2yjPGlokgOzox4zUVzswCiVipLyg-ZDWNlaL3qrQLY5h0_iXcB3elO1XLzBXxhRhahKX9V34ECSjk1ztgeGkWpdnFBFCaVcCO8RzaqVyzcsHQGj_WMwoGVON_6c9a2e1X5mRGI-vI2V9gXilm_1Ct92Nx84zOo6tT7xz1iUXJh9WF6s1QQNYGQmr4QOQkeZXGvcwaCezfg3V_Aj7YXHTfMDZDXfYpKNBXE1TCG4wlrbXKNWZZuPsWxhKUvf86DEYUktUVpoqTmybSiyZlUGPWQ18Gzz5w,https://github.com/kubernetes/kubernetes
158,42780.0,"Generate NPD token during upgrade.

I could not fully verify this change because of https://github.com/kubernetes/kubernetes/issues/42199. However, at least I tried upgrade master, and the corresponding environment variables are correctly generated.
```
...
ENABLE_NODE_PROBLEM_DETECTOR: 'standalone'
...
KUBELET_TOKEN: 'PKNgAaVXeL3VojND2s0KMleELjzGK0oW'
```

@maisem @dchen1107 ",PKNgAaVXeL3VojND2s0KMleELjzGK0oW,https://github.com/kubernetes/kubernetes
159,41388.0,"ref to https://github.com/kubernetes/kubeadm/issues/157

```
#kubeadm init
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[init] Using Kubernetes version: v1.6.0-alpha.1
[init] Using Authorization mode: RBAC
[init] A token has not been provided, generating one
[preflight] Running pre-flight checks
[preflight] Starting the kubelet service
[certificates] Generated CA certificate and key.
[certificates] Generated API server certificate and key.
[certificates] Generated API server kubelet client certificate and key.
[certificates] Generated service account token signing key.
[certificates] Generated service account token signing public key.
[certificates] Valid certificates and keys now exist in ""/etc/kubernetes/pki""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/admin.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/kubelet.conf""
[apiclient] Created API client, waiting for the control plane to become ready
[apiclient] Waiting for API server authorization
[apiclient] Waiting for API server authorization
[apiclient] Waiting for API server authorization
[apiclient] All control plane components are healthy after 18.260284 seconds
[apiclient] Waiting for at least one node to register and become ready
[apiclient] First node is ready after 3.509666 seconds
[apiclient] Test deployment succeeded
[apiconfig] Created RBAC rules
[token-discovery] Using token: e4b248:0563c2e3d0635b02
[token-discovery] Created the kube-discovery deployment, waiting for it to become ready
[token-discovery] kube-discovery is ready after 2.503299 seconds
[addons] Created essential addon: kube-proxy
[addons] Created essential addon: kube-dns

Your Kubernetes master has initialized successfully!

You should now deploy a pod network to the cluster.
Run ""kubectl apply -f [podnetwork].yaml"" with one of the options listed at:
    http://kubernetes.io/docs/admin/addons/

You can now join any number of machines by running the following on each node:

kubeadm join --discovery token://e4b248:0563c2e3d0635b02@192.168.122.100:9898
```",0563c2e3d0635b02,https://github.com/kubernetes/kubernetes
160,40008.0,"**What this PR does / why we need it**: `kubeadm init` must validate or generate a token before anything else. Otherwise, if token validation or generation fail, one will need to run `kubeadm reset && systemctl restart kubelet` before re-running `kubeadm init`.

**Which issue this PR fixes**: fixes kubernetes/kubeadm#112

**Special notes for your reviewer**: /cc @luxas

Tested manually.

### With no token

```
$ sudo ./kubeadm init
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[preflight] Running pre-flight checks
[init] Using Kubernetes version: v1.5.2
[token-discovery] A token has not been provided, generating one
[certificates] Generated Certificate Authority key and certificate.
[certificates] Generated API Server key and certificate
[certificates] Generated Service Account signing keys
[certificates] Created keys and certificates in ""/etc/kubernetes/pki""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/admin.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/kubelet.conf""
[apiclient] Created API client, waiting for the control plane to become ready
[apiclient] All control plane components are healthy after 7.762803 seconds
[apiclient] Waiting for at least one node to register and become ready
[apiclient] First node is ready after 1.003148 seconds
[apiclient] Creating a test deployment
[apiclient] Test deployment succeeded
[token-discovery] Using token: 8321b6:a535ba541af7623c
[token-discovery] Created the kube-discovery deployment, waiting for it to become ready
[token-discovery] kube-discovery is ready after 1.003423 seconds
[addons] Created essential addon: kube-proxy
[addons] Created essential addon: kube-dns
Your Kubernetes master has initialized successfully!
You should now deploy a pod network to the cluster.
Run ""kubectl apply -f [podnetwork].yaml"" with one of the options listed at:
    http://kubernetes.io/docs/admin/addons/
You can now join any number of machines by running the following on each node:
kubeadm join --discovery token://8321b6:a535ba541af7623c@10.142.0.6:9898
```

### With invalid token

```
$ sudo ./kubeadm init --discovery token://12345:12345
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[preflight] Running pre-flight checks
[init] Using Kubernetes version: v1.5.2
[token-discovery] A token has been provided, validating [&{ID:12345 Secret:12345 Addresses:[]}]
token [""12345:12345""] was not of form [""^([a-z0-9]{6})\\:([a-z0-9]{16})$""]
```

### With valid token

```
$ sudo ./kubeadm ex token generate
cd540e:c0e0318e2f4a63b1

$ sudo ./kubeadm init --discovery token://cd540e:c0e0318e2f4a63b1
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[preflight] Running pre-flight checks
[init] Using Kubernetes version: v1.5.2
[token-discovery] A token has been provided, validating [&{ID:cd540e Secret:c0e0318e2f4a63b1 Addresses:[]}]
[certificates] Generated Certificate Authority key and certificate.
[certificates] Generated API Server key and certificate
[certificates] Generated Service Account signing keys
[certificates] Created keys and certificates in ""/etc/kubernetes/pki""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/admin.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/kubelet.conf""
[apiclient] Created API client, waiting for the control plane to become ready
[apiclient] All control plane components are healthy after 13.513305 seconds
[apiclient] Waiting for at least one node to register and become ready
[apiclient] First node is ready after 0.502656 seconds
[apiclient] Creating a test deployment
[apiclient] Test deployment succeeded
[token-discovery] Using token: cd540e:c0e0318e2f4a63b1
[token-discovery] Created the kube-discovery deployment, waiting for it to become ready
[token-discovery] kube-discovery is ready after 2.002457 seconds
[addons] Created essential addon: kube-proxy
[addons] Created essential addon: kube-dns
Your Kubernetes master has initialized successfully!
You should now deploy a pod network to the cluster.
Run ""kubectl apply -f [podnetwork].yaml"" with one of the options listed at:
    http://kubernetes.io/docs/admin/addons/
You can now join any number of machines by running the following on each node:
kubeadm join --discovery token://cd540e:c0e0318e2f4a63b1@10.142.0.6:9898
```

**Release note**:
```release-note
NONE
```
",c0e0318e2f4a63b1,https://github.com/kubernetes/kubernetes
161,40008.0,"**What this PR does / why we need it**: `kubeadm init` must validate or generate a token before anything else. Otherwise, if token validation or generation fail, one will need to run `kubeadm reset && systemctl restart kubelet` before re-running `kubeadm init`.

**Which issue this PR fixes**: fixes kubernetes/kubeadm#112

**Special notes for your reviewer**: /cc @luxas

Tested manually.

### With no token

```
$ sudo ./kubeadm init
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[preflight] Running pre-flight checks
[init] Using Kubernetes version: v1.5.2
[token-discovery] A token has not been provided, generating one
[certificates] Generated Certificate Authority key and certificate.
[certificates] Generated API Server key and certificate
[certificates] Generated Service Account signing keys
[certificates] Created keys and certificates in ""/etc/kubernetes/pki""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/admin.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/kubelet.conf""
[apiclient] Created API client, waiting for the control plane to become ready
[apiclient] All control plane components are healthy after 7.762803 seconds
[apiclient] Waiting for at least one node to register and become ready
[apiclient] First node is ready after 1.003148 seconds
[apiclient] Creating a test deployment
[apiclient] Test deployment succeeded
[token-discovery] Using token: 8321b6:a535ba541af7623c
[token-discovery] Created the kube-discovery deployment, waiting for it to become ready
[token-discovery] kube-discovery is ready after 1.003423 seconds
[addons] Created essential addon: kube-proxy
[addons] Created essential addon: kube-dns
Your Kubernetes master has initialized successfully!
You should now deploy a pod network to the cluster.
Run ""kubectl apply -f [podnetwork].yaml"" with one of the options listed at:
    http://kubernetes.io/docs/admin/addons/
You can now join any number of machines by running the following on each node:
kubeadm join --discovery token://8321b6:a535ba541af7623c@10.142.0.6:9898
```

### With invalid token

```
$ sudo ./kubeadm init --discovery token://12345:12345
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[preflight] Running pre-flight checks
[init] Using Kubernetes version: v1.5.2
[token-discovery] A token has been provided, validating [&{ID:12345 Secret:12345 Addresses:[]}]
token [""12345:12345""] was not of form [""^([a-z0-9]{6})\\:([a-z0-9]{16})$""]
```

### With valid token

```
$ sudo ./kubeadm ex token generate
cd540e:c0e0318e2f4a63b1

$ sudo ./kubeadm init --discovery token://cd540e:c0e0318e2f4a63b1
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[preflight] Running pre-flight checks
[init] Using Kubernetes version: v1.5.2
[token-discovery] A token has been provided, validating [&{ID:cd540e Secret:c0e0318e2f4a63b1 Addresses:[]}]
[certificates] Generated Certificate Authority key and certificate.
[certificates] Generated API Server key and certificate
[certificates] Generated Service Account signing keys
[certificates] Created keys and certificates in ""/etc/kubernetes/pki""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/admin.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/kubelet.conf""
[apiclient] Created API client, waiting for the control plane to become ready
[apiclient] All control plane components are healthy after 13.513305 seconds
[apiclient] Waiting for at least one node to register and become ready
[apiclient] First node is ready after 0.502656 seconds
[apiclient] Creating a test deployment
[apiclient] Test deployment succeeded
[token-discovery] Using token: cd540e:c0e0318e2f4a63b1
[token-discovery] Created the kube-discovery deployment, waiting for it to become ready
[token-discovery] kube-discovery is ready after 2.002457 seconds
[addons] Created essential addon: kube-proxy
[addons] Created essential addon: kube-dns
Your Kubernetes master has initialized successfully!
You should now deploy a pod network to the cluster.
Run ""kubectl apply -f [podnetwork].yaml"" with one of the options listed at:
    http://kubernetes.io/docs/admin/addons/
You can now join any number of machines by running the following on each node:
kubeadm join --discovery token://cd540e:c0e0318e2f4a63b1@10.142.0.6:9898
```

**Release note**:
```release-note
NONE
```
",9]{6})\\:([a-z0-9]{16,https://github.com/kubernetes/kubernetes
162,40008.0,"**What this PR does / why we need it**: `kubeadm init` must validate or generate a token before anything else. Otherwise, if token validation or generation fail, one will need to run `kubeadm reset && systemctl restart kubelet` before re-running `kubeadm init`.

**Which issue this PR fixes**: fixes kubernetes/kubeadm#112

**Special notes for your reviewer**: /cc @luxas

Tested manually.

### With no token

```
$ sudo ./kubeadm init
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[preflight] Running pre-flight checks
[init] Using Kubernetes version: v1.5.2
[token-discovery] A token has not been provided, generating one
[certificates] Generated Certificate Authority key and certificate.
[certificates] Generated API Server key and certificate
[certificates] Generated Service Account signing keys
[certificates] Created keys and certificates in ""/etc/kubernetes/pki""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/admin.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/kubelet.conf""
[apiclient] Created API client, waiting for the control plane to become ready
[apiclient] All control plane components are healthy after 7.762803 seconds
[apiclient] Waiting for at least one node to register and become ready
[apiclient] First node is ready after 1.003148 seconds
[apiclient] Creating a test deployment
[apiclient] Test deployment succeeded
[token-discovery] Using token: 8321b6:a535ba541af7623c
[token-discovery] Created the kube-discovery deployment, waiting for it to become ready
[token-discovery] kube-discovery is ready after 1.003423 seconds
[addons] Created essential addon: kube-proxy
[addons] Created essential addon: kube-dns
Your Kubernetes master has initialized successfully!
You should now deploy a pod network to the cluster.
Run ""kubectl apply -f [podnetwork].yaml"" with one of the options listed at:
    http://kubernetes.io/docs/admin/addons/
You can now join any number of machines by running the following on each node:
kubeadm join --discovery token://8321b6:a535ba541af7623c@10.142.0.6:9898
```

### With invalid token

```
$ sudo ./kubeadm init --discovery token://12345:12345
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[preflight] Running pre-flight checks
[init] Using Kubernetes version: v1.5.2
[token-discovery] A token has been provided, validating [&{ID:12345 Secret:12345 Addresses:[]}]
token [""12345:12345""] was not of form [""^([a-z0-9]{6})\\:([a-z0-9]{16})$""]
```

### With valid token

```
$ sudo ./kubeadm ex token generate
cd540e:c0e0318e2f4a63b1

$ sudo ./kubeadm init --discovery token://cd540e:c0e0318e2f4a63b1
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[preflight] Running pre-flight checks
[init] Using Kubernetes version: v1.5.2
[token-discovery] A token has been provided, validating [&{ID:cd540e Secret:c0e0318e2f4a63b1 Addresses:[]}]
[certificates] Generated Certificate Authority key and certificate.
[certificates] Generated API Server key and certificate
[certificates] Generated Service Account signing keys
[certificates] Created keys and certificates in ""/etc/kubernetes/pki""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/admin.conf""
[kubeconfig] Wrote KubeConfig file to disk: ""/etc/kubernetes/kubelet.conf""
[apiclient] Created API client, waiting for the control plane to become ready
[apiclient] All control plane components are healthy after 13.513305 seconds
[apiclient] Waiting for at least one node to register and become ready
[apiclient] First node is ready after 0.502656 seconds
[apiclient] Creating a test deployment
[apiclient] Test deployment succeeded
[token-discovery] Using token: cd540e:c0e0318e2f4a63b1
[token-discovery] Created the kube-discovery deployment, waiting for it to become ready
[token-discovery] kube-discovery is ready after 2.002457 seconds
[addons] Created essential addon: kube-proxy
[addons] Created essential addon: kube-dns
Your Kubernetes master has initialized successfully!
You should now deploy a pod network to the cluster.
Run ""kubectl apply -f [podnetwork].yaml"" with one of the options listed at:
    http://kubernetes.io/docs/admin/addons/
You can now join any number of machines by running the following on each node:
kubeadm join --discovery token://cd540e:c0e0318e2f4a63b1@10.142.0.6:9898
```

**Release note**:
```release-note
NONE
```
",a535ba541af7623c,https://github.com/kubernetes/kubernetes
163,39508.0,"**Kubernetes version** (use `kubectl version`):
```
kubectl version
Client Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.6"", GitCommit:""e569a27d02001e343cb68086bc06d47804f62af6"", GitTreeState:""clean"", BuildDate:""2016-11-12T05:22:15Z"", GoVersion:""go1.7.1"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.6"", GitCommit:""e569a27d02001e343cb68086bc06d47804f62af6"", GitTreeState:""clean"", BuildDate:""2016-11-12T05:16:27Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: Azure CS
- **OS** (e.g. from /etc/os-release): Ubuntu 16.04 LTS 
- **Kernel** (e.g. `uname -a`): 4.4.0-57-generic 
- **Install tools**: Azure cli (2 agents, standard d2)

**What happened**:
Everything were working, until I decided to update deployment (using `replace`). After that pods stop starting and staid on `ContainerCreating` status.
In 'kubectl describe pod` I got:
```
  28m		40s		22	{kubelet k8s-agent-941b4238-0}			Warning		FailedMount	MountVolume.SetUp failed for volume ""kubernetes.io/secret/71af8ad3-d3c6-11e6-9251-000d3a216b93-default-token-i9xv0"" (spec.Name: ""default-token-i9xv0"") pod ""71af8ad3-d3c6-11e6-9251-000d3a216b93"" (UID: ""71af8ad3-d3c6-11e6-9251-000d3a216b93"") with: mount failed: exit status 32
Mounting arguments: tmpfs /var/lib/kubelet/pods/71af8ad3-d3c6-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0 tmpfs []
Output: mount: mount tmpfs on /var/lib/kubelet/pods/71af8ad3-d3c6-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0 failed: No space left on device


  26m	23s	13	{kubelet k8s-agent-941b4238-0}		Warning	FailedMount	Unable to mount volumes for pod ""router-4284723421-h9de2_default(71af8ad3-d3c6-11e6-9251-000d3a216b93)"": timeout expired waiting for volumes to attach/mount for pod ""router-4284723421-h9de2""/""default"". list of unattached/unmounted volumes=[default-token-i9xv0]
  26m	23s	13	{kubelet k8s-agent-941b4238-0}		Warning	FailedSync	Error syncing pod, skipping: timeout expired waiting for volumes to attach/mount for pod ""router-4284723421-h9de2""/""default"". list of unattached/unmounted volumes=[default-token-i9xv0]
```

However, on agents there is definitely enough space:
```
Filesystem      Size  Used Avail Use% Mounted on
udev            3.4G     0  3.4G   0% /dev
tmpfs           697M   17M  680M   3% /run
/dev/sda1        29G  7.6G   21G  27% /
tmpfs           3.5G     0  3.5G   0% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs           3.5G     0  3.5G   0% /sys/fs/cgroup
none             64K     0   64K   0% /etc/network/interfaces.dynamic.d
/dev/sdb1        14G   40M   14G   1% /mnt
tmpfs           100K     0  100K   0% /run/lxcfs/controllers
tmpfs           3.5G   12K  3.5G   1% /var/lib/kubelet/pods/b7abae8f-cefc-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-zk98l
shm              64M     0   64M   0% /var/lib/docker/containers/70f1efbd4a5564208ca79a329dfc3828f0315aba5d7b5034b6784e9341e96a10/shm
tmpfs           3.5G   12K  3.5G   1% /var/lib/kubelet/pods/9c1dfead-d393-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0
shm              64M     0   64M   0% /var/lib/docker/containers/b050aafce252bb14f65db18315ff2957ae3c08854806a65693d96534749f9d73/shm
shm              64M     0   64M   0% /var/lib/docker/containers/7643840c8ea7465697ccbc3966a6ce7d5a6b852ae924787b1892945458e4695e/shm
tmpfs           697M     0  697M   0% /run/user/1000
```
(I even see that token was mounted)

It looks very close to https://github.com/kubernetes/kubernetes/issues/13174 but seems it is somehow Azure related...

Also on same agent I have another pod running with a volume mounted:
```
Volumes:
  default-token-i9xv0:
    Type:	Secret (a volume populated by a Secret)
    SecretName:	default-token-i9xv0
```",71af8ad3-d3c6-11e6-9251-000d3a216b93,https://github.com/kubernetes/kubernetes
164,39508.0,"**Kubernetes version** (use `kubectl version`):
```
kubectl version
Client Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.6"", GitCommit:""e569a27d02001e343cb68086bc06d47804f62af6"", GitTreeState:""clean"", BuildDate:""2016-11-12T05:22:15Z"", GoVersion:""go1.7.1"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.6"", GitCommit:""e569a27d02001e343cb68086bc06d47804f62af6"", GitTreeState:""clean"", BuildDate:""2016-11-12T05:16:27Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: Azure CS
- **OS** (e.g. from /etc/os-release): Ubuntu 16.04 LTS 
- **Kernel** (e.g. `uname -a`): 4.4.0-57-generic 
- **Install tools**: Azure cli (2 agents, standard d2)

**What happened**:
Everything were working, until I decided to update deployment (using `replace`). After that pods stop starting and staid on `ContainerCreating` status.
In 'kubectl describe pod` I got:
```
  28m		40s		22	{kubelet k8s-agent-941b4238-0}			Warning		FailedMount	MountVolume.SetUp failed for volume ""kubernetes.io/secret/71af8ad3-d3c6-11e6-9251-000d3a216b93-default-token-i9xv0"" (spec.Name: ""default-token-i9xv0"") pod ""71af8ad3-d3c6-11e6-9251-000d3a216b93"" (UID: ""71af8ad3-d3c6-11e6-9251-000d3a216b93"") with: mount failed: exit status 32
Mounting arguments: tmpfs /var/lib/kubelet/pods/71af8ad3-d3c6-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0 tmpfs []
Output: mount: mount tmpfs on /var/lib/kubelet/pods/71af8ad3-d3c6-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0 failed: No space left on device


  26m	23s	13	{kubelet k8s-agent-941b4238-0}		Warning	FailedMount	Unable to mount volumes for pod ""router-4284723421-h9de2_default(71af8ad3-d3c6-11e6-9251-000d3a216b93)"": timeout expired waiting for volumes to attach/mount for pod ""router-4284723421-h9de2""/""default"". list of unattached/unmounted volumes=[default-token-i9xv0]
  26m	23s	13	{kubelet k8s-agent-941b4238-0}		Warning	FailedSync	Error syncing pod, skipping: timeout expired waiting for volumes to attach/mount for pod ""router-4284723421-h9de2""/""default"". list of unattached/unmounted volumes=[default-token-i9xv0]
```

However, on agents there is definitely enough space:
```
Filesystem      Size  Used Avail Use% Mounted on
udev            3.4G     0  3.4G   0% /dev
tmpfs           697M   17M  680M   3% /run
/dev/sda1        29G  7.6G   21G  27% /
tmpfs           3.5G     0  3.5G   0% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs           3.5G     0  3.5G   0% /sys/fs/cgroup
none             64K     0   64K   0% /etc/network/interfaces.dynamic.d
/dev/sdb1        14G   40M   14G   1% /mnt
tmpfs           100K     0  100K   0% /run/lxcfs/controllers
tmpfs           3.5G   12K  3.5G   1% /var/lib/kubelet/pods/b7abae8f-cefc-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-zk98l
shm              64M     0   64M   0% /var/lib/docker/containers/70f1efbd4a5564208ca79a329dfc3828f0315aba5d7b5034b6784e9341e96a10/shm
tmpfs           3.5G   12K  3.5G   1% /var/lib/kubelet/pods/9c1dfead-d393-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0
shm              64M     0   64M   0% /var/lib/docker/containers/b050aafce252bb14f65db18315ff2957ae3c08854806a65693d96534749f9d73/shm
shm              64M     0   64M   0% /var/lib/docker/containers/7643840c8ea7465697ccbc3966a6ce7d5a6b852ae924787b1892945458e4695e/shm
tmpfs           697M     0  697M   0% /run/user/1000
```
(I even see that token was mounted)

It looks very close to https://github.com/kubernetes/kubernetes/issues/13174 but seems it is somehow Azure related...

Also on same agent I have another pod running with a volume mounted:
```
Volumes:
  default-token-i9xv0:
    Type:	Secret (a volume populated by a Secret)
    SecretName:	default-token-i9xv0
```",agent-941b4238-0,https://github.com/kubernetes/kubernetes
165,39508.0,"**Kubernetes version** (use `kubectl version`):
```
kubectl version
Client Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.6"", GitCommit:""e569a27d02001e343cb68086bc06d47804f62af6"", GitTreeState:""clean"", BuildDate:""2016-11-12T05:22:15Z"", GoVersion:""go1.7.1"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.6"", GitCommit:""e569a27d02001e343cb68086bc06d47804f62af6"", GitTreeState:""clean"", BuildDate:""2016-11-12T05:16:27Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: Azure CS
- **OS** (e.g. from /etc/os-release): Ubuntu 16.04 LTS 
- **Kernel** (e.g. `uname -a`): 4.4.0-57-generic 
- **Install tools**: Azure cli (2 agents, standard d2)

**What happened**:
Everything were working, until I decided to update deployment (using `replace`). After that pods stop starting and staid on `ContainerCreating` status.
In 'kubectl describe pod` I got:
```
  28m		40s		22	{kubelet k8s-agent-941b4238-0}			Warning		FailedMount	MountVolume.SetUp failed for volume ""kubernetes.io/secret/71af8ad3-d3c6-11e6-9251-000d3a216b93-default-token-i9xv0"" (spec.Name: ""default-token-i9xv0"") pod ""71af8ad3-d3c6-11e6-9251-000d3a216b93"" (UID: ""71af8ad3-d3c6-11e6-9251-000d3a216b93"") with: mount failed: exit status 32
Mounting arguments: tmpfs /var/lib/kubelet/pods/71af8ad3-d3c6-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0 tmpfs []
Output: mount: mount tmpfs on /var/lib/kubelet/pods/71af8ad3-d3c6-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0 failed: No space left on device


  26m	23s	13	{kubelet k8s-agent-941b4238-0}		Warning	FailedMount	Unable to mount volumes for pod ""router-4284723421-h9de2_default(71af8ad3-d3c6-11e6-9251-000d3a216b93)"": timeout expired waiting for volumes to attach/mount for pod ""router-4284723421-h9de2""/""default"". list of unattached/unmounted volumes=[default-token-i9xv0]
  26m	23s	13	{kubelet k8s-agent-941b4238-0}		Warning	FailedSync	Error syncing pod, skipping: timeout expired waiting for volumes to attach/mount for pod ""router-4284723421-h9de2""/""default"". list of unattached/unmounted volumes=[default-token-i9xv0]
```

However, on agents there is definitely enough space:
```
Filesystem      Size  Used Avail Use% Mounted on
udev            3.4G     0  3.4G   0% /dev
tmpfs           697M   17M  680M   3% /run
/dev/sda1        29G  7.6G   21G  27% /
tmpfs           3.5G     0  3.5G   0% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs           3.5G     0  3.5G   0% /sys/fs/cgroup
none             64K     0   64K   0% /etc/network/interfaces.dynamic.d
/dev/sdb1        14G   40M   14G   1% /mnt
tmpfs           100K     0  100K   0% /run/lxcfs/controllers
tmpfs           3.5G   12K  3.5G   1% /var/lib/kubelet/pods/b7abae8f-cefc-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-zk98l
shm              64M     0   64M   0% /var/lib/docker/containers/70f1efbd4a5564208ca79a329dfc3828f0315aba5d7b5034b6784e9341e96a10/shm
tmpfs           3.5G   12K  3.5G   1% /var/lib/kubelet/pods/9c1dfead-d393-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0
shm              64M     0   64M   0% /var/lib/docker/containers/b050aafce252bb14f65db18315ff2957ae3c08854806a65693d96534749f9d73/shm
shm              64M     0   64M   0% /var/lib/docker/containers/7643840c8ea7465697ccbc3966a6ce7d5a6b852ae924787b1892945458e4695e/shm
tmpfs           697M     0  697M   0% /run/user/1000
```
(I even see that token was mounted)

It looks very close to https://github.com/kubernetes/kubernetes/issues/13174 but seems it is somehow Azure related...

Also on same agent I have another pod running with a volume mounted:
```
Volumes:
  default-token-i9xv0:
    Type:	Secret (a volume populated by a Secret)
    SecretName:	default-token-i9xv0
```",default-token-i9xv0,https://github.com/kubernetes/kubernetes
166,39508.0,"**Kubernetes version** (use `kubectl version`):
```
kubectl version
Client Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.6"", GitCommit:""e569a27d02001e343cb68086bc06d47804f62af6"", GitTreeState:""clean"", BuildDate:""2016-11-12T05:22:15Z"", GoVersion:""go1.7.1"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""4"", GitVersion:""v1.4.6"", GitCommit:""e569a27d02001e343cb68086bc06d47804f62af6"", GitTreeState:""clean"", BuildDate:""2016-11-12T05:16:27Z"", GoVersion:""go1.6.3"", Compiler:""gc"", Platform:""linux/amd64""}
```

**Environment**:
- **Cloud provider or hardware configuration**: Azure CS
- **OS** (e.g. from /etc/os-release): Ubuntu 16.04 LTS 
- **Kernel** (e.g. `uname -a`): 4.4.0-57-generic 
- **Install tools**: Azure cli (2 agents, standard d2)

**What happened**:
Everything were working, until I decided to update deployment (using `replace`). After that pods stop starting and staid on `ContainerCreating` status.
In 'kubectl describe pod` I got:
```
  28m		40s		22	{kubelet k8s-agent-941b4238-0}			Warning		FailedMount	MountVolume.SetUp failed for volume ""kubernetes.io/secret/71af8ad3-d3c6-11e6-9251-000d3a216b93-default-token-i9xv0"" (spec.Name: ""default-token-i9xv0"") pod ""71af8ad3-d3c6-11e6-9251-000d3a216b93"" (UID: ""71af8ad3-d3c6-11e6-9251-000d3a216b93"") with: mount failed: exit status 32
Mounting arguments: tmpfs /var/lib/kubelet/pods/71af8ad3-d3c6-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0 tmpfs []
Output: mount: mount tmpfs on /var/lib/kubelet/pods/71af8ad3-d3c6-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0 failed: No space left on device


  26m	23s	13	{kubelet k8s-agent-941b4238-0}		Warning	FailedMount	Unable to mount volumes for pod ""router-4284723421-h9de2_default(71af8ad3-d3c6-11e6-9251-000d3a216b93)"": timeout expired waiting for volumes to attach/mount for pod ""router-4284723421-h9de2""/""default"". list of unattached/unmounted volumes=[default-token-i9xv0]
  26m	23s	13	{kubelet k8s-agent-941b4238-0}		Warning	FailedSync	Error syncing pod, skipping: timeout expired waiting for volumes to attach/mount for pod ""router-4284723421-h9de2""/""default"". list of unattached/unmounted volumes=[default-token-i9xv0]
```

However, on agents there is definitely enough space:
```
Filesystem      Size  Used Avail Use% Mounted on
udev            3.4G     0  3.4G   0% /dev
tmpfs           697M   17M  680M   3% /run
/dev/sda1        29G  7.6G   21G  27% /
tmpfs           3.5G     0  3.5G   0% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs           3.5G     0  3.5G   0% /sys/fs/cgroup
none             64K     0   64K   0% /etc/network/interfaces.dynamic.d
/dev/sdb1        14G   40M   14G   1% /mnt
tmpfs           100K     0  100K   0% /run/lxcfs/controllers
tmpfs           3.5G   12K  3.5G   1% /var/lib/kubelet/pods/b7abae8f-cefc-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-zk98l
shm              64M     0   64M   0% /var/lib/docker/containers/70f1efbd4a5564208ca79a329dfc3828f0315aba5d7b5034b6784e9341e96a10/shm
tmpfs           3.5G   12K  3.5G   1% /var/lib/kubelet/pods/9c1dfead-d393-11e6-9251-000d3a216b93/volumes/kubernetes.io~secret/default-token-i9xv0
shm              64M     0   64M   0% /var/lib/docker/containers/b050aafce252bb14f65db18315ff2957ae3c08854806a65693d96534749f9d73/shm
shm              64M     0   64M   0% /var/lib/docker/containers/7643840c8ea7465697ccbc3966a6ce7d5a6b852ae924787b1892945458e4695e/shm
tmpfs           697M     0  697M   0% /run/user/1000
```
(I even see that token was mounted)

It looks very close to https://github.com/kubernetes/kubernetes/issues/13174 but seems it is somehow Azure related...

Also on same agent I have another pod running with a volume mounted:
```
Volumes:
  default-token-i9xv0:
    Type:	Secret (a volume populated by a Secret)
    SecretName:	default-token-i9xv0
```",9251-000d3a216b93,https://github.com/kubernetes/kubernetes
167,36856.0,"From my current understanding, once I create a secret using something like the following, it is available to all pods in the namespace if they mount it into an environment variable or volume:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  password: MWYyZDFlMmU2N2Rm
  username: YWRtaW4=
```

It would be nice if we could scope secrets to a deployment. In this case, the secrets can only be mounted into a volume or environment variables for the deployment. When the deployment is deleted, the secrets are also deleted. This way, another pod would not be able to get access to secrets owned by the deployment.

The current work around is to use an environment variable directly in the deployment configuration, but environment variables are very insecure because a user can easily see the secret in plain text if he/she retrieves the deployment as a yaml file from the api server.",MWYyZDFlMmU2N2Rm,https://github.com/kubernetes/kubernetes
168,36856.0,"From my current understanding, once I create a secret using something like the following, it is available to all pods in the namespace if they mount it into an environment variable or volume:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  password: MWYyZDFlMmU2N2Rm
  username: YWRtaW4=
```

It would be nice if we could scope secrets to a deployment. In this case, the secrets can only be mounted into a volume or environment variables for the deployment. When the deployment is deleted, the secrets are also deleted. This way, another pod would not be able to get access to secrets owned by the deployment.

The current work around is to use an environment variable directly in the deployment configuration, but environment variables are very insecure because a user can easily see the secret in plain text if he/she retrieves the deployment as a yaml file from the api server.",YWRtaW4=,https://github.com/kubernetes/kubernetes
169,36143.0,"It happens once in a while in different tests. This is a meta issue for failures like
```
Type: ""UnexpectedServerResponse"",
                        Message: ""Error: 'No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-d33fce984358c2fdde6b\""?'\nTrying to reach: 'https://gke-jenkins-e2e-default-pool-f5e91d0a-2zb3:10250/metrics'"",
                        Field: """",
```

cc @kubernetes/goog-gke @wojtek-t",d33fce984358c2fdde6b\?\nTrying,https://github.com/kubernetes/kubernetes
170,35378.0,"Hi, 

I'm trying to build a test environment which in the end will be deployed to our production server; however I encountered some issues that, for now, I consider bugs.

Please find my install scripts @ https://github.com/itmcdev/solaris.

Following the valid code from `provision.sh`, I followed the instructions from http://kubernetes.io/docs/getting-started-guides/kubeadm/ running:

``` bash
# for master
kubeadm init --api-advertise-addresses=10.0.3.10

# for workers
kubeadm join --token e36420.92c6e1d4959fb8eb 10.0.3.10

# than, for master
kubectl apply -f https://git.io/weave-kube
```

Following this, I noticed that kube-dns and weave-kube images were persisting in ContainerCreating and CrashLoopBackOff statuses. I will let you decide further, whether this is a bug, or a mistake generated by me. To note, I'm a total noob under the process of learning.

``` bash
root@solaris-master:~# kubectl get nodes
NAME               STATUS    AGE
solaris-master     Ready     52m
solaris-worker-1   Ready     50m
solaris-worker-2   Ready     50m
```

``` bash
root@solaris-master:~# kubectl get pods --all-namespaces
NAMESPACE     NAME                                     READY     STATUS              RESTARTS   AGE
kube-system   etcd-solaris-master                      1/1       Running             0          51m
kube-system   kube-apiserver-solaris-master            1/1       Running             0          52m
kube-system   kube-controller-manager-solaris-master   1/1       Running             0          52m
kube-system   kube-discovery-982812725-wm3o2           1/1       Running             0          29m
kube-system   kube-dns-2247936740-ubzbm                0/3       ContainerCreating   0          51m
kube-system   kube-proxy-amd64-3jabb                   1/1       Running             0          50m
kube-system   kube-proxy-amd64-rt2f9                   1/1       Running             0          50m
kube-system   kube-proxy-amd64-ygc2p                   1/1       Running             0          51m
kube-system   kube-scheduler-solaris-master            1/1       Running             0          51m
kube-system   weave-net-4c3a4                          1/2       CrashLoopBackOff    11         37m
kube-system   weave-net-yvv71                          2/2       Running             0          37m
kube-system   weave-net-z9aqe                          1/2       CrashLoopBackOff    11         37m
```

``` bash
root@solaris-master:~# kubectl describe pod weave-net-4c3a4 --namespace=kube-system
Name:           weave-net-4c3a4
Namespace:      kube-system
Node:           solaris-worker-1/10.0.2.15
Start Time:     Sat, 22 Oct 2016 16:33:32 -0400
Labels:         name=weave-net
Status:         Running
IP:             10.0.2.15
Controllers:    DaemonSet/weave-net
Containers:
  weave:
    Container ID:       docker://a254dce85d8a027d9d3de1de080ca6322806e05fda2bc72a87f1d5b3cc936f35
    Image:              weaveworks/weave-kube:1.7.2
    Image ID:           docker://sha256:9a2aa48020f51f3bc736e63a888441979dbcba394debdc1abb8e43cf95449168
    Port:
    Command:
      /home/weave/launch.sh
    Requests:
      cpu:              10m
    State:              Waiting
      Reason:           CrashLoopBackOff
    Last State:         Terminated
      Reason:           Error
      Exit Code:        1
      Started:          Sat, 22 Oct 2016 17:11:32 -0400
      Finished:         Sat, 22 Oct 2016 17:11:32 -0400
    Ready:              False
    Restart Count:      12
    Liveness:           http-get http://127.0.0.1:6784/status delay=30s timeout=1s period=10s #success=1 #failure=3
    Volume Mounts:
      /etc from cni-conf (rw)
      /host_home from cni-bin2 (rw)
      /opt from cni-bin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
      /weavedb from weavedb (rw)
    Environment Variables:      <none>
  weave-npc:
    Container ID:       docker://2395225b64692a0a4caf63903ec5cd4ca55b0fc60989623c165a747acd456068
    Image:              weaveworks/weave-npc:1.7.2
    Image ID:           docker://sha256:63a7347dde435cccc849d0eacc1a11c17733b6e557e67ce56efd8264f11e5d8c
    Port:
    Requests:
      cpu:              10m
    State:              Running
      Started:          Sat, 22 Oct 2016 16:34:49 -0400
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
Conditions:
  Type          Status
  Initialized   True
  Ready         False
  PodScheduled  True
Volumes:
  weavedb:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
  cni-bin:
    Type:       HostPath (bare host directory volume)
    Path:       /opt
  cni-bin2:
    Type:       HostPath (bare host directory volume)
    Path:       /home
  cni-conf:
    Type:       HostPath (bare host directory volume)
    Path:       /etc
  default-token-eejd4:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-eejd4
QoS Class:      Burstable
Tolerations:    dedicated=master:Equal:NoSchedule
Events:
  FirstSeen     LastSeen        Count   From                            SubobjectPath                   Type            Reason          Message
  ---------     --------        -----   ----                            -------------                   --------        ------          -------
  38m           38m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Pulling         pulling image ""weaveworks/weave-kube:1.7.2""
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Pulled          Successfully pulled image ""weaveworks/weave-kube:1.7.2""
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Created         Created container with docker id 06f090f2775f; Security:[seccomp=unconfined]
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Started         Started container with docker id 06f090f2775f
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Pulling         pulling image ""weaveworks/weave-npc:1.7.2""
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Pulled          Successfully pulled image ""weaveworks/weave-npc:1.7.2""

  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Created         Created container with docker id 2395225b6469; Security:[seccomp=unconfined]
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Started         Started container with docker id 2395225b6469
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Created         Created container with docker id 8c4256f19b85; Security:[seccomp=unconfined]
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Started         Started container with docker id 8c4256f19b85
  36m           36m             3       {kubelet solaris-worker-1}                                      Warning         FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 10s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 8bba1d69d16b
  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 8bba1d69d16b; Security:[seccomp=unconfined]
  36m   36m     2       {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 20s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 88c6006d13c3
  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 88c6006d13c3; Security:[seccomp=unconfined]
  36m   35m     5       {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 40s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  35m   35m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 06dad5fefd0c; Security:[seccomp=unconfined]
  35m   35m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 06dad5fefd0c
  35m   34m     7       {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 1m20s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  34m   34m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id d8e315b09473; Security:[seccomp=unconfined]
  34m   34m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id d8e315b09473
  33m   31m     14      {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 2m40s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  31m   31m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 49ca69fe4aa7; Security:[seccomp=unconfined]
  31m   31m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 49ca69fe4aa7
  26m   26m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 5f4f6e79bf1b; Security:[seccomp=unconfined]
  26m   26m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 5f4f6e79bf1b
  20m   16s     5       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         (events with common reason combined)
  20m   16s     5       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         (events with common reason combined)
  36m   16s     12      {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Pulled          Container image ""weaveworks/weave-kube:1.7.2"" already present on machine
  31m   6s      142     {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 5m0s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  36m   6s      173     {kubelet solaris-worker-1}      spec.containers{weave}  Warning BackOff Back-off restarting failed docker container
```

``` bash
root@solaris-master:~# kubectl logs weave-net-4c3a4 --namespace=kube-system
Error from server: a container name must be specified for pod weave-net-4c3a4, choose one of: [weave weave-npc]
```

``` bash
root@solaris-master:~# kubectl describe pod kube-dns-2247936740-ubzbm --namespace=kube-system
Name:           kube-dns-2247936740-ubzbm
Namespace:      kube-system
Node:           solaris-master/10.0.2.15
Start Time:     Sat, 22 Oct 2016 16:19:37 -0400
Labels:         component=kube-dns
                k8s-app=kube-dns
                kubernetes.io/cluster-service=true
                name=kube-dns
                pod-template-hash=2247936740
                tier=node
Status:         Pending
IP:
Controllers:    ReplicaSet/kube-dns-2247936740
Containers:
  kube-dns:
    Container ID:
    Image:              gcr.io/google_containers/kubedns-amd64:1.7
    Image ID:
    Ports:              10053/UDP, 10053/TCP
    Args:
      --domain=cluster.local
      --dns-port=10053
    Limits:
      cpu:      100m
      memory:   170Mi
    Requests:
      cpu:              100m
      memory:           170Mi
    State:              Waiting
      Reason:           ContainerCreating
    Ready:              False
    Restart Count:      0
    Liveness:           http-get http://:8080/healthz delay=60s timeout=5s period=10s #success=1 #failure=1
    Readiness:          http-get http://:8081/readiness delay=30s timeout=5s period=10s #success=1 #failure=3
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
  dnsmasq:
    Container ID:
    Image:              gcr.io/google_containers/kube-dnsmasq-amd64:1.3
    Image ID:
    Ports:              53/UDP, 53/TCP
    Args:
      --cache-size=1000
      --no-resolv
      --server=127.0.0.1#10053
    Limits:
      cpu:      100m
      memory:   170Mi
    Requests:
      cpu:              100m
      memory:           170Mi
    State:              Waiting
      Reason:           ContainerCreating
    Ready:              False
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
  healthz:
    Container ID:
    Image:              gcr.io/google_containers/exechealthz-amd64:1.1
    Image ID:
    Port:               8080/TCP
    Args:
      -cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:53 >/dev/null && nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 >/dev/null
      -port=8080
      -quiet
    Limits:
      cpu:      10m
      memory:   50Mi
    Requests:
      cpu:              10m
      memory:           50Mi
    State:              Waiting
      Reason:           ContainerCreating
    Ready:              False
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
Conditions:
  Type          Status
  Initialized   True
  Ready         False
  PodScheduled  True
Volumes:
  default-token-eejd4:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-eejd4
QoS Class:      Guaranteed
Tolerations:    dedicated=master:NoSchedule
Events:
  FirstSeen     LastSeen        Count   From                            SubobjectPath   Type            Reason          Message
  ---------     --------        -----   ----                            -------------   --------        ------          -------
  55m           55m             1       {default-scheduler }                            Normal          Scheduled       Successfully assigned kube-dns-2247936740-ubzbm to solaris-master
  55m           39m             694     {kubelet solaris-master}                        Warning         FailedSync      Error syncing pod, skipping: failed to ""SetupNetwork"" for ""kube-dns-2247936740-ubzbm_kube-system"" with SetupNetworkError: ""Failed to setup network for pod \""kube-dns-2247936740-ubzbm_kube-system(dac6ff63-9894-11e6-b6af-080027d4fd28)\"" using network plugins \""cni\"": cni config unintialized; Skipping pod""
```

``` bash
root@solaris-master:~# kubectl logs kube-dns-2247936740-ubzbm --namespace=kube-system
Error from server: a container name must be specified for pod kube-dns-2247936740-ubzbm, choose one of: [kube-dns dnsmasq healthz]
```
",cc47b290-9896-11e6-b6af-080027d4fd28,https://github.com/kubernetes/kubernetes
171,35378.0,"Hi, 

I'm trying to build a test environment which in the end will be deployed to our production server; however I encountered some issues that, for now, I consider bugs.

Please find my install scripts @ https://github.com/itmcdev/solaris.

Following the valid code from `provision.sh`, I followed the instructions from http://kubernetes.io/docs/getting-started-guides/kubeadm/ running:

``` bash
# for master
kubeadm init --api-advertise-addresses=10.0.3.10

# for workers
kubeadm join --token e36420.92c6e1d4959fb8eb 10.0.3.10

# than, for master
kubectl apply -f https://git.io/weave-kube
```

Following this, I noticed that kube-dns and weave-kube images were persisting in ContainerCreating and CrashLoopBackOff statuses. I will let you decide further, whether this is a bug, or a mistake generated by me. To note, I'm a total noob under the process of learning.

``` bash
root@solaris-master:~# kubectl get nodes
NAME               STATUS    AGE
solaris-master     Ready     52m
solaris-worker-1   Ready     50m
solaris-worker-2   Ready     50m
```

``` bash
root@solaris-master:~# kubectl get pods --all-namespaces
NAMESPACE     NAME                                     READY     STATUS              RESTARTS   AGE
kube-system   etcd-solaris-master                      1/1       Running             0          51m
kube-system   kube-apiserver-solaris-master            1/1       Running             0          52m
kube-system   kube-controller-manager-solaris-master   1/1       Running             0          52m
kube-system   kube-discovery-982812725-wm3o2           1/1       Running             0          29m
kube-system   kube-dns-2247936740-ubzbm                0/3       ContainerCreating   0          51m
kube-system   kube-proxy-amd64-3jabb                   1/1       Running             0          50m
kube-system   kube-proxy-amd64-rt2f9                   1/1       Running             0          50m
kube-system   kube-proxy-amd64-ygc2p                   1/1       Running             0          51m
kube-system   kube-scheduler-solaris-master            1/1       Running             0          51m
kube-system   weave-net-4c3a4                          1/2       CrashLoopBackOff    11         37m
kube-system   weave-net-yvv71                          2/2       Running             0          37m
kube-system   weave-net-z9aqe                          1/2       CrashLoopBackOff    11         37m
```

``` bash
root@solaris-master:~# kubectl describe pod weave-net-4c3a4 --namespace=kube-system
Name:           weave-net-4c3a4
Namespace:      kube-system
Node:           solaris-worker-1/10.0.2.15
Start Time:     Sat, 22 Oct 2016 16:33:32 -0400
Labels:         name=weave-net
Status:         Running
IP:             10.0.2.15
Controllers:    DaemonSet/weave-net
Containers:
  weave:
    Container ID:       docker://a254dce85d8a027d9d3de1de080ca6322806e05fda2bc72a87f1d5b3cc936f35
    Image:              weaveworks/weave-kube:1.7.2
    Image ID:           docker://sha256:9a2aa48020f51f3bc736e63a888441979dbcba394debdc1abb8e43cf95449168
    Port:
    Command:
      /home/weave/launch.sh
    Requests:
      cpu:              10m
    State:              Waiting
      Reason:           CrashLoopBackOff
    Last State:         Terminated
      Reason:           Error
      Exit Code:        1
      Started:          Sat, 22 Oct 2016 17:11:32 -0400
      Finished:         Sat, 22 Oct 2016 17:11:32 -0400
    Ready:              False
    Restart Count:      12
    Liveness:           http-get http://127.0.0.1:6784/status delay=30s timeout=1s period=10s #success=1 #failure=3
    Volume Mounts:
      /etc from cni-conf (rw)
      /host_home from cni-bin2 (rw)
      /opt from cni-bin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
      /weavedb from weavedb (rw)
    Environment Variables:      <none>
  weave-npc:
    Container ID:       docker://2395225b64692a0a4caf63903ec5cd4ca55b0fc60989623c165a747acd456068
    Image:              weaveworks/weave-npc:1.7.2
    Image ID:           docker://sha256:63a7347dde435cccc849d0eacc1a11c17733b6e557e67ce56efd8264f11e5d8c
    Port:
    Requests:
      cpu:              10m
    State:              Running
      Started:          Sat, 22 Oct 2016 16:34:49 -0400
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
Conditions:
  Type          Status
  Initialized   True
  Ready         False
  PodScheduled  True
Volumes:
  weavedb:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
  cni-bin:
    Type:       HostPath (bare host directory volume)
    Path:       /opt
  cni-bin2:
    Type:       HostPath (bare host directory volume)
    Path:       /home
  cni-conf:
    Type:       HostPath (bare host directory volume)
    Path:       /etc
  default-token-eejd4:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-eejd4
QoS Class:      Burstable
Tolerations:    dedicated=master:Equal:NoSchedule
Events:
  FirstSeen     LastSeen        Count   From                            SubobjectPath                   Type            Reason          Message
  ---------     --------        -----   ----                            -------------                   --------        ------          -------
  38m           38m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Pulling         pulling image ""weaveworks/weave-kube:1.7.2""
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Pulled          Successfully pulled image ""weaveworks/weave-kube:1.7.2""
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Created         Created container with docker id 06f090f2775f; Security:[seccomp=unconfined]
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Started         Started container with docker id 06f090f2775f
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Pulling         pulling image ""weaveworks/weave-npc:1.7.2""
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Pulled          Successfully pulled image ""weaveworks/weave-npc:1.7.2""

  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Created         Created container with docker id 2395225b6469; Security:[seccomp=unconfined]
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Started         Started container with docker id 2395225b6469
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Created         Created container with docker id 8c4256f19b85; Security:[seccomp=unconfined]
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Started         Started container with docker id 8c4256f19b85
  36m           36m             3       {kubelet solaris-worker-1}                                      Warning         FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 10s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 8bba1d69d16b
  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 8bba1d69d16b; Security:[seccomp=unconfined]
  36m   36m     2       {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 20s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 88c6006d13c3
  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 88c6006d13c3; Security:[seccomp=unconfined]
  36m   35m     5       {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 40s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  35m   35m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 06dad5fefd0c; Security:[seccomp=unconfined]
  35m   35m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 06dad5fefd0c
  35m   34m     7       {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 1m20s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  34m   34m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id d8e315b09473; Security:[seccomp=unconfined]
  34m   34m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id d8e315b09473
  33m   31m     14      {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 2m40s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  31m   31m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 49ca69fe4aa7; Security:[seccomp=unconfined]
  31m   31m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 49ca69fe4aa7
  26m   26m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 5f4f6e79bf1b; Security:[seccomp=unconfined]
  26m   26m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 5f4f6e79bf1b
  20m   16s     5       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         (events with common reason combined)
  20m   16s     5       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         (events with common reason combined)
  36m   16s     12      {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Pulled          Container image ""weaveworks/weave-kube:1.7.2"" already present on machine
  31m   6s      142     {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 5m0s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  36m   6s      173     {kubelet solaris-worker-1}      spec.containers{weave}  Warning BackOff Back-off restarting failed docker container
```

``` bash
root@solaris-master:~# kubectl logs weave-net-4c3a4 --namespace=kube-system
Error from server: a container name must be specified for pod weave-net-4c3a4, choose one of: [weave weave-npc]
```

``` bash
root@solaris-master:~# kubectl describe pod kube-dns-2247936740-ubzbm --namespace=kube-system
Name:           kube-dns-2247936740-ubzbm
Namespace:      kube-system
Node:           solaris-master/10.0.2.15
Start Time:     Sat, 22 Oct 2016 16:19:37 -0400
Labels:         component=kube-dns
                k8s-app=kube-dns
                kubernetes.io/cluster-service=true
                name=kube-dns
                pod-template-hash=2247936740
                tier=node
Status:         Pending
IP:
Controllers:    ReplicaSet/kube-dns-2247936740
Containers:
  kube-dns:
    Container ID:
    Image:              gcr.io/google_containers/kubedns-amd64:1.7
    Image ID:
    Ports:              10053/UDP, 10053/TCP
    Args:
      --domain=cluster.local
      --dns-port=10053
    Limits:
      cpu:      100m
      memory:   170Mi
    Requests:
      cpu:              100m
      memory:           170Mi
    State:              Waiting
      Reason:           ContainerCreating
    Ready:              False
    Restart Count:      0
    Liveness:           http-get http://:8080/healthz delay=60s timeout=5s period=10s #success=1 #failure=1
    Readiness:          http-get http://:8081/readiness delay=30s timeout=5s period=10s #success=1 #failure=3
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
  dnsmasq:
    Container ID:
    Image:              gcr.io/google_containers/kube-dnsmasq-amd64:1.3
    Image ID:
    Ports:              53/UDP, 53/TCP
    Args:
      --cache-size=1000
      --no-resolv
      --server=127.0.0.1#10053
    Limits:
      cpu:      100m
      memory:   170Mi
    Requests:
      cpu:              100m
      memory:           170Mi
    State:              Waiting
      Reason:           ContainerCreating
    Ready:              False
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
  healthz:
    Container ID:
    Image:              gcr.io/google_containers/exechealthz-amd64:1.1
    Image ID:
    Port:               8080/TCP
    Args:
      -cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:53 >/dev/null && nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 >/dev/null
      -port=8080
      -quiet
    Limits:
      cpu:      10m
      memory:   50Mi
    Requests:
      cpu:              10m
      memory:           50Mi
    State:              Waiting
      Reason:           ContainerCreating
    Ready:              False
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
Conditions:
  Type          Status
  Initialized   True
  Ready         False
  PodScheduled  True
Volumes:
  default-token-eejd4:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-eejd4
QoS Class:      Guaranteed
Tolerations:    dedicated=master:NoSchedule
Events:
  FirstSeen     LastSeen        Count   From                            SubobjectPath   Type            Reason          Message
  ---------     --------        -----   ----                            -------------   --------        ------          -------
  55m           55m             1       {default-scheduler }                            Normal          Scheduled       Successfully assigned kube-dns-2247936740-ubzbm to solaris-master
  55m           39m             694     {kubelet solaris-master}                        Warning         FailedSync      Error syncing pod, skipping: failed to ""SetupNetwork"" for ""kube-dns-2247936740-ubzbm_kube-system"" with SetupNetworkError: ""Failed to setup network for pod \""kube-dns-2247936740-ubzbm_kube-system(dac6ff63-9894-11e6-b6af-080027d4fd28)\"" using network plugins \""cni\"": cni config unintialized; Skipping pod""
```

``` bash
root@solaris-master:~# kubectl logs kube-dns-2247936740-ubzbm --namespace=kube-system
Error from server: a container name must be specified for pod kube-dns-2247936740-ubzbm, choose one of: [kube-dns dnsmasq healthz]
```
",dac6ff63-9894-11e6-b6af-080027d4fd28,https://github.com/kubernetes/kubernetes
172,35378.0,"Hi, 

I'm trying to build a test environment which in the end will be deployed to our production server; however I encountered some issues that, for now, I consider bugs.

Please find my install scripts @ https://github.com/itmcdev/solaris.

Following the valid code from `provision.sh`, I followed the instructions from http://kubernetes.io/docs/getting-started-guides/kubeadm/ running:

``` bash
# for master
kubeadm init --api-advertise-addresses=10.0.3.10

# for workers
kubeadm join --token e36420.92c6e1d4959fb8eb 10.0.3.10

# than, for master
kubectl apply -f https://git.io/weave-kube
```

Following this, I noticed that kube-dns and weave-kube images were persisting in ContainerCreating and CrashLoopBackOff statuses. I will let you decide further, whether this is a bug, or a mistake generated by me. To note, I'm a total noob under the process of learning.

``` bash
root@solaris-master:~# kubectl get nodes
NAME               STATUS    AGE
solaris-master     Ready     52m
solaris-worker-1   Ready     50m
solaris-worker-2   Ready     50m
```

``` bash
root@solaris-master:~# kubectl get pods --all-namespaces
NAMESPACE     NAME                                     READY     STATUS              RESTARTS   AGE
kube-system   etcd-solaris-master                      1/1       Running             0          51m
kube-system   kube-apiserver-solaris-master            1/1       Running             0          52m
kube-system   kube-controller-manager-solaris-master   1/1       Running             0          52m
kube-system   kube-discovery-982812725-wm3o2           1/1       Running             0          29m
kube-system   kube-dns-2247936740-ubzbm                0/3       ContainerCreating   0          51m
kube-system   kube-proxy-amd64-3jabb                   1/1       Running             0          50m
kube-system   kube-proxy-amd64-rt2f9                   1/1       Running             0          50m
kube-system   kube-proxy-amd64-ygc2p                   1/1       Running             0          51m
kube-system   kube-scheduler-solaris-master            1/1       Running             0          51m
kube-system   weave-net-4c3a4                          1/2       CrashLoopBackOff    11         37m
kube-system   weave-net-yvv71                          2/2       Running             0          37m
kube-system   weave-net-z9aqe                          1/2       CrashLoopBackOff    11         37m
```

``` bash
root@solaris-master:~# kubectl describe pod weave-net-4c3a4 --namespace=kube-system
Name:           weave-net-4c3a4
Namespace:      kube-system
Node:           solaris-worker-1/10.0.2.15
Start Time:     Sat, 22 Oct 2016 16:33:32 -0400
Labels:         name=weave-net
Status:         Running
IP:             10.0.2.15
Controllers:    DaemonSet/weave-net
Containers:
  weave:
    Container ID:       docker://a254dce85d8a027d9d3de1de080ca6322806e05fda2bc72a87f1d5b3cc936f35
    Image:              weaveworks/weave-kube:1.7.2
    Image ID:           docker://sha256:9a2aa48020f51f3bc736e63a888441979dbcba394debdc1abb8e43cf95449168
    Port:
    Command:
      /home/weave/launch.sh
    Requests:
      cpu:              10m
    State:              Waiting
      Reason:           CrashLoopBackOff
    Last State:         Terminated
      Reason:           Error
      Exit Code:        1
      Started:          Sat, 22 Oct 2016 17:11:32 -0400
      Finished:         Sat, 22 Oct 2016 17:11:32 -0400
    Ready:              False
    Restart Count:      12
    Liveness:           http-get http://127.0.0.1:6784/status delay=30s timeout=1s period=10s #success=1 #failure=3
    Volume Mounts:
      /etc from cni-conf (rw)
      /host_home from cni-bin2 (rw)
      /opt from cni-bin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
      /weavedb from weavedb (rw)
    Environment Variables:      <none>
  weave-npc:
    Container ID:       docker://2395225b64692a0a4caf63903ec5cd4ca55b0fc60989623c165a747acd456068
    Image:              weaveworks/weave-npc:1.7.2
    Image ID:           docker://sha256:63a7347dde435cccc849d0eacc1a11c17733b6e557e67ce56efd8264f11e5d8c
    Port:
    Requests:
      cpu:              10m
    State:              Running
      Started:          Sat, 22 Oct 2016 16:34:49 -0400
    Ready:              True
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
Conditions:
  Type          Status
  Initialized   True
  Ready         False
  PodScheduled  True
Volumes:
  weavedb:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
  cni-bin:
    Type:       HostPath (bare host directory volume)
    Path:       /opt
  cni-bin2:
    Type:       HostPath (bare host directory volume)
    Path:       /home
  cni-conf:
    Type:       HostPath (bare host directory volume)
    Path:       /etc
  default-token-eejd4:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-eejd4
QoS Class:      Burstable
Tolerations:    dedicated=master:Equal:NoSchedule
Events:
  FirstSeen     LastSeen        Count   From                            SubobjectPath                   Type            Reason          Message
  ---------     --------        -----   ----                            -------------                   --------        ------          -------
  38m           38m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Pulling         pulling image ""weaveworks/weave-kube:1.7.2""
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Pulled          Successfully pulled image ""weaveworks/weave-kube:1.7.2""
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Created         Created container with docker id 06f090f2775f; Security:[seccomp=unconfined]
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Started         Started container with docker id 06f090f2775f
  37m           37m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Pulling         pulling image ""weaveworks/weave-npc:1.7.2""
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Pulled          Successfully pulled image ""weaveworks/weave-npc:1.7.2""

  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Created         Created container with docker id 2395225b6469; Security:[seccomp=unconfined]
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave-npc}      Normal          Started         Started container with docker id 2395225b6469
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Created         Created container with docker id 8c4256f19b85; Security:[seccomp=unconfined]
  36m           36m             1       {kubelet solaris-worker-1}      spec.containers{weave}          Normal          Started         Started container with docker id 8c4256f19b85
  36m           36m             3       {kubelet solaris-worker-1}                                      Warning         FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 10s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 8bba1d69d16b
  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 8bba1d69d16b; Security:[seccomp=unconfined]
  36m   36m     2       {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 20s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 88c6006d13c3
  36m   36m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 88c6006d13c3; Security:[seccomp=unconfined]
  36m   35m     5       {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 40s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  35m   35m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 06dad5fefd0c; Security:[seccomp=unconfined]
  35m   35m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 06dad5fefd0c
  35m   34m     7       {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 1m20s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  34m   34m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id d8e315b09473; Security:[seccomp=unconfined]
  34m   34m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id d8e315b09473
  33m   31m     14      {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 2m40s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  31m   31m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 49ca69fe4aa7; Security:[seccomp=unconfined]
  31m   31m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 49ca69fe4aa7
  26m   26m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         Created container with docker id 5f4f6e79bf1b; Security:[seccomp=unconfined]
  26m   26m     1       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         Started container with docker id 5f4f6e79bf1b
  20m   16s     5       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Started         (events with common reason combined)
  20m   16s     5       {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Created         (events with common reason combined)
  36m   16s     12      {kubelet solaris-worker-1}      spec.containers{weave}  Normal  Pulled          Container image ""weaveworks/weave-kube:1.7.2"" already present on machine
  31m   6s      142     {kubelet solaris-worker-1}                              Warning FailedSync      Error syncing pod, skipping: failed to ""StartContainer"" for ""weave"" with CrashLoopBackOff: ""Back-off 5m0s restarting failed container=weave pod=weave-net-4c3a4_kube-system(cc47b290-9896-11e6-b6af-080027d4fd28)""

  36m   6s      173     {kubelet solaris-worker-1}      spec.containers{weave}  Warning BackOff Back-off restarting failed docker container
```

``` bash
root@solaris-master:~# kubectl logs weave-net-4c3a4 --namespace=kube-system
Error from server: a container name must be specified for pod weave-net-4c3a4, choose one of: [weave weave-npc]
```

``` bash
root@solaris-master:~# kubectl describe pod kube-dns-2247936740-ubzbm --namespace=kube-system
Name:           kube-dns-2247936740-ubzbm
Namespace:      kube-system
Node:           solaris-master/10.0.2.15
Start Time:     Sat, 22 Oct 2016 16:19:37 -0400
Labels:         component=kube-dns
                k8s-app=kube-dns
                kubernetes.io/cluster-service=true
                name=kube-dns
                pod-template-hash=2247936740
                tier=node
Status:         Pending
IP:
Controllers:    ReplicaSet/kube-dns-2247936740
Containers:
  kube-dns:
    Container ID:
    Image:              gcr.io/google_containers/kubedns-amd64:1.7
    Image ID:
    Ports:              10053/UDP, 10053/TCP
    Args:
      --domain=cluster.local
      --dns-port=10053
    Limits:
      cpu:      100m
      memory:   170Mi
    Requests:
      cpu:              100m
      memory:           170Mi
    State:              Waiting
      Reason:           ContainerCreating
    Ready:              False
    Restart Count:      0
    Liveness:           http-get http://:8080/healthz delay=60s timeout=5s period=10s #success=1 #failure=1
    Readiness:          http-get http://:8081/readiness delay=30s timeout=5s period=10s #success=1 #failure=3
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
  dnsmasq:
    Container ID:
    Image:              gcr.io/google_containers/kube-dnsmasq-amd64:1.3
    Image ID:
    Ports:              53/UDP, 53/TCP
    Args:
      --cache-size=1000
      --no-resolv
      --server=127.0.0.1#10053
    Limits:
      cpu:      100m
      memory:   170Mi
    Requests:
      cpu:              100m
      memory:           170Mi
    State:              Waiting
      Reason:           ContainerCreating
    Ready:              False
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
  healthz:
    Container ID:
    Image:              gcr.io/google_containers/exechealthz-amd64:1.1
    Image ID:
    Port:               8080/TCP
    Args:
      -cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:53 >/dev/null && nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 >/dev/null
      -port=8080
      -quiet
    Limits:
      cpu:      10m
      memory:   50Mi
    Requests:
      cpu:              10m
      memory:           50Mi
    State:              Waiting
      Reason:           ContainerCreating
    Ready:              False
    Restart Count:      0
    Volume Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-eejd4 (ro)
    Environment Variables:      <none>
Conditions:
  Type          Status
  Initialized   True
  Ready         False
  PodScheduled  True
Volumes:
  default-token-eejd4:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-eejd4
QoS Class:      Guaranteed
Tolerations:    dedicated=master:NoSchedule
Events:
  FirstSeen     LastSeen        Count   From                            SubobjectPath   Type            Reason          Message
  ---------     --------        -----   ----                            -------------   --------        ------          -------
  55m           55m             1       {default-scheduler }                            Normal          Scheduled       Successfully assigned kube-dns-2247936740-ubzbm to solaris-master
  55m           39m             694     {kubelet solaris-master}                        Warning         FailedSync      Error syncing pod, skipping: failed to ""SetupNetwork"" for ""kube-dns-2247936740-ubzbm_kube-system"" with SetupNetworkError: ""Failed to setup network for pod \""kube-dns-2247936740-ubzbm_kube-system(dac6ff63-9894-11e6-b6af-080027d4fd28)\"" using network plugins \""cni\"": cni config unintialized; Skipping pod""
```

``` bash
root@solaris-master:~# kubectl logs kube-dns-2247936740-ubzbm --namespace=kube-system
Error from server: a container name must be specified for pod kube-dns-2247936740-ubzbm, choose one of: [kube-dns dnsmasq healthz]
```
",default-token-eejd4,https://github.com/kubernetes/kubernetes
173,33617.0,"It hits the apiserver proxy on GKE, which fails with: 

```
'http://10.244.0.6:80/guestbook.php?cmd=get&key=messages&value='"") has prevented the request from succeeding (get services frontend), response: 
Sep 27 15:36:19.157: INFO: Failed to get response from guestbook. err: an error on the server (""Error: 'No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-4b1917fb2f43ed3cbed7\""?'\nTrying to reach: 'http://10.244.0.6:80/guestbook.php?cmd=get&key=messages&value='"") has prevented the request from succeeding (get services frontend), response: 
Sep 27 15:36:24.162: INFO: Failed to get response from guestbook. err: an error on the server (""Error: 'No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-4b1917fb2f43ed3cbed7\""?'\nTrying to reach: 'http://10.244.0.6:80/guestbook.php?cmd=get&key=messages&value='"") has prevented the request from succeeding (get services frontend), response: 
Sep 27 15:36:29.166: INFO: Failed to get response from guestbook. err: an error on the server (""Error: 'No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-4b1917fb2f43ed3cbed7\""?'\nTrying to reach: 'http://10.244.0.6:80/guestbook.php?cmd=get&key=messages&value='"") has prevented the request from succeeding (get services frontend), response: 
Sep 27 15:36:34.173: INFO: Failed to get response from guestbook. err: an error on the server (""Error: 'No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-4b1917fb2f43ed3cbed7\""?'\nTrying to reach: 'http://10.244.1.6:80/guestbook.php?cmd=get&key=messages&value='"") has prevented the request from succeeding (get services frontend), response: 
Sep 27 15:36:39.179: INFO: Failed to get response from guestbook. err: an error on the server (""Error: 'No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-4b1917fb2f43ed3cbed7\""?'\nTrying to reach: 'http://10.244.1.6:80/guestbook.php?cmd=get&key=messages&value='"") has prevented the request from succeeding (get services frontend), response: 
Sep 27 15:36:44.184: INFO: Failed to get response from guestbook. err: an error on the server (""Error: 'No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-4b1917fb2f43ed3cbed7\""?'\nTrying to reach: 'http://10.244.1.6:80/guestbook.php?cmd=get&key=messages&value='"") has prevented the request from succeeding (get services frontend), response: 
```

We could just not hit the apiserver proxy but create a service of type=lb instead. That is actually more representative of real world app usage, but the proxy issue seems specific to gke and ssh-tunnels. 
",4b1917fb2f43ed3cbed7\?\nTrying,https://github.com/kubernetes/kubernetes
174,28617.0,"Syptom: loading environment variables from keys within a secret does not allow loading of keys with a value that is a BOOL or an INT of any size.

Version:  1.2.4 confirmed

Explaination:

sample secret:
testbool=true
testint=8080
teststring=hello

```
apiVersion: v1
kind: Secret
metadata:
  name: test.secret
type: Opaque
data:
  testbool: dHJ1ZQ==
  testint: ODA4MA==
  teststring: aGVsbG8=
```

in kubernetes:

```
apiVersion: v1
data:
  testbool: dHJ1ZQ==
  testint: ODA4MA==
  teststring: aGVsbG8=
kind: Secret
metadata:
  annotations:
  name: test.secret
  namespace: default
  resourceVersion: ""4178761""
  selfLink: /api/v1/namespaces/default/secrets/test.secret
  uid: b258f770-42e2-11e6-9fee-0ac53f35edc7
type: Opaque
```

Sample deployment:

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: test
  labels:
    app: test
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: test
        tier: dev
    spec:
      imagePullSecrets:
      - name: mypullsecret
      containers:
      - name: testapp1
        image: us.gcr.io/account-name/app:version
        ports:
        - containerPort: 8080
        env:
          - name: testbool
            valueFrom:
              secretKeyRef:
                name: test.secret
                key: testbool
          - name: testint
            valueFrom:
              secretKeyRef:
                name: test.secret
                key: testint
          - name: teststring
            valueFrom:
              secretKeyRef:
                name: test.secret
                key: teststring
```

in Kubernetes:

```
metadata:
  generation: 104
  labels:
    app: test
  name: test
  namespace: default
  resourceVersion: ""4178941""
  selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/test
  uid: f74be004-3ca4-11e6-b887-06500d373dfb
spec:
  replicas: 3
  selector:
    matchLabels:
      app: test
      tier: dev
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: test
        tier: dev
    spec:
      containers:
        env:
          - name: teststring
            valueFrom:
              secretKeyRef:
                name: test.secret
                key: teststring
        image: us.gcr.io/account-name/app:version
        imagePullPolicy: IfNotPresent
        name: test1
        ports:
        - containerPort: 8080
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
      dnsPolicy: ClusterFirst
      imagePullSecrets:
      - name: mypullsecret
      restartPolicy: Always
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 3
  observedGeneration: 104
  replicas: 3
  updatedReplicas: 3
```

As you can see, using a kubectl apply -f to upload my deployment, the deployment was created completely correctly except they environment variables that were pure INT or BOOL types. The secret is uploaded correctly as seen above, and the deployment works flawlessly except it does not pass those environment vars to the container. I have logged into the container and verified it does not set them at all.

On top of this, Kubectl does not mention any errors with the deployment or the secret- as well as the logs for Kubernetes saying nothing at all bout anything. I have no idea where the vars are being dropped, in all honestly.

This is very repeatable, I can change the bool to ""false"" and it does the same, and well as changing the ""int"" to any other number of any size (verified random numbers up to 1000000000000). Understandably, raw strings and bools wouldn't typically be considered secrets, however we have a software key we need to consume that is a pure string, as well as us wishing to keep certain true/false configurations sensitive at times.

I can load these BOOLs and INTs on secret volumes, but that is far from ideal.
",teststring=hello,https://github.com/kubernetes/kubernetes
175,23149.0,"**KubeProxy should test kube-proxy [Slow]**

_/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubeproxy.go:105 Expected error: <errors.errorString | 0xc208554840>: { s: ""Error running &{/jenkins-master-data/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.105.26 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/.kube/config exec --namespace=e2e-tests-e2e-kubeproxy-dtjqf host-test-container-pod -- /bin/sh -c for i in $(seq 1 5); do echo 'hostName' | timeout -t 3 nc -w 1 -u 10.180.0.5 8081; echo; sleep 1s; done | grep -v '^\s*$' |sort | uniq -c | wc -l] [] Error from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-c5d9e45b81848219014b\""?\n [] 0xc2084e48e0 exit status 1 true [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82e0 0xc2083e8300] [0x95de00 0x95de00] 0xc2083d5f20}:\nCommand stdout:\n\nstderr:\nError from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-c5d9e45b81848219014b\""?\n\nerror:\nexit status 1\n"", } Error running &{/jenkins-master-data/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.105.26 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/.kube/config exec --namespace=e2e-tests-e2e-kubeproxy-dtjqf host-test-container-pod -- /bin/sh -c for i in $(seq 1 5); do echo 'hostName' | timeout -t 3 nc -w 1 -u 10.180.0.5 8081; echo; sleep 1s; done | grep -v '^\s*$' |sort | uniq -c | wc -l] [] Error from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user ""gke-c5d9e45b81848219014b""? [] 0xc2084e48e0 exit status 1 true [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82e0 0xc2083e8300] [0x95de00 0x95de00] 0xc2083d5f20}: Command stdout: stderr: Error from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user ""gke-c5d9e45b81848219014b""? error: exit status 1 not to have occurred_

e.g.:
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/97
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/80
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/55
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/46
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/14
",c5d9e45b81848219014b\?\n,https://github.com/kubernetes/kubernetes
176,23149.0,"**KubeProxy should test kube-proxy [Slow]**

_/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubeproxy.go:105 Expected error: <errors.errorString | 0xc208554840>: { s: ""Error running &{/jenkins-master-data/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.105.26 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/.kube/config exec --namespace=e2e-tests-e2e-kubeproxy-dtjqf host-test-container-pod -- /bin/sh -c for i in $(seq 1 5); do echo 'hostName' | timeout -t 3 nc -w 1 -u 10.180.0.5 8081; echo; sleep 1s; done | grep -v '^\s*$' |sort | uniq -c | wc -l] [] Error from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-c5d9e45b81848219014b\""?\n [] 0xc2084e48e0 exit status 1 true [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82e0 0xc2083e8300] [0x95de00 0x95de00] 0xc2083d5f20}:\nCommand stdout:\n\nstderr:\nError from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-c5d9e45b81848219014b\""?\n\nerror:\nexit status 1\n"", } Error running &{/jenkins-master-data/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.105.26 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/.kube/config exec --namespace=e2e-tests-e2e-kubeproxy-dtjqf host-test-container-pod -- /bin/sh -c for i in $(seq 1 5); do echo 'hostName' | timeout -t 3 nc -w 1 -u 10.180.0.5 8081; echo; sleep 1s; done | grep -v '^\s*$' |sort | uniq -c | wc -l] [] Error from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user ""gke-c5d9e45b81848219014b""? [] 0xc2084e48e0 exit status 1 true [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82e0 0xc2083e8300] [0x95de00 0x95de00] 0xc2083d5f20}: Command stdout: stderr: Error from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user ""gke-c5d9e45b81848219014b""? error: exit status 1 not to have occurred_

e.g.:
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/97
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/80
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/55
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/46
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/14
",\?\n\nerror:\nexit,https://github.com/kubernetes/kubernetes
177,23149.0,"**KubeProxy should test kube-proxy [Slow]**

_/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubeproxy.go:105 Expected error: <errors.errorString | 0xc208554840>: { s: ""Error running &{/jenkins-master-data/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.105.26 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/.kube/config exec --namespace=e2e-tests-e2e-kubeproxy-dtjqf host-test-container-pod -- /bin/sh -c for i in $(seq 1 5); do echo 'hostName' | timeout -t 3 nc -w 1 -u 10.180.0.5 8081; echo; sleep 1s; done | grep -v '^\s*$' |sort | uniq -c | wc -l] [] Error from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-c5d9e45b81848219014b\""?\n [] 0xc2084e48e0 exit status 1 true [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82e0 0xc2083e8300] [0x95de00 0x95de00] 0xc2083d5f20}:\nCommand stdout:\n\nstderr:\nError from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user \""gke-c5d9e45b81848219014b\""?\n\nerror:\nexit status 1\n"", } Error running &{/jenkins-master-data/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://104.154.105.26 --kubeconfig=/var/lib/jenkins/jobs/kubernetes-e2e-gke-slow-release-1.2/workspace/.kube/config exec --namespace=e2e-tests-e2e-kubeproxy-dtjqf host-test-container-pod -- /bin/sh -c for i in $(seq 1 5); do echo 'hostName' | timeout -t 3 nc -w 1 -u 10.180.0.5 8081; echo; sleep 1s; done | grep -v '^\s*$' |sort | uniq -c | wc -l] [] Error from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user ""gke-c5d9e45b81848219014b""? [] 0xc2084e48e0 exit status 1 true [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82c0 0xc2083e82e8 0xc2083e8308] [0xc2083e82e0 0xc2083e8300] [0x95de00 0x95de00] 0xc2083d5f20}: Command stdout: stderr: Error from server: No SSH tunnels currently open. Were the targets able to accept an ssh-key for user ""gke-c5d9e45b81848219014b""? error: exit status 1 not to have occurred_

e.g.:
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/97
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/80
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/55
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/46
- https://storage.cloud.google.com/kubernetes-jenkins/logs/kubernetes-e2e-gke-slow-release-1.2/14
",c5d9e45b81848219014b,https://github.com/kubernetes/kubernetes
178,21530.0,"[https://console.cloud.google.com/m/cloudstorage/b/kubernetes-jenkins/o/pr-logs/pull/21369/kubernetes-pull-build-test-e2e-gce/29340/build-log.txt](https://console.cloud.google.com/m/cloudstorage/b/kubernetes-jenkins/o/pr-logs/pull/21369/kubernetes-pull-build-test-e2e-gce/29340/build-log.txt)

```
鈥� Failure [11.605 seconds]
ServiceAccounts
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/service_accounts.go:124
  should mount an API token into pods [Conformance] [It]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/service_accounts.go:123

  ""content of file \""/var/run/secrets/kubernetes.io/serviceaccount/token\"": eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJlMmUtdGVzdHMtc3ZjYWNjb3VudHMtY2pkZDIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiZGVmYXVsdC10b2tlbi1ldWE1ZiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYTQyMjI4ZjEtZDZkMi0xMWU1LWE0MmYtNDIwMTBhZjAwMDAyIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmUyZS10ZXN0cy1zdmNhY2NvdW50cy1jamRkMjpkZWZhdWx0In0.T_dLwS5K_x3rrmuC3iP-LY_lmPPiTxTnjais6tQkR-wkSR5XGzTnJW1xBCwsBXS2uGZi97Dug_l6i29cWt8TvRITMgQjLF5IT5Fi68FyRB0JfrBkCcDKRhPP8tH69rSNMWkuntpVZs3cXARlWPPuEXmxUPxUfPuhM8H1baYLg7i0hJveHVqNY5tlWEgYe7ujPUdxNvwUwTWtG3Lna-MgMtq7m5A5iEe9yh5ixeFM8URA_kHoAEQlxdKMR83XVkCfbKJxAUGuTvuA8PQ5fi8Wr6vFz1sAX8_NRTk0hstIYtCh-Y5xa5aHot75bKLkBFcGSFqgtUGX2_4beGIIOxZ3Cg"" in container output
  Expected
      <string>: content of file ""/var/run/secrets/kubernetes.io/serviceaccount/token"": eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJlMmUtdGVzdHMtc3ZjYWNjb3VudHMtY2pkZDIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiZGVmYXVsdC10b2tlbi1vMzZ5OCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYTQyMjI4ZjEtZDZkMi0xMWU1LWE0MmYtNDIwMTBhZjAwMDAyIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmUyZS10ZXN0cy1zdmNhY2NvdW50cy1jamRkMjpkZWZhdWx0In0.Wy5Iksh4SgXa32ERH5VRVbg-DsZ4SCEdfXGRdM6Nh_a5uhYRRyhCo5w6vRdi39dKeiVTFBwiHVJap2RQUQ6ivMo8_0O_lWlEsbVtFO7CC2cWKIEm-Pjx5958_3sv6kw8QW3ZM6-CWk7669idz-ApGzx-hkrlREvle0HoO2wErPAR4s_X5VIveOjDT5eW1shzazAKSWs1FjxHIw_oeVntyxcRW6i_PDMEDpXOJTC3vjIY2bTYJ9eHDEblXNgiwuIvb3lTQbTnV-1GDs0OPkhB52-jyHMOhK0PzkQp8jerGSF3fffiTu6h47riUWE5UfuCLHAU9RM0AKSZtDx5Z64xPA

  to contain substring
      <string>: content of file ""/var/run/secrets/kubernetes.io/serviceaccount/token"": eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJlMmUtdGVzdHMtc3ZjYWNjb3VudHMtY2pkZDIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiZGVmYXVsdC10b2tlbi1ldWE1ZiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYTQyMjI4ZjEtZDZkMi0xMWU1LWE0MmYtNDIwMTBhZjAwMDAyIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmUyZS10ZXN0cy1zdmNhY2NvdW50cy1jamRkMjpkZWZhdWx0In0.T_dLwS5K_x3rrmuC3iP-LY_lmPPiTxTnjais6tQkR-wkSR5XGzTnJW1xBCwsBXS2uGZi97Dug_l6i29cWt8TvRITMgQjLF5IT5Fi68FyRB0JfrBkCcDKRhPP8tH69rSNMWkuntpVZs3cXARlWPPuEXmxUPxUfPuhM8H1baYLg7i0hJveHVqNY5tlWEgYe7ujPUdxNvwUwTWtG3Lna-MgMtq7m5A5iEe9yh5ixeFM8URA_kHoAEQlxdKMR83XVkCfbKJxAUGuTvuA8PQ5fi8Wr6vFz1sAX8_NRTk0hstIYtCh-Y5xa5aHot75bKLkBFcGSFqgtUGX2_4beGIIOxZ3Cg
```
",LY_lmPPiTxTnjais6tQkR-wkSR5XGzTnJW1xBCwsBXS2uGZi97Dug_l6i29cWt8TvRITMgQjLF5IT5Fi68FyRB0JfrBkCcDKRhPP8tH69rSNMWkuntpVZs3cXARlWPPuEXmxUPxUfPuhM8H1baYLg7i0hJveHVqNY5tlWEgYe7ujPUdxNvwUwTWtG3Lna-MgMtq7m5A5iEe9yh5ixeFM8URA_kHoAEQlxdKMR83XVkCfbKJxAUGuTvuA8PQ5fi8Wr6vFz1sAX8_NRTk0hstIYtCh-Y5xa5aHot75bKLkBFcGSFqgtUGX2_4beGIIOxZ3Cg,https://github.com/kubernetes/kubernetes
179,20787.0,"https://console.developers.google.com/storage/browser/kubernetes-jenkins/logs/kubernetes-e2e-gce/11138/

```
• Failure [44.213 seconds]
ServiceAccounts
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/service_accounts.go:104
  should mount an API token into pods [Conformance] [It]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/service_accounts.go:103

  ""content of file \""/var/run/secrets/kubernetes.io/serviceaccount/token\"": eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJlMmUtdGVzdHMtc3ZjYWNjb3VudHMtNWQxZjMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiZGVmYXVsdC10b2tlbi15NGJ2aiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZTQyMDA3YWEtY2RiMS0xMWU1LWFlYmUtNDIwMTBhZjAwMDAyIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmUyZS10ZXN0cy1zdmNhY2NvdW50cy01ZDFmMzpkZWZhdWx0In0.ey9Ak0dgOEIsSwjEUwi__bOk999g2iU5wT-FDAegxF6Dlk4z3e01LTQYD6xx0B0OulAP_G3RI32QgP1lmIPtiChe3YXmGwLyez4mPzU_BnWcZu2X9jsXxjWNqsU2yZeG4wKlJJBmWmIqZDJDZplmcqR7pCnBeoQwqkFgbf00DhCRr6b9RX5UlUUayqQNf6kK9ewpGkCUxbeQ1F8Y401VhcUJSXAOJSxYWpk1TN0NL88kboQZ1kDa2958EFc7voObAtC4loSooJk1AsWGiL6d3YdoNdiPWICOvPlO_J2I-fGv8dkuap0y9Jp6O0YmwDsJjUrqZ8_FmhUJugQ5CyCRvA"" in container output
  Expected
      <string>: content of file ""/var/run/secrets/kubernetes.io/serviceaccount/token"": eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJlMmUtdGVzdHMtc3ZjYWNjb3VudHMtNWQxZjMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiZGVmYXVsdC10b2tlbi05emx2NiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZTQyMDA3YWEtY2RiMS0xMWU1LWFlYmUtNDIwMTBhZjAwMDAyIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmUyZS10ZXN0cy1zdmNhY2NvdW50cy01ZDFmMzpkZWZhdWx0In0.RFby-5cko4yvIoyY0cQwSvSi2sFzWIJRI5NryI1bAhcoQlSez51--beWUe1JWh8i3pV_CLN3P4qE_M_47o3sT6wNgQ1GuZ1UDY6z5xaNwpSQOvfpFOT5QxxP0ZD-s6kFluKsslt_A9tG1l4UV2YODmT_5CEO_xo0rKih6HHeYn3q965zLa47lY8nzv7-__dNoYdwDeCxmiErsm-31uedw3zsOj9FqH71Ij0L_9Tax72BYssP0Hey4WF_q8Xq0N-WPNPROCqiFYHePIyfSBfj5LkKi2jTt59bm8Lqp-2bc29ayt1YsY8Ofjnz98XPCapX5WG30O6ed3-OJOqHJFCooQ

  to contain substring
      <string>: content of file ""/var/run/secrets/kubernetes.io/serviceaccount/token"": eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJlMmUtdGVzdHMtc3ZjYWNjb3VudHMtNWQxZjMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiZGVmYXVsdC10b2tlbi15NGJ2aiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZTQyMDA3YWEtY2RiMS0xMWU1LWFlYmUtNDIwMTBhZjAwMDAyIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmUyZS10ZXN0cy1zdmNhY2NvdW50cy01ZDFmMzpkZWZhdWx0In0.ey9Ak0dgOEIsSwjEUwi__bOk999g2iU5wT-FDAegxF6Dlk4z3e01LTQYD6xx0B0OulAP_G3RI32QgP1lmIPtiChe3YXmGwLyez4mPzU_BnWcZu2X9jsXxjWNqsU2yZeG4wKlJJBmWmIqZDJDZplmcqR7pCnBeoQwqkFgbf00DhCRr6b9RX5UlUUayqQNf6kK9ewpGkCUxbeQ1F8Y401VhcUJSXAOJSxYWpk1TN0NL88kboQZ1kDa2958EFc7voObAtC4loSooJk1AsWGiL6d3YdoNdiPWICOvPlO_J2I-fGv8dkuap0y9Jp6O0YmwDsJjUrqZ8_FmhUJugQ5CyCRvA

```

Maybe similar to #20494 ?

`/var/run/secrets/kubernetes.io/serviceaccount/token` does contain a token, but it looks like it is a different token.
",FDAegxF6Dlk4z3e01LTQYD6xx0B0OulAP_G3RI32QgP1lmIPtiChe3YXmGwLyez4mPzU_BnWcZu2X9jsXxjWNqsU2yZeG4wKlJJBmWmIqZDJDZplmcqR7pCnBeoQwqkFgbf00DhCRr6b9RX5UlUUayqQNf6kK9ewpGkCUxbeQ1F8Y401VhcUJSXAOJSxYWpk1TN0NL88kboQZ1kDa2958EFc7voObAtC4loSooJk1AsWGiL6d3YdoNdiPWICOvPlO_J2I-fGv8dkuap0y9Jp6O0YmwDsJjUrqZ8_FmhUJugQ5CyCRvA,https://github.com/kubernetes/kubernetes
180,20787.0,"https://console.developers.google.com/storage/browser/kubernetes-jenkins/logs/kubernetes-e2e-gce/11138/

```
• Failure [44.213 seconds]
ServiceAccounts
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/service_accounts.go:104
  should mount an API token into pods [Conformance] [It]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/service_accounts.go:103

  ""content of file \""/var/run/secrets/kubernetes.io/serviceaccount/token\"": eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJlMmUtdGVzdHMtc3ZjYWNjb3VudHMtNWQxZjMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiZGVmYXVsdC10b2tlbi15NGJ2aiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZTQyMDA3YWEtY2RiMS0xMWU1LWFlYmUtNDIwMTBhZjAwMDAyIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmUyZS10ZXN0cy1zdmNhY2NvdW50cy01ZDFmMzpkZWZhdWx0In0.ey9Ak0dgOEIsSwjEUwi__bOk999g2iU5wT-FDAegxF6Dlk4z3e01LTQYD6xx0B0OulAP_G3RI32QgP1lmIPtiChe3YXmGwLyez4mPzU_BnWcZu2X9jsXxjWNqsU2yZeG4wKlJJBmWmIqZDJDZplmcqR7pCnBeoQwqkFgbf00DhCRr6b9RX5UlUUayqQNf6kK9ewpGkCUxbeQ1F8Y401VhcUJSXAOJSxYWpk1TN0NL88kboQZ1kDa2958EFc7voObAtC4loSooJk1AsWGiL6d3YdoNdiPWICOvPlO_J2I-fGv8dkuap0y9Jp6O0YmwDsJjUrqZ8_FmhUJugQ5CyCRvA"" in container output
  Expected
      <string>: content of file ""/var/run/secrets/kubernetes.io/serviceaccount/token"": eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJlMmUtdGVzdHMtc3ZjYWNjb3VudHMtNWQxZjMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiZGVmYXVsdC10b2tlbi05emx2NiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZTQyMDA3YWEtY2RiMS0xMWU1LWFlYmUtNDIwMTBhZjAwMDAyIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmUyZS10ZXN0cy1zdmNhY2NvdW50cy01ZDFmMzpkZWZhdWx0In0.RFby-5cko4yvIoyY0cQwSvSi2sFzWIJRI5NryI1bAhcoQlSez51--beWUe1JWh8i3pV_CLN3P4qE_M_47o3sT6wNgQ1GuZ1UDY6z5xaNwpSQOvfpFOT5QxxP0ZD-s6kFluKsslt_A9tG1l4UV2YODmT_5CEO_xo0rKih6HHeYn3q965zLa47lY8nzv7-__dNoYdwDeCxmiErsm-31uedw3zsOj9FqH71Ij0L_9Tax72BYssP0Hey4WF_q8Xq0N-WPNPROCqiFYHePIyfSBfj5LkKi2jTt59bm8Lqp-2bc29ayt1YsY8Ofjnz98XPCapX5WG30O6ed3-OJOqHJFCooQ

  to contain substring
      <string>: content of file ""/var/run/secrets/kubernetes.io/serviceaccount/token"": eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJlMmUtdGVzdHMtc3ZjYWNjb3VudHMtNWQxZjMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiZGVmYXVsdC10b2tlbi15NGJ2aiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZTQyMDA3YWEtY2RiMS0xMWU1LWFlYmUtNDIwMTBhZjAwMDAyIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmUyZS10ZXN0cy1zdmNhY2NvdW50cy01ZDFmMzpkZWZhdWx0In0.ey9Ak0dgOEIsSwjEUwi__bOk999g2iU5wT-FDAegxF6Dlk4z3e01LTQYD6xx0B0OulAP_G3RI32QgP1lmIPtiChe3YXmGwLyez4mPzU_BnWcZu2X9jsXxjWNqsU2yZeG4wKlJJBmWmIqZDJDZplmcqR7pCnBeoQwqkFgbf00DhCRr6b9RX5UlUUayqQNf6kK9ewpGkCUxbeQ1F8Y401VhcUJSXAOJSxYWpk1TN0NL88kboQZ1kDa2958EFc7voObAtC4loSooJk1AsWGiL6d3YdoNdiPWICOvPlO_J2I-fGv8dkuap0y9Jp6O0YmwDsJjUrqZ8_FmhUJugQ5CyCRvA

```

Maybe similar to #20494 ?

`/var/run/secrets/kubernetes.io/serviceaccount/token` does contain a token, but it looks like it is a different token.
",00DhCRr6b9RX5UlUUayqQNf6kK9ewpGkCUxbeQ1F8Y,https://github.com/kubernetes/kubernetes
181,14701.0,"The api doc shows:Kubernetes uses client certificates, tokens, or http basic auth to authenticate users for API calls.
So I tried to use basic auth to authenticate kubelet by set the --kubeconfig as config.yaml which generted by kubectl under ~/.kube/:

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: http://master:8080
  name: ubuntu
contexts:
- context:
    cluster: """"
    user: """"
  name: development
- context:
    cluster: ubuntu
    user: ubuntu
  name: ubuntu
current-context: ubuntu
kind: Config
preferences: {}
users:
- name: ubuntu
  user:
    password: wpJjc2rKyCoiP7tb
    username: admin
```

But it did not work, so I thought whether the kubelet can use basic auth?
",wpJjc2rKyCoiP7tb,https://github.com/kubernetes/kubernetes
182,14701.0,"The api doc shows:Kubernetes uses client certificates, tokens, or http basic auth to authenticate users for API calls.
So I tried to use basic auth to authenticate kubelet by set the --kubeconfig as config.yaml which generted by kubectl under ~/.kube/:

```
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: http://master:8080
  name: ubuntu
contexts:
- context:
    cluster: """"
    user: """"
  name: development
- context:
    cluster: ubuntu
    user: ubuntu
  name: ubuntu
current-context: ubuntu
kind: Config
preferences: {}
users:
- name: ubuntu
  user:
    password: wpJjc2rKyCoiP7tb
    username: admin
```

But it did not work, so I thought whether the kubelet can use basic auth?
",username,https://github.com/kubernetes/kubernetes
183,10247.0,"```
$ kubectl config view --minify
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://173.255.113.219
  name: kubernetes-satnam_e2e-test-satnam
contexts:
- context:
    cluster: kubernetes-satnam_e2e-test-satnam
    user: kubernetes-satnam_e2e-test-satnam
  name: kubernetes-satnam_e2e-test-satnam
current-context: kubernetes-satnam_e2e-test-satnam
kind: Config
preferences: {}
users:
- name: kubernetes-satnam_e2e-test-satnam
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
    token: LsUe2Z4cXqa61UQqQ2qWGGf7nOSLw9np
```
",LsUe2Z4cXqa61UQqQ2qWGGf7nOSLw9np,https://github.com/kubernetes/kubernetes
184,9483.0,"It seems creating a secret is only allowed if it's base64 encoded?  I have a secret that's PEM encoded, e.g.

```
apiVersion: v1beta3
kind: Secret
metadata:
  name: kube-keypair
data:
  kube-registry.crt: ""-----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm kvGkVwkfj+gR8NomUZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wY XYrhLEM357BxFRM1/G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ ltdI07XEPIgrGr5wvRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7 PutHNg3TIPSM4F4pmmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNb oHUT2RArHR1zJV47+NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O2 1+NM4DsjAgMBAAECggEAWNpciaJDWX2CUAMHsBbNfQgv1V02JeHaUebr4LvDJMCw tl0j/zdVwtby4ZbKcX96UdnmshTosPuZTUlPt2anmRv+0hrDX2pbUxTrUZ7U4yXc WIggQLebgP5pAS2qJOWS5Sk5K+kszJVE3PJuj3QIQ+by7ABahZPnS14usOyi2QUg eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB d7+9eypks7NA48hJt+NZZC9ZZXvNsRHdlVTueExtM9lJPZEDgkAUsd8wTx49pIs3 T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr 16cwb5Q38qu01o7R5pZ6KkigRt0FlMDn92hEnipPd/zJesoSTKGSeoTu7SsGF9IC kg7y6cO7RrWNQNwbM7dwN9jIq/kJN45oxClVNW73lxEwenYy4AIx5boeALoktToz tJ2Zj36b/LvDA//kyznqBa+JEwKBgQDD2gU8B46Jika6H2g+5Hmk9mNtHj6ZZIHc ijwkDXiq5uLLRwa+RX1OhhVFnStVoJ3QrhFJAGqJbz0GHiDnTu3yfQL7iQE/Oc0K HMk415o3pjk3VIYchOENlaYx6mfr5Og6+ZXlcgXUvN4lRdwDphwyM3L4EKrfnc3v C5TYv/9XsQKBgB7UWJeEz+mbPv1KuWjXpEBz22kHjyQq8hpIFbCex96AbrteuocV R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus 0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ UDJMfO0Tci8NbA7QBuRZnr5a5PNOfT3IO2ZcAGM94GKEznHSWt2d9yOZHc/xiBAn S39t7eoDxnzTzGjvP1qgGvMJUP6F09U2fEhkQ2ebWadHTkZ5srSW/WhYGK2veN81 A7Tuf9gkm/2OhwI01bIZdfbGo7refKz7btSPvcECgYEApN3h9grnhhZ9lvrGjPgj XAS6oAKmRZtLvlmvbKlZNdQceHDBDzUEn7JA4ASgGaq+WPQPimfmzdoXmRfEltUA BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2 Y9QiC6fdz+koJqq33yMwCy0= -----END PRIVATE KEY-----""
  kube-registry.key: ""-----BEGIN CERTIFICATE----- MIIDUDCCAjigAwIBAgIJAK87AEpK+g1OMA0GCSqGSIb3DQEBCwUAMB8xHTAbBgNV BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC95VjiVBvZCZLmkvGkVwkfj+gR8Nom UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1 /G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ltdI07XEPIgrGr5w vRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7PutHNg3TIPSM4F4p mmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNboHUT2RArHR1zJV47 +NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O21+NM4DsjAgMBAAGj gY4wgYswHQYDVR0OBBYEFBe5fmodCcmMKa3VU/dvLmf3AK7RME8GA1UdIwRIMEaA FBe5fmodCcmMKa3VU/dvLmf3AK7RoSOkITAfMR0wGwYDVQQDFBQxMjcuMC4wLjFA MTQzMjg0MDgzMoIJAK87AEpK+g1OMAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEG MA0GCSqGSIb3DQEBCwUAA4IBAQAQ2wCPmH9kTXC5JGNN/w6qOhVe4O48EnmI3ceL XNvb5aPHztZ73uP+NH5vrdJOr+86w3m6QeiAiivUlr5GmcDWutgaWhElrrwo9tY/ oJpJf3E8zzCGixONYlFQEFtDA1oUl8Nz8yvO3Eq58YJr5Gqwmbmf3CjxAuBA+GX5 YDPXr3YWu/AMekA6pcNZg92mYfDdNI8GzAkGXmGgSZfUELCXIDPpn2XIjuQvN+4J tg12xMOnDPAoFGXmqnQzwhHy68aMYzGUeHcDuGbYNUgd+AMey71uau6j76NW0JFl 9wZ2hsIRH18oZwAJFbOHPkHaRhXSvzFauKqYc6k629yUF0s+ -----END CERTIFICATE-----""
type: Opaque
```

kubectl create reports error:

```
error: unable to load file ""/tmp/secret.yaml"": unable to load ""/tmp/secret.yaml"": illegal base64 data at input byte 0
```

BTW, I'm not sure if puting certificate in secret is the right approach in k8s (or will work).  My use case is to mount certificates as secret volume, and set ssl_certificate directive in nginx to use that certificates. e.g.

```
      containers:
        - name: kube-nginx
          image: kube-nginx
          volumeMounts:
            - name: kube-keypair
              readOnly: true
              mountPath: /etc/nginx/ssl
      volumes:
        - name: kube-keypair
          secret:
            secretName: kube-keypair
```

and nginx conf

```
  ssl_certificate /etc/nginx/ssl/kube-registry.crt;
  ssl_certificate_key /etc/nginx/ssl/kube-registry.key;
```
",MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm,https://github.com/kubernetes/kubernetes
185,9483.0,"It seems creating a secret is only allowed if it's base64 encoded?  I have a secret that's PEM encoded, e.g.

```
apiVersion: v1beta3
kind: Secret
metadata:
  name: kube-keypair
data:
  kube-registry.crt: ""-----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm kvGkVwkfj+gR8NomUZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wY XYrhLEM357BxFRM1/G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ ltdI07XEPIgrGr5wvRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7 PutHNg3TIPSM4F4pmmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNb oHUT2RArHR1zJV47+NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O2 1+NM4DsjAgMBAAECggEAWNpciaJDWX2CUAMHsBbNfQgv1V02JeHaUebr4LvDJMCw tl0j/zdVwtby4ZbKcX96UdnmshTosPuZTUlPt2anmRv+0hrDX2pbUxTrUZ7U4yXc WIggQLebgP5pAS2qJOWS5Sk5K+kszJVE3PJuj3QIQ+by7ABahZPnS14usOyi2QUg eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB d7+9eypks7NA48hJt+NZZC9ZZXvNsRHdlVTueExtM9lJPZEDgkAUsd8wTx49pIs3 T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr 16cwb5Q38qu01o7R5pZ6KkigRt0FlMDn92hEnipPd/zJesoSTKGSeoTu7SsGF9IC kg7y6cO7RrWNQNwbM7dwN9jIq/kJN45oxClVNW73lxEwenYy4AIx5boeALoktToz tJ2Zj36b/LvDA//kyznqBa+JEwKBgQDD2gU8B46Jika6H2g+5Hmk9mNtHj6ZZIHc ijwkDXiq5uLLRwa+RX1OhhVFnStVoJ3QrhFJAGqJbz0GHiDnTu3yfQL7iQE/Oc0K HMk415o3pjk3VIYchOENlaYx6mfr5Og6+ZXlcgXUvN4lRdwDphwyM3L4EKrfnc3v C5TYv/9XsQKBgB7UWJeEz+mbPv1KuWjXpEBz22kHjyQq8hpIFbCex96AbrteuocV R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus 0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ UDJMfO0Tci8NbA7QBuRZnr5a5PNOfT3IO2ZcAGM94GKEznHSWt2d9yOZHc/xiBAn S39t7eoDxnzTzGjvP1qgGvMJUP6F09U2fEhkQ2ebWadHTkZ5srSW/WhYGK2veN81 A7Tuf9gkm/2OhwI01bIZdfbGo7refKz7btSPvcECgYEApN3h9grnhhZ9lvrGjPgj XAS6oAKmRZtLvlmvbKlZNdQceHDBDzUEn7JA4ASgGaq+WPQPimfmzdoXmRfEltUA BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2 Y9QiC6fdz+koJqq33yMwCy0= -----END PRIVATE KEY-----""
  kube-registry.key: ""-----BEGIN CERTIFICATE----- MIIDUDCCAjigAwIBAgIJAK87AEpK+g1OMA0GCSqGSIb3DQEBCwUAMB8xHTAbBgNV BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC95VjiVBvZCZLmkvGkVwkfj+gR8Nom UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1 /G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ltdI07XEPIgrGr5w vRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7PutHNg3TIPSM4F4p mmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNboHUT2RArHR1zJV47 +NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O21+NM4DsjAgMBAAGj gY4wgYswHQYDVR0OBBYEFBe5fmodCcmMKa3VU/dvLmf3AK7RME8GA1UdIwRIMEaA FBe5fmodCcmMKa3VU/dvLmf3AK7RoSOkITAfMR0wGwYDVQQDFBQxMjcuMC4wLjFA MTQzMjg0MDgzMoIJAK87AEpK+g1OMAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEG MA0GCSqGSIb3DQEBCwUAA4IBAQAQ2wCPmH9kTXC5JGNN/w6qOhVe4O48EnmI3ceL XNvb5aPHztZ73uP+NH5vrdJOr+86w3m6QeiAiivUlr5GmcDWutgaWhElrrwo9tY/ oJpJf3E8zzCGixONYlFQEFtDA1oUl8Nz8yvO3Eq58YJr5Gqwmbmf3CjxAuBA+GX5 YDPXr3YWu/AMekA6pcNZg92mYfDdNI8GzAkGXmGgSZfUELCXIDPpn2XIjuQvN+4J tg12xMOnDPAoFGXmqnQzwhHy68aMYzGUeHcDuGbYNUgd+AMey71uau6j76NW0JFl 9wZ2hsIRH18oZwAJFbOHPkHaRhXSvzFauKqYc6k629yUF0s+ -----END CERTIFICATE-----""
type: Opaque
```

kubectl create reports error:

```
error: unable to load file ""/tmp/secret.yaml"": unable to load ""/tmp/secret.yaml"": illegal base64 data at input byte 0
```

BTW, I'm not sure if puting certificate in secret is the right approach in k8s (or will work).  My use case is to mount certificates as secret volume, and set ssl_certificate directive in nginx to use that certificates. e.g.

```
      containers:
        - name: kube-nginx
          image: kube-nginx
          volumeMounts:
            - name: kube-keypair
              readOnly: true
              mountPath: /etc/nginx/ssl
      volumes:
        - name: kube-keypair
          secret:
            secretName: kube-keypair
```

and nginx conf

```
  ssl_certificate /etc/nginx/ssl/kube-registry.crt;
  ssl_certificate_key /etc/nginx/ssl/kube-registry.key;
```
",0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ,https://github.com/kubernetes/kubernetes
186,9483.0,"It seems creating a secret is only allowed if it's base64 encoded?  I have a secret that's PEM encoded, e.g.

```
apiVersion: v1beta3
kind: Secret
metadata:
  name: kube-keypair
data:
  kube-registry.crt: ""-----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm kvGkVwkfj+gR8NomUZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wY XYrhLEM357BxFRM1/G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ ltdI07XEPIgrGr5wvRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7 PutHNg3TIPSM4F4pmmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNb oHUT2RArHR1zJV47+NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O2 1+NM4DsjAgMBAAECggEAWNpciaJDWX2CUAMHsBbNfQgv1V02JeHaUebr4LvDJMCw tl0j/zdVwtby4ZbKcX96UdnmshTosPuZTUlPt2anmRv+0hrDX2pbUxTrUZ7U4yXc WIggQLebgP5pAS2qJOWS5Sk5K+kszJVE3PJuj3QIQ+by7ABahZPnS14usOyi2QUg eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB d7+9eypks7NA48hJt+NZZC9ZZXvNsRHdlVTueExtM9lJPZEDgkAUsd8wTx49pIs3 T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr 16cwb5Q38qu01o7R5pZ6KkigRt0FlMDn92hEnipPd/zJesoSTKGSeoTu7SsGF9IC kg7y6cO7RrWNQNwbM7dwN9jIq/kJN45oxClVNW73lxEwenYy4AIx5boeALoktToz tJ2Zj36b/LvDA//kyznqBa+JEwKBgQDD2gU8B46Jika6H2g+5Hmk9mNtHj6ZZIHc ijwkDXiq5uLLRwa+RX1OhhVFnStVoJ3QrhFJAGqJbz0GHiDnTu3yfQL7iQE/Oc0K HMk415o3pjk3VIYchOENlaYx6mfr5Og6+ZXlcgXUvN4lRdwDphwyM3L4EKrfnc3v C5TYv/9XsQKBgB7UWJeEz+mbPv1KuWjXpEBz22kHjyQq8hpIFbCex96AbrteuocV R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus 0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ UDJMfO0Tci8NbA7QBuRZnr5a5PNOfT3IO2ZcAGM94GKEznHSWt2d9yOZHc/xiBAn S39t7eoDxnzTzGjvP1qgGvMJUP6F09U2fEhkQ2ebWadHTkZ5srSW/WhYGK2veN81 A7Tuf9gkm/2OhwI01bIZdfbGo7refKz7btSPvcECgYEApN3h9grnhhZ9lvrGjPgj XAS6oAKmRZtLvlmvbKlZNdQceHDBDzUEn7JA4ASgGaq+WPQPimfmzdoXmRfEltUA BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2 Y9QiC6fdz+koJqq33yMwCy0= -----END PRIVATE KEY-----""
  kube-registry.key: ""-----BEGIN CERTIFICATE----- MIIDUDCCAjigAwIBAgIJAK87AEpK+g1OMA0GCSqGSIb3DQEBCwUAMB8xHTAbBgNV BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC95VjiVBvZCZLmkvGkVwkfj+gR8Nom UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1 /G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ltdI07XEPIgrGr5w vRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7PutHNg3TIPSM4F4p mmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNboHUT2RArHR1zJV47 +NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O21+NM4DsjAgMBAAGj gY4wgYswHQYDVR0OBBYEFBe5fmodCcmMKa3VU/dvLmf3AK7RME8GA1UdIwRIMEaA FBe5fmodCcmMKa3VU/dvLmf3AK7RoSOkITAfMR0wGwYDVQQDFBQxMjcuMC4wLjFA MTQzMjg0MDgzMoIJAK87AEpK+g1OMAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEG MA0GCSqGSIb3DQEBCwUAA4IBAQAQ2wCPmH9kTXC5JGNN/w6qOhVe4O48EnmI3ceL XNvb5aPHztZ73uP+NH5vrdJOr+86w3m6QeiAiivUlr5GmcDWutgaWhElrrwo9tY/ oJpJf3E8zzCGixONYlFQEFtDA1oUl8Nz8yvO3Eq58YJr5Gqwmbmf3CjxAuBA+GX5 YDPXr3YWu/AMekA6pcNZg92mYfDdNI8GzAkGXmGgSZfUELCXIDPpn2XIjuQvN+4J tg12xMOnDPAoFGXmqnQzwhHy68aMYzGUeHcDuGbYNUgd+AMey71uau6j76NW0JFl 9wZ2hsIRH18oZwAJFbOHPkHaRhXSvzFauKqYc6k629yUF0s+ -----END CERTIFICATE-----""
type: Opaque
```

kubectl create reports error:

```
error: unable to load file ""/tmp/secret.yaml"": unable to load ""/tmp/secret.yaml"": illegal base64 data at input byte 0
```

BTW, I'm not sure if puting certificate in secret is the right approach in k8s (or will work).  My use case is to mount certificates as secret volume, and set ssl_certificate directive in nginx to use that certificates. e.g.

```
      containers:
        - name: kube-nginx
          image: kube-nginx
          volumeMounts:
            - name: kube-keypair
              readOnly: true
              mountPath: /etc/nginx/ssl
      volumes:
        - name: kube-keypair
          secret:
            secretName: kube-keypair
```

and nginx conf

```
  ssl_certificate /etc/nginx/ssl/kube-registry.crt;
  ssl_certificate_key /etc/nginx/ssl/kube-registry.key;
```
",BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2,https://github.com/kubernetes/kubernetes
187,9483.0,"It seems creating a secret is only allowed if it's base64 encoded?  I have a secret that's PEM encoded, e.g.

```
apiVersion: v1beta3
kind: Secret
metadata:
  name: kube-keypair
data:
  kube-registry.crt: ""-----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm kvGkVwkfj+gR8NomUZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wY XYrhLEM357BxFRM1/G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ ltdI07XEPIgrGr5wvRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7 PutHNg3TIPSM4F4pmmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNb oHUT2RArHR1zJV47+NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O2 1+NM4DsjAgMBAAECggEAWNpciaJDWX2CUAMHsBbNfQgv1V02JeHaUebr4LvDJMCw tl0j/zdVwtby4ZbKcX96UdnmshTosPuZTUlPt2anmRv+0hrDX2pbUxTrUZ7U4yXc WIggQLebgP5pAS2qJOWS5Sk5K+kszJVE3PJuj3QIQ+by7ABahZPnS14usOyi2QUg eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB d7+9eypks7NA48hJt+NZZC9ZZXvNsRHdlVTueExtM9lJPZEDgkAUsd8wTx49pIs3 T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr 16cwb5Q38qu01o7R5pZ6KkigRt0FlMDn92hEnipPd/zJesoSTKGSeoTu7SsGF9IC kg7y6cO7RrWNQNwbM7dwN9jIq/kJN45oxClVNW73lxEwenYy4AIx5boeALoktToz tJ2Zj36b/LvDA//kyznqBa+JEwKBgQDD2gU8B46Jika6H2g+5Hmk9mNtHj6ZZIHc ijwkDXiq5uLLRwa+RX1OhhVFnStVoJ3QrhFJAGqJbz0GHiDnTu3yfQL7iQE/Oc0K HMk415o3pjk3VIYchOENlaYx6mfr5Og6+ZXlcgXUvN4lRdwDphwyM3L4EKrfnc3v C5TYv/9XsQKBgB7UWJeEz+mbPv1KuWjXpEBz22kHjyQq8hpIFbCex96AbrteuocV R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus 0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ UDJMfO0Tci8NbA7QBuRZnr5a5PNOfT3IO2ZcAGM94GKEznHSWt2d9yOZHc/xiBAn S39t7eoDxnzTzGjvP1qgGvMJUP6F09U2fEhkQ2ebWadHTkZ5srSW/WhYGK2veN81 A7Tuf9gkm/2OhwI01bIZdfbGo7refKz7btSPvcECgYEApN3h9grnhhZ9lvrGjPgj XAS6oAKmRZtLvlmvbKlZNdQceHDBDzUEn7JA4ASgGaq+WPQPimfmzdoXmRfEltUA BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2 Y9QiC6fdz+koJqq33yMwCy0= -----END PRIVATE KEY-----""
  kube-registry.key: ""-----BEGIN CERTIFICATE----- MIIDUDCCAjigAwIBAgIJAK87AEpK+g1OMA0GCSqGSIb3DQEBCwUAMB8xHTAbBgNV BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC95VjiVBvZCZLmkvGkVwkfj+gR8Nom UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1 /G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ltdI07XEPIgrGr5w vRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7PutHNg3TIPSM4F4p mmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNboHUT2RArHR1zJV47 +NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O21+NM4DsjAgMBAAGj gY4wgYswHQYDVR0OBBYEFBe5fmodCcmMKa3VU/dvLmf3AK7RME8GA1UdIwRIMEaA FBe5fmodCcmMKa3VU/dvLmf3AK7RoSOkITAfMR0wGwYDVQQDFBQxMjcuMC4wLjFA MTQzMjg0MDgzMoIJAK87AEpK+g1OMAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEG MA0GCSqGSIb3DQEBCwUAA4IBAQAQ2wCPmH9kTXC5JGNN/w6qOhVe4O48EnmI3ceL XNvb5aPHztZ73uP+NH5vrdJOr+86w3m6QeiAiivUlr5GmcDWutgaWhElrrwo9tY/ oJpJf3E8zzCGixONYlFQEFtDA1oUl8Nz8yvO3Eq58YJr5Gqwmbmf3CjxAuBA+GX5 YDPXr3YWu/AMekA6pcNZg92mYfDdNI8GzAkGXmGgSZfUELCXIDPpn2XIjuQvN+4J tg12xMOnDPAoFGXmqnQzwhHy68aMYzGUeHcDuGbYNUgd+AMey71uau6j76NW0JFl 9wZ2hsIRH18oZwAJFbOHPkHaRhXSvzFauKqYc6k629yUF0s+ -----END CERTIFICATE-----""
type: Opaque
```

kubectl create reports error:

```
error: unable to load file ""/tmp/secret.yaml"": unable to load ""/tmp/secret.yaml"": illegal base64 data at input byte 0
```

BTW, I'm not sure if puting certificate in secret is the right approach in k8s (or will work).  My use case is to mount certificates as secret volume, and set ssl_certificate directive in nginx to use that certificates. e.g.

```
      containers:
        - name: kube-nginx
          image: kube-nginx
          volumeMounts:
            - name: kube-keypair
              readOnly: true
              mountPath: /etc/nginx/ssl
      volumes:
        - name: kube-keypair
          secret:
            secretName: kube-keypair
```

and nginx conf

```
  ssl_certificate /etc/nginx/ssl/kube-registry.crt;
  ssl_certificate_key /etc/nginx/ssl/kube-registry.key;
```
",BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy,https://github.com/kubernetes/kubernetes
188,9483.0,"It seems creating a secret is only allowed if it's base64 encoded?  I have a secret that's PEM encoded, e.g.

```
apiVersion: v1beta3
kind: Secret
metadata:
  name: kube-keypair
data:
  kube-registry.crt: ""-----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm kvGkVwkfj+gR8NomUZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wY XYrhLEM357BxFRM1/G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ ltdI07XEPIgrGr5wvRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7 PutHNg3TIPSM4F4pmmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNb oHUT2RArHR1zJV47+NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O2 1+NM4DsjAgMBAAECggEAWNpciaJDWX2CUAMHsBbNfQgv1V02JeHaUebr4LvDJMCw tl0j/zdVwtby4ZbKcX96UdnmshTosPuZTUlPt2anmRv+0hrDX2pbUxTrUZ7U4yXc WIggQLebgP5pAS2qJOWS5Sk5K+kszJVE3PJuj3QIQ+by7ABahZPnS14usOyi2QUg eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB d7+9eypks7NA48hJt+NZZC9ZZXvNsRHdlVTueExtM9lJPZEDgkAUsd8wTx49pIs3 T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr 16cwb5Q38qu01o7R5pZ6KkigRt0FlMDn92hEnipPd/zJesoSTKGSeoTu7SsGF9IC kg7y6cO7RrWNQNwbM7dwN9jIq/kJN45oxClVNW73lxEwenYy4AIx5boeALoktToz tJ2Zj36b/LvDA//kyznqBa+JEwKBgQDD2gU8B46Jika6H2g+5Hmk9mNtHj6ZZIHc ijwkDXiq5uLLRwa+RX1OhhVFnStVoJ3QrhFJAGqJbz0GHiDnTu3yfQL7iQE/Oc0K HMk415o3pjk3VIYchOENlaYx6mfr5Og6+ZXlcgXUvN4lRdwDphwyM3L4EKrfnc3v C5TYv/9XsQKBgB7UWJeEz+mbPv1KuWjXpEBz22kHjyQq8hpIFbCex96AbrteuocV R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus 0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ UDJMfO0Tci8NbA7QBuRZnr5a5PNOfT3IO2ZcAGM94GKEznHSWt2d9yOZHc/xiBAn S39t7eoDxnzTzGjvP1qgGvMJUP6F09U2fEhkQ2ebWadHTkZ5srSW/WhYGK2veN81 A7Tuf9gkm/2OhwI01bIZdfbGo7refKz7btSPvcECgYEApN3h9grnhhZ9lvrGjPgj XAS6oAKmRZtLvlmvbKlZNdQceHDBDzUEn7JA4ASgGaq+WPQPimfmzdoXmRfEltUA BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2 Y9QiC6fdz+koJqq33yMwCy0= -----END PRIVATE KEY-----""
  kube-registry.key: ""-----BEGIN CERTIFICATE----- MIIDUDCCAjigAwIBAgIJAK87AEpK+g1OMA0GCSqGSIb3DQEBCwUAMB8xHTAbBgNV BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC95VjiVBvZCZLmkvGkVwkfj+gR8Nom UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1 /G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ltdI07XEPIgrGr5w vRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7PutHNg3TIPSM4F4p mmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNboHUT2RArHR1zJV47 +NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O21+NM4DsjAgMBAAGj gY4wgYswHQYDVR0OBBYEFBe5fmodCcmMKa3VU/dvLmf3AK7RME8GA1UdIwRIMEaA FBe5fmodCcmMKa3VU/dvLmf3AK7RoSOkITAfMR0wGwYDVQQDFBQxMjcuMC4wLjFA MTQzMjg0MDgzMoIJAK87AEpK+g1OMAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEG MA0GCSqGSIb3DQEBCwUAA4IBAQAQ2wCPmH9kTXC5JGNN/w6qOhVe4O48EnmI3ceL XNvb5aPHztZ73uP+NH5vrdJOr+86w3m6QeiAiivUlr5GmcDWutgaWhElrrwo9tY/ oJpJf3E8zzCGixONYlFQEFtDA1oUl8Nz8yvO3Eq58YJr5Gqwmbmf3CjxAuBA+GX5 YDPXr3YWu/AMekA6pcNZg92mYfDdNI8GzAkGXmGgSZfUELCXIDPpn2XIjuQvN+4J tg12xMOnDPAoFGXmqnQzwhHy68aMYzGUeHcDuGbYNUgd+AMey71uau6j76NW0JFl 9wZ2hsIRH18oZwAJFbOHPkHaRhXSvzFauKqYc6k629yUF0s+ -----END CERTIFICATE-----""
type: Opaque
```

kubectl create reports error:

```
error: unable to load file ""/tmp/secret.yaml"": unable to load ""/tmp/secret.yaml"": illegal base64 data at input byte 0
```

BTW, I'm not sure if puting certificate in secret is the right approach in k8s (or will work).  My use case is to mount certificates as secret volume, and set ssl_certificate directive in nginx to use that certificates. e.g.

```
      containers:
        - name: kube-nginx
          image: kube-nginx
          volumeMounts:
            - name: kube-keypair
              readOnly: true
              mountPath: /etc/nginx/ssl
      volumes:
        - name: kube-keypair
          secret:
            secretName: kube-keypair
```

and nginx conf

```
  ssl_certificate /etc/nginx/ssl/kube-registry.crt;
  ssl_certificate_key /etc/nginx/ssl/kube-registry.key;
```
",UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1,https://github.com/kubernetes/kubernetes
189,9483.0,"It seems creating a secret is only allowed if it's base64 encoded?  I have a secret that's PEM encoded, e.g.

```
apiVersion: v1beta3
kind: Secret
metadata:
  name: kube-keypair
data:
  kube-registry.crt: ""-----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm kvGkVwkfj+gR8NomUZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wY XYrhLEM357BxFRM1/G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ ltdI07XEPIgrGr5wvRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7 PutHNg3TIPSM4F4pmmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNb oHUT2RArHR1zJV47+NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O2 1+NM4DsjAgMBAAECggEAWNpciaJDWX2CUAMHsBbNfQgv1V02JeHaUebr4LvDJMCw tl0j/zdVwtby4ZbKcX96UdnmshTosPuZTUlPt2anmRv+0hrDX2pbUxTrUZ7U4yXc WIggQLebgP5pAS2qJOWS5Sk5K+kszJVE3PJuj3QIQ+by7ABahZPnS14usOyi2QUg eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB d7+9eypks7NA48hJt+NZZC9ZZXvNsRHdlVTueExtM9lJPZEDgkAUsd8wTx49pIs3 T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr 16cwb5Q38qu01o7R5pZ6KkigRt0FlMDn92hEnipPd/zJesoSTKGSeoTu7SsGF9IC kg7y6cO7RrWNQNwbM7dwN9jIq/kJN45oxClVNW73lxEwenYy4AIx5boeALoktToz tJ2Zj36b/LvDA//kyznqBa+JEwKBgQDD2gU8B46Jika6H2g+5Hmk9mNtHj6ZZIHc ijwkDXiq5uLLRwa+RX1OhhVFnStVoJ3QrhFJAGqJbz0GHiDnTu3yfQL7iQE/Oc0K HMk415o3pjk3VIYchOENlaYx6mfr5Og6+ZXlcgXUvN4lRdwDphwyM3L4EKrfnc3v C5TYv/9XsQKBgB7UWJeEz+mbPv1KuWjXpEBz22kHjyQq8hpIFbCex96AbrteuocV R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus 0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ UDJMfO0Tci8NbA7QBuRZnr5a5PNOfT3IO2ZcAGM94GKEznHSWt2d9yOZHc/xiBAn S39t7eoDxnzTzGjvP1qgGvMJUP6F09U2fEhkQ2ebWadHTkZ5srSW/WhYGK2veN81 A7Tuf9gkm/2OhwI01bIZdfbGo7refKz7btSPvcECgYEApN3h9grnhhZ9lvrGjPgj XAS6oAKmRZtLvlmvbKlZNdQceHDBDzUEn7JA4ASgGaq+WPQPimfmzdoXmRfEltUA BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2 Y9QiC6fdz+koJqq33yMwCy0= -----END PRIVATE KEY-----""
  kube-registry.key: ""-----BEGIN CERTIFICATE----- MIIDUDCCAjigAwIBAgIJAK87AEpK+g1OMA0GCSqGSIb3DQEBCwUAMB8xHTAbBgNV BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC95VjiVBvZCZLmkvGkVwkfj+gR8Nom UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1 /G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ltdI07XEPIgrGr5w vRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7PutHNg3TIPSM4F4p mmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNboHUT2RArHR1zJV47 +NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O21+NM4DsjAgMBAAGj gY4wgYswHQYDVR0OBBYEFBe5fmodCcmMKa3VU/dvLmf3AK7RME8GA1UdIwRIMEaA FBe5fmodCcmMKa3VU/dvLmf3AK7RoSOkITAfMR0wGwYDVQQDFBQxMjcuMC4wLjFA MTQzMjg0MDgzMoIJAK87AEpK+g1OMAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEG MA0GCSqGSIb3DQEBCwUAA4IBAQAQ2wCPmH9kTXC5JGNN/w6qOhVe4O48EnmI3ceL XNvb5aPHztZ73uP+NH5vrdJOr+86w3m6QeiAiivUlr5GmcDWutgaWhElrrwo9tY/ oJpJf3E8zzCGixONYlFQEFtDA1oUl8Nz8yvO3Eq58YJr5Gqwmbmf3CjxAuBA+GX5 YDPXr3YWu/AMekA6pcNZg92mYfDdNI8GzAkGXmGgSZfUELCXIDPpn2XIjuQvN+4J tg12xMOnDPAoFGXmqnQzwhHy68aMYzGUeHcDuGbYNUgd+AMey71uau6j76NW0JFl 9wZ2hsIRH18oZwAJFbOHPkHaRhXSvzFauKqYc6k629yUF0s+ -----END CERTIFICATE-----""
type: Opaque
```

kubectl create reports error:

```
error: unable to load file ""/tmp/secret.yaml"": unable to load ""/tmp/secret.yaml"": illegal base64 data at input byte 0
```

BTW, I'm not sure if puting certificate in secret is the right approach in k8s (or will work).  My use case is to mount certificates as secret volume, and set ssl_certificate directive in nginx to use that certificates. e.g.

```
      containers:
        - name: kube-nginx
          image: kube-nginx
          volumeMounts:
            - name: kube-keypair
              readOnly: true
              mountPath: /etc/nginx/ssl
      volumes:
        - name: kube-keypair
          secret:
            secretName: kube-keypair
```

and nginx conf

```
  ssl_certificate /etc/nginx/ssl/kube-registry.crt;
  ssl_certificate_key /etc/nginx/ssl/kube-registry.key;
```
",NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G,https://github.com/kubernetes/kubernetes
190,9483.0,"It seems creating a secret is only allowed if it's base64 encoded?  I have a secret that's PEM encoded, e.g.

```
apiVersion: v1beta3
kind: Secret
metadata:
  name: kube-keypair
data:
  kube-registry.crt: ""-----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm kvGkVwkfj+gR8NomUZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wY XYrhLEM357BxFRM1/G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ ltdI07XEPIgrGr5wvRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7 PutHNg3TIPSM4F4pmmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNb oHUT2RArHR1zJV47+NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O2 1+NM4DsjAgMBAAECggEAWNpciaJDWX2CUAMHsBbNfQgv1V02JeHaUebr4LvDJMCw tl0j/zdVwtby4ZbKcX96UdnmshTosPuZTUlPt2anmRv+0hrDX2pbUxTrUZ7U4yXc WIggQLebgP5pAS2qJOWS5Sk5K+kszJVE3PJuj3QIQ+by7ABahZPnS14usOyi2QUg eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB d7+9eypks7NA48hJt+NZZC9ZZXvNsRHdlVTueExtM9lJPZEDgkAUsd8wTx49pIs3 T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr 16cwb5Q38qu01o7R5pZ6KkigRt0FlMDn92hEnipPd/zJesoSTKGSeoTu7SsGF9IC kg7y6cO7RrWNQNwbM7dwN9jIq/kJN45oxClVNW73lxEwenYy4AIx5boeALoktToz tJ2Zj36b/LvDA//kyznqBa+JEwKBgQDD2gU8B46Jika6H2g+5Hmk9mNtHj6ZZIHc ijwkDXiq5uLLRwa+RX1OhhVFnStVoJ3QrhFJAGqJbz0GHiDnTu3yfQL7iQE/Oc0K HMk415o3pjk3VIYchOENlaYx6mfr5Og6+ZXlcgXUvN4lRdwDphwyM3L4EKrfnc3v C5TYv/9XsQKBgB7UWJeEz+mbPv1KuWjXpEBz22kHjyQq8hpIFbCex96AbrteuocV R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus 0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ UDJMfO0Tci8NbA7QBuRZnr5a5PNOfT3IO2ZcAGM94GKEznHSWt2d9yOZHc/xiBAn S39t7eoDxnzTzGjvP1qgGvMJUP6F09U2fEhkQ2ebWadHTkZ5srSW/WhYGK2veN81 A7Tuf9gkm/2OhwI01bIZdfbGo7refKz7btSPvcECgYEApN3h9grnhhZ9lvrGjPgj XAS6oAKmRZtLvlmvbKlZNdQceHDBDzUEn7JA4ASgGaq+WPQPimfmzdoXmRfEltUA BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2 Y9QiC6fdz+koJqq33yMwCy0= -----END PRIVATE KEY-----""
  kube-registry.key: ""-----BEGIN CERTIFICATE----- MIIDUDCCAjigAwIBAgIJAK87AEpK+g1OMA0GCSqGSIb3DQEBCwUAMB8xHTAbBgNV BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC95VjiVBvZCZLmkvGkVwkfj+gR8Nom UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1 /G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ltdI07XEPIgrGr5w vRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7PutHNg3TIPSM4F4p mmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNboHUT2RArHR1zJV47 +NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O21+NM4DsjAgMBAAGj gY4wgYswHQYDVR0OBBYEFBe5fmodCcmMKa3VU/dvLmf3AK7RME8GA1UdIwRIMEaA FBe5fmodCcmMKa3VU/dvLmf3AK7RoSOkITAfMR0wGwYDVQQDFBQxMjcuMC4wLjFA MTQzMjg0MDgzMoIJAK87AEpK+g1OMAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEG MA0GCSqGSIb3DQEBCwUAA4IBAQAQ2wCPmH9kTXC5JGNN/w6qOhVe4O48EnmI3ceL XNvb5aPHztZ73uP+NH5vrdJOr+86w3m6QeiAiivUlr5GmcDWutgaWhElrrwo9tY/ oJpJf3E8zzCGixONYlFQEFtDA1oUl8Nz8yvO3Eq58YJr5Gqwmbmf3CjxAuBA+GX5 YDPXr3YWu/AMekA6pcNZg92mYfDdNI8GzAkGXmGgSZfUELCXIDPpn2XIjuQvN+4J tg12xMOnDPAoFGXmqnQzwhHy68aMYzGUeHcDuGbYNUgd+AMey71uau6j76NW0JFl 9wZ2hsIRH18oZwAJFbOHPkHaRhXSvzFauKqYc6k629yUF0s+ -----END CERTIFICATE-----""
type: Opaque
```

kubectl create reports error:

```
error: unable to load file ""/tmp/secret.yaml"": unable to load ""/tmp/secret.yaml"": illegal base64 data at input byte 0
```

BTW, I'm not sure if puting certificate in secret is the right approach in k8s (or will work).  My use case is to mount certificates as secret volume, and set ssl_certificate directive in nginx to use that certificates. e.g.

```
      containers:
        - name: kube-nginx
          image: kube-nginx
          volumeMounts:
            - name: kube-keypair
              readOnly: true
              mountPath: /etc/nginx/ssl
      volumes:
        - name: kube-keypair
          secret:
            secretName: kube-keypair
```

and nginx conf

```
  ssl_certificate /etc/nginx/ssl/kube-registry.crt;
  ssl_certificate_key /etc/nginx/ssl/kube-registry.key;
```
",T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr,https://github.com/kubernetes/kubernetes
191,9483.0,"It seems creating a secret is only allowed if it's base64 encoded?  I have a secret that's PEM encoded, e.g.

```
apiVersion: v1beta3
kind: Secret
metadata:
  name: kube-keypair
data:
  kube-registry.crt: ""-----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm kvGkVwkfj+gR8NomUZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wY XYrhLEM357BxFRM1/G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ ltdI07XEPIgrGr5wvRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7 PutHNg3TIPSM4F4pmmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNb oHUT2RArHR1zJV47+NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O2 1+NM4DsjAgMBAAECggEAWNpciaJDWX2CUAMHsBbNfQgv1V02JeHaUebr4LvDJMCw tl0j/zdVwtby4ZbKcX96UdnmshTosPuZTUlPt2anmRv+0hrDX2pbUxTrUZ7U4yXc WIggQLebgP5pAS2qJOWS5Sk5K+kszJVE3PJuj3QIQ+by7ABahZPnS14usOyi2QUg eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB d7+9eypks7NA48hJt+NZZC9ZZXvNsRHdlVTueExtM9lJPZEDgkAUsd8wTx49pIs3 T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr 16cwb5Q38qu01o7R5pZ6KkigRt0FlMDn92hEnipPd/zJesoSTKGSeoTu7SsGF9IC kg7y6cO7RrWNQNwbM7dwN9jIq/kJN45oxClVNW73lxEwenYy4AIx5boeALoktToz tJ2Zj36b/LvDA//kyznqBa+JEwKBgQDD2gU8B46Jika6H2g+5Hmk9mNtHj6ZZIHc ijwkDXiq5uLLRwa+RX1OhhVFnStVoJ3QrhFJAGqJbz0GHiDnTu3yfQL7iQE/Oc0K HMk415o3pjk3VIYchOENlaYx6mfr5Og6+ZXlcgXUvN4lRdwDphwyM3L4EKrfnc3v C5TYv/9XsQKBgB7UWJeEz+mbPv1KuWjXpEBz22kHjyQq8hpIFbCex96AbrteuocV R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus 0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ UDJMfO0Tci8NbA7QBuRZnr5a5PNOfT3IO2ZcAGM94GKEznHSWt2d9yOZHc/xiBAn S39t7eoDxnzTzGjvP1qgGvMJUP6F09U2fEhkQ2ebWadHTkZ5srSW/WhYGK2veN81 A7Tuf9gkm/2OhwI01bIZdfbGo7refKz7btSPvcECgYEApN3h9grnhhZ9lvrGjPgj XAS6oAKmRZtLvlmvbKlZNdQceHDBDzUEn7JA4ASgGaq+WPQPimfmzdoXmRfEltUA BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2 Y9QiC6fdz+koJqq33yMwCy0= -----END PRIVATE KEY-----""
  kube-registry.key: ""-----BEGIN CERTIFICATE----- MIIDUDCCAjigAwIBAgIJAK87AEpK+g1OMA0GCSqGSIb3DQEBCwUAMB8xHTAbBgNV BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC95VjiVBvZCZLmkvGkVwkfj+gR8Nom UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1 /G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ltdI07XEPIgrGr5w vRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7PutHNg3TIPSM4F4p mmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNboHUT2RArHR1zJV47 +NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O21+NM4DsjAgMBAAGj gY4wgYswHQYDVR0OBBYEFBe5fmodCcmMKa3VU/dvLmf3AK7RME8GA1UdIwRIMEaA FBe5fmodCcmMKa3VU/dvLmf3AK7RoSOkITAfMR0wGwYDVQQDFBQxMjcuMC4wLjFA MTQzMjg0MDgzMoIJAK87AEpK+g1OMAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEG MA0GCSqGSIb3DQEBCwUAA4IBAQAQ2wCPmH9kTXC5JGNN/w6qOhVe4O48EnmI3ceL XNvb5aPHztZ73uP+NH5vrdJOr+86w3m6QeiAiivUlr5GmcDWutgaWhElrrwo9tY/ oJpJf3E8zzCGixONYlFQEFtDA1oUl8Nz8yvO3Eq58YJr5Gqwmbmf3CjxAuBA+GX5 YDPXr3YWu/AMekA6pcNZg92mYfDdNI8GzAkGXmGgSZfUELCXIDPpn2XIjuQvN+4J tg12xMOnDPAoFGXmqnQzwhHy68aMYzGUeHcDuGbYNUgd+AMey71uau6j76NW0JFl 9wZ2hsIRH18oZwAJFbOHPkHaRhXSvzFauKqYc6k629yUF0s+ -----END CERTIFICATE-----""
type: Opaque
```

kubectl create reports error:

```
error: unable to load file ""/tmp/secret.yaml"": unable to load ""/tmp/secret.yaml"": illegal base64 data at input byte 0
```

BTW, I'm not sure if puting certificate in secret is the right approach in k8s (or will work).  My use case is to mount certificates as secret volume, and set ssl_certificate directive in nginx to use that certificates. e.g.

```
      containers:
        - name: kube-nginx
          image: kube-nginx
          volumeMounts:
            - name: kube-keypair
              readOnly: true
              mountPath: /etc/nginx/ssl
      volumes:
        - name: kube-keypair
          secret:
            secretName: kube-keypair
```

and nginx conf

```
  ssl_certificate /etc/nginx/ssl/kube-registry.crt;
  ssl_certificate_key /etc/nginx/ssl/kube-registry.key;
```
",R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus,https://github.com/kubernetes/kubernetes
192,9483.0,"It seems creating a secret is only allowed if it's base64 encoded?  I have a secret that's PEM encoded, e.g.

```
apiVersion: v1beta3
kind: Secret
metadata:
  name: kube-keypair
data:
  kube-registry.crt: ""-----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm kvGkVwkfj+gR8NomUZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wY XYrhLEM357BxFRM1/G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ ltdI07XEPIgrGr5wvRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7 PutHNg3TIPSM4F4pmmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNb oHUT2RArHR1zJV47+NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O2 1+NM4DsjAgMBAAECggEAWNpciaJDWX2CUAMHsBbNfQgv1V02JeHaUebr4LvDJMCw tl0j/zdVwtby4ZbKcX96UdnmshTosPuZTUlPt2anmRv+0hrDX2pbUxTrUZ7U4yXc WIggQLebgP5pAS2qJOWS5Sk5K+kszJVE3PJuj3QIQ+by7ABahZPnS14usOyi2QUg eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB d7+9eypks7NA48hJt+NZZC9ZZXvNsRHdlVTueExtM9lJPZEDgkAUsd8wTx49pIs3 T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr 16cwb5Q38qu01o7R5pZ6KkigRt0FlMDn92hEnipPd/zJesoSTKGSeoTu7SsGF9IC kg7y6cO7RrWNQNwbM7dwN9jIq/kJN45oxClVNW73lxEwenYy4AIx5boeALoktToz tJ2Zj36b/LvDA//kyznqBa+JEwKBgQDD2gU8B46Jika6H2g+5Hmk9mNtHj6ZZIHc ijwkDXiq5uLLRwa+RX1OhhVFnStVoJ3QrhFJAGqJbz0GHiDnTu3yfQL7iQE/Oc0K HMk415o3pjk3VIYchOENlaYx6mfr5Og6+ZXlcgXUvN4lRdwDphwyM3L4EKrfnc3v C5TYv/9XsQKBgB7UWJeEz+mbPv1KuWjXpEBz22kHjyQq8hpIFbCex96AbrteuocV R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus 0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ UDJMfO0Tci8NbA7QBuRZnr5a5PNOfT3IO2ZcAGM94GKEznHSWt2d9yOZHc/xiBAn S39t7eoDxnzTzGjvP1qgGvMJUP6F09U2fEhkQ2ebWadHTkZ5srSW/WhYGK2veN81 A7Tuf9gkm/2OhwI01bIZdfbGo7refKz7btSPvcECgYEApN3h9grnhhZ9lvrGjPgj XAS6oAKmRZtLvlmvbKlZNdQceHDBDzUEn7JA4ASgGaq+WPQPimfmzdoXmRfEltUA BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2 Y9QiC6fdz+koJqq33yMwCy0= -----END PRIVATE KEY-----""
  kube-registry.key: ""-----BEGIN CERTIFICATE----- MIIDUDCCAjigAwIBAgIJAK87AEpK+g1OMA0GCSqGSIb3DQEBCwUAMB8xHTAbBgNV BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC95VjiVBvZCZLmkvGkVwkfj+gR8Nom UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1 /G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ltdI07XEPIgrGr5w vRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7PutHNg3TIPSM4F4p mmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNboHUT2RArHR1zJV47 +NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O21+NM4DsjAgMBAAGj gY4wgYswHQYDVR0OBBYEFBe5fmodCcmMKa3VU/dvLmf3AK7RME8GA1UdIwRIMEaA FBe5fmodCcmMKa3VU/dvLmf3AK7RoSOkITAfMR0wGwYDVQQDFBQxMjcuMC4wLjFA MTQzMjg0MDgzMoIJAK87AEpK+g1OMAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEG MA0GCSqGSIb3DQEBCwUAA4IBAQAQ2wCPmH9kTXC5JGNN/w6qOhVe4O48EnmI3ceL XNvb5aPHztZ73uP+NH5vrdJOr+86w3m6QeiAiivUlr5GmcDWutgaWhElrrwo9tY/ oJpJf3E8zzCGixONYlFQEFtDA1oUl8Nz8yvO3Eq58YJr5Gqwmbmf3CjxAuBA+GX5 YDPXr3YWu/AMekA6pcNZg92mYfDdNI8GzAkGXmGgSZfUELCXIDPpn2XIjuQvN+4J tg12xMOnDPAoFGXmqnQzwhHy68aMYzGUeHcDuGbYNUgd+AMey71uau6j76NW0JFl 9wZ2hsIRH18oZwAJFbOHPkHaRhXSvzFauKqYc6k629yUF0s+ -----END CERTIFICATE-----""
type: Opaque
```

kubectl create reports error:

```
error: unable to load file ""/tmp/secret.yaml"": unable to load ""/tmp/secret.yaml"": illegal base64 data at input byte 0
```

BTW, I'm not sure if puting certificate in secret is the right approach in k8s (or will work).  My use case is to mount certificates as secret volume, and set ssl_certificate directive in nginx to use that certificates. e.g.

```
      containers:
        - name: kube-nginx
          image: kube-nginx
          volumeMounts:
            - name: kube-keypair
              readOnly: true
              mountPath: /etc/nginx/ssl
      volumes:
        - name: kube-keypair
          secret:
            secretName: kube-keypair
```

and nginx conf

```
  ssl_certificate /etc/nginx/ssl/kube-registry.crt;
  ssl_certificate_key /etc/nginx/ssl/kube-registry.key;
```
",eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB,https://github.com/kubernetes/kubernetes
193,9483.0,"It seems creating a secret is only allowed if it's base64 encoded?  I have a secret that's PEM encoded, e.g.

```
apiVersion: v1beta3
kind: Secret
metadata:
  name: kube-keypair
data:
  kube-registry.crt: ""-----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm kvGkVwkfj+gR8NomUZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wY XYrhLEM357BxFRM1/G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ ltdI07XEPIgrGr5wvRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7 PutHNg3TIPSM4F4pmmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNb oHUT2RArHR1zJV47+NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O2 1+NM4DsjAgMBAAECggEAWNpciaJDWX2CUAMHsBbNfQgv1V02JeHaUebr4LvDJMCw tl0j/zdVwtby4ZbKcX96UdnmshTosPuZTUlPt2anmRv+0hrDX2pbUxTrUZ7U4yXc WIggQLebgP5pAS2qJOWS5Sk5K+kszJVE3PJuj3QIQ+by7ABahZPnS14usOyi2QUg eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB d7+9eypks7NA48hJt+NZZC9ZZXvNsRHdlVTueExtM9lJPZEDgkAUsd8wTx49pIs3 T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr 16cwb5Q38qu01o7R5pZ6KkigRt0FlMDn92hEnipPd/zJesoSTKGSeoTu7SsGF9IC kg7y6cO7RrWNQNwbM7dwN9jIq/kJN45oxClVNW73lxEwenYy4AIx5boeALoktToz tJ2Zj36b/LvDA//kyznqBa+JEwKBgQDD2gU8B46Jika6H2g+5Hmk9mNtHj6ZZIHc ijwkDXiq5uLLRwa+RX1OhhVFnStVoJ3QrhFJAGqJbz0GHiDnTu3yfQL7iQE/Oc0K HMk415o3pjk3VIYchOENlaYx6mfr5Og6+ZXlcgXUvN4lRdwDphwyM3L4EKrfnc3v C5TYv/9XsQKBgB7UWJeEz+mbPv1KuWjXpEBz22kHjyQq8hpIFbCex96AbrteuocV R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus 0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ UDJMfO0Tci8NbA7QBuRZnr5a5PNOfT3IO2ZcAGM94GKEznHSWt2d9yOZHc/xiBAn S39t7eoDxnzTzGjvP1qgGvMJUP6F09U2fEhkQ2ebWadHTkZ5srSW/WhYGK2veN81 A7Tuf9gkm/2OhwI01bIZdfbGo7refKz7btSPvcECgYEApN3h9grnhhZ9lvrGjPgj XAS6oAKmRZtLvlmvbKlZNdQceHDBDzUEn7JA4ASgGaq+WPQPimfmzdoXmRfEltUA BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2 Y9QiC6fdz+koJqq33yMwCy0= -----END PRIVATE KEY-----""
  kube-registry.key: ""-----BEGIN CERTIFICATE----- MIIDUDCCAjigAwIBAgIJAK87AEpK+g1OMA0GCSqGSIb3DQEBCwUAMB8xHTAbBgNV BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC95VjiVBvZCZLmkvGkVwkfj+gR8Nom UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1 /G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ltdI07XEPIgrGr5w vRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7PutHNg3TIPSM4F4p mmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNboHUT2RArHR1zJV47 +NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O21+NM4DsjAgMBAAGj gY4wgYswHQYDVR0OBBYEFBe5fmodCcmMKa3VU/dvLmf3AK7RME8GA1UdIwRIMEaA FBe5fmodCcmMKa3VU/dvLmf3AK7RoSOkITAfMR0wGwYDVQQDFBQxMjcuMC4wLjFA MTQzMjg0MDgzMoIJAK87AEpK+g1OMAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEG MA0GCSqGSIb3DQEBCwUAA4IBAQAQ2wCPmH9kTXC5JGNN/w6qOhVe4O48EnmI3ceL XNvb5aPHztZ73uP+NH5vrdJOr+86w3m6QeiAiivUlr5GmcDWutgaWhElrrwo9tY/ oJpJf3E8zzCGixONYlFQEFtDA1oUl8Nz8yvO3Eq58YJr5Gqwmbmf3CjxAuBA+GX5 YDPXr3YWu/AMekA6pcNZg92mYfDdNI8GzAkGXmGgSZfUELCXIDPpn2XIjuQvN+4J tg12xMOnDPAoFGXmqnQzwhHy68aMYzGUeHcDuGbYNUgd+AMey71uau6j76NW0JFl 9wZ2hsIRH18oZwAJFbOHPkHaRhXSvzFauKqYc6k629yUF0s+ -----END CERTIFICATE-----""
type: Opaque
```

kubectl create reports error:

```
error: unable to load file ""/tmp/secret.yaml"": unable to load ""/tmp/secret.yaml"": illegal base64 data at input byte 0
```

BTW, I'm not sure if puting certificate in secret is the right approach in k8s (or will work).  My use case is to mount certificates as secret volume, and set ssl_certificate directive in nginx to use that certificates. e.g.

```
      containers:
        - name: kube-nginx
          image: kube-nginx
          volumeMounts:
            - name: kube-keypair
              readOnly: true
              mountPath: /etc/nginx/ssl
      volumes:
        - name: kube-keypair
          secret:
            secretName: kube-keypair
```

and nginx conf

```
  ssl_certificate /etc/nginx/ssl/kube-registry.crt;
  ssl_certificate_key /etc/nginx/ssl/kube-registry.key;
```
",#NAME?,https://github.com/kubernetes/kubernetes
194,8362.0,"kubectl cannot process the following listed objects (copied from [here](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/secrets.md#use-case-pods-with-prod--test-credentials))

```
[{
  ""apiVersion"": ""v1beta2"",
  ""kind"": ""Secret"",
  ""id"": ""prod-db-secret"",
  ""data"": {
    ""username"": ""dmFsdWUtMQ0K"",
    ""password"": ""dmFsdWUtMg0KDQo=""
  }
},
{
  ""apiVersion"": ""v1beta2"",
  ""kind"": ""Secret"",
  ""id"": ""test-db-secret"",
  ""data"": {
    ""username"": ""dmFsdWUtMQ0K"",
    ""password"": ""dmFsdWUtMg0KDQo=""
  }
}]
```

```
$ kubectl create -f ...
Error: unable to get type info from ""double.json"": couldn't get version/kind: json: cannot unmarshal array into Go value of type struct { APIVersion string ""json:\""apiVersion,omitempty\""""; Kind string ""json:\""kind,omitempty\"""" }
```
#7257 states ""Group related objects together in a single file. This currently requires a List object in YAML, but it's still better than separate files. Almost nobody knows about this feature."" Does this feature only work for YAML but not JSON? Thanks.

@bgrant0607 @krousey @nikhiljindal 
",apiVersion,https://github.com/kubernetes/kubernetes
195,8362.0,"kubectl cannot process the following listed objects (copied from [here](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/secrets.md#use-case-pods-with-prod--test-credentials))

```
[{
  ""apiVersion"": ""v1beta2"",
  ""kind"": ""Secret"",
  ""id"": ""prod-db-secret"",
  ""data"": {
    ""username"": ""dmFsdWUtMQ0K"",
    ""password"": ""dmFsdWUtMg0KDQo=""
  }
},
{
  ""apiVersion"": ""v1beta2"",
  ""kind"": ""Secret"",
  ""id"": ""test-db-secret"",
  ""data"": {
    ""username"": ""dmFsdWUtMQ0K"",
    ""password"": ""dmFsdWUtMg0KDQo=""
  }
}]
```

```
$ kubectl create -f ...
Error: unable to get type info from ""double.json"": couldn't get version/kind: json: cannot unmarshal array into Go value of type struct { APIVersion string ""json:\""apiVersion,omitempty\""""; Kind string ""json:\""kind,omitempty\"""" }
```
#7257 states ""Group related objects together in a single file. This currently requires a List object in YAML, but it's still better than separate files. Almost nobody knows about this feature."" Does this feature only work for YAML but not JSON? Thanks.

@bgrant0607 @krousey @nikhiljindal 
",dmFsdWUtMg0KDQo=,https://github.com/kubernetes/kubernetes
196,8147.0,"In the step 1) of docs https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/getting-started-guides/ubuntu.md

Clone repo kubernetes via ssh would fail:

```
$ git clone git@github.com:GoogleCloudPlatform/kubernetes.git
Cloning into 'kubernetes'...
The authenticity of host 'github.com (192.30.252.129)' can't be established.
RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'github.com,192.30.252.129' (RSA) to the list of known hosts.
Permission denied (publickey).
fatal: Could not read from remote repository.

Please make sure you have the correct access rights
and the repository exists.
```

Better to change it to the [https one](https://github.com/GoogleCloudPlatform/kubernetes.git).
",63:1b:56:4d:eb:df:a6:48,https://github.com/kubernetes/kubernetes
197,6387.0,"Accessing `https://<master-ip>/api/v1beta1/proxy/services/monitoring-heapster/` redirects to `https://<master-ip>/validate/` instead of ` `https://<master-ip>/api/v1beta1/proxy/services/monitoring-heapster/validate`

Here is a snapshot of the request and response headers:

```
Remote Address:104.197.10.177:443
Request URL:https://104.197.10.177/api/v1beta1/proxy/services/monitoring-heapster/
Request Method:GET
Status Code:307 Temporary Redirect
Request Headersview source
Accept:text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8
Accept-Encoding:gzip, deflate, sdch
Accept-Language:en-US,en;q=0.8
Authorization:Basic YWRtaW46SXMwQkV0RDFrTmVRTENEcw==
Cache-Control:no-cache
Connection:keep-alive
DNT:1
Host:104.197.10.177
Pragma:no-cache
User-Agent:Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36

Response Headers
Connection:keep-alive
Content-Length:46
Content-Type:text/html; charset=utf-8
Date:Thu, 02 Apr 2015 21:21:32 GMT
Location:/validate/
Server:nginx/1.2.1
```
",YWRtaW46SXMwQkV0RDFrTmVRTENEcw,https://github.com/kubernetes/kubernetes
198,1212.0,"The vCenter Server Appliance (VCSA) is automatically configured with self-signed certificates which default to localhost.localdomain and may not include the final hostname which would yield a mismatch in the x509 Certificates. In case a new certificate is not re-generated, we should perhaps consider feature enhancement to allow override, this maybe quite common for home lab/dev environments. 

```
└─[0] cluster/kube-up.sh
Starting cluster using provider: vsphere
Using password: admin:ez3hgL2DU7WuIHTh

Starting master VM (this can take a minute)...
Error: Post https://vcsa.primp-industries.com/sdk: x509: certificate is valid for localhost.primp-industries.com, localhost, not vcsa.primp-industries.com
```
",ez3hgL2DU7WuIHTh,https://github.com/kubernetes/kubernetes
199,1212.0,"The vCenter Server Appliance (VCSA) is automatically configured with self-signed certificates which default to localhost.localdomain and may not include the final hostname which would yield a mismatch in the x509 Certificates. In case a new certificate is not re-generated, we should perhaps consider feature enhancement to allow override, this maybe quite common for home lab/dev environments. 

```
└─[0] cluster/kube-up.sh
Starting cluster using provider: vsphere
Using password: admin:ez3hgL2DU7WuIHTh

Starting master VM (this can take a minute)...
Error: Post https://vcsa.primp-industries.com/sdk: x509: certificate is valid for localhost.primp-industries.com, localhost, not vcsa.primp-industries.com
```
",Starting,https://github.com/kubernetes/kubernetes
0,,,,https://github.com/open-meteo/open-meteo
0,5259.0,"Visit https://redis.io for more details.

> openssl s_client -alpn h2 -servername redis.io -connect redis.io:443

```bash
Server Temp Key: ECDH, P-256, 256 bits
---
SSL handshake has read 3417 bytes and written 352 bytes
---
New, TLSv1/SSLv3, Cipher is ECDHE-RSA-AES256-GCM-SHA384
Server public key is 2048 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
No ALPN negotiated
SSL-Session:
    Protocol  : TLSv1.2
    Cipher    : ECDHE-RSA-AES256-GCM-SHA384
    Session-ID: 166A873199C0AB78F0209Ex0E1CDA6297C410DD81xxx95CF9
    Session-ID-ctx:
    Master-Key: 32A9B7E8DB11647956B5B4D23CC7280B63CFD7xxx383A7C4C716E42AB2D31AC2C72D61
    TLS session ticket lifetime hint: 300 (seconds)
    TLS session ticket:

    Start Time: 1534580406
    Timeout   : 7200 (sec)
    Verify return code: 10 (certificate has expired)
---
```


FYI",32A9B7E8DB11647956B5B4D23CC7280B63CFD7xxx383A7C4C716E42AB2D31AC2C72D61,https://github.com/redis/redis
1,5259.0,"Visit https://redis.io for more details.

> openssl s_client -alpn h2 -servername redis.io -connect redis.io:443

```bash
Server Temp Key: ECDH, P-256, 256 bits
---
SSL handshake has read 3417 bytes and written 352 bytes
---
New, TLSv1/SSLv3, Cipher is ECDHE-RSA-AES256-GCM-SHA384
Server public key is 2048 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
No ALPN negotiated
SSL-Session:
    Protocol  : TLSv1.2
    Cipher    : ECDHE-RSA-AES256-GCM-SHA384
    Session-ID: 166A873199C0AB78F0209Ex0E1CDA6297C410DD81xxx95CF9
    Session-ID-ctx:
    Master-Key: 32A9B7E8DB11647956B5B4D23CC7280B63CFD7xxx383A7C4C716E42AB2D31AC2C72D61
    TLS session ticket lifetime hint: 300 (seconds)
    TLS session ticket:

    Start Time: 1534580406
    Timeout   : 7200 (sec)
    Verify return code: 10 (certificate has expired)
---
```


FYI",166A873199C0AB78F0209Ex0E1CDA6297C410DD81xxx95CF9,https://github.com/redis/redis
0,15867.0,"Am up and going on Docker with no problems and trying to make an ""MVP"" for self-hosted auth within my company.

Trying to run a vanilla JS auth:
```
var SUPABASE_URL = 'http://10.195.246.14:3000/';
var SUPABASE_KEY = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJhbm9uIiwKICAgICJpc3MiOiAic3VwYWJhc2UtZGVtbyIsCiAgICAiaWF0IjogMTY0MTc2OTIwMCwKICAgICJleHAiOiAxNzk5NTM1NjAwCn0.dc_X5iR_VP_qT0zsiyj_I_OZ2T9FtRU2BBNWN8Bu4GE';
var supabase = supabase.createClient(SUPABASE_URL, SUPABASE_KEY)


window.userToken = null

document.addEventListener('DOMContentLoaded', function (event) {
  var signUpForm = document.querySelector('#sign-up')
  signUpForm.onsubmit = signUpSubmitted.bind(signUpForm)

  var logInForm = document.querySelector('#log-in')
  logInForm.onsubmit = logInSubmitted.bind(logInForm)

  var userDetailsButton = document.querySelector('#user-button')
  userDetailsButton.onclick = fetchUserDetails.bind(userDetailsButton)

  var logoutButton = document.querySelector('#logout-button')
  logoutButton.onclick = logoutSubmitted.bind(logoutButton)
})

const signUpSubmitted = (event) => {
  event.preventDefault()
  const email = event.target[0].value
  const password = event.target[1].value

  supabase.auth
    .signUp({ email, password })
    .then((response) => {
      response.error ? alert(response.error.message) : setToken(response)
    })
    .catch((err) => {
      alert(err)
    })
}
```

Was hit with the classic CORS error

<img width=""551"" alt=""image"" src=""https://github.com/supabase/supabase/assets/12734718/68ea97d7-6c86-406d-b30e-923da2306170"">

I stumbled upon [this tutorial](https://dev.to/the_cozma/kongplugin-cors-fixing-access-control-allow-origin-header-missing-error-iao) which suggested a modification to `kong.yml` but it did not work at all.

Where in the config can I specify to allow cors for the auth server?",DOMContentLoaded,https://github.com/supabase/supabase
1,15867.0,"Am up and going on Docker with no problems and trying to make an ""MVP"" for self-hosted auth within my company.

Trying to run a vanilla JS auth:
```
var SUPABASE_URL = 'http://10.195.246.14:3000/';
var SUPABASE_KEY = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJhbm9uIiwKICAgICJpc3MiOiAic3VwYWJhc2UtZGVtbyIsCiAgICAiaWF0IjogMTY0MTc2OTIwMCwKICAgICJleHAiOiAxNzk5NTM1NjAwCn0.dc_X5iR_VP_qT0zsiyj_I_OZ2T9FtRU2BBNWN8Bu4GE';
var supabase = supabase.createClient(SUPABASE_URL, SUPABASE_KEY)


window.userToken = null

document.addEventListener('DOMContentLoaded', function (event) {
  var signUpForm = document.querySelector('#sign-up')
  signUpForm.onsubmit = signUpSubmitted.bind(signUpForm)

  var logInForm = document.querySelector('#log-in')
  logInForm.onsubmit = logInSubmitted.bind(logInForm)

  var userDetailsButton = document.querySelector('#user-button')
  userDetailsButton.onclick = fetchUserDetails.bind(userDetailsButton)

  var logoutButton = document.querySelector('#logout-button')
  logoutButton.onclick = logoutSubmitted.bind(logoutButton)
})

const signUpSubmitted = (event) => {
  event.preventDefault()
  const email = event.target[0].value
  const password = event.target[1].value

  supabase.auth
    .signUp({ email, password })
    .then((response) => {
      response.error ? alert(response.error.message) : setToken(response)
    })
    .catch((err) => {
      alert(err)
    })
}
```

Was hit with the classic CORS error

<img width=""551"" alt=""image"" src=""https://github.com/supabase/supabase/assets/12734718/68ea97d7-6c86-406d-b30e-923da2306170"">

I stumbled upon [this tutorial](https://dev.to/the_cozma/kongplugin-cors-fixing-access-control-allow-origin-header-missing-error-iao) which suggested a modification to `kong.yml` but it did not work at all.

Where in the config can I specify to allow cors for the auth server?",response,https://github.com/supabase/supabase
2,9666.0,"# Bug report


## Describe the bug
when using following JWT_SECRET in /docker/.env file -> it keeps connecting as per screenshot
![image](https://user-images.githubusercontent.com/10296400/196603215-bf08be2d-2fc5-4b4b-aa1a-15545ba30837.png)
`JWT_SECRET=7dxxx5f1-xxxx-xxxx-xxxx-f6xxx27aa398`

issue is related with size of jwt_secret. it works good on using default jwt_secret provided in /docker/.env.example. Also note that value for jwt_secret I am using is 36 chars so it is > 32. so basically it won't allow any similar key with any other chars which is of this length.


## To Reproduce


1. change /docker/.env file to following: (I have only changed JWT_SECRET)
```
############
# Secrets 
# YOU MUST CHANGE THESE BEFORE GOING INTO PRODUCTION
############

POSTGRES_PASSWORD=your-super-secret-and-long-postgres-password
JWT_SECRET=7dxxx5f1-xxxx-xxxx-xxxx-f6xxx27aa398
ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJhbm9uIiwKICAgICJpc3MiOiAic3VwYWJhc2UtZGVtbyIsCiAgICAiaWF0IjogMTY0MTc2OTIwMCwKICAgICJleHAiOiAxNzk5NTM1NjAwCn0.dc_X5iR_VP_qT0zsiyj_I_OZ2T9FtRU2BBNWN8Bu4GE
SERVICE_ROLE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJzZXJ2aWNlX3JvbGUiLAogICAgImlzcyI6ICJzdXBhYmFzZS1kZW1vIiwKICAgICJpYXQiOiAxNjQxNzY5MjAwLAogICAgImV4cCI6IDE3OTk1MzU2MDAKfQ.DaYlNEoUrrEn2Ig7tqibS-PHK5vgusbcbo7X36XVt4Q


############
# Database - You can change these to any PostgreSQL database that has logical replication enabled.
############

POSTGRES_HOST=db
POSTGRES_DB=postgres
POSTGRES_USER=postgres
POSTGRES_PORT=5432


############
# API Proxy - Configuration for the Kong Reverse proxy.
############

KONG_HTTP_PORT=8000
KONG_HTTPS_PORT=8443


############
# API - Configuration for PostgREST.
############

PGRST_DB_SCHEMAS=public,storage,graphql_public


############
# Auth - Configuration for the GoTrue authentication server.
############

## General
SITE_URL=http://localhost:3000
ADDITIONAL_REDIRECT_URLS=
JWT_EXPIRY=3600
DISABLE_SIGNUP=false
API_EXTERNAL_URL=http://localhost:8000

## Mailer Config
MAILER_URLPATHS_CONFIRMATION=""/auth/v1/verify""
MAILER_URLPATHS_INVITE=""/auth/v1/verify""
MAILER_URLPATHS_RECOVERY=""/auth/v1/verify""
MAILER_URLPATHS_EMAIL_CHANGE=""/auth/v1/verify""

## Email auth
ENABLE_EMAIL_SIGNUP=true
ENABLE_EMAIL_AUTOCONFIRM=false
SMTP_ADMIN_EMAIL=admin@example.com
SMTP_HOST=mail
SMTP_PORT=2500
SMTP_USER=fake_mail_user
SMTP_PASS=fake_mail_password
SMTP_SENDER_NAME=fake_sender

## Phone auth
ENABLE_PHONE_SIGNUP=true
ENABLE_PHONE_AUTOCONFIRM=true


############
# Studio - Configuration for the Dashboard
############

STUDIO_ORGANIZATION_NAME=Default Organization
STUDIO_PROJECT_NAME=Default Project

STUDIO_PORT=3000
PUBLIC_REST_URL=http://localhost:8000/rest/v1/ # replace if you intend to use Studio outside of localhost

```
2. run `docker-compose up`
3. goto http://localhost:3000 & select default project
4. it keeps loading.....

## Expected behavior

it should load the project. Please allow shorter jwt_secret so we can use secrets generated by supabase cloud locally. 
 

## System information

- windows 11
- latest chrome
- master branch latest
- not relevant as I am using docker
 
",7dxxx5f1-xxxx-xxxx-xxxx-f6xxx27aa398,https://github.com/supabase/supabase
3,9666.0,"# Bug report


## Describe the bug
when using following JWT_SECRET in /docker/.env file -> it keeps connecting as per screenshot
![image](https://user-images.githubusercontent.com/10296400/196603215-bf08be2d-2fc5-4b4b-aa1a-15545ba30837.png)
`JWT_SECRET=7dxxx5f1-xxxx-xxxx-xxxx-f6xxx27aa398`

issue is related with size of jwt_secret. it works good on using default jwt_secret provided in /docker/.env.example. Also note that value for jwt_secret I am using is 36 chars so it is > 32. so basically it won't allow any similar key with any other chars which is of this length.


## To Reproduce


1. change /docker/.env file to following: (I have only changed JWT_SECRET)
```
############
# Secrets 
# YOU MUST CHANGE THESE BEFORE GOING INTO PRODUCTION
############

POSTGRES_PASSWORD=your-super-secret-and-long-postgres-password
JWT_SECRET=7dxxx5f1-xxxx-xxxx-xxxx-f6xxx27aa398
ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJhbm9uIiwKICAgICJpc3MiOiAic3VwYWJhc2UtZGVtbyIsCiAgICAiaWF0IjogMTY0MTc2OTIwMCwKICAgICJleHAiOiAxNzk5NTM1NjAwCn0.dc_X5iR_VP_qT0zsiyj_I_OZ2T9FtRU2BBNWN8Bu4GE
SERVICE_ROLE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJzZXJ2aWNlX3JvbGUiLAogICAgImlzcyI6ICJzdXBhYmFzZS1kZW1vIiwKICAgICJpYXQiOiAxNjQxNzY5MjAwLAogICAgImV4cCI6IDE3OTk1MzU2MDAKfQ.DaYlNEoUrrEn2Ig7tqibS-PHK5vgusbcbo7X36XVt4Q


############
# Database - You can change these to any PostgreSQL database that has logical replication enabled.
############

POSTGRES_HOST=db
POSTGRES_DB=postgres
POSTGRES_USER=postgres
POSTGRES_PORT=5432


############
# API Proxy - Configuration for the Kong Reverse proxy.
############

KONG_HTTP_PORT=8000
KONG_HTTPS_PORT=8443


############
# API - Configuration for PostgREST.
############

PGRST_DB_SCHEMAS=public,storage,graphql_public


############
# Auth - Configuration for the GoTrue authentication server.
############

## General
SITE_URL=http://localhost:3000
ADDITIONAL_REDIRECT_URLS=
JWT_EXPIRY=3600
DISABLE_SIGNUP=false
API_EXTERNAL_URL=http://localhost:8000

## Mailer Config
MAILER_URLPATHS_CONFIRMATION=""/auth/v1/verify""
MAILER_URLPATHS_INVITE=""/auth/v1/verify""
MAILER_URLPATHS_RECOVERY=""/auth/v1/verify""
MAILER_URLPATHS_EMAIL_CHANGE=""/auth/v1/verify""

## Email auth
ENABLE_EMAIL_SIGNUP=true
ENABLE_EMAIL_AUTOCONFIRM=false
SMTP_ADMIN_EMAIL=admin@example.com
SMTP_HOST=mail
SMTP_PORT=2500
SMTP_USER=fake_mail_user
SMTP_PASS=fake_mail_password
SMTP_SENDER_NAME=fake_sender

## Phone auth
ENABLE_PHONE_SIGNUP=true
ENABLE_PHONE_AUTOCONFIRM=true


############
# Studio - Configuration for the Dashboard
############

STUDIO_ORGANIZATION_NAME=Default Organization
STUDIO_PROJECT_NAME=Default Project

STUDIO_PORT=3000
PUBLIC_REST_URL=http://localhost:8000/rest/v1/ # replace if you intend to use Studio outside of localhost

```
2. run `docker-compose up`
3. goto http://localhost:3000 & select default project
4. it keeps loading.....

## Expected behavior

it should load the project. Please allow shorter jwt_secret so we can use secrets generated by supabase cloud locally. 
 

## System information

- windows 11
- latest chrome
- master branch latest
- not relevant as I am using docker
 
",PHK5vgusbcbo7X36XVt4Q,https://github.com/supabase/supabase
4,9666.0,"# Bug report


## Describe the bug
when using following JWT_SECRET in /docker/.env file -> it keeps connecting as per screenshot
![image](https://user-images.githubusercontent.com/10296400/196603215-bf08be2d-2fc5-4b4b-aa1a-15545ba30837.png)
`JWT_SECRET=7dxxx5f1-xxxx-xxxx-xxxx-f6xxx27aa398`

issue is related with size of jwt_secret. it works good on using default jwt_secret provided in /docker/.env.example. Also note that value for jwt_secret I am using is 36 chars so it is > 32. so basically it won't allow any similar key with any other chars which is of this length.


## To Reproduce


1. change /docker/.env file to following: (I have only changed JWT_SECRET)
```
############
# Secrets 
# YOU MUST CHANGE THESE BEFORE GOING INTO PRODUCTION
############

POSTGRES_PASSWORD=your-super-secret-and-long-postgres-password
JWT_SECRET=7dxxx5f1-xxxx-xxxx-xxxx-f6xxx27aa398
ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJhbm9uIiwKICAgICJpc3MiOiAic3VwYWJhc2UtZGVtbyIsCiAgICAiaWF0IjogMTY0MTc2OTIwMCwKICAgICJleHAiOiAxNzk5NTM1NjAwCn0.dc_X5iR_VP_qT0zsiyj_I_OZ2T9FtRU2BBNWN8Bu4GE
SERVICE_ROLE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJzZXJ2aWNlX3JvbGUiLAogICAgImlzcyI6ICJzdXBhYmFzZS1kZW1vIiwKICAgICJpYXQiOiAxNjQxNzY5MjAwLAogICAgImV4cCI6IDE3OTk1MzU2MDAKfQ.DaYlNEoUrrEn2Ig7tqibS-PHK5vgusbcbo7X36XVt4Q


############
# Database - You can change these to any PostgreSQL database that has logical replication enabled.
############

POSTGRES_HOST=db
POSTGRES_DB=postgres
POSTGRES_USER=postgres
POSTGRES_PORT=5432


############
# API Proxy - Configuration for the Kong Reverse proxy.
############

KONG_HTTP_PORT=8000
KONG_HTTPS_PORT=8443


############
# API - Configuration for PostgREST.
############

PGRST_DB_SCHEMAS=public,storage,graphql_public


############
# Auth - Configuration for the GoTrue authentication server.
############

## General
SITE_URL=http://localhost:3000
ADDITIONAL_REDIRECT_URLS=
JWT_EXPIRY=3600
DISABLE_SIGNUP=false
API_EXTERNAL_URL=http://localhost:8000

## Mailer Config
MAILER_URLPATHS_CONFIRMATION=""/auth/v1/verify""
MAILER_URLPATHS_INVITE=""/auth/v1/verify""
MAILER_URLPATHS_RECOVERY=""/auth/v1/verify""
MAILER_URLPATHS_EMAIL_CHANGE=""/auth/v1/verify""

## Email auth
ENABLE_EMAIL_SIGNUP=true
ENABLE_EMAIL_AUTOCONFIRM=false
SMTP_ADMIN_EMAIL=admin@example.com
SMTP_HOST=mail
SMTP_PORT=2500
SMTP_USER=fake_mail_user
SMTP_PASS=fake_mail_password
SMTP_SENDER_NAME=fake_sender

## Phone auth
ENABLE_PHONE_SIGNUP=true
ENABLE_PHONE_AUTOCONFIRM=true


############
# Studio - Configuration for the Dashboard
############

STUDIO_ORGANIZATION_NAME=Default Organization
STUDIO_PROJECT_NAME=Default Project

STUDIO_PORT=3000
PUBLIC_REST_URL=http://localhost:8000/rest/v1/ # replace if you intend to use Studio outside of localhost

```
2. run `docker-compose up`
3. goto http://localhost:3000 & select default project
4. it keeps loading.....

## Expected behavior

it should load the project. Please allow shorter jwt_secret so we can use secrets generated by supabase cloud locally. 
 

## System information

- windows 11
- latest chrome
- master branch latest
- not relevant as I am using docker
 
",postgres-password,https://github.com/supabase/supabase
5,9666.0,"# Bug report


## Describe the bug
when using following JWT_SECRET in /docker/.env file -> it keeps connecting as per screenshot
![image](https://user-images.githubusercontent.com/10296400/196603215-bf08be2d-2fc5-4b4b-aa1a-15545ba30837.png)
`JWT_SECRET=7dxxx5f1-xxxx-xxxx-xxxx-f6xxx27aa398`

issue is related with size of jwt_secret. it works good on using default jwt_secret provided in /docker/.env.example. Also note that value for jwt_secret I am using is 36 chars so it is > 32. so basically it won't allow any similar key with any other chars which is of this length.


## To Reproduce


1. change /docker/.env file to following: (I have only changed JWT_SECRET)
```
############
# Secrets 
# YOU MUST CHANGE THESE BEFORE GOING INTO PRODUCTION
############

POSTGRES_PASSWORD=your-super-secret-and-long-postgres-password
JWT_SECRET=7dxxx5f1-xxxx-xxxx-xxxx-f6xxx27aa398
ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJhbm9uIiwKICAgICJpc3MiOiAic3VwYWJhc2UtZGVtbyIsCiAgICAiaWF0IjogMTY0MTc2OTIwMCwKICAgICJleHAiOiAxNzk5NTM1NjAwCn0.dc_X5iR_VP_qT0zsiyj_I_OZ2T9FtRU2BBNWN8Bu4GE
SERVICE_ROLE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJzZXJ2aWNlX3JvbGUiLAogICAgImlzcyI6ICJzdXBhYmFzZS1kZW1vIiwKICAgICJpYXQiOiAxNjQxNzY5MjAwLAogICAgImV4cCI6IDE3OTk1MzU2MDAKfQ.DaYlNEoUrrEn2Ig7tqibS-PHK5vgusbcbo7X36XVt4Q


############
# Database - You can change these to any PostgreSQL database that has logical replication enabled.
############

POSTGRES_HOST=db
POSTGRES_DB=postgres
POSTGRES_USER=postgres
POSTGRES_PORT=5432


############
# API Proxy - Configuration for the Kong Reverse proxy.
############

KONG_HTTP_PORT=8000
KONG_HTTPS_PORT=8443


############
# API - Configuration for PostgREST.
############

PGRST_DB_SCHEMAS=public,storage,graphql_public


############
# Auth - Configuration for the GoTrue authentication server.
############

## General
SITE_URL=http://localhost:3000
ADDITIONAL_REDIRECT_URLS=
JWT_EXPIRY=3600
DISABLE_SIGNUP=false
API_EXTERNAL_URL=http://localhost:8000

## Mailer Config
MAILER_URLPATHS_CONFIRMATION=""/auth/v1/verify""
MAILER_URLPATHS_INVITE=""/auth/v1/verify""
MAILER_URLPATHS_RECOVERY=""/auth/v1/verify""
MAILER_URLPATHS_EMAIL_CHANGE=""/auth/v1/verify""

## Email auth
ENABLE_EMAIL_SIGNUP=true
ENABLE_EMAIL_AUTOCONFIRM=false
SMTP_ADMIN_EMAIL=admin@example.com
SMTP_HOST=mail
SMTP_PORT=2500
SMTP_USER=fake_mail_user
SMTP_PASS=fake_mail_password
SMTP_SENDER_NAME=fake_sender

## Phone auth
ENABLE_PHONE_SIGNUP=true
ENABLE_PHONE_AUTOCONFIRM=true


############
# Studio - Configuration for the Dashboard
############

STUDIO_ORGANIZATION_NAME=Default Organization
STUDIO_PROJECT_NAME=Default Project

STUDIO_PORT=3000
PUBLIC_REST_URL=http://localhost:8000/rest/v1/ # replace if you intend to use Studio outside of localhost

```
2. run `docker-compose up`
3. goto http://localhost:3000 & select default project
4. it keeps loading.....

## Expected behavior

it should load the project. Please allow shorter jwt_secret so we can use secrets generated by supabase cloud locally. 
 

## System information

- windows 11
- latest chrome
- master branch latest
- not relevant as I am using docker
 
",xxxx-f6xxx27aa398,https://github.com/supabase/supabase
6,9666.0,"# Bug report


## Describe the bug
when using following JWT_SECRET in /docker/.env file -> it keeps connecting as per screenshot
![image](https://user-images.githubusercontent.com/10296400/196603215-bf08be2d-2fc5-4b4b-aa1a-15545ba30837.png)
`JWT_SECRET=7dxxx5f1-xxxx-xxxx-xxxx-f6xxx27aa398`

issue is related with size of jwt_secret. it works good on using default jwt_secret provided in /docker/.env.example. Also note that value for jwt_secret I am using is 36 chars so it is > 32. so basically it won't allow any similar key with any other chars which is of this length.


## To Reproduce


1. change /docker/.env file to following: (I have only changed JWT_SECRET)
```
############
# Secrets 
# YOU MUST CHANGE THESE BEFORE GOING INTO PRODUCTION
############

POSTGRES_PASSWORD=your-super-secret-and-long-postgres-password
JWT_SECRET=7dxxx5f1-xxxx-xxxx-xxxx-f6xxx27aa398
ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJhbm9uIiwKICAgICJpc3MiOiAic3VwYWJhc2UtZGVtbyIsCiAgICAiaWF0IjogMTY0MTc2OTIwMCwKICAgICJleHAiOiAxNzk5NTM1NjAwCn0.dc_X5iR_VP_qT0zsiyj_I_OZ2T9FtRU2BBNWN8Bu4GE
SERVICE_ROLE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJzZXJ2aWNlX3JvbGUiLAogICAgImlzcyI6ICJzdXBhYmFzZS1kZW1vIiwKICAgICJpYXQiOiAxNjQxNzY5MjAwLAogICAgImV4cCI6IDE3OTk1MzU2MDAKfQ.DaYlNEoUrrEn2Ig7tqibS-PHK5vgusbcbo7X36XVt4Q


############
# Database - You can change these to any PostgreSQL database that has logical replication enabled.
############

POSTGRES_HOST=db
POSTGRES_DB=postgres
POSTGRES_USER=postgres
POSTGRES_PORT=5432


############
# API Proxy - Configuration for the Kong Reverse proxy.
############

KONG_HTTP_PORT=8000
KONG_HTTPS_PORT=8443


############
# API - Configuration for PostgREST.
############

PGRST_DB_SCHEMAS=public,storage,graphql_public


############
# Auth - Configuration for the GoTrue authentication server.
############

## General
SITE_URL=http://localhost:3000
ADDITIONAL_REDIRECT_URLS=
JWT_EXPIRY=3600
DISABLE_SIGNUP=false
API_EXTERNAL_URL=http://localhost:8000

## Mailer Config
MAILER_URLPATHS_CONFIRMATION=""/auth/v1/verify""
MAILER_URLPATHS_INVITE=""/auth/v1/verify""
MAILER_URLPATHS_RECOVERY=""/auth/v1/verify""
MAILER_URLPATHS_EMAIL_CHANGE=""/auth/v1/verify""

## Email auth
ENABLE_EMAIL_SIGNUP=true
ENABLE_EMAIL_AUTOCONFIRM=false
SMTP_ADMIN_EMAIL=admin@example.com
SMTP_HOST=mail
SMTP_PORT=2500
SMTP_USER=fake_mail_user
SMTP_PASS=fake_mail_password
SMTP_SENDER_NAME=fake_sender

## Phone auth
ENABLE_PHONE_SIGNUP=true
ENABLE_PHONE_AUTOCONFIRM=true


############
# Studio - Configuration for the Dashboard
############

STUDIO_ORGANIZATION_NAME=Default Organization
STUDIO_PROJECT_NAME=Default Project

STUDIO_PORT=3000
PUBLIC_REST_URL=http://localhost:8000/rest/v1/ # replace if you intend to use Studio outside of localhost

```
2. run `docker-compose up`
3. goto http://localhost:3000 & select default project
4. it keeps loading.....

## Expected behavior

it should load the project. Please allow shorter jwt_secret so we can use secrets generated by supabase cloud locally. 
 

## System information

- windows 11
- latest chrome
- master branch latest
- not relevant as I am using docker
 
",SMTP_SENDER_NAME=fake_sender,https://github.com/supabase/supabase
7,9666.0,"# Bug report


## Describe the bug
when using following JWT_SECRET in /docker/.env file -> it keeps connecting as per screenshot
![image](https://user-images.githubusercontent.com/10296400/196603215-bf08be2d-2fc5-4b4b-aa1a-15545ba30837.png)
`JWT_SECRET=7dxxx5f1-xxxx-xxxx-xxxx-f6xxx27aa398`

issue is related with size of jwt_secret. it works good on using default jwt_secret provided in /docker/.env.example. Also note that value for jwt_secret I am using is 36 chars so it is > 32. so basically it won't allow any similar key with any other chars which is of this length.


## To Reproduce


1. change /docker/.env file to following: (I have only changed JWT_SECRET)
```
############
# Secrets 
# YOU MUST CHANGE THESE BEFORE GOING INTO PRODUCTION
############

POSTGRES_PASSWORD=your-super-secret-and-long-postgres-password
JWT_SECRET=7dxxx5f1-xxxx-xxxx-xxxx-f6xxx27aa398
ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJhbm9uIiwKICAgICJpc3MiOiAic3VwYWJhc2UtZGVtbyIsCiAgICAiaWF0IjogMTY0MTc2OTIwMCwKICAgICJleHAiOiAxNzk5NTM1NjAwCn0.dc_X5iR_VP_qT0zsiyj_I_OZ2T9FtRU2BBNWN8Bu4GE
SERVICE_ROLE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJzZXJ2aWNlX3JvbGUiLAogICAgImlzcyI6ICJzdXBhYmFzZS1kZW1vIiwKICAgICJpYXQiOiAxNjQxNzY5MjAwLAogICAgImV4cCI6IDE3OTk1MzU2MDAKfQ.DaYlNEoUrrEn2Ig7tqibS-PHK5vgusbcbo7X36XVt4Q


############
# Database - You can change these to any PostgreSQL database that has logical replication enabled.
############

POSTGRES_HOST=db
POSTGRES_DB=postgres
POSTGRES_USER=postgres
POSTGRES_PORT=5432


############
# API Proxy - Configuration for the Kong Reverse proxy.
############

KONG_HTTP_PORT=8000
KONG_HTTPS_PORT=8443


############
# API - Configuration for PostgREST.
############

PGRST_DB_SCHEMAS=public,storage,graphql_public


############
# Auth - Configuration for the GoTrue authentication server.
############

## General
SITE_URL=http://localhost:3000
ADDITIONAL_REDIRECT_URLS=
JWT_EXPIRY=3600
DISABLE_SIGNUP=false
API_EXTERNAL_URL=http://localhost:8000

## Mailer Config
MAILER_URLPATHS_CONFIRMATION=""/auth/v1/verify""
MAILER_URLPATHS_INVITE=""/auth/v1/verify""
MAILER_URLPATHS_RECOVERY=""/auth/v1/verify""
MAILER_URLPATHS_EMAIL_CHANGE=""/auth/v1/verify""

## Email auth
ENABLE_EMAIL_SIGNUP=true
ENABLE_EMAIL_AUTOCONFIRM=false
SMTP_ADMIN_EMAIL=admin@example.com
SMTP_HOST=mail
SMTP_PORT=2500
SMTP_USER=fake_mail_user
SMTP_PASS=fake_mail_password
SMTP_SENDER_NAME=fake_sender

## Phone auth
ENABLE_PHONE_SIGNUP=true
ENABLE_PHONE_AUTOCONFIRM=true


############
# Studio - Configuration for the Dashboard
############

STUDIO_ORGANIZATION_NAME=Default Organization
STUDIO_PROJECT_NAME=Default Project

STUDIO_PORT=3000
PUBLIC_REST_URL=http://localhost:8000/rest/v1/ # replace if you intend to use Studio outside of localhost

```
2. run `docker-compose up`
3. goto http://localhost:3000 & select default project
4. it keeps loading.....

## Expected behavior

it should load the project. Please allow shorter jwt_secret so we can use secrets generated by supabase cloud locally. 
 

## System information

- windows 11
- latest chrome
- master branch latest
- not relevant as I am using docker
 
",password,https://github.com/supabase/supabase
8,4040.0,"The user signup with his email, A confirmation email was sent in his inbox
```js
const {
    session: _session,
    error
} = await supabase.auth.signUp({
    email,
    password
}, {
    redirectTo: 'http://localhost:3000/login?message=Congratulations! Your email now is confirmed.'
});
if (error) {
    alert(error.message);
    return;
}
alert(`check your email!`);
```

When the user click on the verification link he get this:

```
http://localhost:3000/#error_code=404&error_description=Confirmation+Token+not+found
```

The link was like that:
```
https://glkylfhlgbxdibbqaqyg.supabase.co/auth/v1/verify?token=qFJNoSmSncg5cdZ0Bw6CXQ&type=signup&redirect_to=http://localhost:3000/
```

Another thing! The `{ redirectTo }` option is worked with the providers correctly, But with *email & password* it doesn't work!

#### Note
> The email verification link worked at the first try, But when i removed the user and trying again, the error above is happened",redirectTo,https://github.com/supabase/supabase
9,2706.0,"### Discussed in https://github.com/supabase/supabase/discussions/2620

<div type='discussions-op-text'>

<sup>Originally posted by **IzMichael** July 29, 2021</sup>
Hi! I'm trying to create my first database, but whatever password I use, it says it's not strong enough. I've tried using passwords like 'wzyijmX6ql$V7ghHCXWWtmi' and it's not strong enough apparently.</div>",wzyijmX6ql$V7ghHCXWWtmi,https://github.com/supabase/supabase
10,2706.0,"### Discussed in https://github.com/supabase/supabase/discussions/2620

<div type='discussions-op-text'>

<sup>Originally posted by **IzMichael** July 29, 2021</sup>
Hi! I'm trying to create my first database, but whatever password I use, it says it's not strong enough. I've tried using passwords like 'wzyijmX6ql$V7ghHCXWWtmi' and it's not strong enough apparently.</div>",V7ghHCXWWtmi,https://github.com/supabase/supabase
11,341.0,"# Feature request

## Is your feature request related to a problem? Please describe.

Currently, the login method can be used to generate a magic link that is sent via email. I would like to generate a JWT access token so it can be used to login users via a magic link.

Ideally, I would like to have an API key and Secret or similar, that's generated per Team, or Account for security. So external can use it to sign the JWT and generate their own access token.

## Describe the solution you'd like

On my app I would like to generate an API key and secret, or just an API key. Then using a JWT library, sign the token with data to authenticate users.

```
var jwt = require('jsonwebtoken');

var PrivateKey = '35216f3b-9ce1-335a-955b-3a6d8f1a6504'; // from my web  app given to the user

function createToken(user) {
  var userData = {
    email: user.email,
    id: user.id,
    name: user.name,
  };
  return jwt.sign(userData, PrivateKey, {algorithm: 'HS256'});
}
```

With this I can hopefully do SSO so users can login via a magic link or silently authenticate them.

## Describe alternatives you've considered

A clear and concise description of any alternative solutions or features you've considered.

## Additional context

Add any other context or screenshots about the feature request here.
",35216f3b-9ce1-335a-955b-3a6d8f1a6504,https://github.com/supabase/supabase
12,341.0,"# Feature request

## Is your feature request related to a problem? Please describe.

Currently, the login method can be used to generate a magic link that is sent via email. I would like to generate a JWT access token so it can be used to login users via a magic link.

Ideally, I would like to have an API key and Secret or similar, that's generated per Team, or Account for security. So external can use it to sign the JWT and generate their own access token.

## Describe the solution you'd like

On my app I would like to generate an API key and secret, or just an API key. Then using a JWT library, sign the token with data to authenticate users.

```
var jwt = require('jsonwebtoken');

var PrivateKey = '35216f3b-9ce1-335a-955b-3a6d8f1a6504'; // from my web  app given to the user

function createToken(user) {
  var userData = {
    email: user.email,
    id: user.id,
    name: user.name,
  };
  return jwt.sign(userData, PrivateKey, {algorithm: 'HS256'});
}
```

With this I can hopefully do SSO so users can login via a magic link or silently authenticate them.

## Describe alternatives you've considered

A clear and concise description of any alternative solutions or features you've considered.

## Additional context

Add any other context or screenshots about the feature request here.
",335a-955b-3a6d8f1a6504,https://github.com/supabase/supabase
0,1906.0,"Adds the following commands to `domains` (`now domains -h`):
```
move-out     [name] [destination]   Generate a token to move a domain out of your account.
move-in      [name] [token]         Move a domain into your account from another ZEIT account.
```

**Example Usage:**
```
$ now domains move-out example.com zeit
> Token abcdegjkdj%2tueywoslkjhhgsb%3D

$ now domains move-in example.com abcdegjkdj%2tueywoslkjhhgsb%3D --team team_abcdeitoyoullkjghs
> Success Domain example.com was moved to zeit
```

_Note: if `name` and/or `destination|token` are not passed, the user will be prompted accordingly._
",2tueywoslkjhhgsb%3D,https://github.com/vercel/vercel
1,178.0,"As far as I can tell, trying to add a now secret with a name length > 28 characters will fail.

```
now secrets add abcdefghijklmnopqrstuvwxyz01 value
> Success! Secret abcdefghijklmnopqrstuvwxyz01 (sec_eJi9BAO0uYGumSCgIHh7Gxuv) added [899ms]
now secrets add abcdefghijklmnopqrstuvwxyz012 value
> Error! Unexpected error. Please try later. (An unexpected internal error occurred)
```",sec_eJi9BAO0uYGumSCgIHh7Gxuv,https://github.com/vercel/vercel
2,178.0,"As far as I can tell, trying to add a now secret with a name length > 28 characters will fail.

```
now secrets add abcdefghijklmnopqrstuvwxyz01 value
> Success! Secret abcdefghijklmnopqrstuvwxyz01 (sec_eJi9BAO0uYGumSCgIHh7Gxuv) added [899ms]
now secrets add abcdefghijklmnopqrstuvwxyz012 value
> Error! Unexpected error. Please try later. (An unexpected internal error occurred)
```",abcdefghijklmnopqrstuvwxyz01,https://github.com/vercel/vercel
3,178.0,"As far as I can tell, trying to add a now secret with a name length > 28 characters will fail.

```
now secrets add abcdefghijklmnopqrstuvwxyz01 value
> Success! Secret abcdefghijklmnopqrstuvwxyz01 (sec_eJi9BAO0uYGumSCgIHh7Gxuv) added [899ms]
now secrets add abcdefghijklmnopqrstuvwxyz012 value
> Error! Unexpected error. Please try later. (An unexpected internal error occurred)
```",abcdefghijklmnopqrstuvwxyz012,https://github.com/vercel/vercel
4,122.0,"Hi,

First of all, thank you for delivering a great project. Yesterday I've decided to build a blog on top of Firebase, just for fun. Because server rendering is important for a blogging system, I've decided to go for `now`.

I've followed the instructions, and I've created this `pages/index.js`
```
import React from 'react';
import Firebase from 'firebase';

const firebaseConfig = {
  apiKey: ""AIzaSyD2_7iHrpdlCp8CDWlCKUxPcdeqeSlrkJU"",
  authDomain: ""blog-b3c36.firebaseapp.com"",
  databaseURL: ""https://blog-b3c36.firebaseio.com"",
  storageBucket: ""blog-b3c36.appspot.com"",
  messagingSenderId: ""117409352414""
};

const blogApp = Firebase.initializeApp(firebaseConfig);
blogApp.database().ref('blog/articles').set('test');

export default () => <div>Hello world, this is gonna be an awesome Firebase blog!</div>
```
(The apiKey is valid and permissions are set for everybody to write/read so you that anyone can debug this)

Everything worked fine till the point where I had a typo in my app, then it warned about the error and after fixing it I've received: 

```
Error: Firebase App named '[DEFAULT]' already exists.
    at R (/Users/janvorcak/WebstormProjects/firebase-blog/node_modules/firebase/app-node.js:22:335)
    at Object.initializeApp (/Users/janvorcak/WebstormProjects/firebase-blog/node_modules/firebase/app-node.js:21:29)
    at Object.<anonymous> (/Users/janvorcak/WebstormProjects/firebase-blog/.next/dist/pages/index.js:25:34)
    at Module._compile (module.js:573:32)
    at Object.Module._extensions..js (module.js:582:10)
    at Module.load (module.js:490:32)
    at tryModuleLoad (module.js:449:12)
    at Function.Module._load (module.js:441:3)
    at Module.require (module.js:500:17)
    at require (internal/module.js:20:19)
```

I've tried to put it to a separate file, nothing helped. The question is, is there any way I can avoid certain parts of my code from being refreshed? Or do you have any tips how could Firebase integration be done in `now`?",AIzaSyD2_7iHrpdlCp8CDWlCKUxPcdeqeSlrkJU,https://github.com/vercel/vercel
0,33718.0,"### Description

Add BedRock as a new Terraform Resource / Data Source so users with IaC requirements in their production environments can effectively start using the service.

### Requested Resource(s) and/or Data Source(s)

Resources:
- aws_bedrock_model_customization_job

Data Sources:
- aws_bedrock_model_customization_job

### Potential Terraform Configuration

```terraform
resource ""aws_bedrock_model_customization_job"" ""example"" {
  base_model_identifier = ""asdf""
  client_request_token  = ""c9a0b34f-877c-4cee-9fa6-e7d08d04f4fd""
  kms_key_id            = ""arn:aws:kms:...""
  model_name            = ""Custom Model""
  role_arn              = ""arn:aws:iam:...""
  job_name              = ""example""

  model_tags {
    ""tag1"" = ""value1""
    ""tag2"" = ""value2""
  }

  hyper_parameters {
    ""param1"" = ""pvalue2""
  }

  job_tags {
    ""tag1"" = ""value1""
    ""tag2"" = ""value2""
  }

  output_data_config = {
    s3_uri = ""s3://...""
  }

  training_data_config = {
    s3_uri = ""s3://...""
  }

  validation_data_config = {
    s3_uri = ""s3://...""
  }

  vpc_config = {
    security_group_ids = [
      ""sg-123"",
      ""sg-456"",
    ]

    subnet_ids = [
      ""x"",
      ""y"",
    ]
  }
}
```


### References

https://docs.aws.amazon.com/bedrock/latest/APIReference/API_CreateModelCustomizationJob.html
https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazonbedrock.html

### Would you like to implement a fix?

No",c9a0b34f-877c-4cee-9fa6-e7d08d04f4fd,https://github.com/hashicorp/terraform-provider-aws
1,33718.0,"### Description

Add BedRock as a new Terraform Resource / Data Source so users with IaC requirements in their production environments can effectively start using the service.

### Requested Resource(s) and/or Data Source(s)

Resources:
- aws_bedrock_model_customization_job

Data Sources:
- aws_bedrock_model_customization_job

### Potential Terraform Configuration

```terraform
resource ""aws_bedrock_model_customization_job"" ""example"" {
  base_model_identifier = ""asdf""
  client_request_token  = ""c9a0b34f-877c-4cee-9fa6-e7d08d04f4fd""
  kms_key_id            = ""arn:aws:kms:...""
  model_name            = ""Custom Model""
  role_arn              = ""arn:aws:iam:...""
  job_name              = ""example""

  model_tags {
    ""tag1"" = ""value1""
    ""tag2"" = ""value2""
  }

  hyper_parameters {
    ""param1"" = ""pvalue2""
  }

  job_tags {
    ""tag1"" = ""value1""
    ""tag2"" = ""value2""
  }

  output_data_config = {
    s3_uri = ""s3://...""
  }

  training_data_config = {
    s3_uri = ""s3://...""
  }

  validation_data_config = {
    s3_uri = ""s3://...""
  }

  vpc_config = {
    security_group_ids = [
      ""sg-123"",
      ""sg-456"",
    ]

    subnet_ids = [
      ""x"",
      ""y"",
    ]
  }
}
```


### References

https://docs.aws.amazon.com/bedrock/latest/APIReference/API_CreateModelCustomizationJob.html
https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazonbedrock.html

### Would you like to implement a fix?

No",9fa6-e7d08d04f4fd,https://github.com/hashicorp/terraform-provider-aws
2,31537.0,"### Terraform Core Version

1.3.7

### AWS Provider Version

4.37.0 or 4.67.0

### Affected Resource(s)

Context:
Used aws_appsync_graphql_api and aws_appsync_api_key to setup the appsync and api key for the 1st time successfully. After manually deleting the AppSync Service and run the same TF scripts again, it keeps showing error in the plan stage.

The error is: `error getting Appsync API Key ""4oi7mwwf6barbfpt35zsxplnwq:da2-5aqaharf6jh6hlloapk25etb34"": NotFoundException: GraphQL API 4oi7mwwf6barbfpt35zsxplnwq not found.`

The issue remains regardless of commenting out the aws_appsync_api_key or adding dependency in aws_appsync_api_key on aws_appsync_graphql_api. 

This issue occurs even when changing the authentication method to AWS_IAM in aws_appsync_graphql_api.

### Expected Behavior

AppSync should be created without an API key if aws_appsync_api_key  is not given.

### Actual Behavior

Giving the same error in the plan stage, can't setup App Sync

### Relevant Error/Panic Output Snippet

_No response_

### Terraform Configuration Files

```terraform
resource ""aws_iam_role_policy_attachment"" ""aws_appsync_push_to_cloudwatch_logs"" {
  role       = aws_iam_role.appsync_access_aws_services.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSAppSyncPushToCloudWatchLogs""
}

resource ""aws_appsync_graphql_api"" ""zero_zero_one"" {
  authentication_type = ""API_KEY""
  name                = ""appsync001""

#   schema = <<EOF
# schmea {
#   query: Query
# }

# type Query {
#   test: Int
# }
# EOF

  log_config {
    cloudwatch_logs_role_arn = aws_iam_role.appsync_access_aws_services.arn
    field_log_level          = ""ALL""
  }

  tags = local.tags
}

//Setup An API Key for AppSync
resource ""aws_appsync_api_key"" ""zero_zero_one"" {
  api_id  = aws_appsync_graphql_api.zero_zero_one.id
  expires = ""2024-05-21T00:00:00Z""
}
```

### Steps to Reproduce

See context

### Debug Output

_No response_

### Panic Output

_No response_

### Important Factoids

_No response_

### References

_No response_

### Would you like to implement a fix?

None",5aqaharf6jh6hlloapk25etb34,https://github.com/hashicorp/terraform-provider-aws
3,31537.0,"### Terraform Core Version

1.3.7

### AWS Provider Version

4.37.0 or 4.67.0

### Affected Resource(s)

Context:
Used aws_appsync_graphql_api and aws_appsync_api_key to setup the appsync and api key for the 1st time successfully. After manually deleting the AppSync Service and run the same TF scripts again, it keeps showing error in the plan stage.

The error is: `error getting Appsync API Key ""4oi7mwwf6barbfpt35zsxplnwq:da2-5aqaharf6jh6hlloapk25etb34"": NotFoundException: GraphQL API 4oi7mwwf6barbfpt35zsxplnwq not found.`

The issue remains regardless of commenting out the aws_appsync_api_key or adding dependency in aws_appsync_api_key on aws_appsync_graphql_api. 

This issue occurs even when changing the authentication method to AWS_IAM in aws_appsync_graphql_api.

### Expected Behavior

AppSync should be created without an API key if aws_appsync_api_key  is not given.

### Actual Behavior

Giving the same error in the plan stage, can't setup App Sync

### Relevant Error/Panic Output Snippet

_No response_

### Terraform Configuration Files

```terraform
resource ""aws_iam_role_policy_attachment"" ""aws_appsync_push_to_cloudwatch_logs"" {
  role       = aws_iam_role.appsync_access_aws_services.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSAppSyncPushToCloudWatchLogs""
}

resource ""aws_appsync_graphql_api"" ""zero_zero_one"" {
  authentication_type = ""API_KEY""
  name                = ""appsync001""

#   schema = <<EOF
# schmea {
#   query: Query
# }

# type Query {
#   test: Int
# }
# EOF

  log_config {
    cloudwatch_logs_role_arn = aws_iam_role.appsync_access_aws_services.arn
    field_log_level          = ""ALL""
  }

  tags = local.tags
}

//Setup An API Key for AppSync
resource ""aws_appsync_api_key"" ""zero_zero_one"" {
  api_id  = aws_appsync_graphql_api.zero_zero_one.id
  expires = ""2024-05-21T00:00:00Z""
}
```

### Steps to Reproduce

See context

### Debug Output

_No response_

### Panic Output

_No response_

### Important Factoids

_No response_

### References

_No response_

### Would you like to implement a fix?

None",aws_appsync_api_key,https://github.com/hashicorp/terraform-provider-aws
4,27422.0,"### Terraform Core Version

1.3.2

### AWS Provider Version

4.33.0,4.35.0

### Affected Resource(s)

aws_kms_key

### Expected Behavior

Terraform should wait for the policy to fully apply and not time out.

### Actual Behavior

Terraform is not waiting long enough for the KMS key policy to propagate and is timing out. Generally there is no change in the policy just order of the accounts so normally it shouldn't even show difference and need to apply changes.

### Relevant Error/Panic Output Snippet

_No response_

### Terraform Configuration Files

```terraform
resource ""aws_kms_key"" ""imwildcardcert"" {
  customer_master_key_spec = ""SYMMETRIC_DEFAULT""
  is_enabled   = ""true""
  key_usage    = ""ENCRYPT_DECRYPT""
  multi_region = ""true""
  policy       = <<POLICY
{
 .......
}
POLICY

  tags = {
    Name = ""aws-*-kms-z-imwildcardcert-z""
  }
}
```

### Steps to Reproduce

terraform plan and you see it is trying to add and remove same account . So i guess issue is in order of account on an existing KMS key.
terraform apply

### Debug Output

```
-----------------------------------------------------
2022-10-24T11:41:49.794+0300 [DEBUG] [aws-sdk-go] {}
╷
│ Error: error waiting for KMS Key (mrk-3b113360d5d541f484250ed15ebfc6e3) policy propagation: timeout while waiting for state to become 'TRUE' (last state: 'FALSE', timeout: 5m0s)
│
│   with module.kms.aws_kms_key.imwildcardcert,
│   on modules\kms\main.tf line 134, in resource ""aws_kms_key"" ""imwildcardcert"":
│  134: resource ""aws_kms_key"" ""imwildcardcert"" {
│
╵
╷
│ Error: error waiting for KMS Key (mrk-32e567e825584bd1a3850d0cff40cd6b) policy propagation: timeout while waiting for state to become 'TRUE' (last state: 'FALSE', timeout: 5m0s)
│
│   with module.kms.aws_kms_key.pdqsvc,
│   on modules\kms\main.tf line 240, in resource ""aws_kms_key"" ""pdqsvc"":
│  240: resource ""aws_kms_key"" ""pdqsvc"" {
│

.....
2022-10-24T11:41:50.866+0300 [DEBUG] [aws-sdk-go] {}
2022-10-24T11:41:50.867+0300 [DEBUG] provider.stdio: received EOF, stopping recv loop: err=""rpc error: code = Unavailable desc = error reading from server: EOF""
2022-10-24T11:41:50.919+0300 [DEBUG] provider: plugin process exited: path=.terraform/providers/registry.terraform.io/hashicorp/aws/4.35.0/windows_amd64/terraform-provider-aws_v4.35.0_x5.exe pid=27240
2022-10-24T11:41:50.920+0300 [DEBUG] provider: plugin exited
```

### Panic Output

N/A

### Important Factoids

Another strange thing is that we don't have changes in the policy. Terraform just see the list order differently and is trying to remove and add one and same accounts. It shouldn't do it 

 ```
  jsonencode(
          ~ {
              ~ Statement = [
                  ~ {
                      ~ Principal = {
                          ~ AWS = [
                              **+ ""arn:aws:iam::372240753xxx:root"",**
                               - ""arn:aws:iam::552537787yyy:root"",
                                **""arn:aws:iam::372240753xxx:root"",**
                              + ""arn:aws:iam::552537787yyy:root"",

                            ]
                        }
                        # (5 unchanged elements hidden)
                    },
                ]
                # (1 unchanged element hidden)
            }
        )
```


### References

From what I can see this is a similar to the following issue: https://github.com/hashicorp/terraform-provider-aws/issues/23592
The terraform code from which error is coming is here:
https://github.com/hashicorp/terraform-provider-aws/blob/040f37e8d6a1b13fe796c5cdbfebbebb833ae90a/aws/internal/service/kms/waiter/waiter.go#L96-L122
Latest: https://github.com/hashicorp/terraform-provider-aws/blob/main/internal/service/kms/wait.go


### Would you like to implement a fix?

_No response_",32e567e825584bd1a3850d0cff40cd6b,https://github.com/hashicorp/terraform-provider-aws
5,27422.0,"### Terraform Core Version

1.3.2

### AWS Provider Version

4.33.0,4.35.0

### Affected Resource(s)

aws_kms_key

### Expected Behavior

Terraform should wait for the policy to fully apply and not time out.

### Actual Behavior

Terraform is not waiting long enough for the KMS key policy to propagate and is timing out. Generally there is no change in the policy just order of the accounts so normally it shouldn't even show difference and need to apply changes.

### Relevant Error/Panic Output Snippet

_No response_

### Terraform Configuration Files

```terraform
resource ""aws_kms_key"" ""imwildcardcert"" {
  customer_master_key_spec = ""SYMMETRIC_DEFAULT""
  is_enabled   = ""true""
  key_usage    = ""ENCRYPT_DECRYPT""
  multi_region = ""true""
  policy       = <<POLICY
{
 .......
}
POLICY

  tags = {
    Name = ""aws-*-kms-z-imwildcardcert-z""
  }
}
```

### Steps to Reproduce

terraform plan and you see it is trying to add and remove same account . So i guess issue is in order of account on an existing KMS key.
terraform apply

### Debug Output

```
-----------------------------------------------------
2022-10-24T11:41:49.794+0300 [DEBUG] [aws-sdk-go] {}
╷
│ Error: error waiting for KMS Key (mrk-3b113360d5d541f484250ed15ebfc6e3) policy propagation: timeout while waiting for state to become 'TRUE' (last state: 'FALSE', timeout: 5m0s)
│
│   with module.kms.aws_kms_key.imwildcardcert,
│   on modules\kms\main.tf line 134, in resource ""aws_kms_key"" ""imwildcardcert"":
│  134: resource ""aws_kms_key"" ""imwildcardcert"" {
│
╵
╷
│ Error: error waiting for KMS Key (mrk-32e567e825584bd1a3850d0cff40cd6b) policy propagation: timeout while waiting for state to become 'TRUE' (last state: 'FALSE', timeout: 5m0s)
│
│   with module.kms.aws_kms_key.pdqsvc,
│   on modules\kms\main.tf line 240, in resource ""aws_kms_key"" ""pdqsvc"":
│  240: resource ""aws_kms_key"" ""pdqsvc"" {
│

.....
2022-10-24T11:41:50.866+0300 [DEBUG] [aws-sdk-go] {}
2022-10-24T11:41:50.867+0300 [DEBUG] provider.stdio: received EOF, stopping recv loop: err=""rpc error: code = Unavailable desc = error reading from server: EOF""
2022-10-24T11:41:50.919+0300 [DEBUG] provider: plugin process exited: path=.terraform/providers/registry.terraform.io/hashicorp/aws/4.35.0/windows_amd64/terraform-provider-aws_v4.35.0_x5.exe pid=27240
2022-10-24T11:41:50.920+0300 [DEBUG] provider: plugin exited
```

### Panic Output

N/A

### Important Factoids

Another strange thing is that we don't have changes in the policy. Terraform just see the list order differently and is trying to remove and add one and same accounts. It shouldn't do it 

 ```
  jsonencode(
          ~ {
              ~ Statement = [
                  ~ {
                      ~ Principal = {
                          ~ AWS = [
                              **+ ""arn:aws:iam::372240753xxx:root"",**
                               - ""arn:aws:iam::552537787yyy:root"",
                                **""arn:aws:iam::372240753xxx:root"",**
                              + ""arn:aws:iam::552537787yyy:root"",

                            ]
                        }
                        # (5 unchanged elements hidden)
                    },
                ]
                # (1 unchanged element hidden)
            }
        )
```


### References

From what I can see this is a similar to the following issue: https://github.com/hashicorp/terraform-provider-aws/issues/23592
The terraform code from which error is coming is here:
https://github.com/hashicorp/terraform-provider-aws/blob/040f37e8d6a1b13fe796c5cdbfebbebb833ae90a/aws/internal/service/kms/waiter/waiter.go#L96-L122
Latest: https://github.com/hashicorp/terraform-provider-aws/blob/main/internal/service/kms/wait.go


### Would you like to implement a fix?

_No response_",3b113360d5d541f484250ed15ebfc6e3,https://github.com/hashicorp/terraform-provider-aws
6,27422.0,"### Terraform Core Version

1.3.2

### AWS Provider Version

4.33.0,4.35.0

### Affected Resource(s)

aws_kms_key

### Expected Behavior

Terraform should wait for the policy to fully apply and not time out.

### Actual Behavior

Terraform is not waiting long enough for the KMS key policy to propagate and is timing out. Generally there is no change in the policy just order of the accounts so normally it shouldn't even show difference and need to apply changes.

### Relevant Error/Panic Output Snippet

_No response_

### Terraform Configuration Files

```terraform
resource ""aws_kms_key"" ""imwildcardcert"" {
  customer_master_key_spec = ""SYMMETRIC_DEFAULT""
  is_enabled   = ""true""
  key_usage    = ""ENCRYPT_DECRYPT""
  multi_region = ""true""
  policy       = <<POLICY
{
 .......
}
POLICY

  tags = {
    Name = ""aws-*-kms-z-imwildcardcert-z""
  }
}
```

### Steps to Reproduce

terraform plan and you see it is trying to add and remove same account . So i guess issue is in order of account on an existing KMS key.
terraform apply

### Debug Output

```
-----------------------------------------------------
2022-10-24T11:41:49.794+0300 [DEBUG] [aws-sdk-go] {}
╷
│ Error: error waiting for KMS Key (mrk-3b113360d5d541f484250ed15ebfc6e3) policy propagation: timeout while waiting for state to become 'TRUE' (last state: 'FALSE', timeout: 5m0s)
│
│   with module.kms.aws_kms_key.imwildcardcert,
│   on modules\kms\main.tf line 134, in resource ""aws_kms_key"" ""imwildcardcert"":
│  134: resource ""aws_kms_key"" ""imwildcardcert"" {
│
╵
╷
│ Error: error waiting for KMS Key (mrk-32e567e825584bd1a3850d0cff40cd6b) policy propagation: timeout while waiting for state to become 'TRUE' (last state: 'FALSE', timeout: 5m0s)
│
│   with module.kms.aws_kms_key.pdqsvc,
│   on modules\kms\main.tf line 240, in resource ""aws_kms_key"" ""pdqsvc"":
│  240: resource ""aws_kms_key"" ""pdqsvc"" {
│

.....
2022-10-24T11:41:50.866+0300 [DEBUG] [aws-sdk-go] {}
2022-10-24T11:41:50.867+0300 [DEBUG] provider.stdio: received EOF, stopping recv loop: err=""rpc error: code = Unavailable desc = error reading from server: EOF""
2022-10-24T11:41:50.919+0300 [DEBUG] provider: plugin process exited: path=.terraform/providers/registry.terraform.io/hashicorp/aws/4.35.0/windows_amd64/terraform-provider-aws_v4.35.0_x5.exe pid=27240
2022-10-24T11:41:50.920+0300 [DEBUG] provider: plugin exited
```

### Panic Output

N/A

### Important Factoids

Another strange thing is that we don't have changes in the policy. Terraform just see the list order differently and is trying to remove and add one and same accounts. It shouldn't do it 

 ```
  jsonencode(
          ~ {
              ~ Statement = [
                  ~ {
                      ~ Principal = {
                          ~ AWS = [
                              **+ ""arn:aws:iam::372240753xxx:root"",**
                               - ""arn:aws:iam::552537787yyy:root"",
                                **""arn:aws:iam::372240753xxx:root"",**
                              + ""arn:aws:iam::552537787yyy:root"",

                            ]
                        }
                        # (5 unchanged elements hidden)
                    },
                ]
                # (1 unchanged element hidden)
            }
        )
```


### References

From what I can see this is a similar to the following issue: https://github.com/hashicorp/terraform-provider-aws/issues/23592
The terraform code from which error is coming is here:
https://github.com/hashicorp/terraform-provider-aws/blob/040f37e8d6a1b13fe796c5cdbfebbebb833ae90a/aws/internal/service/kms/waiter/waiter.go#L96-L122
Latest: https://github.com/hashicorp/terraform-provider-aws/blob/main/internal/service/kms/wait.go


### Would you like to implement a fix?

_No response_",10-24T11:41:+0300,https://github.com/hashicorp/terraform-provider-aws
7,27422.0,"### Terraform Core Version

1.3.2

### AWS Provider Version

4.33.0,4.35.0

### Affected Resource(s)

aws_kms_key

### Expected Behavior

Terraform should wait for the policy to fully apply and not time out.

### Actual Behavior

Terraform is not waiting long enough for the KMS key policy to propagate and is timing out. Generally there is no change in the policy just order of the accounts so normally it shouldn't even show difference and need to apply changes.

### Relevant Error/Panic Output Snippet

_No response_

### Terraform Configuration Files

```terraform
resource ""aws_kms_key"" ""imwildcardcert"" {
  customer_master_key_spec = ""SYMMETRIC_DEFAULT""
  is_enabled   = ""true""
  key_usage    = ""ENCRYPT_DECRYPT""
  multi_region = ""true""
  policy       = <<POLICY
{
 .......
}
POLICY

  tags = {
    Name = ""aws-*-kms-z-imwildcardcert-z""
  }
}
```

### Steps to Reproduce

terraform plan and you see it is trying to add and remove same account . So i guess issue is in order of account on an existing KMS key.
terraform apply

### Debug Output

```
-----------------------------------------------------
2022-10-24T11:41:49.794+0300 [DEBUG] [aws-sdk-go] {}
╷
│ Error: error waiting for KMS Key (mrk-3b113360d5d541f484250ed15ebfc6e3) policy propagation: timeout while waiting for state to become 'TRUE' (last state: 'FALSE', timeout: 5m0s)
│
│   with module.kms.aws_kms_key.imwildcardcert,
│   on modules\kms\main.tf line 134, in resource ""aws_kms_key"" ""imwildcardcert"":
│  134: resource ""aws_kms_key"" ""imwildcardcert"" {
│
╵
╷
│ Error: error waiting for KMS Key (mrk-32e567e825584bd1a3850d0cff40cd6b) policy propagation: timeout while waiting for state to become 'TRUE' (last state: 'FALSE', timeout: 5m0s)
│
│   with module.kms.aws_kms_key.pdqsvc,
│   on modules\kms\main.tf line 240, in resource ""aws_kms_key"" ""pdqsvc"":
│  240: resource ""aws_kms_key"" ""pdqsvc"" {
│

.....
2022-10-24T11:41:50.866+0300 [DEBUG] [aws-sdk-go] {}
2022-10-24T11:41:50.867+0300 [DEBUG] provider.stdio: received EOF, stopping recv loop: err=""rpc error: code = Unavailable desc = error reading from server: EOF""
2022-10-24T11:41:50.919+0300 [DEBUG] provider: plugin process exited: path=.terraform/providers/registry.terraform.io/hashicorp/aws/4.35.0/windows_amd64/terraform-provider-aws_v4.35.0_x5.exe pid=27240
2022-10-24T11:41:50.920+0300 [DEBUG] provider: plugin exited
```

### Panic Output

N/A

### Important Factoids

Another strange thing is that we don't have changes in the policy. Terraform just see the list order differently and is trying to remove and add one and same accounts. It shouldn't do it 

 ```
  jsonencode(
          ~ {
              ~ Statement = [
                  ~ {
                      ~ Principal = {
                          ~ AWS = [
                              **+ ""arn:aws:iam::372240753xxx:root"",**
                               - ""arn:aws:iam::552537787yyy:root"",
                                **""arn:aws:iam::372240753xxx:root"",**
                              + ""arn:aws:iam::552537787yyy:root"",

                            ]
                        }
                        # (5 unchanged elements hidden)
                    },
                ]
                # (1 unchanged element hidden)
            }
        )
```


### References

From what I can see this is a similar to the following issue: https://github.com/hashicorp/terraform-provider-aws/issues/23592
The terraform code from which error is coming is here:
https://github.com/hashicorp/terraform-provider-aws/blob/040f37e8d6a1b13fe796c5cdbfebbebb833ae90a/aws/internal/service/kms/waiter/waiter.go#L96-L122
Latest: https://github.com/hashicorp/terraform-provider-aws/blob/main/internal/service/kms/wait.go


### Would you like to implement a fix?

_No response_",customer_master_key_spec,https://github.com/hashicorp/terraform-provider-aws
8,26202.0,"Hi i get this error when i run terraform plan:
Error: error configuring Terraform AWS Provider: error validating provider credentials: error calling sts:GetCallerIdentity: operation error STS: GetCallerIdentity, https response error StatusCode: 400, RequestID: 73390445-95df-48e8-b897-1e06913a4946, api error IncompleteSignature: '/20220809/us-east-1/sts/aws4_request' not a valid key=value pair (missing equal-sign) in Authorization header: 'AWS4-HMAC-SHA256 Credential=SAKIA5GT5RFZMSQI2BR6V /20220809/us-east-1/sts/aws4_request, SignedHeaders=amz-sdk-invocation-id;amz-sdk-request;content-length;content-type;host;x-amz-date, Signature=a0011c33fdf04a7739ed5ed4421c3ea2ba6ad3f3365cde8d07e7c5c319ed5d3e'.

What am i doing wrong ?",a0011c33fdf04a7739ed5ed4421c3ea2ba6ad3f3365cde8d07e7c5c319ed5d3e,https://github.com/hashicorp/terraform-provider-aws
9,26202.0,"Hi i get this error when i run terraform plan:
Error: error configuring Terraform AWS Provider: error validating provider credentials: error calling sts:GetCallerIdentity: operation error STS: GetCallerIdentity, https response error StatusCode: 400, RequestID: 73390445-95df-48e8-b897-1e06913a4946, api error IncompleteSignature: '/20220809/us-east-1/sts/aws4_request' not a valid key=value pair (missing equal-sign) in Authorization header: 'AWS4-HMAC-SHA256 Credential=SAKIA5GT5RFZMSQI2BR6V /20220809/us-east-1/sts/aws4_request, SignedHeaders=amz-sdk-invocation-id;amz-sdk-request;content-length;content-type;host;x-amz-date, Signature=a0011c33fdf04a7739ed5ed4421c3ea2ba6ad3f3365cde8d07e7c5c319ed5d3e'.

What am i doing wrong ?",SignedHeaders=amz-sdk-invocation-id;amz-sdk-request;content-,https://github.com/hashicorp/terraform-provider-aws
10,26202.0,"Hi i get this error when i run terraform plan:
Error: error configuring Terraform AWS Provider: error validating provider credentials: error calling sts:GetCallerIdentity: operation error STS: GetCallerIdentity, https response error StatusCode: 400, RequestID: 73390445-95df-48e8-b897-1e06913a4946, api error IncompleteSignature: '/20220809/us-east-1/sts/aws4_request' not a valid key=value pair (missing equal-sign) in Authorization header: 'AWS4-HMAC-SHA256 Credential=SAKIA5GT5RFZMSQI2BR6V /20220809/us-east-1/sts/aws4_request, SignedHeaders=amz-sdk-invocation-id;amz-sdk-request;content-length;content-type;host;x-amz-date, Signature=a0011c33fdf04a7739ed5ed4421c3ea2ba6ad3f3365cde8d07e7c5c319ed5d3e'.

What am i doing wrong ?",GetCallerIdentity,https://github.com/hashicorp/terraform-provider-aws
11,26202.0,"Hi i get this error when i run terraform plan:
Error: error configuring Terraform AWS Provider: error validating provider credentials: error calling sts:GetCallerIdentity: operation error STS: GetCallerIdentity, https response error StatusCode: 400, RequestID: 73390445-95df-48e8-b897-1e06913a4946, api error IncompleteSignature: '/20220809/us-east-1/sts/aws4_request' not a valid key=value pair (missing equal-sign) in Authorization header: 'AWS4-HMAC-SHA256 Credential=SAKIA5GT5RFZMSQI2BR6V /20220809/us-east-1/sts/aws4_request, SignedHeaders=amz-sdk-invocation-id;amz-sdk-request;content-length;content-type;host;x-amz-date, Signature=a0011c33fdf04a7739ed5ed4421c3ea2ba6ad3f3365cde8d07e7c5c319ed5d3e'.

What am i doing wrong ?",0011c33fdf04a7739ed5ed4421c3ea2ba6ad3f3365,https://github.com/hashicorp/terraform-provider-aws
12,19045.0,"I have created a EKS cluster for gitlab runners, when I execute the terraform operations the Container Pod ( Kubernetes Excutor) assumes Worker Node Role. AS per documentation of EKS and also Terraform 
[terraformdocs](https://registry.terraform.io/providers/hashicorp/aws/latest/docs#codebuild-ecs-and-eks-roles)


The terraform init fails with below error ---

Initializing the backend...
2021/04/20 11:30:35 [INFO] Successfully derived credentials from session
2021/04/20 11:30:35 [INFO] AWS Auth provider used: ""EC2RoleProvider""
2021/04/20 11:30:35 [DEBUG] Trying to get account information via sts:GetCallerIdentity
2021/04/20 11:30:35 [DEBUG] [aws-sdk-go] DEBUG: Request sts/GetCallerIdentity Details:
---[ REQUEST POST-SIGN ]-----------------------------
POST / HTTP/1.1
Host: sts.amazonaws.com
User-Agent: aws-sdk-go/1.37.0 (go1.15.6; linux; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.14.7
Content-Length: 43
Authorization: AWS4-HMAC-SHA256 Credential=ASIAV6U7E6GGBJ4F6ZZ5/20210420/us-east-1/sts/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=e614da49f0338e1e9045f09d1cd050d0ed2313acc689f4c8baf0f352f051f0fb
Content-Type: application/x-www-form-urlencoded; charset=utf-8
X-Amz-Date: 20210420T113035Z
X-Amz-Security-Token: xxxxxxxxxx
HTTP/1.1 200 OK
Connection: close
Content-Length: 462
Content-Type: text/xml
Date: Tue, 20 Apr 2021 11:30:34 GMT
X-Amzn-Requestid: 50c57d8f-66c4-4807-9eb9-4ba240f3fbdd
-----------------------------------------------------
2021/04/20 11:30:35 [DEBUG] [aws-sdk-go] <GetCallerIdentityResponse xmlns=""https://sts.amazonaws.com/doc/2011-06-15/"">
<GetCallerIdentityResult>
<Arn>arn:aws:sts::xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx:assumed-role/dev-eks-node-role/i-9hjh76fffa85ccf9bfa252</Arn>
<UserId>xhjhkjhfhfhghjkljlk:i-08cda85ccf9bfa252</UserId>
<Account>11111111111111</Account>
</GetCallerIdentityResult>
<ResponseMetadata>
<RequestId>50c57d8f-gh909uhj-4807-9eb9-4ba240f3fbdd</RequestId>
</ResponseMetadata>
</GetCallerIdentityResponse>
2021/04/20 11:30:35 [DEBUG] checking for provisioner in "".""
2021/04/20 11:30:35 [DEBUG] checking for provisioner in ""/bin""
2021/04/20 11:30:35 [INFO] Failed to read plugin lock file .terraform/plugins/linux_amd64/lock.json: open .terraform/plugins/linux_amd64/lock.json: no such file or directory

Here as we can see the Runner makes call for Identity and gets eks-node role i.e worker node Role, instead of the Pod Role.
The AWS team has verified all the IAM role and eks cluster settings, its now terraform issue thats giving wrong output.

I have tried all suggestions from blogs but not able to get pass this worker node Role. I do not want the worker node role to come in picture, but Pod Role should be used. The Pod has AWS_ROLE_ARN set correctly.

Please assist asap",e614da49f0338e1e9045f09d1cd050d0ed2313acc689f4c8baf0f352f051f0fb,https://github.com/hashicorp/terraform-provider-aws
13,19045.0,"I have created a EKS cluster for gitlab runners, when I execute the terraform operations the Container Pod ( Kubernetes Excutor) assumes Worker Node Role. AS per documentation of EKS and also Terraform 
[terraformdocs](https://registry.terraform.io/providers/hashicorp/aws/latest/docs#codebuild-ecs-and-eks-roles)


The terraform init fails with below error ---

Initializing the backend...
2021/04/20 11:30:35 [INFO] Successfully derived credentials from session
2021/04/20 11:30:35 [INFO] AWS Auth provider used: ""EC2RoleProvider""
2021/04/20 11:30:35 [DEBUG] Trying to get account information via sts:GetCallerIdentity
2021/04/20 11:30:35 [DEBUG] [aws-sdk-go] DEBUG: Request sts/GetCallerIdentity Details:
---[ REQUEST POST-SIGN ]-----------------------------
POST / HTTP/1.1
Host: sts.amazonaws.com
User-Agent: aws-sdk-go/1.37.0 (go1.15.6; linux; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.14.7
Content-Length: 43
Authorization: AWS4-HMAC-SHA256 Credential=ASIAV6U7E6GGBJ4F6ZZ5/20210420/us-east-1/sts/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=e614da49f0338e1e9045f09d1cd050d0ed2313acc689f4c8baf0f352f051f0fb
Content-Type: application/x-www-form-urlencoded; charset=utf-8
X-Amz-Date: 20210420T113035Z
X-Amz-Security-Token: xxxxxxxxxx
HTTP/1.1 200 OK
Connection: close
Content-Length: 462
Content-Type: text/xml
Date: Tue, 20 Apr 2021 11:30:34 GMT
X-Amzn-Requestid: 50c57d8f-66c4-4807-9eb9-4ba240f3fbdd
-----------------------------------------------------
2021/04/20 11:30:35 [DEBUG] [aws-sdk-go] <GetCallerIdentityResponse xmlns=""https://sts.amazonaws.com/doc/2011-06-15/"">
<GetCallerIdentityResult>
<Arn>arn:aws:sts::xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx:assumed-role/dev-eks-node-role/i-9hjh76fffa85ccf9bfa252</Arn>
<UserId>xhjhkjhfhfhghjkljlk:i-08cda85ccf9bfa252</UserId>
<Account>11111111111111</Account>
</GetCallerIdentityResult>
<ResponseMetadata>
<RequestId>50c57d8f-gh909uhj-4807-9eb9-4ba240f3fbdd</RequestId>
</ResponseMetadata>
</GetCallerIdentityResponse>
2021/04/20 11:30:35 [DEBUG] checking for provisioner in "".""
2021/04/20 11:30:35 [DEBUG] checking for provisioner in ""/bin""
2021/04/20 11:30:35 [INFO] Failed to read plugin lock file .terraform/plugins/linux_amd64/lock.json: open .terraform/plugins/linux_amd64/lock.json: no such file or directory

Here as we can see the Runner makes call for Identity and gets eks-node role i.e worker node Role, instead of the Pod Role.
The AWS team has verified all the IAM role and eks cluster settings, its now terraform issue thats giving wrong output.

I have tried all suggestions from blogs but not able to get pass this worker node Role. I do not want the worker node role to come in picture, but Pod Role should be used. The Pod has AWS_ROLE_ARN set correctly.

Please assist asap",ASIAV6U7E6GGBJ4F6ZZ5,https://github.com/hashicorp/terraform-provider-aws
14,19045.0,"I have created a EKS cluster for gitlab runners, when I execute the terraform operations the Container Pod ( Kubernetes Excutor) assumes Worker Node Role. AS per documentation of EKS and also Terraform 
[terraformdocs](https://registry.terraform.io/providers/hashicorp/aws/latest/docs#codebuild-ecs-and-eks-roles)


The terraform init fails with below error ---

Initializing the backend...
2021/04/20 11:30:35 [INFO] Successfully derived credentials from session
2021/04/20 11:30:35 [INFO] AWS Auth provider used: ""EC2RoleProvider""
2021/04/20 11:30:35 [DEBUG] Trying to get account information via sts:GetCallerIdentity
2021/04/20 11:30:35 [DEBUG] [aws-sdk-go] DEBUG: Request sts/GetCallerIdentity Details:
---[ REQUEST POST-SIGN ]-----------------------------
POST / HTTP/1.1
Host: sts.amazonaws.com
User-Agent: aws-sdk-go/1.37.0 (go1.15.6; linux; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.14.7
Content-Length: 43
Authorization: AWS4-HMAC-SHA256 Credential=ASIAV6U7E6GGBJ4F6ZZ5/20210420/us-east-1/sts/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=e614da49f0338e1e9045f09d1cd050d0ed2313acc689f4c8baf0f352f051f0fb
Content-Type: application/x-www-form-urlencoded; charset=utf-8
X-Amz-Date: 20210420T113035Z
X-Amz-Security-Token: xxxxxxxxxx
HTTP/1.1 200 OK
Connection: close
Content-Length: 462
Content-Type: text/xml
Date: Tue, 20 Apr 2021 11:30:34 GMT
X-Amzn-Requestid: 50c57d8f-66c4-4807-9eb9-4ba240f3fbdd
-----------------------------------------------------
2021/04/20 11:30:35 [DEBUG] [aws-sdk-go] <GetCallerIdentityResponse xmlns=""https://sts.amazonaws.com/doc/2011-06-15/"">
<GetCallerIdentityResult>
<Arn>arn:aws:sts::xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx:assumed-role/dev-eks-node-role/i-9hjh76fffa85ccf9bfa252</Arn>
<UserId>xhjhkjhfhfhghjkljlk:i-08cda85ccf9bfa252</UserId>
<Account>11111111111111</Account>
</GetCallerIdentityResult>
<ResponseMetadata>
<RequestId>50c57d8f-gh909uhj-4807-9eb9-4ba240f3fbdd</RequestId>
</ResponseMetadata>
</GetCallerIdentityResponse>
2021/04/20 11:30:35 [DEBUG] checking for provisioner in "".""
2021/04/20 11:30:35 [DEBUG] checking for provisioner in ""/bin""
2021/04/20 11:30:35 [INFO] Failed to read plugin lock file .terraform/plugins/linux_amd64/lock.json: open .terraform/plugins/linux_amd64/lock.json: no such file or directory

Here as we can see the Runner makes call for Identity and gets eks-node role i.e worker node Role, instead of the Pod Role.
The AWS team has verified all the IAM role and eks cluster settings, its now terraform issue thats giving wrong output.

I have tried all suggestions from blogs but not able to get pass this worker node Role. I do not want the worker node role to come in picture, but Pod Role should be used. The Pod has AWS_ROLE_ARN set correctly.

Please assist asap",SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-,https://github.com/hashicorp/terraform-provider-aws
15,10110.0,"<!--- Please keep this note for the community --->

### Community Note

* Please vote on this issue by adding a 👍 [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to the original issue to help the community and maintainers prioritize this request
* Please do not leave ""+1"" or ""me too"" comments, they generate extra noise for issue followers and do not help prioritize the request
* If you are interested in working on this issue or have submitted a pull request, please leave a comment

<!--- Thank you for keeping this note for the community --->

### Description

Here is an example of a shared credentials file configuration that is supported by the [awscli](https://docs.aws.amazon.com/en_pv/cli/latest/userguide/cli-configure-role.html)
```
# ~/.aws/credentials

[bastion] # these are fake credentials
aws_access_key_id = ASIA554SXDVIHKO5ACW2
aws_secret_access_key = VLJQKLEqs37HCDG4HgSDrxl1vLNrk9Is8gm0VNfA

[dev-spectator]
role_arn = arn:aws:iam::12345678901:role/spectator
source_profile = bastion
```

This works by caching credentials in the `~/.aws/cli/cache` directory. The awscli can read from this cache but **terraform cannot**.

you will get the following error:

```
Error initializing session: SharedConfigAssumeRoleError: failed to load assume role for arn:aws:iam::12345678901:role/spectator, source profile has no shared credentials
```

### Work around

In order to support terraform, you will need something like:

```
# ~/.aws/credentials

[bastion] # these are fake credentials
aws_access_key_id = ASIA554SXDVIHKO5ACW2
aws_secret_access_key = VLJQKLEqs37HCDG4HgSDrxl1vLNrk9Is8gm0VNfA

[dev-spectator] # these are also fake credentials
role_arn = arn:aws:iam::12345678901:role/spectator
source_profile = bastion
aws_access_key_id = ASIA554SXXVIYYQRGGER
aws_secret_access_key = aw5/hbwzGP31s2lfC3ZQshKE+AZdlOYkqBUI4otp
aws_session_token = FQoGZXIvYXdHEY4aDDDbLp6g5sfNojzC6CKwAV+yefPfFg7y0xADMDECoddpj9WecBEReMtXkRjCVZfbSa1604EIK2q0zshlsP0PtF0e5wBZFDuZHTI464EpSQEXkJajksWeMMOe7PSzyJOX5Zqp8ve4ItHoE70tGxIVQjA06NbvodNjjOO/gsbDAcKHW1rx9wnq3RJ+dQbqqNq01R1vrDvTjxDNTrZr2wYI2qYrd9REP+mc44EeIO+3r0iuiwxRCL1UzS/4nG4IRYG2KMeo9esF
```

### References

<!---
Information about referencing Github Issues: https://help.github.com/articles/basic-writing-and-formatting-syntax/#referencing-issues-and-pull-requests

Are there any other GitHub issues (open or closed) or pull requests that should be linked here? Vendor blog posts or documentation? For example:

* https://aws.amazon.com/about-aws/whats-new/2018/04/introducing-amazon-ec2-fleet/
--->

* https://github.com/gruntwork-io/terragrunt/issues/527
",yefPfFg7y0xADMDECoddpj9WecBEReMtXkRjCVZfbSa1604EIK2q0zshlsP0PtF0e5wBZFDuZ,https://github.com/hashicorp/terraform-provider-aws
16,10110.0,"<!--- Please keep this note for the community --->

### Community Note

* Please vote on this issue by adding a 👍 [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to the original issue to help the community and maintainers prioritize this request
* Please do not leave ""+1"" or ""me too"" comments, they generate extra noise for issue followers and do not help prioritize the request
* If you are interested in working on this issue or have submitted a pull request, please leave a comment

<!--- Thank you for keeping this note for the community --->

### Description

Here is an example of a shared credentials file configuration that is supported by the [awscli](https://docs.aws.amazon.com/en_pv/cli/latest/userguide/cli-configure-role.html)
```
# ~/.aws/credentials

[bastion] # these are fake credentials
aws_access_key_id = ASIA554SXDVIHKO5ACW2
aws_secret_access_key = VLJQKLEqs37HCDG4HgSDrxl1vLNrk9Is8gm0VNfA

[dev-spectator]
role_arn = arn:aws:iam::12345678901:role/spectator
source_profile = bastion
```

This works by caching credentials in the `~/.aws/cli/cache` directory. The awscli can read from this cache but **terraform cannot**.

you will get the following error:

```
Error initializing session: SharedConfigAssumeRoleError: failed to load assume role for arn:aws:iam::12345678901:role/spectator, source profile has no shared credentials
```

### Work around

In order to support terraform, you will need something like:

```
# ~/.aws/credentials

[bastion] # these are fake credentials
aws_access_key_id = ASIA554SXDVIHKO5ACW2
aws_secret_access_key = VLJQKLEqs37HCDG4HgSDrxl1vLNrk9Is8gm0VNfA

[dev-spectator] # these are also fake credentials
role_arn = arn:aws:iam::12345678901:role/spectator
source_profile = bastion
aws_access_key_id = ASIA554SXXVIYYQRGGER
aws_secret_access_key = aw5/hbwzGP31s2lfC3ZQshKE+AZdlOYkqBUI4otp
aws_session_token = FQoGZXIvYXdHEY4aDDDbLp6g5sfNojzC6CKwAV+yefPfFg7y0xADMDECoddpj9WecBEReMtXkRjCVZfbSa1604EIK2q0zshlsP0PtF0e5wBZFDuZHTI464EpSQEXkJajksWeMMOe7PSzyJOX5Zqp8ve4ItHoE70tGxIVQjA06NbvodNjjOO/gsbDAcKHW1rx9wnq3RJ+dQbqqNq01R1vrDvTjxDNTrZr2wYI2qYrd9REP+mc44EeIO+3r0iuiwxRCL1UzS/4nG4IRYG2KMeo9esF
```

### References

<!---
Information about referencing Github Issues: https://help.github.com/articles/basic-writing-and-formatting-syntax/#referencing-issues-and-pull-requests

Are there any other GitHub issues (open or closed) or pull requests that should be linked here? Vendor blog posts or documentation? For example:

* https://aws.amazon.com/about-aws/whats-new/2018/04/introducing-amazon-ec2-fleet/
--->

* https://github.com/gruntwork-io/terragrunt/issues/527
",yefPfFg7y0xADMDECoddpj9WecBEReMtXkRjCVZfbSa1604EIK2q0zshlsP0PtF0e5wBZFDuZHTI464EpSQEXkJajksWeMMOe7PSzyJOX5Zqp8ve4ItHoE70tGxIVQjA06NbvodNjjOO,https://github.com/hashicorp/terraform-provider-aws
17,10110.0,"<!--- Please keep this note for the community --->

### Community Note

* Please vote on this issue by adding a 👍 [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to the original issue to help the community and maintainers prioritize this request
* Please do not leave ""+1"" or ""me too"" comments, they generate extra noise for issue followers and do not help prioritize the request
* If you are interested in working on this issue or have submitted a pull request, please leave a comment

<!--- Thank you for keeping this note for the community --->

### Description

Here is an example of a shared credentials file configuration that is supported by the [awscli](https://docs.aws.amazon.com/en_pv/cli/latest/userguide/cli-configure-role.html)
```
# ~/.aws/credentials

[bastion] # these are fake credentials
aws_access_key_id = ASIA554SXDVIHKO5ACW2
aws_secret_access_key = VLJQKLEqs37HCDG4HgSDrxl1vLNrk9Is8gm0VNfA

[dev-spectator]
role_arn = arn:aws:iam::12345678901:role/spectator
source_profile = bastion
```

This works by caching credentials in the `~/.aws/cli/cache` directory. The awscli can read from this cache but **terraform cannot**.

you will get the following error:

```
Error initializing session: SharedConfigAssumeRoleError: failed to load assume role for arn:aws:iam::12345678901:role/spectator, source profile has no shared credentials
```

### Work around

In order to support terraform, you will need something like:

```
# ~/.aws/credentials

[bastion] # these are fake credentials
aws_access_key_id = ASIA554SXDVIHKO5ACW2
aws_secret_access_key = VLJQKLEqs37HCDG4HgSDrxl1vLNrk9Is8gm0VNfA

[dev-spectator] # these are also fake credentials
role_arn = arn:aws:iam::12345678901:role/spectator
source_profile = bastion
aws_access_key_id = ASIA554SXXVIYYQRGGER
aws_secret_access_key = aw5/hbwzGP31s2lfC3ZQshKE+AZdlOYkqBUI4otp
aws_session_token = FQoGZXIvYXdHEY4aDDDbLp6g5sfNojzC6CKwAV+yefPfFg7y0xADMDECoddpj9WecBEReMtXkRjCVZfbSa1604EIK2q0zshlsP0PtF0e5wBZFDuZHTI464EpSQEXkJajksWeMMOe7PSzyJOX5Zqp8ve4ItHoE70tGxIVQjA06NbvodNjjOO/gsbDAcKHW1rx9wnq3RJ+dQbqqNq01R1vrDvTjxDNTrZr2wYI2qYrd9REP+mc44EeIO+3r0iuiwxRCL1UzS/4nG4IRYG2KMeo9esF
```

### References

<!---
Information about referencing Github Issues: https://help.github.com/articles/basic-writing-and-formatting-syntax/#referencing-issues-and-pull-requests

Are there any other GitHub issues (open or closed) or pull requests that should be linked here? Vendor blog posts or documentation? For example:

* https://aws.amazon.com/about-aws/whats-new/2018/04/introducing-amazon-ec2-fleet/
--->

* https://github.com/gruntwork-io/terragrunt/issues/527
",ASIA554SXDVIHKO5ACW2,https://github.com/hashicorp/terraform-provider-aws
18,10110.0,"<!--- Please keep this note for the community --->

### Community Note

* Please vote on this issue by adding a 👍 [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to the original issue to help the community and maintainers prioritize this request
* Please do not leave ""+1"" or ""me too"" comments, they generate extra noise for issue followers and do not help prioritize the request
* If you are interested in working on this issue or have submitted a pull request, please leave a comment

<!--- Thank you for keeping this note for the community --->

### Description

Here is an example of a shared credentials file configuration that is supported by the [awscli](https://docs.aws.amazon.com/en_pv/cli/latest/userguide/cli-configure-role.html)
```
# ~/.aws/credentials

[bastion] # these are fake credentials
aws_access_key_id = ASIA554SXDVIHKO5ACW2
aws_secret_access_key = VLJQKLEqs37HCDG4HgSDrxl1vLNrk9Is8gm0VNfA

[dev-spectator]
role_arn = arn:aws:iam::12345678901:role/spectator
source_profile = bastion
```

This works by caching credentials in the `~/.aws/cli/cache` directory. The awscli can read from this cache but **terraform cannot**.

you will get the following error:

```
Error initializing session: SharedConfigAssumeRoleError: failed to load assume role for arn:aws:iam::12345678901:role/spectator, source profile has no shared credentials
```

### Work around

In order to support terraform, you will need something like:

```
# ~/.aws/credentials

[bastion] # these are fake credentials
aws_access_key_id = ASIA554SXDVIHKO5ACW2
aws_secret_access_key = VLJQKLEqs37HCDG4HgSDrxl1vLNrk9Is8gm0VNfA

[dev-spectator] # these are also fake credentials
role_arn = arn:aws:iam::12345678901:role/spectator
source_profile = bastion
aws_access_key_id = ASIA554SXXVIYYQRGGER
aws_secret_access_key = aw5/hbwzGP31s2lfC3ZQshKE+AZdlOYkqBUI4otp
aws_session_token = FQoGZXIvYXdHEY4aDDDbLp6g5sfNojzC6CKwAV+yefPfFg7y0xADMDECoddpj9WecBEReMtXkRjCVZfbSa1604EIK2q0zshlsP0PtF0e5wBZFDuZHTI464EpSQEXkJajksWeMMOe7PSzyJOX5Zqp8ve4ItHoE70tGxIVQjA06NbvodNjjOO/gsbDAcKHW1rx9wnq3RJ+dQbqqNq01R1vrDvTjxDNTrZr2wYI2qYrd9REP+mc44EeIO+3r0iuiwxRCL1UzS/4nG4IRYG2KMeo9esF
```

### References

<!---
Information about referencing Github Issues: https://help.github.com/articles/basic-writing-and-formatting-syntax/#referencing-issues-and-pull-requests

Are there any other GitHub issues (open or closed) or pull requests that should be linked here? Vendor blog posts or documentation? For example:

* https://aws.amazon.com/about-aws/whats-new/2018/04/introducing-amazon-ec2-fleet/
--->

* https://github.com/gruntwork-io/terragrunt/issues/527
",ASIA554SXXVIYYQRGGER,https://github.com/hashicorp/terraform-provider-aws
19,10110.0,"<!--- Please keep this note for the community --->

### Community Note

* Please vote on this issue by adding a 👍 [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to the original issue to help the community and maintainers prioritize this request
* Please do not leave ""+1"" or ""me too"" comments, they generate extra noise for issue followers and do not help prioritize the request
* If you are interested in working on this issue or have submitted a pull request, please leave a comment

<!--- Thank you for keeping this note for the community --->

### Description

Here is an example of a shared credentials file configuration that is supported by the [awscli](https://docs.aws.amazon.com/en_pv/cli/latest/userguide/cli-configure-role.html)
```
# ~/.aws/credentials

[bastion] # these are fake credentials
aws_access_key_id = ASIA554SXDVIHKO5ACW2
aws_secret_access_key = VLJQKLEqs37HCDG4HgSDrxl1vLNrk9Is8gm0VNfA

[dev-spectator]
role_arn = arn:aws:iam::12345678901:role/spectator
source_profile = bastion
```

This works by caching credentials in the `~/.aws/cli/cache` directory. The awscli can read from this cache but **terraform cannot**.

you will get the following error:

```
Error initializing session: SharedConfigAssumeRoleError: failed to load assume role for arn:aws:iam::12345678901:role/spectator, source profile has no shared credentials
```

### Work around

In order to support terraform, you will need something like:

```
# ~/.aws/credentials

[bastion] # these are fake credentials
aws_access_key_id = ASIA554SXDVIHKO5ACW2
aws_secret_access_key = VLJQKLEqs37HCDG4HgSDrxl1vLNrk9Is8gm0VNfA

[dev-spectator] # these are also fake credentials
role_arn = arn:aws:iam::12345678901:role/spectator
source_profile = bastion
aws_access_key_id = ASIA554SXXVIYYQRGGER
aws_secret_access_key = aw5/hbwzGP31s2lfC3ZQshKE+AZdlOYkqBUI4otp
aws_session_token = FQoGZXIvYXdHEY4aDDDbLp6g5sfNojzC6CKwAV+yefPfFg7y0xADMDECoddpj9WecBEReMtXkRjCVZfbSa1604EIK2q0zshlsP0PtF0e5wBZFDuZHTI464EpSQEXkJajksWeMMOe7PSzyJOX5Zqp8ve4ItHoE70tGxIVQjA06NbvodNjjOO/gsbDAcKHW1rx9wnq3RJ+dQbqqNq01R1vrDvTjxDNTrZr2wYI2qYrd9REP+mc44EeIO+3r0iuiwxRCL1UzS/4nG4IRYG2KMeo9esF
```

### References

<!---
Information about referencing Github Issues: https://help.github.com/articles/basic-writing-and-formatting-syntax/#referencing-issues-and-pull-requests

Are there any other GitHub issues (open or closed) or pull requests that should be linked here? Vendor blog posts or documentation? For example:

* https://aws.amazon.com/about-aws/whats-new/2018/04/introducing-amazon-ec2-fleet/
--->

* https://github.com/gruntwork-io/terragrunt/issues/527
",VLJQKLEqs37HCDG4HgSDrxl1vLNrk9Is8gm0VNfA,https://github.com/hashicorp/terraform-provider-aws
20,10110.0,"<!--- Please keep this note for the community --->

### Community Note

* Please vote on this issue by adding a 👍 [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to the original issue to help the community and maintainers prioritize this request
* Please do not leave ""+1"" or ""me too"" comments, they generate extra noise for issue followers and do not help prioritize the request
* If you are interested in working on this issue or have submitted a pull request, please leave a comment

<!--- Thank you for keeping this note for the community --->

### Description

Here is an example of a shared credentials file configuration that is supported by the [awscli](https://docs.aws.amazon.com/en_pv/cli/latest/userguide/cli-configure-role.html)
```
# ~/.aws/credentials

[bastion] # these are fake credentials
aws_access_key_id = ASIA554SXDVIHKO5ACW2
aws_secret_access_key = VLJQKLEqs37HCDG4HgSDrxl1vLNrk9Is8gm0VNfA

[dev-spectator]
role_arn = arn:aws:iam::12345678901:role/spectator
source_profile = bastion
```

This works by caching credentials in the `~/.aws/cli/cache` directory. The awscli can read from this cache but **terraform cannot**.

you will get the following error:

```
Error initializing session: SharedConfigAssumeRoleError: failed to load assume role for arn:aws:iam::12345678901:role/spectator, source profile has no shared credentials
```

### Work around

In order to support terraform, you will need something like:

```
# ~/.aws/credentials

[bastion] # these are fake credentials
aws_access_key_id = ASIA554SXDVIHKO5ACW2
aws_secret_access_key = VLJQKLEqs37HCDG4HgSDrxl1vLNrk9Is8gm0VNfA

[dev-spectator] # these are also fake credentials
role_arn = arn:aws:iam::12345678901:role/spectator
source_profile = bastion
aws_access_key_id = ASIA554SXXVIYYQRGGER
aws_secret_access_key = aw5/hbwzGP31s2lfC3ZQshKE+AZdlOYkqBUI4otp
aws_session_token = FQoGZXIvYXdHEY4aDDDbLp6g5sfNojzC6CKwAV+yefPfFg7y0xADMDECoddpj9WecBEReMtXkRjCVZfbSa1604EIK2q0zshlsP0PtF0e5wBZFDuZHTI464EpSQEXkJajksWeMMOe7PSzyJOX5Zqp8ve4ItHoE70tGxIVQjA06NbvodNjjOO/gsbDAcKHW1rx9wnq3RJ+dQbqqNq01R1vrDvTjxDNTrZr2wYI2qYrd9REP+mc44EeIO+3r0iuiwxRCL1UzS/4nG4IRYG2KMeo9esF
```

### References

<!---
Information about referencing Github Issues: https://help.github.com/articles/basic-writing-and-formatting-syntax/#referencing-issues-and-pull-requests

Are there any other GitHub issues (open or closed) or pull requests that should be linked here? Vendor blog posts or documentation? For example:

* https://aws.amazon.com/about-aws/whats-new/2018/04/introducing-amazon-ec2-fleet/
--->

* https://github.com/gruntwork-io/terragrunt/issues/527
",FQoGZXIvYXdHEY4aDDDbLp6g5sfNojzC6CKwAV+,https://github.com/hashicorp/terraform-provider-aws
21,10110.0,"<!--- Please keep this note for the community --->

### Community Note

* Please vote on this issue by adding a 👍 [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to the original issue to help the community and maintainers prioritize this request
* Please do not leave ""+1"" or ""me too"" comments, they generate extra noise for issue followers and do not help prioritize the request
* If you are interested in working on this issue or have submitted a pull request, please leave a comment

<!--- Thank you for keeping this note for the community --->

### Description

Here is an example of a shared credentials file configuration that is supported by the [awscli](https://docs.aws.amazon.com/en_pv/cli/latest/userguide/cli-configure-role.html)
```
# ~/.aws/credentials

[bastion] # these are fake credentials
aws_access_key_id = ASIA554SXDVIHKO5ACW2
aws_secret_access_key = VLJQKLEqs37HCDG4HgSDrxl1vLNrk9Is8gm0VNfA

[dev-spectator]
role_arn = arn:aws:iam::12345678901:role/spectator
source_profile = bastion
```

This works by caching credentials in the `~/.aws/cli/cache` directory. The awscli can read from this cache but **terraform cannot**.

you will get the following error:

```
Error initializing session: SharedConfigAssumeRoleError: failed to load assume role for arn:aws:iam::12345678901:role/spectator, source profile has no shared credentials
```

### Work around

In order to support terraform, you will need something like:

```
# ~/.aws/credentials

[bastion] # these are fake credentials
aws_access_key_id = ASIA554SXDVIHKO5ACW2
aws_secret_access_key = VLJQKLEqs37HCDG4HgSDrxl1vLNrk9Is8gm0VNfA

[dev-spectator] # these are also fake credentials
role_arn = arn:aws:iam::12345678901:role/spectator
source_profile = bastion
aws_access_key_id = ASIA554SXXVIYYQRGGER
aws_secret_access_key = aw5/hbwzGP31s2lfC3ZQshKE+AZdlOYkqBUI4otp
aws_session_token = FQoGZXIvYXdHEY4aDDDbLp6g5sfNojzC6CKwAV+yefPfFg7y0xADMDECoddpj9WecBEReMtXkRjCVZfbSa1604EIK2q0zshlsP0PtF0e5wBZFDuZHTI464EpSQEXkJajksWeMMOe7PSzyJOX5Zqp8ve4ItHoE70tGxIVQjA06NbvodNjjOO/gsbDAcKHW1rx9wnq3RJ+dQbqqNq01R1vrDvTjxDNTrZr2wYI2qYrd9REP+mc44EeIO+3r0iuiwxRCL1UzS/4nG4IRYG2KMeo9esF
```

### References

<!---
Information about referencing Github Issues: https://help.github.com/articles/basic-writing-and-formatting-syntax/#referencing-issues-and-pull-requests

Are there any other GitHub issues (open or closed) or pull requests that should be linked here? Vendor blog posts or documentation? For example:

* https://aws.amazon.com/about-aws/whats-new/2018/04/introducing-amazon-ec2-fleet/
--->

* https://github.com/gruntwork-io/terragrunt/issues/527
",aws_secret_access_key,https://github.com/hashicorp/terraform-provider-aws
22,10110.0,"<!--- Please keep this note for the community --->

### Community Note

* Please vote on this issue by adding a 👍 [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to the original issue to help the community and maintainers prioritize this request
* Please do not leave ""+1"" or ""me too"" comments, they generate extra noise for issue followers and do not help prioritize the request
* If you are interested in working on this issue or have submitted a pull request, please leave a comment

<!--- Thank you for keeping this note for the community --->

### Description

Here is an example of a shared credentials file configuration that is supported by the [awscli](https://docs.aws.amazon.com/en_pv/cli/latest/userguide/cli-configure-role.html)
```
# ~/.aws/credentials

[bastion] # these are fake credentials
aws_access_key_id = ASIA554SXDVIHKO5ACW2
aws_secret_access_key = VLJQKLEqs37HCDG4HgSDrxl1vLNrk9Is8gm0VNfA

[dev-spectator]
role_arn = arn:aws:iam::12345678901:role/spectator
source_profile = bastion
```

This works by caching credentials in the `~/.aws/cli/cache` directory. The awscli can read from this cache but **terraform cannot**.

you will get the following error:

```
Error initializing session: SharedConfigAssumeRoleError: failed to load assume role for arn:aws:iam::12345678901:role/spectator, source profile has no shared credentials
```

### Work around

In order to support terraform, you will need something like:

```
# ~/.aws/credentials

[bastion] # these are fake credentials
aws_access_key_id = ASIA554SXDVIHKO5ACW2
aws_secret_access_key = VLJQKLEqs37HCDG4HgSDrxl1vLNrk9Is8gm0VNfA

[dev-spectator] # these are also fake credentials
role_arn = arn:aws:iam::12345678901:role/spectator
source_profile = bastion
aws_access_key_id = ASIA554SXXVIYYQRGGER
aws_secret_access_key = aw5/hbwzGP31s2lfC3ZQshKE+AZdlOYkqBUI4otp
aws_session_token = FQoGZXIvYXdHEY4aDDDbLp6g5sfNojzC6CKwAV+yefPfFg7y0xADMDECoddpj9WecBEReMtXkRjCVZfbSa1604EIK2q0zshlsP0PtF0e5wBZFDuZHTI464EpSQEXkJajksWeMMOe7PSzyJOX5Zqp8ve4ItHoE70tGxIVQjA06NbvodNjjOO/gsbDAcKHW1rx9wnq3RJ+dQbqqNq01R1vrDvTjxDNTrZr2wYI2qYrd9REP+mc44EeIO+3r0iuiwxRCL1UzS/4nG4IRYG2KMeo9esF
```

### References

<!---
Information about referencing Github Issues: https://help.github.com/articles/basic-writing-and-formatting-syntax/#referencing-issues-and-pull-requests

Are there any other GitHub issues (open or closed) or pull requests that should be linked here? Vendor blog posts or documentation? For example:

* https://aws.amazon.com/about-aws/whats-new/2018/04/introducing-amazon-ec2-fleet/
--->

* https://github.com/gruntwork-io/terragrunt/issues/527
",12345678901:role,https://github.com/hashicorp/terraform-provider-aws
23,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",b9c3c084-789e-7472-0b18-8698b4afc854,https://github.com/hashicorp/terraform-provider-aws
24,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",35e6cd4e-cb70-4016-a250-53f0880cc9be,https://github.com/hashicorp/terraform-provider-aws
25,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",53ace3cc-2597-49a0-99b2-3b9f1cb8567d,https://github.com/hashicorp/terraform-provider-aws
26,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",e2bfb730-ecaa-11e6-8f88-34363bc7c4c0,https://github.com/hashicorp/terraform-provider-aws
27,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfa,https://github.com/hashicorp/terraform-provider-aws
28,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgL,https://github.com/hashicorp/terraform-provider-aws
29,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JR,https://github.com/hashicorp/terraform-provider-aws
30,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA,https://github.com/hashicorp/terraform-provider-aws
31,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA,https://github.com/hashicorp/terraform-provider-aws
32,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef,https://github.com/hashicorp/terraform-provider-aws
33,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2,https://github.com/hashicorp/terraform-provider-aws
34,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo,https://github.com/hashicorp/terraform-provider-aws
35,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d,https://github.com/hashicorp/terraform-provider-aws
36,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2,https://github.com/hashicorp/terraform-provider-aws
37,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",ASIAOS43C3BRNG62F4KQ,https://github.com/hashicorp/terraform-provider-aws
38,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",disable_api_termination,https://github.com/hashicorp/terraform-provider-aws
39,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-,https://github.com/hashicorp/terraform-provider-aws
40,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",-side-encryption,https://github.com/hashicorp/terraform-provider-aws
41,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",yqOKaSKBAunC9koGEWSi+,https://github.com/hashicorp/terraform-provider-aws
42,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",ServerSideEncryption,https://github.com/hashicorp/terraform-provider-aws
43,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",true\n\nwrite_files:\n,https://github.com/hashicorp/terraform-provider-aws
44,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",Version=2016-11-15,https://github.com/hashicorp/terraform-provider-aws
45,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",SignedHeaders=content-length;content-type;host;x-amz-content-,https://github.com/hashicorp/terraform-provider-aws
46,4359.0,"# Terraform Version

Terraform v0.11.6
provider.aws v1.16.0

# Affected Resource(s)
aws_instance_ec2
# Terraform Configuration Files
module ""docdb"" {
  source = ""../../../../../../tradeshift-puppet/terraform/modules/aws/instances-cn/riak""
  environment = ""${var.pull_request}""
  stackname = ""${var.stackname}""
  role = ""docdb""
  owner = ""${var.owner}""
  decomission_date = ""${var.decomission_date}""
  instance_type = ""t2.large""
  instance_ami = ""${module.regional-common.base_ami_1604}""
  instance_key_name = ""${module.regional-common.key_name}""
  disable_api_termination = false
  ebs_volume_size = ""100""
  ebs_delete_on_termination = ""false""
  ebs_optimized = false
  vpc_security_groups = ""${data.terraform_remote_state.security_groups.stack_it_run_access_sg_id}""
  vpc_subnets = ""${data.terraform_remote_state.vpc.private_subnets}""
  postfix = "".${var.stackname}""
  dns_region_name = ""${module.regional-common.region_dns}""
  disk_encryption_key = ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx""
  count = ""1""
  cert_name = ""${module.environment-common.cert_name}""
}

# Debug Output
```
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 200 OK
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: text/xml;charset=UTF-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:36 GMT
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Vary: Accept-Encoding
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <DescribeInstanceAttributeResponse xmlns=""http://ec2.amazonaws.com/doc/2016-11-15/"">
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <requestId>35e6cd4e-cb70-4016-a250-53f0880cc9be</requestId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <instanceId>i-022a9894321a6d374</instanceId>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     <userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:         <value>I2Nsb3VkLWNvbmZpZwptb3VudHM6CiAtIFtlcGhlbWVyYWwwLCBudWxsXQoKcHJlc2VydmVfaG9zdG5hbWU6IHRydWUKbWFuYWdlX2V0Y19ob3N0czogZmFsc2UKCmJvb3RjbWQ6CiAtICJlY2hvIHBheSA+IC9ldGMvdHJhZGVzaGlmdC9zdGFja25hbWUiCiAtICJlY2hvIGRvY2RiLSRJTlNUQU5DRV9JRCA+IC9ldGMvaG9zdG5hbWU7IGhvc3RuYW1lIC1GIC9ldGMvaG9zdG5hbWUiCiAtICJzZWQgLWkgLWUgJy9eMTI3LjAuMC4xL2QnIC9ldGMvaG9zdHM7IGVjaG8gMTI3LjAuMC4xIGRvY2RiLSRJTlNUQU5DRV9JRC5jbi1ub3J0aHdlc3QtMS50ZXN0LmJ3dHNpLmNuIGRvY2RiLSRJTlNUQU5DRV9JRCBsb2NhbGhvc3QgPj4gL2V0Yy9ob3N0cyIKICMgRGlzayBlbmNyeXB0aW9uIGtleSBmb3IgZXBoZW1lcmFsIGRpc2sgZHJpdmVzLCBzYXZlZCB0byByYW0gZHJpdmUKICMgVGhlIHNlZCBwYXJ0IHJlbW92ZXMgdGhlIChzdGRpbik9IG91dHB1dCBmcm9tIG9wZW5zc2wgZGdzdAogLSAiZWNobyAkSU5TVEFOQ0VfSUQgfCBvcGVuc3NsIGRnc3QgLXNoYTI1NiAtaG1hYyAnZHI2OVpVR1ZtMEVwMUVvL3hCb09UY2VmdUtWeCcgfCBzZWQgJ3MvXi4qPSAvLycgPiAvZGV2L3NobS9lbmNyeXB0ZWQtZGlzay5sdWtzLmtleSAmJiBjaG1vZCA0MDAgL2Rldi9zaG0vZW5jcnlwdGVkLWRpc2subHVrcy5rZXkiCgojIERvbid0IG92ZXJyaWRlIG91ciBhcHQgcmVwb3NpdG9yeSBsaXN0IG9uIGZpcnN0IGJvb3QKYXB0X3ByZXNlcnZlX3NvdXJjZXNfbGlzdDogdHJ1ZQoKIyBEb24ndCBwcmludCB0aGUgbmV3IHNzaCBrZXlzIG9uIHRoZSBjb25zb2xlCm5vX3NzaF9maW5nZXJwcmludHM6IHRydWUKCndyaXRlX2ZpbGVzOgogLSBwYXRoOiAvZXRjL3B1cHBldGxhYnMvcHVwcGV0L3B1cHBldC5jb25mCiAgIG93bmVyOiByb290OnJvb3QKICAgcGVybWlzc2lvbnM6ICcwNDQ0JwogICBjb250ZW50OiB8CiAgICAgW21haW5dCiAgICAgY2VydG5hbWUgPSBiYXNlLnRlc3RpbmcuYnd0c2kuY24KICAgICBlbnZpcm9ubWVudCA9IGNoaW5hcGF5CiAgICAgW2FnZW50XQogICAgIHJ1bmludGVydmFsID0gMTAweQogICAgIHNlcnZlciA9IHB1cHBldC5id3RzaS5jbgogICAgIG5vZGVfbmFtZSA9IGZhY3RlcgogICAgIG5vZGVfbmFtZV9mYWN0ID0gZnFkbgogICAgIHNwbGF5ID0gdHJ1ZQogICAgIHNwbGF5bGltaXQgPSA1bQo=</value>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:     </userData>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: </DescribeInstanceAttributeResponse>
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:36 [DEBUG] [aws-sdk-go] DEBUG: Request ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ REQUEST POST-SIGN ]-----------------------------
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: POST / HTTP/1.1
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Host: ec2.cn-northwest-1.amazonaws.com.cn
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: User-Agent: aws-sdk-go/1.13.32 (go1.9.2; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.7
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Length: 95
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/ec2/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=60ac431f06b221bf7fe6d53ee72de1babb1aad5a5a4d17917e4d2f78a7533fef
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Content-Type: application/x-www-form-urlencoded; charset=utf-8
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Date: 20180426T110036Z
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Accept-Encoding: gzip
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Action=DescribeInstanceCreditSpecifications&InstanceId.1=i-022a9894321a6d374&Version=2016-11-15
2018-04-26T19:00:36.718+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Response ec2/DescribeInstanceCreditSpecifications Details:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: ---[ RESPONSE ]--------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: HTTP/1.1 400 Bad Request
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Connection: close
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Transfer-Encoding: chunked
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Date: Thu, 26 Apr 2018 11:00:37 GMT
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: Server: AmazonEC2
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4:
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: -----------------------------------------------------
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] <?xml version=""1.0"" encoding=""UTF-8""?>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: <Response><Errors><Error><Code>UnsupportedOperation</Code><Message>The functionality you requested is not available in this region.</Message></Error></Errors><RequestID>53ace3cc-2597-49a0-99b2-3b9f1cb8567d</RequestID></Response>
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Validate Response ec2/DescribeInstanceCreditSpecifications failed, not retrying, error UnsupportedOperation: The functionality you requested is not available in this region.
2018-04-26T19:00:37.370+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalApplyPost, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [ERROR] root.docdb: eval: *terraform.EvalSequence, err: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d
2018/04/26 19:00:37 [DEBUG] Uploading remote state to S3: {
  Body: buffer(0xc4203e17d0),
  Bucket: ""bwts-terraform-bucket-chinapay"",
  ContentLength: 20792,
  ContentType: ""application/json"",
  Key: ""cn-northwest-1/riak/terraform.tfstate"",
  ServerSideEncryption: ""AES256""
}
2018/04/26 19:00:37 [DEBUG] [aws-sdk-go] DEBUG: Request s3/PutObject Details:
---[ REQUEST POST-SIGN ]-----------------------------
PUT /cn-northwest-1/riak/terraform.tfstate HTTP/1.1
Host: bwts-terraform-bucket-chinapay.s3.cn-northwest-1.amazonaws.com.cn
User-Agent: aws-sdk-go/1.12.75 (go1.10.1; darwin; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.11.6
Content-Length: 20792
Authorization: AWS4-HMAC-SHA256 Credential=ASIAOS43C3BRNG62F4KQ/20180426/cn-northwest-1/s3/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-server-side-encryption, Signature=8aa16110ae06a99b503e1f0546a4d6cf208fdf658ca29420baa2d649cec043e2
Content-Type: application/json
X-Amz-Content-Sha256: 04110e3e4cd97aa3e89cfe2f03487922bfd6b6346ec6dfe3fbe8fad9755e18c2
X-Amz-Date: 20180426T110037Z
X-Amz-Security-Token: FQoDYXdzECMaDBn0dTR5Adi+yqOKaSKBAunC9koGEWSi+glPaQcXYEBDxkkbz8wEPzhtRgEOofTKazqo1aOGnVCw4KltCP8JviHPrnGes/0BAfN0CSfANVZfkUGRRczLEtMWKj1j9gFkXPELpfxO10DI/iX4lxe3vdzQv2LeY0xFLLKW8PXNq4/nbjrNgr/2VTiL8IYECd9lzDlKsgwYY7W1IdMqA1KcXX41Q1CDifRou5KCBehUg9NE9mqwKcHMtikEcvM76PIZ33CkXWX9OacMjlk+YNf0/Ae1lhcq8qAAXDwPgJdb4X7OQiYyOh1HI/HbHIRw/Ae/V9t6OAFmz1662A2xHLSysyV74zuc17EYgN4t+Hpp9feLKLPVhtcF
X-Amz-Server-Side-Encryption: AES256
Accept-Encoding: gzip

{
    ""version"": 3,
    ""terraform_version"": ""0.11.6"",
    ""serial"": 1,
    ""lineage"": ""b9c3c084-789e-7472-0b18-8698b4afc854"",
    ""modules"": [
        {
            ""path"": [
                ""root""
            ],
            ""outputs"": {},
            ""resources"": {
                ""data.terraform_remote_state.security_groups"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""bastion_host"": ""sg-e29feb8b"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/dev-stack-sgs/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""external_lb_sg_id"": ""sg-a55027cc"",
                            ""id"": ""2018-04-26 11:00:04.859497096 +0000 UTC"",
                            ""rundeck"": ""sg-b39eeada"",
                            ""stack_it_run_access_sg_id"": ""sg-9d5f28f4"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                },
                ""data.terraform_remote_state.vpc"": {
                    ""type"": ""terraform_remote_state"",
                    ""depends_on"": [
                        ""module.regional-common""
                    ],
                    ""primary"": {
                        ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                        ""attributes"": {
                            ""backend"": ""s3"",
                            ""config.%"": ""4"",
                            ""config.bucket"": ""bwts-terraform-bucket-chinapay"",
                            ""config.encrypt"": ""1"",
                            ""config.key"": ""cn-northwest-1/vpc/terraform.tfstate"",
                            ""config.region"": ""cn-northwest-1"",
                            ""environment"": ""default"",
                            ""id"": ""2018-04-26 11:00:04.858615081 +0000 UTC"",
                            ""private_subnets"": ""subnet-454c962c,subnet-9722ddec"",
                            ""public_subnets"": ""subnet-5e4d9737,subnet-9022ddeb"",
                            ""vpc_cidr"": ""192.168.0.0/19"",
                            ""vpc_id"": ""vpc-34c61f5d"",
                            ""workspace"": ""default""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.terraform""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""docdb""
            ],
            ""outputs"": {},
            ""resources"": {
                ""aws_instance.riak"": {
                    ""type"": ""aws_instance"",
                    ""depends_on"": [
                        ""data.template_file.cloud-init-riak""
                    ],
                    ""primary"": {
                        ""id"": ""i-022a9894321a6d374"",
                        ""attributes"": {
                            ""ami"": ""ami-6ec1d50c"",
                            ""associate_public_ip_address"": ""false"",
                            ""availability_zone"": ""cn-northwest-1a"",
                            ""disable_api_termination"": ""false"",
                            ""ebs_block_device.#"": ""4"",
                            ""ebs_block_device.3965576335.delete_on_termination"": ""false"",
                            ""ebs_block_device.3965576335.device_name"": ""/dev/xvdd"",
                            ""ebs_block_device.3965576335.encrypted"": ""false"",
                            ""ebs_block_device.3965576335.iops"": ""300"",
                            ""ebs_block_device.3965576335.snapshot_id"": """",
                            ""ebs_block_device.3965576335.volume_id"": ""vol-09cde364159031c5d"",
                            ""ebs_block_device.3965576335.volume_size"": ""100"",
                            ""ebs_block_device.3965576335.volume_type"": ""gp2"",
                            ""ebs_block_device.3986656952.delete_on_termination"": ""false"",
                            ""ebs_block_device.3986656952.device_name"": ""/dev/xvde"",
                            ""ebs_block_device.3986656952.encrypted"": ""false"",
                            ""ebs_block_device.3986656952.iops"": ""300"",
                            ""ebs_block_device.3986656952.snapshot_id"": """",
                            ""ebs_block_device.3986656952.volume_id"": ""vol-09c14e768bdd6dd4b"",
                            ""ebs_block_device.3986656952.volume_size"": ""100"",
                            ""ebs_block_device.3986656952.volume_type"": ""gp2"",
                            ""ebs_block_device.3994770134.delete_on_termination"": ""false"",
                            ""ebs_block_device.3994770134.device_name"": ""/dev/xvdg"",
                            ""ebs_block_device.3994770134.encrypted"": ""false"",
                            ""ebs_block_device.3994770134.iops"": ""300"",
                            ""ebs_block_device.3994770134.snapshot_id"": """",
                            ""ebs_block_device.3994770134.volume_id"": ""vol-0e1b3ff63114f3003"",
                            ""ebs_block_device.3994770134.volume_size"": ""100"",
                            ""ebs_block_device.3994770134.volume_type"": ""gp2"",
                            ""ebs_block_device.4023988449.delete_on_termination"": ""false"",
                            ""ebs_block_device.4023988449.device_name"": ""/dev/xvdf"",
                            ""ebs_block_device.4023988449.encrypted"": ""false"",
                            ""ebs_block_device.4023988449.iops"": ""300"",
                            ""ebs_block_device.4023988449.snapshot_id"": """",
                            ""ebs_block_device.4023988449.volume_id"": ""vol-015467a2fec64cfdd"",
                            ""ebs_block_device.4023988449.volume_size"": ""100"",
                            ""ebs_block_device.4023988449.volume_type"": ""gp2"",
                            ""ebs_optimized"": ""false"",
                            ""ephemeral_block_device.#"": ""0"",
                            ""get_password_data"": ""false"",
                            ""iam_instance_profile"": """",
                            ""id"": ""i-022a9894321a6d374"",
                            ""instance_state"": ""running"",
                            ""instance_type"": ""t2.large"",
                            ""ipv6_addresses.#"": ""0"",
                            ""key_name"": ""pay_china"",
                            ""monitoring"": ""false"",
                            ""network_interface.#"": ""0"",
                            ""network_interface_id"": ""eni-fd55c3a4"",
                            ""placement_group"": """",
                            ""primary_network_interface_id"": ""eni-fd55c3a4"",
                            ""private_dns"": ""ip-192-168-14-135.cn-northwest-1.compute.internal"",
                            ""private_ip"": ""192.168.14.135"",
                            ""public_dns"": """",
                            ""public_ip"": """",
                            ""root_block_device.#"": ""1"",
                            ""root_block_device.0.delete_on_termination"": ""true"",
                            ""root_block_device.0.iops"": ""100"",
                            ""root_block_device.0.volume_id"": ""vol-0884c92d17ea04d83"",
                            ""root_block_device.0.volume_size"": ""20"",
                            ""root_block_device.0.volume_type"": ""gp2"",
                            ""security_groups.#"": ""0"",
                            ""source_dest_check"": ""true"",
                            ""subnet_id"": ""subnet-454c962c"",
                            ""tags.%"": ""9"",
                            ""tags.Decomission_Date"": ""never"",
                            ""tags.Environment"": ""chinapay"",
                            ""tags.Name"": ""docdb.pay.cn-northwest-1.test.bwtsi.cn"",
                            ""tags.Owner"": ""operations"",
                            ""tags.Purpose"": ""Riak instance"",
                            ""tags.Role"": ""docdb"",
                            ""tags.Stackname"": ""pay"",
                            ""tags.Started_By"": ""operations"",
                            ""tags.Warning"": ""Managed by terraform, do not edit"",
                            ""tenancy"": ""default"",
                            ""user_data"": ""f35050bdeefa2a2d9f354c0e699a5f2d7c393365"",
                            ""volume_tags.%"": ""0"",
                            ""vpc_security_group_ids.#"": ""1"",
                            ""vpc_security_group_ids.2301982502"": ""sg-9d5f28f4""
                        },
                        ""meta"": {
                            ""e2bfb730-ecaa-11e6-8f88-34363bc7c4c0"": {
                                ""create"": 600000000000,
                                ""delete"": 1200000000000,
                                ""update"": 600000000000
                            },
                            ""schema_version"": ""1""
                        },
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.aws""
                },
                ""data.template_file.cloud-init-riak"": {
                    ""type"": ""template_file"",
                    ""depends_on"": [],
                    ""primary"": {
                        ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                        ""attributes"": {
                            ""id"": ""a5f1f2b9d367e2b212c58642fd07eb0b9d6f956c763b7bc5e787b162bc75538d"",
                            ""rendered"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo pay \u003e /etc/tradeshift/stackname\""\n - \""echo docdb-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 docdb-$INSTANCE_ID.cn-northwest-1.test.bwtsi.cn docdb-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac 'dr69ZUGVm0Ep1Eo/xBoOTcefuKVx' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = base.testing.bwtsi.cn\n     environment = chinapay\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""template"": ""#cloud-config\nmounts:\n - [ephemeral0, null]\n\npreserve_hostname: true\nmanage_etc_hosts: false\n\nbootcmd:\n - \""echo ${STACK_NAME} \u003e /etc/tradeshift/stackname\""\n - \""echo ${HOSTNAME}-$INSTANCE_ID \u003e /etc/hostname; hostname -F /etc/hostname\""\n - \""sed -i -e '/^127.0.0.1/d' /etc/hosts; echo 127.0.0.1 ${HOSTNAME}-$INSTANCE_ID.${REGION_DNS} ${HOSTNAME}-$INSTANCE_ID localhost \u003e\u003e /etc/hosts\""\n # Disk encryption key for ephemeral disk drives, saved to ram drive\n # The sed part removes the (stdin)= output from openssl dgst\n - \""echo $INSTANCE_ID | openssl dgst -sha256 -hmac '${DISK_ENCRYPTION_KEY}' | sed 's/^.*= //' \u003e /dev/shm/encrypted-disk.luks.key \u0026\u0026 chmod 400 /dev/shm/encrypted-disk.luks.key\""\n\n# Don't override our apt repository list on first boot\napt_preserve_sources_list: true\n\n# Don't print the new ssh keys on the console\nno_ssh_fingerprints: true\n\nwrite_files:\n - path: /etc/puppetlabs/puppet/puppet.conf\n   owner: root:root\n   permissions: '0444'\n   content: |\n     [main]\n     certname = ${CERT_NAME}\n     environment = ${ENVIRONMENT}\n     [agent]\n     runinterval = 100y\n     server = puppet.bwtsi.cn\n     node_name = facter\n     node_name_fact = fqdn\n     splay = true\n     splaylimit = 5m\n"",
                            ""vars.%"": ""6"",
                            ""vars.CERT_NAME"": ""base.testing.bwtsi.cn"",
                            ""vars.DISK_ENCRYPTION_KEY"": ""dr69ZUGVm0Ep1Eo/xBoOTcefuKVx"",
                            ""vars.ENVIRONMENT"": ""chinapay"",
                            ""vars.HOSTNAME"": ""docdb"",
                            ""vars.REGION_DNS"": ""cn-northwest-1.test.bwtsi.cn"",
                            ""vars.STACK_NAME"": ""pay""
                        },
                        ""meta"": {},
                        ""tainted"": false
                    },
                    ""deposed"": [],
                    ""provider"": ""provider.template""
                }
            },
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""environment-common""
            ],
            ""outputs"": {
                ""cert_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""base.testing.bwtsi.cn""
                },
                ""domain_name_search"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test.bwtsi.cn""
                },
                ""environment_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""chinapay""
                },
                ""environment_shortname"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                },
                ""security_group_name_postfix"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": "".chinapay""
                },
                ""site"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""global-common""
            ],
            ""outputs"": {
                ""all_cidr_eu_west_1"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16,172.25.128.0/21,172.25.120.0/21,172.25.112.0/21,172.31.248.0/21,172.31.240.0/21,172.21.0.0/19,172.19.0.0/19,172.22.0.0/19,172.18.0.0/19""
                },
                ""base_tradeshift_net_account_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""694518486591""
                },
                ""gcsops_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.17.22.143/32""
                },
                ""gcsops_cn_cidr"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""54.223.95.7/32,54.223.198.170/32,54.223.211.136/32,52.80.34.103/32""
                },
                ""office_ips"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,178.49.148.100/32,87.243.3.34/32,109.166.189.50/32,78.40.84.83/32,4.53.137.82/32,173.247.199.142/32,58.211.225.90/32,202.181.248.19/32,109.70.48.99/32""
                },
                ""prod_base_tradeshift_net_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""prod.base.tradeshift.net""
                },
                ""prod_base_tradeshift_net_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""10.88.0.0/16""
                },
                ""prod_base_tradeshift_net_vpc_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""vpc-264c9042""
                },
                ""prod_ts_sv_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""52.50.175.210/32,52.30.44.136/32,52.50.189.21/32""
                },
                ""test_bwtsi_cn_vpc_cdir"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/19""
                },
                ""test_bwtsi_cn_vpc_private_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.12.0/22,192.168.16.0/22""
                },
                ""test_bwtsi_cn_vpc_public_subnets"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""192.168.0.0/22,192.168.4.0/22""
                },
                ""tradeshift_com_public_zone_id"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""Z2OX0I8SM94425""
                },
                ""translation_service_clients"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""5.56.144.196/32,5.56.144.198/32,4.53.137.82/32,173.247.199.142/32,130.185.137.106/32,202.181.248.19/32,52.17.87.187/32,52.30.44.136/32,52.30.58.232/32,52.31.203.95/32,52.49.119.80/32,52.49.49.111/32,52.49.5.171/32,52.50.108.6/32,52.50.175.210/32,52.50.189.21/32,52.50.255.154/32,52.50.91.189/32,54.195.199.108/32,54.78.39.106/32,58.211.225.90/32,89.150.143.118/32,54.222.150.122/32,54.222.194.56/32,109.70.48.99/32""
                },
                ""truebn_org_nat_gateways"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""34.253.176.198/32,34.241.59.39/32""
                },
                ""userdata_path"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/modules/eff596488a3055adf7024741dffb9ca1/..""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        },
        {
            ""path"": [
                ""root"",
                ""regional-common""
            ],
            ""outputs"": {
                ""availability_zones"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1a,cn-northwest-1b""
                },
                ""backend_region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""base_ami_1604"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-6ec1d50c""
                },
                ""base_ami_1604_zfs"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""ami-edf8ec8f""
                },
                ""chinapay_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""key_name"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""pay_china""
                },
                ""region"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1""
                },
                ""region_dns"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test.bwtsi.cn""
                },
                ""region_env"": {
                    ""sensitive"": false,
                    ""type"": ""string"",
                    ""value"": ""cn-northwest-1.test""
                }
            },
            ""resources"": {},
            ""depends_on"": []
        }
    ]
}

-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go] DEBUG: Response s3/PutObject Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 0
Date: Thu, 26 Apr 2018 11:00:39 GMT
Etag: ""807c608ab629b9b73914da058c923c20""
Server: AmazonS3
X-Amz-Id-2: 5Af+DuU/y0Ngtkb6wCq1R0Oz+ZrOYi4apbPsCZTSlwhSqnqBNB64rFL4HfBG6WB9GETdRmQyKnw=
X-Amz-Request-Id: 7984823D8787C039
X-Amz-Server-Side-Encryption: AES256


-----------------------------------------------------
2018/04/26 19:00:38 [DEBUG] [aws-sdk-go]
2018/04/26 19:00:38 [DEBUG] plugin: waiting for all plugin processes to complete...

Error: Error applying plan:

1 error(s) occurred:

* module.docdb.aws_instance.riak: 1 error(s) occurred:

* aws_instance.riak: UnsupportedOperation: The functionality you requested is not available in this region.
	status code: 400, request id: 53ace3cc-2597-49a0-99b2-3b9f1cb8567d

2018-04-26T19:00:38.547+0800 [DEBUG] plugin.terraform-provider-aws_v1.16.0_x4: 2018/04/26 19:00:38 [ERR] plugin: plugin server: accept unix /var/folders/k0/hk5r4k696x901sk4mh_vwcg40000gn/T/plugin243311916: use of closed network connection
Terraform does not automatically rollback in the face of errors.
Instead, your Terraform state file has been partially updated with
any resources that successfully completed. Please address the error
above and apply again to incrementally change your infrastructure.


2018-04-26T19:00:38.550+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-template_v1.0.0_x4
2018-04-26T19:00:38.551+0800 [DEBUG] plugin: plugin process exited: path=/Users/jordanhuang/bwts-orchestration/terraform/bwtsi.cn/china-pay/cn-northwest-1/riak/.terraform/plugins/darwin_amd64/terraform-provider-aws_v1.16.0_x4
2018-04-26T19:00:38.555+0800 [DEBUG] plugin: plugin process exited: path=/usr/local/bin/terraform-provider-alicloud

```
# Expected Behavior
can create aws instance in China AWS
# Actual Behavior
I can create aws instance in China AWS with provider.aws v1.15.0
but I can't create aws instance with provider.aws v1.16.0",f35050bdeefa2a2d9f354c0e699a5f2d7c393365,https://github.com/hashicorp/terraform-provider-aws
47,3383.0,"When running `terraform apply` the console runs indefinitely if the user has an invalid `aws_session_token` but a valid `aws_access_key_id` and `aws_secret_access_key`. 

### Terraform Version
```
$  terraform -v
Terraform v0.11.3
+ provider.aws v1.9.0
+ provider.external v1.0.0
+ provider.null v1.0.0
+ provider.template v1.0.0
```
### AWS Example Credentials File 
```
$ cat ~/.aws/credentials
[default]
aws_access_key_id     = <VALID> OSIAJO22MN2TVEXAMPLE 
aws_secret_access_key = <VALID> erHSKM98c+afa418tkewJRjsdaJMxuiSgO99Xi7cE
aws_session_token     = <INVALID> FQoDYXdzEH8aDiudh41nfgrZUgNBtSL8AbD/RaUtmT3amBVTPQkjQqgNUXLhVjFGmq3Sn/1sXxL+gn4znkesHnYy3oTi4AsqoFnpZAjK95OC9+Z/7kJbZUmW05u4fGy7xO9ONqEh7eb+tJNjfXS4zwKrigy86HYEqo9omWLvNFsY5/rmHT3IIBtKfduKmbo0UehLCcBWymjlEZBn5xGiac/F4xG52WirAf8cF5JWH6u6b9A3aRdjFq2+Q//tBdcN6D/6OYH9AMMHVSQtaccFqCB8IOoK4+YLbn+qweo26LZOIpsx6XJMb/E+Lw9M4pBLKVCkY0KjemaIjloozWHL9KHO0a0OyFkesammoXDKmA/o4tivxyiz15LUBQ==
``` 

### Debug Output
```
$ terraform apply -var-file desired_cluster_profile.tfvars -var aws_profile=359820441116_Mesosphere-PowerUsers -var state=upgrade
data.external.whoami: Refreshing state...
data.template_file.os-setup: Refreshing state...
data.template_file.aws_ami: Refreshing state...
data.template_file.aws_ami_user: Refreshing state...
data.template_file.os-setup: Refreshing state...
data.template_file.aws_ami: Refreshing state...
data.template_file.aws_ami_user: Refreshing state...
^CInterrupt received.
Please wait for Terraform to exit or data loss may occur.
Gracefully shutting down...
v^CTwo interrupts received. Exiting immediately. Note that data
loss may have occurred.
Error: Error refreshing state: 2 error(s) occurred:

* provider.aws: NoCredentialProviders: no valid providers in chain. Deprecated.
	For verbose messaging see aws.Config.CredentialsChainVerboseErrors
* provider.aws.bursted-vpc: NoCredentialProviders: no valid providers in chain. Deprecated.
	For verbose messaging see aws.Config.CredentialsChainVerboseErrors

```


### Expected Behavior
What should have happened?
Error out asap.

### Actual Behavior
What actually happened?
Runs indefinitely. 

### Steps to Reproduce
Please list the steps required to reproduce the issue, for example:
1. `terraform apply`
",FQoDYXdzEH8aDiudh41nfgrZUgNBtSL8AbD+,https://github.com/hashicorp/terraform-provider-aws
48,3383.0,"When running `terraform apply` the console runs indefinitely if the user has an invalid `aws_session_token` but a valid `aws_access_key_id` and `aws_secret_access_key`. 

### Terraform Version
```
$  terraform -v
Terraform v0.11.3
+ provider.aws v1.9.0
+ provider.external v1.0.0
+ provider.null v1.0.0
+ provider.template v1.0.0
```
### AWS Example Credentials File 
```
$ cat ~/.aws/credentials
[default]
aws_access_key_id     = <VALID> OSIAJO22MN2TVEXAMPLE 
aws_secret_access_key = <VALID> erHSKM98c+afa418tkewJRjsdaJMxuiSgO99Xi7cE
aws_session_token     = <INVALID> FQoDYXdzEH8aDiudh41nfgrZUgNBtSL8AbD/RaUtmT3amBVTPQkjQqgNUXLhVjFGmq3Sn/1sXxL+gn4znkesHnYy3oTi4AsqoFnpZAjK95OC9+Z/7kJbZUmW05u4fGy7xO9ONqEh7eb+tJNjfXS4zwKrigy86HYEqo9omWLvNFsY5/rmHT3IIBtKfduKmbo0UehLCcBWymjlEZBn5xGiac/F4xG52WirAf8cF5JWH6u6b9A3aRdjFq2+Q//tBdcN6D/6OYH9AMMHVSQtaccFqCB8IOoK4+YLbn+qweo26LZOIpsx6XJMb/E+Lw9M4pBLKVCkY0KjemaIjloozWHL9KHO0a0OyFkesammoXDKmA/o4tivxyiz15LUBQ==
``` 

### Debug Output
```
$ terraform apply -var-file desired_cluster_profile.tfvars -var aws_profile=359820441116_Mesosphere-PowerUsers -var state=upgrade
data.external.whoami: Refreshing state...
data.template_file.os-setup: Refreshing state...
data.template_file.aws_ami: Refreshing state...
data.template_file.aws_ami_user: Refreshing state...
data.template_file.os-setup: Refreshing state...
data.template_file.aws_ami: Refreshing state...
data.template_file.aws_ami_user: Refreshing state...
^CInterrupt received.
Please wait for Terraform to exit or data loss may occur.
Gracefully shutting down...
v^CTwo interrupts received. Exiting immediately. Note that data
loss may have occurred.
Error: Error refreshing state: 2 error(s) occurred:

* provider.aws: NoCredentialProviders: no valid providers in chain. Deprecated.
	For verbose messaging see aws.Config.CredentialsChainVerboseErrors
* provider.aws.bursted-vpc: NoCredentialProviders: no valid providers in chain. Deprecated.
	For verbose messaging see aws.Config.CredentialsChainVerboseErrors

```


### Expected Behavior
What should have happened?
Error out asap.

### Actual Behavior
What actually happened?
Runs indefinitely. 

### Steps to Reproduce
Please list the steps required to reproduce the issue, for example:
1. `terraform apply`
",afa418tkewJRjsdaJMxuiSgO99Xi7cE,https://github.com/hashicorp/terraform-provider-aws
49,3383.0,"When running `terraform apply` the console runs indefinitely if the user has an invalid `aws_session_token` but a valid `aws_access_key_id` and `aws_secret_access_key`. 

### Terraform Version
```
$  terraform -v
Terraform v0.11.3
+ provider.aws v1.9.0
+ provider.external v1.0.0
+ provider.null v1.0.0
+ provider.template v1.0.0
```
### AWS Example Credentials File 
```
$ cat ~/.aws/credentials
[default]
aws_access_key_id     = <VALID> OSIAJO22MN2TVEXAMPLE 
aws_secret_access_key = <VALID> erHSKM98c+afa418tkewJRjsdaJMxuiSgO99Xi7cE
aws_session_token     = <INVALID> FQoDYXdzEH8aDiudh41nfgrZUgNBtSL8AbD/RaUtmT3amBVTPQkjQqgNUXLhVjFGmq3Sn/1sXxL+gn4znkesHnYy3oTi4AsqoFnpZAjK95OC9+Z/7kJbZUmW05u4fGy7xO9ONqEh7eb+tJNjfXS4zwKrigy86HYEqo9omWLvNFsY5/rmHT3IIBtKfduKmbo0UehLCcBWymjlEZBn5xGiac/F4xG52WirAf8cF5JWH6u6b9A3aRdjFq2+Q//tBdcN6D/6OYH9AMMHVSQtaccFqCB8IOoK4+YLbn+qweo26LZOIpsx6XJMb/E+Lw9M4pBLKVCkY0KjemaIjloozWHL9KHO0a0OyFkesammoXDKmA/o4tivxyiz15LUBQ==
``` 

### Debug Output
```
$ terraform apply -var-file desired_cluster_profile.tfvars -var aws_profile=359820441116_Mesosphere-PowerUsers -var state=upgrade
data.external.whoami: Refreshing state...
data.template_file.os-setup: Refreshing state...
data.template_file.aws_ami: Refreshing state...
data.template_file.aws_ami_user: Refreshing state...
data.template_file.os-setup: Refreshing state...
data.template_file.aws_ami: Refreshing state...
data.template_file.aws_ami_user: Refreshing state...
^CInterrupt received.
Please wait for Terraform to exit or data loss may occur.
Gracefully shutting down...
v^CTwo interrupts received. Exiting immediately. Note that data
loss may have occurred.
Error: Error refreshing state: 2 error(s) occurred:

* provider.aws: NoCredentialProviders: no valid providers in chain. Deprecated.
	For verbose messaging see aws.Config.CredentialsChainVerboseErrors
* provider.aws.bursted-vpc: NoCredentialProviders: no valid providers in chain. Deprecated.
	For verbose messaging see aws.Config.CredentialsChainVerboseErrors

```


### Expected Behavior
What should have happened?
Error out asap.

### Actual Behavior
What actually happened?
Runs indefinitely. 

### Steps to Reproduce
Please list the steps required to reproduce the issue, for example:
1. `terraform apply`
",aws_access_key_id,https://github.com/hashicorp/terraform-provider-aws
50,3383.0,"When running `terraform apply` the console runs indefinitely if the user has an invalid `aws_session_token` but a valid `aws_access_key_id` and `aws_secret_access_key`. 

### Terraform Version
```
$  terraform -v
Terraform v0.11.3
+ provider.aws v1.9.0
+ provider.external v1.0.0
+ provider.null v1.0.0
+ provider.template v1.0.0
```
### AWS Example Credentials File 
```
$ cat ~/.aws/credentials
[default]
aws_access_key_id     = <VALID> OSIAJO22MN2TVEXAMPLE 
aws_secret_access_key = <VALID> erHSKM98c+afa418tkewJRjsdaJMxuiSgO99Xi7cE
aws_session_token     = <INVALID> FQoDYXdzEH8aDiudh41nfgrZUgNBtSL8AbD/RaUtmT3amBVTPQkjQqgNUXLhVjFGmq3Sn/1sXxL+gn4znkesHnYy3oTi4AsqoFnpZAjK95OC9+Z/7kJbZUmW05u4fGy7xO9ONqEh7eb+tJNjfXS4zwKrigy86HYEqo9omWLvNFsY5/rmHT3IIBtKfduKmbo0UehLCcBWymjlEZBn5xGiac/F4xG52WirAf8cF5JWH6u6b9A3aRdjFq2+Q//tBdcN6D/6OYH9AMMHVSQtaccFqCB8IOoK4+YLbn+qweo26LZOIpsx6XJMb/E+Lw9M4pBLKVCkY0KjemaIjloozWHL9KHO0a0OyFkesammoXDKmA/o4tivxyiz15LUBQ==
``` 

### Debug Output
```
$ terraform apply -var-file desired_cluster_profile.tfvars -var aws_profile=359820441116_Mesosphere-PowerUsers -var state=upgrade
data.external.whoami: Refreshing state...
data.template_file.os-setup: Refreshing state...
data.template_file.aws_ami: Refreshing state...
data.template_file.aws_ami_user: Refreshing state...
data.template_file.os-setup: Refreshing state...
data.template_file.aws_ami: Refreshing state...
data.template_file.aws_ami_user: Refreshing state...
^CInterrupt received.
Please wait for Terraform to exit or data loss may occur.
Gracefully shutting down...
v^CTwo interrupts received. Exiting immediately. Note that data
loss may have occurred.
Error: Error refreshing state: 2 error(s) occurred:

* provider.aws: NoCredentialProviders: no valid providers in chain. Deprecated.
	For verbose messaging see aws.Config.CredentialsChainVerboseErrors
* provider.aws.bursted-vpc: NoCredentialProviders: no valid providers in chain. Deprecated.
	For verbose messaging see aws.Config.CredentialsChainVerboseErrors

```


### Expected Behavior
What should have happened?
Error out asap.

### Actual Behavior
What actually happened?
Runs indefinitely. 

### Steps to Reproduce
Please list the steps required to reproduce the issue, for example:
1. `terraform apply`
",aws_secret_access_key,https://github.com/hashicorp/terraform-provider-aws
51,3383.0,"When running `terraform apply` the console runs indefinitely if the user has an invalid `aws_session_token` but a valid `aws_access_key_id` and `aws_secret_access_key`. 

### Terraform Version
```
$  terraform -v
Terraform v0.11.3
+ provider.aws v1.9.0
+ provider.external v1.0.0
+ provider.null v1.0.0
+ provider.template v1.0.0
```
### AWS Example Credentials File 
```
$ cat ~/.aws/credentials
[default]
aws_access_key_id     = <VALID> OSIAJO22MN2TVEXAMPLE 
aws_secret_access_key = <VALID> erHSKM98c+afa418tkewJRjsdaJMxuiSgO99Xi7cE
aws_session_token     = <INVALID> FQoDYXdzEH8aDiudh41nfgrZUgNBtSL8AbD/RaUtmT3amBVTPQkjQqgNUXLhVjFGmq3Sn/1sXxL+gn4znkesHnYy3oTi4AsqoFnpZAjK95OC9+Z/7kJbZUmW05u4fGy7xO9ONqEh7eb+tJNjfXS4zwKrigy86HYEqo9omWLvNFsY5/rmHT3IIBtKfduKmbo0UehLCcBWymjlEZBn5xGiac/F4xG52WirAf8cF5JWH6u6b9A3aRdjFq2+Q//tBdcN6D/6OYH9AMMHVSQtaccFqCB8IOoK4+YLbn+qweo26LZOIpsx6XJMb/E+Lw9M4pBLKVCkY0KjemaIjloozWHL9KHO0a0OyFkesammoXDKmA/o4tivxyiz15LUBQ==
``` 

### Debug Output
```
$ terraform apply -var-file desired_cluster_profile.tfvars -var aws_profile=359820441116_Mesosphere-PowerUsers -var state=upgrade
data.external.whoami: Refreshing state...
data.template_file.os-setup: Refreshing state...
data.template_file.aws_ami: Refreshing state...
data.template_file.aws_ami_user: Refreshing state...
data.template_file.os-setup: Refreshing state...
data.template_file.aws_ami: Refreshing state...
data.template_file.aws_ami_user: Refreshing state...
^CInterrupt received.
Please wait for Terraform to exit or data loss may occur.
Gracefully shutting down...
v^CTwo interrupts received. Exiting immediately. Note that data
loss may have occurred.
Error: Error refreshing state: 2 error(s) occurred:

* provider.aws: NoCredentialProviders: no valid providers in chain. Deprecated.
	For verbose messaging see aws.Config.CredentialsChainVerboseErrors
* provider.aws.bursted-vpc: NoCredentialProviders: no valid providers in chain. Deprecated.
	For verbose messaging see aws.Config.CredentialsChainVerboseErrors

```


### Expected Behavior
What should have happened?
Error out asap.

### Actual Behavior
What actually happened?
Runs indefinitely. 

### Steps to Reproduce
Please list the steps required to reproduce the issue, for example:
1. `terraform apply`
",OSIAJO22MN2TVEXAMPLE,https://github.com/hashicorp/terraform-provider-aws
52,1854.0,"This is to address the following test failure from this morning:

```
=== RUN   TestAccAWSEBSVolume_kmsKey
--- FAIL: TestAccAWSEBSVolume_kmsKey (22.84s)
    testing.go:434: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40' does not exist
            status code: 400, request id: f2006223-ad80-11e7-8981-d922c7ad0e59
```

Snippet from related debug log:

```
-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Response kms/DescribeKey Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 373
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: f1fe8d88-ad80-11e7-a270-333f20b6475c


-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] {""KeyMetadata"":{""AWSAccountId"":""187416307283"",""Arn"":""arn:aws:kms:us-west-2:187416307283:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40"",""CreationDate"":1.507615498459E9,""Description"":""Terraform acc test 4796457295038973391"",""Enabled"":true,""KeyId"":""eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40"",""KeyManager"":""CUSTOMER"",""KeyState"":""Enabled"",""KeyUsage"":""ENCRYPT_DECRYPT"",""Origin"":""AWS_KMS""}}
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Request kms/GetKeyPolicy Details:
---[ REQUEST POST-SIGN ]-----------------------------
POST / HTTP/1.1
Host: kms.us-west-2.amazonaws.com
User-Agent: aws-sdk-go/1.12.6 (go1.9; linux; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.10.0-dev
Content-Length: 71
Authorization: AWS4-HMAC-SHA256 Credential=*REDACTED/20171010/us-west-2/kms/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-target, Signature=*REDACTED*
Content-Type: application/x-amz-json-1.1
X-Amz-Date: 20171010T060458Z
X-Amz-Target: TrentService.GetKeyPolicy
Accept-Encoding: gzip

{""KeyId"":""eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40"",""PolicyName"":""default""}
-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Response kms/GetKeyPolicy Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 400 Bad Request
Connection: close
Content-Length: 139
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: f2006223-ad80-11e7-8981-d922c7ad0e59


-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] {""__type"":""NotFoundException"",""message"":""Key 'arn:aws:kms:us-west-2:187416307283:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40' does not exist""}
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Validate Response kms/GetKeyPolicy failed, not retrying, error NotFoundException: Key 'arn:aws:kms:us-west-2:187416307283:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40' does not exist
	status code: 400, request id: f2006223-ad80-11e7-8981-d922c7ad0e59
```

![kitty-whack-a-mole1](https://user-images.githubusercontent.com/287584/31406016-b5c46140-adf7-11e7-8bd5-57357966668c.gif)
",TestAccAWSEBSVolume_kmsKey,https://github.com/hashicorp/terraform-provider-aws
53,1854.0,"This is to address the following test failure from this morning:

```
=== RUN   TestAccAWSEBSVolume_kmsKey
--- FAIL: TestAccAWSEBSVolume_kmsKey (22.84s)
    testing.go:434: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40' does not exist
            status code: 400, request id: f2006223-ad80-11e7-8981-d922c7ad0e59
```

Snippet from related debug log:

```
-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Response kms/DescribeKey Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 373
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: f1fe8d88-ad80-11e7-a270-333f20b6475c


-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] {""KeyMetadata"":{""AWSAccountId"":""187416307283"",""Arn"":""arn:aws:kms:us-west-2:187416307283:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40"",""CreationDate"":1.507615498459E9,""Description"":""Terraform acc test 4796457295038973391"",""Enabled"":true,""KeyId"":""eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40"",""KeyManager"":""CUSTOMER"",""KeyState"":""Enabled"",""KeyUsage"":""ENCRYPT_DECRYPT"",""Origin"":""AWS_KMS""}}
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Request kms/GetKeyPolicy Details:
---[ REQUEST POST-SIGN ]-----------------------------
POST / HTTP/1.1
Host: kms.us-west-2.amazonaws.com
User-Agent: aws-sdk-go/1.12.6 (go1.9; linux; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.10.0-dev
Content-Length: 71
Authorization: AWS4-HMAC-SHA256 Credential=*REDACTED/20171010/us-west-2/kms/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-target, Signature=*REDACTED*
Content-Type: application/x-amz-json-1.1
X-Amz-Date: 20171010T060458Z
X-Amz-Target: TrentService.GetKeyPolicy
Accept-Encoding: gzip

{""KeyId"":""eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40"",""PolicyName"":""default""}
-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Response kms/GetKeyPolicy Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 400 Bad Request
Connection: close
Content-Length: 139
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: f2006223-ad80-11e7-8981-d922c7ad0e59


-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] {""__type"":""NotFoundException"",""message"":""Key 'arn:aws:kms:us-west-2:187416307283:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40' does not exist""}
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Validate Response kms/GetKeyPolicy failed, not retrying, error NotFoundException: Key 'arn:aws:kms:us-west-2:187416307283:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40' does not exist
	status code: 400, request id: f2006223-ad80-11e7-8981-d922c7ad0e59
```

![kitty-whack-a-mole1](https://user-images.githubusercontent.com/287584/31406016-b5c46140-adf7-11e7-8bd5-57357966668c.gif)
",187416307283:key,https://github.com/hashicorp/terraform-provider-aws
54,1854.0,"This is to address the following test failure from this morning:

```
=== RUN   TestAccAWSEBSVolume_kmsKey
--- FAIL: TestAccAWSEBSVolume_kmsKey (22.84s)
    testing.go:434: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40' does not exist
            status code: 400, request id: f2006223-ad80-11e7-8981-d922c7ad0e59
```

Snippet from related debug log:

```
-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Response kms/DescribeKey Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 373
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: f1fe8d88-ad80-11e7-a270-333f20b6475c


-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] {""KeyMetadata"":{""AWSAccountId"":""187416307283"",""Arn"":""arn:aws:kms:us-west-2:187416307283:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40"",""CreationDate"":1.507615498459E9,""Description"":""Terraform acc test 4796457295038973391"",""Enabled"":true,""KeyId"":""eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40"",""KeyManager"":""CUSTOMER"",""KeyState"":""Enabled"",""KeyUsage"":""ENCRYPT_DECRYPT"",""Origin"":""AWS_KMS""}}
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Request kms/GetKeyPolicy Details:
---[ REQUEST POST-SIGN ]-----------------------------
POST / HTTP/1.1
Host: kms.us-west-2.amazonaws.com
User-Agent: aws-sdk-go/1.12.6 (go1.9; linux; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.10.0-dev
Content-Length: 71
Authorization: AWS4-HMAC-SHA256 Credential=*REDACTED/20171010/us-west-2/kms/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-target, Signature=*REDACTED*
Content-Type: application/x-amz-json-1.1
X-Amz-Date: 20171010T060458Z
X-Amz-Target: TrentService.GetKeyPolicy
Accept-Encoding: gzip

{""KeyId"":""eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40"",""PolicyName"":""default""}
-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Response kms/GetKeyPolicy Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 400 Bad Request
Connection: close
Content-Length: 139
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: f2006223-ad80-11e7-8981-d922c7ad0e59


-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] {""__type"":""NotFoundException"",""message"":""Key 'arn:aws:kms:us-west-2:187416307283:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40' does not exist""}
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Validate Response kms/GetKeyPolicy failed, not retrying, error NotFoundException: Key 'arn:aws:kms:us-west-2:187416307283:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40' does not exist
	status code: 400, request id: f2006223-ad80-11e7-8981-d922c7ad0e59
```

![kitty-whack-a-mole1](https://user-images.githubusercontent.com/287584/31406016-b5c46140-adf7-11e7-8bd5-57357966668c.gif)
",west-2:*******:key,https://github.com/hashicorp/terraform-provider-aws
55,1854.0,"This is to address the following test failure from this morning:

```
=== RUN   TestAccAWSEBSVolume_kmsKey
--- FAIL: TestAccAWSEBSVolume_kmsKey (22.84s)
    testing.go:434: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40' does not exist
            status code: 400, request id: f2006223-ad80-11e7-8981-d922c7ad0e59
```

Snippet from related debug log:

```
-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Response kms/DescribeKey Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 373
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: f1fe8d88-ad80-11e7-a270-333f20b6475c


-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] {""KeyMetadata"":{""AWSAccountId"":""187416307283"",""Arn"":""arn:aws:kms:us-west-2:187416307283:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40"",""CreationDate"":1.507615498459E9,""Description"":""Terraform acc test 4796457295038973391"",""Enabled"":true,""KeyId"":""eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40"",""KeyManager"":""CUSTOMER"",""KeyState"":""Enabled"",""KeyUsage"":""ENCRYPT_DECRYPT"",""Origin"":""AWS_KMS""}}
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Request kms/GetKeyPolicy Details:
---[ REQUEST POST-SIGN ]-----------------------------
POST / HTTP/1.1
Host: kms.us-west-2.amazonaws.com
User-Agent: aws-sdk-go/1.12.6 (go1.9; linux; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.10.0-dev
Content-Length: 71
Authorization: AWS4-HMAC-SHA256 Credential=*REDACTED/20171010/us-west-2/kms/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-target, Signature=*REDACTED*
Content-Type: application/x-amz-json-1.1
X-Amz-Date: 20171010T060458Z
X-Amz-Target: TrentService.GetKeyPolicy
Accept-Encoding: gzip

{""KeyId"":""eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40"",""PolicyName"":""default""}
-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Response kms/GetKeyPolicy Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 400 Bad Request
Connection: close
Content-Length: 139
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: f2006223-ad80-11e7-8981-d922c7ad0e59


-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] {""__type"":""NotFoundException"",""message"":""Key 'arn:aws:kms:us-west-2:187416307283:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40' does not exist""}
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Validate Response kms/GetKeyPolicy failed, not retrying, error NotFoundException: Key 'arn:aws:kms:us-west-2:187416307283:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40' does not exist
	status code: 400, request id: f2006223-ad80-11e7-8981-d922c7ad0e59
```

![kitty-whack-a-mole1](https://user-images.githubusercontent.com/287584/31406016-b5c46140-adf7-11e7-8bd5-57357966668c.gif)
","KeyUsage:ENCRYPT_DECRYPT,Origin:AWS_KMS",https://github.com/hashicorp/terraform-provider-aws
56,1854.0,"This is to address the following test failure from this morning:

```
=== RUN   TestAccAWSEBSVolume_kmsKey
--- FAIL: TestAccAWSEBSVolume_kmsKey (22.84s)
    testing.go:434: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40' does not exist
            status code: 400, request id: f2006223-ad80-11e7-8981-d922c7ad0e59
```

Snippet from related debug log:

```
-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Response kms/DescribeKey Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 373
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: f1fe8d88-ad80-11e7-a270-333f20b6475c


-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] {""KeyMetadata"":{""AWSAccountId"":""187416307283"",""Arn"":""arn:aws:kms:us-west-2:187416307283:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40"",""CreationDate"":1.507615498459E9,""Description"":""Terraform acc test 4796457295038973391"",""Enabled"":true,""KeyId"":""eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40"",""KeyManager"":""CUSTOMER"",""KeyState"":""Enabled"",""KeyUsage"":""ENCRYPT_DECRYPT"",""Origin"":""AWS_KMS""}}
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Request kms/GetKeyPolicy Details:
---[ REQUEST POST-SIGN ]-----------------------------
POST / HTTP/1.1
Host: kms.us-west-2.amazonaws.com
User-Agent: aws-sdk-go/1.12.6 (go1.9; linux; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.10.0-dev
Content-Length: 71
Authorization: AWS4-HMAC-SHA256 Credential=*REDACTED/20171010/us-west-2/kms/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-target, Signature=*REDACTED*
Content-Type: application/x-amz-json-1.1
X-Amz-Date: 20171010T060458Z
X-Amz-Target: TrentService.GetKeyPolicy
Accept-Encoding: gzip

{""KeyId"":""eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40"",""PolicyName"":""default""}
-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Response kms/GetKeyPolicy Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 400 Bad Request
Connection: close
Content-Length: 139
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: f2006223-ad80-11e7-8981-d922c7ad0e59


-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] {""__type"":""NotFoundException"",""message"":""Key 'arn:aws:kms:us-west-2:187416307283:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40' does not exist""}
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Validate Response kms/GetKeyPolicy failed, not retrying, error NotFoundException: Key 'arn:aws:kms:us-west-2:187416307283:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40' does not exist
	status code: 400, request id: f2006223-ad80-11e7-8981-d922c7ad0e59
```

![kitty-whack-a-mole1](https://user-images.githubusercontent.com/287584/31406016-b5c46140-adf7-11e7-8bd5-57357966668c.gif)
",PolicyName:default,https://github.com/hashicorp/terraform-provider-aws
57,1854.0,"This is to address the following test failure from this morning:

```
=== RUN   TestAccAWSEBSVolume_kmsKey
--- FAIL: TestAccAWSEBSVolume_kmsKey (22.84s)
    testing.go:434: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40' does not exist
            status code: 400, request id: f2006223-ad80-11e7-8981-d922c7ad0e59
```

Snippet from related debug log:

```
-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Response kms/DescribeKey Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 373
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: f1fe8d88-ad80-11e7-a270-333f20b6475c


-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] {""KeyMetadata"":{""AWSAccountId"":""187416307283"",""Arn"":""arn:aws:kms:us-west-2:187416307283:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40"",""CreationDate"":1.507615498459E9,""Description"":""Terraform acc test 4796457295038973391"",""Enabled"":true,""KeyId"":""eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40"",""KeyManager"":""CUSTOMER"",""KeyState"":""Enabled"",""KeyUsage"":""ENCRYPT_DECRYPT"",""Origin"":""AWS_KMS""}}
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Request kms/GetKeyPolicy Details:
---[ REQUEST POST-SIGN ]-----------------------------
POST / HTTP/1.1
Host: kms.us-west-2.amazonaws.com
User-Agent: aws-sdk-go/1.12.6 (go1.9; linux; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.10.0-dev
Content-Length: 71
Authorization: AWS4-HMAC-SHA256 Credential=*REDACTED/20171010/us-west-2/kms/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-target, Signature=*REDACTED*
Content-Type: application/x-amz-json-1.1
X-Amz-Date: 20171010T060458Z
X-Amz-Target: TrentService.GetKeyPolicy
Accept-Encoding: gzip

{""KeyId"":""eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40"",""PolicyName"":""default""}
-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Response kms/GetKeyPolicy Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 400 Bad Request
Connection: close
Content-Length: 139
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: f2006223-ad80-11e7-8981-d922c7ad0e59


-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] {""__type"":""NotFoundException"",""message"":""Key 'arn:aws:kms:us-west-2:187416307283:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40' does not exist""}
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Validate Response kms/GetKeyPolicy failed, not retrying, error NotFoundException: Key 'arn:aws:kms:us-west-2:187416307283:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40' does not exist
	status code: 400, request id: f2006223-ad80-11e7-8981-d922c7ad0e59
```

![kitty-whack-a-mole1](https://user-images.githubusercontent.com/287584/31406016-b5c46140-adf7-11e7-8bd5-57357966668c.gif)
","arn:aws:kms:us-west-2:187416307283:key,CreationDate:,Description",https://github.com/hashicorp/terraform-provider-aws
58,1854.0,"This is to address the following test failure from this morning:

```
=== RUN   TestAccAWSEBSVolume_kmsKey
--- FAIL: TestAccAWSEBSVolume_kmsKey (22.84s)
    testing.go:434: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40' does not exist
            status code: 400, request id: f2006223-ad80-11e7-8981-d922c7ad0e59
```

Snippet from related debug log:

```
-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Response kms/DescribeKey Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 373
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: f1fe8d88-ad80-11e7-a270-333f20b6475c


-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] {""KeyMetadata"":{""AWSAccountId"":""187416307283"",""Arn"":""arn:aws:kms:us-west-2:187416307283:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40"",""CreationDate"":1.507615498459E9,""Description"":""Terraform acc test 4796457295038973391"",""Enabled"":true,""KeyId"":""eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40"",""KeyManager"":""CUSTOMER"",""KeyState"":""Enabled"",""KeyUsage"":""ENCRYPT_DECRYPT"",""Origin"":""AWS_KMS""}}
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Request kms/GetKeyPolicy Details:
---[ REQUEST POST-SIGN ]-----------------------------
POST / HTTP/1.1
Host: kms.us-west-2.amazonaws.com
User-Agent: aws-sdk-go/1.12.6 (go1.9; linux; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.10.0-dev
Content-Length: 71
Authorization: AWS4-HMAC-SHA256 Credential=*REDACTED/20171010/us-west-2/kms/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-target, Signature=*REDACTED*
Content-Type: application/x-amz-json-1.1
X-Amz-Date: 20171010T060458Z
X-Amz-Target: TrentService.GetKeyPolicy
Accept-Encoding: gzip

{""KeyId"":""eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40"",""PolicyName"":""default""}
-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Response kms/GetKeyPolicy Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 400 Bad Request
Connection: close
Content-Length: 139
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: f2006223-ad80-11e7-8981-d922c7ad0e59


-----------------------------------------------------
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] {""__type"":""NotFoundException"",""message"":""Key 'arn:aws:kms:us-west-2:187416307283:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40' does not exist""}
2017/10/10 06:04:58 [DEBUG] [aws-sdk-go] DEBUG: Validate Response kms/GetKeyPolicy failed, not retrying, error NotFoundException: Key 'arn:aws:kms:us-west-2:187416307283:key/eb421bb3-2865-4d7d-b7c6-2c2c3a07dc40' does not exist
	status code: 400, request id: f2006223-ad80-11e7-8981-d922c7ad0e59
```

![kitty-whack-a-mole1](https://user-images.githubusercontent.com/287584/31406016-b5c46140-adf7-11e7-8bd5-57357966668c.gif)
",length;content-type;host;x-amz-date;x-amz-target,https://github.com/hashicorp/terraform-provider-aws
59,1086.0,"### Terraform Version
0.9.11

### Affected Resource(s)
- aws_provider

### Terraform Configuration Files
```hcl
provider ""vault"" {
  address = ""http://111.222.333.444:8200""
  skip_tls_verify = ""true""
}

data ""vault_generic_secret"" ""aws_iam_keys"" {
  path = ""aws/creds/admin""
}

provider ""aws"" {
  region = ""${var.region}""
  access_key = ""${data.vault_generic_secret.aws_iam_keys.data[""access_key""]}""
  secret_key = ""${data.vault_generic_secret.aws_iam_keys.data[""secret_key""]}""
```

### Debug Output
I am watching the traffic between Terraform and Vault with tcpdump, since it's plain text.
I see the AWS keys, generated by Vault, being returned to Terraform in JSON format.

```
GET /v1/aws/creds/admin HTTP/1.1
Host: 111.222.333.444:8200
User-Agent: Go-http-client/1.1
X-Vault-Token: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Accept-Encoding: gzip
Connection: close

HTTP/1.1 200 OK
Cache-Control: no-store
Content-Type: application/json
Date: Sat, 08 Jul 2017 00:48:22 GMT
Content-Length: 325
Connection: close

{""request_id"":""3ea862c3-f46b-81e5-d954-e8aefa1a9a66"",""lease_id"":""aws/creds/admin/23abf456-2cc6-1d7d-8eb9-f145fb7a9995"",""renewable"":true,""lease_duration"":60,""data"":{""access_key"":""AKIAJQT5JHHFVO6KZOLQ"",""secret_key"":""XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"",""security_token"":null},""wrap_info"":null,""warnings"":null,""auth"":null}
```

I know the AWS key generator in Vault works fine because I've tested it with the command line, and the keys it returns are working just fine. The keys are generated with admin privileges, for testing, and so they should be able to do anything.

### Expected Behavior
`terraform plan` should succeed.

### Actual Behavior
```
Refreshing Terraform state in-memory prior to plan...
The refreshed state will be used to calculate this plan, but will not be
persisted to local or remote state storage.

data.vault_generic_secret.aws_iam_keys: Refreshing state...
Releasing state lock. This may take a few moments...
Error refreshing state: 1 error(s) occurred:

* provider.aws: InvalidClientTokenId: The security token included in the request is invalid.
	status code: 403, request id: f477482f-6379-11e7-8ecb-85e000cae410
```

### Steps to Reproduce
1. `terraform plan`",AKIAJQT5JHHFVO6KZOLQ,https://github.com/hashicorp/terraform-provider-aws
60,1086.0,"### Terraform Version
0.9.11

### Affected Resource(s)
- aws_provider

### Terraform Configuration Files
```hcl
provider ""vault"" {
  address = ""http://111.222.333.444:8200""
  skip_tls_verify = ""true""
}

data ""vault_generic_secret"" ""aws_iam_keys"" {
  path = ""aws/creds/admin""
}

provider ""aws"" {
  region = ""${var.region}""
  access_key = ""${data.vault_generic_secret.aws_iam_keys.data[""access_key""]}""
  secret_key = ""${data.vault_generic_secret.aws_iam_keys.data[""secret_key""]}""
```

### Debug Output
I am watching the traffic between Terraform and Vault with tcpdump, since it's plain text.
I see the AWS keys, generated by Vault, being returned to Terraform in JSON format.

```
GET /v1/aws/creds/admin HTTP/1.1
Host: 111.222.333.444:8200
User-Agent: Go-http-client/1.1
X-Vault-Token: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Accept-Encoding: gzip
Connection: close

HTTP/1.1 200 OK
Cache-Control: no-store
Content-Type: application/json
Date: Sat, 08 Jul 2017 00:48:22 GMT
Content-Length: 325
Connection: close

{""request_id"":""3ea862c3-f46b-81e5-d954-e8aefa1a9a66"",""lease_id"":""aws/creds/admin/23abf456-2cc6-1d7d-8eb9-f145fb7a9995"",""renewable"":true,""lease_duration"":60,""data"":{""access_key"":""AKIAJQT5JHHFVO6KZOLQ"",""secret_key"":""XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"",""security_token"":null},""wrap_info"":null,""warnings"":null,""auth"":null}
```

I know the AWS key generator in Vault works fine because I've tested it with the command line, and the keys it returns are working just fine. The keys are generated with admin privileges, for testing, and so they should be able to do anything.

### Expected Behavior
`terraform plan` should succeed.

### Actual Behavior
```
Refreshing Terraform state in-memory prior to plan...
The refreshed state will be used to calculate this plan, but will not be
persisted to local or remote state storage.

data.vault_generic_secret.aws_iam_keys: Refreshing state...
Releasing state lock. This may take a few moments...
Error refreshing state: 1 error(s) occurred:

* provider.aws: InvalidClientTokenId: The security token included in the request is invalid.
	status code: 403, request id: f477482f-6379-11e7-8ecb-85e000cae410
```

### Steps to Reproduce
1. `terraform plan`",XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX,https://github.com/hashicorp/terraform-provider-aws
61,1086.0,"### Terraform Version
0.9.11

### Affected Resource(s)
- aws_provider

### Terraform Configuration Files
```hcl
provider ""vault"" {
  address = ""http://111.222.333.444:8200""
  skip_tls_verify = ""true""
}

data ""vault_generic_secret"" ""aws_iam_keys"" {
  path = ""aws/creds/admin""
}

provider ""aws"" {
  region = ""${var.region}""
  access_key = ""${data.vault_generic_secret.aws_iam_keys.data[""access_key""]}""
  secret_key = ""${data.vault_generic_secret.aws_iam_keys.data[""secret_key""]}""
```

### Debug Output
I am watching the traffic between Terraform and Vault with tcpdump, since it's plain text.
I see the AWS keys, generated by Vault, being returned to Terraform in JSON format.

```
GET /v1/aws/creds/admin HTTP/1.1
Host: 111.222.333.444:8200
User-Agent: Go-http-client/1.1
X-Vault-Token: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Accept-Encoding: gzip
Connection: close

HTTP/1.1 200 OK
Cache-Control: no-store
Content-Type: application/json
Date: Sat, 08 Jul 2017 00:48:22 GMT
Content-Length: 325
Connection: close

{""request_id"":""3ea862c3-f46b-81e5-d954-e8aefa1a9a66"",""lease_id"":""aws/creds/admin/23abf456-2cc6-1d7d-8eb9-f145fb7a9995"",""renewable"":true,""lease_duration"":60,""data"":{""access_key"":""AKIAJQT5JHHFVO6KZOLQ"",""secret_key"":""XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"",""security_token"":null},""wrap_info"":null,""warnings"":null,""auth"":null}
```

I know the AWS key generator in Vault works fine because I've tested it with the command line, and the keys it returns are working just fine. The keys are generated with admin privileges, for testing, and so they should be able to do anything.

### Expected Behavior
`terraform plan` should succeed.

### Actual Behavior
```
Refreshing Terraform state in-memory prior to plan...
The refreshed state will be used to calculate this plan, but will not be
persisted to local or remote state storage.

data.vault_generic_secret.aws_iam_keys: Refreshing state...
Releasing state lock. This may take a few moments...
Error refreshing state: 1 error(s) occurred:

* provider.aws: InvalidClientTokenId: The security token included in the request is invalid.
	status code: 403, request id: f477482f-6379-11e7-8ecb-85e000cae410
```

### Steps to Reproduce
1. `terraform plan`","XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX,security_token:null},wrap_info",https://github.com/hashicorp/terraform-provider-aws
0,1014.0,"I have uploaded a video to YouTube using this api in firebase cloud function, but the response i get in the call back doesn't contains the video id. 

Code for uploading video 

```js
var yt = google.youtube('v3');
yt.videos.insert({
  part: 'status,snippet',
  resource: {
    snippet: {
      title: ""title"",
      description: ""description""
    },
    status: {
      privacyStatus: 'public'
    }
  },
  media: {
    body: fs.createReadStream(file)
  }
}, function (error, data) {
  console.log(util.inspect(data, false, null));
  console.log(error);
});
```

the logged response is like 

```
{ status: 200,
  statusText: 'OK', 
  headers:  
   { 'x-guploader-uploadid': 'AEnB2UqicLz-FhD6KLtuvX_sZYeYQWuXoTM4FFusI6yCSGGlIdxKu3-EIneHhG04CYPqum1Uz8ISVmwIlzZqHrBJ2w-wOmbb6t5hEqOUrPnfRxirEd2tHfM',
 etag: '""_gJQceDMxJ8gP-8T2HLXUoURK8c/-WggxUHM10yHZDXznWuNSxlnSYU""',
 vary: 'Origin, X-Origin',
 'x-goog-correlation-id': 'zaYeqX5ngEo',
 'content-type': 'application/json; charset=UTF-8',
 'cache-control': 'no-cache, no-store, max-age=0, must-revalidate',
 pragma: 'no-cache',
 expires: 'Mon, 01 Jan 1990 00:00:00 GMT',
 date: 'Wed, 21 Feb 2018 15:19:39 GMT',
 'content-length': '975',
 server: 'UploadServer',
 'alt-svc': 'hq="":443""; ma=2592000; quic=51303431; quic=51303339; quic=51303338; quic=51303337; quic=51303335,quic="":443""; ma=2592000; v=""41,39,38,37,35""',
 connection: 'close' },
  config: 
   { adapter: [Function: httpAdapter],
     transformRequest: { '0': [Function: transformRequest] },
     transformResponse: { '0': [Function: transformResponse] },
     timeout: 0,
     xsrfCookieName: 'XSRF-TOKEN', 
```

Why its not working giving  a video resource in the response body.

",AEnB2UqicLz-FhD6KLtuvX_sZYeYQWuXoTM4FFusI6yCSGGlIdxKu3-EIneHhG04CYPqum1Uz8ISVmwIlzZqHrBJ2w-wOmbb6t5hEqOUrPnfRxirEd2tHfM,https://github.com/googleapis/google-api-nodejs-client
0,15336.0,"### Area

oidc

### Describe the bug

On my Dev server if I'm trying to authenicate an existing user with empty password (the user exists but the password is incorrect) I'm getting a weird error, server shows me `500 Internal Server Error`.

### Version

20.0.0

### Expected behavior

401 Unauthorized

### Actual behavior

500 Internal Server Error

### How to Reproduce?

Basically if I'm trying to do something like this (use correct client credentials in Basic Authorization):
```
curl -v --location --request POST 'http://192.168.99.104:8180/realms/quiz/protocol/openid-connect/token' \
--header 'Authorization: Basic cXVpei1zZXJ2aWNlOjdjNzk1MzlkLTBkYTktNDk0YS05OWIyLTVkZjdlN2M2NTBhZg==' \
--header 'Content-Type: application/x-www-form-urlencoded' \
--data-urlencode 'client_id=quiz-service' \
--data-urlencode 'username=quiz_user' \
--data-urlencode 'password=' \
--data-urlencode 'grant_type=password'
```
I'm getting this:
```
< HTTP/1.1 500 Internal Server Error
< Referrer-Policy: no-referrer
< X-Frame-Options: SAMEORIGIN
< Strict-Transport-Security: max-age=31536000; includeSubDomains
< Cache-Control: no-store
< X-Content-Type-Options: nosniff
< Pragma: no-cache
< X-XSS-Protection: 1; mode=block
< Content-Type: application/json
< content-length: 25
<
{""error"":""unknown_error""}
```

### Anything else?

Stack Trace from console log:
```
keycloak_1  | 2022-11-03 22:06:40,520 ERROR [org.keycloak.services.error.KeycloakErrorHandler] (executor-thread-0) Uncaught server error: java.lang.RuntimeException: java.lang.IllegalArgumentException: password empty
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.encodedCredential(Pbkdf2PasswordHashProvider.java:116)
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.verify(Pbkdf2PasswordHashProvider.java:92)
keycloak_1  |   at org.keycloak.credential.PasswordCredentialProvider.isValid(PasswordCredentialProvider.java:177)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.lambda$validate$11(LegacyUserCredentialManager.java:255)
keycloak_1  |   at java.base/java.util.Collection.removeIf(Collection.java:544)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.validate(LegacyUserCredentialManager.java:255)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.lambda$isValid$0(LegacyUserCredentialManager.java:76)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
keycloak_1  |   at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1693)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.isValid(LegacyUserCredentialManager.java:76)
keycloak_1  |   at org.keycloak.models.SubjectCredentialManager.isValid(SubjectCredentialManager.java:45)
keycloak_1  |   at org.keycloak.authentication.authenticators.directgrant.ValidatePassword.authenticate(ValidatePassword.java:47)
keycloak_1  |   at org.keycloak.authentication.DefaultAuthenticationFlow.processSingleFlowExecutionModel(DefaultAuthenticationFlow.java:446)
keycloak_1  |   at org.keycloak.authentication.DefaultAuthenticationFlow.processFlow(DefaultAuthenticationFlow.java:250)
keycloak_1  |   at org.keycloak.authentication.AuthenticationProcessor.authenticateOnly(AuthenticationProcessor.java:1017)
keycloak_1  |   at org.keycloak.protocol.oidc.endpoints.TokenEndpoint.resourceOwnerPasswordCredentialsGrant(TokenEndpoint.java:627)
keycloak_1  |   at org.keycloak.protocol.oidc.endpoints.TokenEndpoint.processGrantRequest(TokenEndpoint.java:208)
keycloak_1  |   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
keycloak_1  |   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
keycloak_1  |   at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
keycloak_1  |   at java.base/java.lang.reflect.Method.invoke(Method.java:566)
keycloak_1  |   at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:170)
keycloak_1  |   at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:130)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.internalInvokeOnTarget(ResourceMethodInvoker.java:660)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTargetAfterFilter(ResourceMethodInvoker.java:524)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.lambda$invokeOnTarget$2(ResourceMethodInvoker.java:474)
keycloak_1  |   at org.jboss.resteasy.core.interception.jaxrs.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:364)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTarget(ResourceMethodInvoker.java:476)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:434)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:192)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:152)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:183)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:141)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:32)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:492)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.lambda$invoke$4(SynchronousDispatcher.java:261)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.lambda$preprocess$0(SynchronousDispatcher.java:161)
keycloak_1  |   at org.jboss.resteasy.core.interception.jaxrs.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:364)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.preprocess(SynchronousDispatcher.java:164)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:247)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.RequestDispatcher.service(RequestDispatcher.java:73)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.dispatch(VertxRequestHandler.java:151)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.handle(VertxRequestHandler.java:82)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.handle(VertxRequestHandler.java:42)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at io.quarkus.vertx.http.runtime.StaticResourcesRecorder$2.handle(StaticResourcesRecorder.java:84)
keycloak_1  |   at io.quarkus.vertx.http.runtime.StaticResourcesRecorder$2.handle(StaticResourcesRecorder.java:71)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at io.quarkus.vertx.http.runtime.VertxHttpRecorder$6.handle(VertxHttpRecorder.java:430)
keycloak_1  |   at io.quarkus.vertx.http.runtime.VertxHttpRecorder$6.handle(VertxHttpRecorder.java:408)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at org.keycloak.quarkus.runtime.integration.web.QuarkusRequestFilter.lambda$createBlockingHandler$0(QuarkusRequestFilter.java:82)
keycloak_1  |   at io.quarkus.vertx.core.runtime.VertxCoreRecorder$14.runWith(VertxCoreRecorder.java:564)
keycloak_1  |   at org.jboss.threads.EnhancedQueueExecutor$Task.run(EnhancedQueueExecutor.java:2449)
keycloak_1  |   at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1478)
keycloak_1  |   at org.jboss.threads.DelegatingRunnable.run(DelegatingRunnable.java:29)
keycloak_1  |   at org.jboss.threads.ThreadLocalResettingRunnable.run(ThreadLocalResettingRunnable.java:29)
keycloak_1  |   at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
keycloak_1  |   at java.base/java.lang.Thread.run(Thread.java:829)
keycloak_1  | Caused by: java.lang.IllegalArgumentException: password empty
keycloak_1  |   at org.bouncycastle.jcajce.provider.symmetric.PBEPBKDF2$BasePBKDF2.engineGenerateSecret(Unknown Source)
keycloak_1  |   at java.base/javax.crypto.SecretKeyFactory.generateSecret(SecretKeyFactory.java:338)
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.encodedCredential(Pbkdf2PasswordHashProvider.java:111)
keycloak_1  |   ... 72 more
```",cXVpei1zZXJ2aWNlOjdjNzk1MzlkLTBkYTktNDk0YS05OWIyLTVkZjdlN2M2NTBhZg,https://github.com/keycloak/keycloak
1,15336.0,"### Area

oidc

### Describe the bug

On my Dev server if I'm trying to authenicate an existing user with empty password (the user exists but the password is incorrect) I'm getting a weird error, server shows me `500 Internal Server Error`.

### Version

20.0.0

### Expected behavior

401 Unauthorized

### Actual behavior

500 Internal Server Error

### How to Reproduce?

Basically if I'm trying to do something like this (use correct client credentials in Basic Authorization):
```
curl -v --location --request POST 'http://192.168.99.104:8180/realms/quiz/protocol/openid-connect/token' \
--header 'Authorization: Basic cXVpei1zZXJ2aWNlOjdjNzk1MzlkLTBkYTktNDk0YS05OWIyLTVkZjdlN2M2NTBhZg==' \
--header 'Content-Type: application/x-www-form-urlencoded' \
--data-urlencode 'client_id=quiz-service' \
--data-urlencode 'username=quiz_user' \
--data-urlencode 'password=' \
--data-urlencode 'grant_type=password'
```
I'm getting this:
```
< HTTP/1.1 500 Internal Server Error
< Referrer-Policy: no-referrer
< X-Frame-Options: SAMEORIGIN
< Strict-Transport-Security: max-age=31536000; includeSubDomains
< Cache-Control: no-store
< X-Content-Type-Options: nosniff
< Pragma: no-cache
< X-XSS-Protection: 1; mode=block
< Content-Type: application/json
< content-length: 25
<
{""error"":""unknown_error""}
```

### Anything else?

Stack Trace from console log:
```
keycloak_1  | 2022-11-03 22:06:40,520 ERROR [org.keycloak.services.error.KeycloakErrorHandler] (executor-thread-0) Uncaught server error: java.lang.RuntimeException: java.lang.IllegalArgumentException: password empty
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.encodedCredential(Pbkdf2PasswordHashProvider.java:116)
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.verify(Pbkdf2PasswordHashProvider.java:92)
keycloak_1  |   at org.keycloak.credential.PasswordCredentialProvider.isValid(PasswordCredentialProvider.java:177)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.lambda$validate$11(LegacyUserCredentialManager.java:255)
keycloak_1  |   at java.base/java.util.Collection.removeIf(Collection.java:544)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.validate(LegacyUserCredentialManager.java:255)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.lambda$isValid$0(LegacyUserCredentialManager.java:76)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
keycloak_1  |   at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1693)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.isValid(LegacyUserCredentialManager.java:76)
keycloak_1  |   at org.keycloak.models.SubjectCredentialManager.isValid(SubjectCredentialManager.java:45)
keycloak_1  |   at org.keycloak.authentication.authenticators.directgrant.ValidatePassword.authenticate(ValidatePassword.java:47)
keycloak_1  |   at org.keycloak.authentication.DefaultAuthenticationFlow.processSingleFlowExecutionModel(DefaultAuthenticationFlow.java:446)
keycloak_1  |   at org.keycloak.authentication.DefaultAuthenticationFlow.processFlow(DefaultAuthenticationFlow.java:250)
keycloak_1  |   at org.keycloak.authentication.AuthenticationProcessor.authenticateOnly(AuthenticationProcessor.java:1017)
keycloak_1  |   at org.keycloak.protocol.oidc.endpoints.TokenEndpoint.resourceOwnerPasswordCredentialsGrant(TokenEndpoint.java:627)
keycloak_1  |   at org.keycloak.protocol.oidc.endpoints.TokenEndpoint.processGrantRequest(TokenEndpoint.java:208)
keycloak_1  |   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
keycloak_1  |   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
keycloak_1  |   at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
keycloak_1  |   at java.base/java.lang.reflect.Method.invoke(Method.java:566)
keycloak_1  |   at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:170)
keycloak_1  |   at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:130)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.internalInvokeOnTarget(ResourceMethodInvoker.java:660)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTargetAfterFilter(ResourceMethodInvoker.java:524)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.lambda$invokeOnTarget$2(ResourceMethodInvoker.java:474)
keycloak_1  |   at org.jboss.resteasy.core.interception.jaxrs.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:364)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTarget(ResourceMethodInvoker.java:476)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:434)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:192)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:152)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:183)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:141)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:32)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:492)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.lambda$invoke$4(SynchronousDispatcher.java:261)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.lambda$preprocess$0(SynchronousDispatcher.java:161)
keycloak_1  |   at org.jboss.resteasy.core.interception.jaxrs.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:364)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.preprocess(SynchronousDispatcher.java:164)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:247)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.RequestDispatcher.service(RequestDispatcher.java:73)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.dispatch(VertxRequestHandler.java:151)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.handle(VertxRequestHandler.java:82)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.handle(VertxRequestHandler.java:42)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at io.quarkus.vertx.http.runtime.StaticResourcesRecorder$2.handle(StaticResourcesRecorder.java:84)
keycloak_1  |   at io.quarkus.vertx.http.runtime.StaticResourcesRecorder$2.handle(StaticResourcesRecorder.java:71)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at io.quarkus.vertx.http.runtime.VertxHttpRecorder$6.handle(VertxHttpRecorder.java:430)
keycloak_1  |   at io.quarkus.vertx.http.runtime.VertxHttpRecorder$6.handle(VertxHttpRecorder.java:408)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at org.keycloak.quarkus.runtime.integration.web.QuarkusRequestFilter.lambda$createBlockingHandler$0(QuarkusRequestFilter.java:82)
keycloak_1  |   at io.quarkus.vertx.core.runtime.VertxCoreRecorder$14.runWith(VertxCoreRecorder.java:564)
keycloak_1  |   at org.jboss.threads.EnhancedQueueExecutor$Task.run(EnhancedQueueExecutor.java:2449)
keycloak_1  |   at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1478)
keycloak_1  |   at org.jboss.threads.DelegatingRunnable.run(DelegatingRunnable.java:29)
keycloak_1  |   at org.jboss.threads.ThreadLocalResettingRunnable.run(ThreadLocalResettingRunnable.java:29)
keycloak_1  |   at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
keycloak_1  |   at java.base/java.lang.Thread.run(Thread.java:829)
keycloak_1  | Caused by: java.lang.IllegalArgumentException: password empty
keycloak_1  |   at org.bouncycastle.jcajce.provider.symmetric.PBEPBKDF2$BasePBKDF2.engineGenerateSecret(Unknown Source)
keycloak_1  |   at java.base/javax.crypto.SecretKeyFactory.generateSecret(SecretKeyFactory.java:338)
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.encodedCredential(Pbkdf2PasswordHashProvider.java:111)
keycloak_1  |   ... 72 more
```",createBlockingHandler$0(:82,https://github.com/keycloak/keycloak
2,15336.0,"### Area

oidc

### Describe the bug

On my Dev server if I'm trying to authenicate an existing user with empty password (the user exists but the password is incorrect) I'm getting a weird error, server shows me `500 Internal Server Error`.

### Version

20.0.0

### Expected behavior

401 Unauthorized

### Actual behavior

500 Internal Server Error

### How to Reproduce?

Basically if I'm trying to do something like this (use correct client credentials in Basic Authorization):
```
curl -v --location --request POST 'http://192.168.99.104:8180/realms/quiz/protocol/openid-connect/token' \
--header 'Authorization: Basic cXVpei1zZXJ2aWNlOjdjNzk1MzlkLTBkYTktNDk0YS05OWIyLTVkZjdlN2M2NTBhZg==' \
--header 'Content-Type: application/x-www-form-urlencoded' \
--data-urlencode 'client_id=quiz-service' \
--data-urlencode 'username=quiz_user' \
--data-urlencode 'password=' \
--data-urlencode 'grant_type=password'
```
I'm getting this:
```
< HTTP/1.1 500 Internal Server Error
< Referrer-Policy: no-referrer
< X-Frame-Options: SAMEORIGIN
< Strict-Transport-Security: max-age=31536000; includeSubDomains
< Cache-Control: no-store
< X-Content-Type-Options: nosniff
< Pragma: no-cache
< X-XSS-Protection: 1; mode=block
< Content-Type: application/json
< content-length: 25
<
{""error"":""unknown_error""}
```

### Anything else?

Stack Trace from console log:
```
keycloak_1  | 2022-11-03 22:06:40,520 ERROR [org.keycloak.services.error.KeycloakErrorHandler] (executor-thread-0) Uncaught server error: java.lang.RuntimeException: java.lang.IllegalArgumentException: password empty
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.encodedCredential(Pbkdf2PasswordHashProvider.java:116)
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.verify(Pbkdf2PasswordHashProvider.java:92)
keycloak_1  |   at org.keycloak.credential.PasswordCredentialProvider.isValid(PasswordCredentialProvider.java:177)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.lambda$validate$11(LegacyUserCredentialManager.java:255)
keycloak_1  |   at java.base/java.util.Collection.removeIf(Collection.java:544)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.validate(LegacyUserCredentialManager.java:255)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.lambda$isValid$0(LegacyUserCredentialManager.java:76)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
keycloak_1  |   at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1693)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.isValid(LegacyUserCredentialManager.java:76)
keycloak_1  |   at org.keycloak.models.SubjectCredentialManager.isValid(SubjectCredentialManager.java:45)
keycloak_1  |   at org.keycloak.authentication.authenticators.directgrant.ValidatePassword.authenticate(ValidatePassword.java:47)
keycloak_1  |   at org.keycloak.authentication.DefaultAuthenticationFlow.processSingleFlowExecutionModel(DefaultAuthenticationFlow.java:446)
keycloak_1  |   at org.keycloak.authentication.DefaultAuthenticationFlow.processFlow(DefaultAuthenticationFlow.java:250)
keycloak_1  |   at org.keycloak.authentication.AuthenticationProcessor.authenticateOnly(AuthenticationProcessor.java:1017)
keycloak_1  |   at org.keycloak.protocol.oidc.endpoints.TokenEndpoint.resourceOwnerPasswordCredentialsGrant(TokenEndpoint.java:627)
keycloak_1  |   at org.keycloak.protocol.oidc.endpoints.TokenEndpoint.processGrantRequest(TokenEndpoint.java:208)
keycloak_1  |   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
keycloak_1  |   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
keycloak_1  |   at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
keycloak_1  |   at java.base/java.lang.reflect.Method.invoke(Method.java:566)
keycloak_1  |   at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:170)
keycloak_1  |   at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:130)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.internalInvokeOnTarget(ResourceMethodInvoker.java:660)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTargetAfterFilter(ResourceMethodInvoker.java:524)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.lambda$invokeOnTarget$2(ResourceMethodInvoker.java:474)
keycloak_1  |   at org.jboss.resteasy.core.interception.jaxrs.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:364)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTarget(ResourceMethodInvoker.java:476)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:434)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:192)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:152)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:183)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:141)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:32)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:492)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.lambda$invoke$4(SynchronousDispatcher.java:261)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.lambda$preprocess$0(SynchronousDispatcher.java:161)
keycloak_1  |   at org.jboss.resteasy.core.interception.jaxrs.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:364)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.preprocess(SynchronousDispatcher.java:164)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:247)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.RequestDispatcher.service(RequestDispatcher.java:73)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.dispatch(VertxRequestHandler.java:151)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.handle(VertxRequestHandler.java:82)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.handle(VertxRequestHandler.java:42)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at io.quarkus.vertx.http.runtime.StaticResourcesRecorder$2.handle(StaticResourcesRecorder.java:84)
keycloak_1  |   at io.quarkus.vertx.http.runtime.StaticResourcesRecorder$2.handle(StaticResourcesRecorder.java:71)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at io.quarkus.vertx.http.runtime.VertxHttpRecorder$6.handle(VertxHttpRecorder.java:430)
keycloak_1  |   at io.quarkus.vertx.http.runtime.VertxHttpRecorder$6.handle(VertxHttpRecorder.java:408)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at org.keycloak.quarkus.runtime.integration.web.QuarkusRequestFilter.lambda$createBlockingHandler$0(QuarkusRequestFilter.java:82)
keycloak_1  |   at io.quarkus.vertx.core.runtime.VertxCoreRecorder$14.runWith(VertxCoreRecorder.java:564)
keycloak_1  |   at org.jboss.threads.EnhancedQueueExecutor$Task.run(EnhancedQueueExecutor.java:2449)
keycloak_1  |   at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1478)
keycloak_1  |   at org.jboss.threads.DelegatingRunnable.run(DelegatingRunnable.java:29)
keycloak_1  |   at org.jboss.threads.ThreadLocalResettingRunnable.run(ThreadLocalResettingRunnable.java:29)
keycloak_1  |   at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
keycloak_1  |   at java.base/java.lang.Thread.run(Thread.java:829)
keycloak_1  | Caused by: java.lang.IllegalArgumentException: password empty
keycloak_1  |   at org.bouncycastle.jcajce.provider.symmetric.PBEPBKDF2$BasePBKDF2.engineGenerateSecret(Unknown Source)
keycloak_1  |   at java.base/javax.crypto.SecretKeyFactory.generateSecret(SecretKeyFactory.java:338)
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.encodedCredential(Pbkdf2PasswordHashProvider.java:111)
keycloak_1  |   ... 72 more
```",validate$11(:255,https://github.com/keycloak/keycloak
3,15336.0,"### Area

oidc

### Describe the bug

On my Dev server if I'm trying to authenicate an existing user with empty password (the user exists but the password is incorrect) I'm getting a weird error, server shows me `500 Internal Server Error`.

### Version

20.0.0

### Expected behavior

401 Unauthorized

### Actual behavior

500 Internal Server Error

### How to Reproduce?

Basically if I'm trying to do something like this (use correct client credentials in Basic Authorization):
```
curl -v --location --request POST 'http://192.168.99.104:8180/realms/quiz/protocol/openid-connect/token' \
--header 'Authorization: Basic cXVpei1zZXJ2aWNlOjdjNzk1MzlkLTBkYTktNDk0YS05OWIyLTVkZjdlN2M2NTBhZg==' \
--header 'Content-Type: application/x-www-form-urlencoded' \
--data-urlencode 'client_id=quiz-service' \
--data-urlencode 'username=quiz_user' \
--data-urlencode 'password=' \
--data-urlencode 'grant_type=password'
```
I'm getting this:
```
< HTTP/1.1 500 Internal Server Error
< Referrer-Policy: no-referrer
< X-Frame-Options: SAMEORIGIN
< Strict-Transport-Security: max-age=31536000; includeSubDomains
< Cache-Control: no-store
< X-Content-Type-Options: nosniff
< Pragma: no-cache
< X-XSS-Protection: 1; mode=block
< Content-Type: application/json
< content-length: 25
<
{""error"":""unknown_error""}
```

### Anything else?

Stack Trace from console log:
```
keycloak_1  | 2022-11-03 22:06:40,520 ERROR [org.keycloak.services.error.KeycloakErrorHandler] (executor-thread-0) Uncaught server error: java.lang.RuntimeException: java.lang.IllegalArgumentException: password empty
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.encodedCredential(Pbkdf2PasswordHashProvider.java:116)
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.verify(Pbkdf2PasswordHashProvider.java:92)
keycloak_1  |   at org.keycloak.credential.PasswordCredentialProvider.isValid(PasswordCredentialProvider.java:177)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.lambda$validate$11(LegacyUserCredentialManager.java:255)
keycloak_1  |   at java.base/java.util.Collection.removeIf(Collection.java:544)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.validate(LegacyUserCredentialManager.java:255)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.lambda$isValid$0(LegacyUserCredentialManager.java:76)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
keycloak_1  |   at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1693)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.isValid(LegacyUserCredentialManager.java:76)
keycloak_1  |   at org.keycloak.models.SubjectCredentialManager.isValid(SubjectCredentialManager.java:45)
keycloak_1  |   at org.keycloak.authentication.authenticators.directgrant.ValidatePassword.authenticate(ValidatePassword.java:47)
keycloak_1  |   at org.keycloak.authentication.DefaultAuthenticationFlow.processSingleFlowExecutionModel(DefaultAuthenticationFlow.java:446)
keycloak_1  |   at org.keycloak.authentication.DefaultAuthenticationFlow.processFlow(DefaultAuthenticationFlow.java:250)
keycloak_1  |   at org.keycloak.authentication.AuthenticationProcessor.authenticateOnly(AuthenticationProcessor.java:1017)
keycloak_1  |   at org.keycloak.protocol.oidc.endpoints.TokenEndpoint.resourceOwnerPasswordCredentialsGrant(TokenEndpoint.java:627)
keycloak_1  |   at org.keycloak.protocol.oidc.endpoints.TokenEndpoint.processGrantRequest(TokenEndpoint.java:208)
keycloak_1  |   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
keycloak_1  |   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
keycloak_1  |   at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
keycloak_1  |   at java.base/java.lang.reflect.Method.invoke(Method.java:566)
keycloak_1  |   at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:170)
keycloak_1  |   at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:130)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.internalInvokeOnTarget(ResourceMethodInvoker.java:660)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTargetAfterFilter(ResourceMethodInvoker.java:524)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.lambda$invokeOnTarget$2(ResourceMethodInvoker.java:474)
keycloak_1  |   at org.jboss.resteasy.core.interception.jaxrs.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:364)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTarget(ResourceMethodInvoker.java:476)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:434)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:192)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:152)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:183)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:141)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:32)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:492)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.lambda$invoke$4(SynchronousDispatcher.java:261)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.lambda$preprocess$0(SynchronousDispatcher.java:161)
keycloak_1  |   at org.jboss.resteasy.core.interception.jaxrs.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:364)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.preprocess(SynchronousDispatcher.java:164)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:247)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.RequestDispatcher.service(RequestDispatcher.java:73)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.dispatch(VertxRequestHandler.java:151)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.handle(VertxRequestHandler.java:82)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.handle(VertxRequestHandler.java:42)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at io.quarkus.vertx.http.runtime.StaticResourcesRecorder$2.handle(StaticResourcesRecorder.java:84)
keycloak_1  |   at io.quarkus.vertx.http.runtime.StaticResourcesRecorder$2.handle(StaticResourcesRecorder.java:71)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at io.quarkus.vertx.http.runtime.VertxHttpRecorder$6.handle(VertxHttpRecorder.java:430)
keycloak_1  |   at io.quarkus.vertx.http.runtime.VertxHttpRecorder$6.handle(VertxHttpRecorder.java:408)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at org.keycloak.quarkus.runtime.integration.web.QuarkusRequestFilter.lambda$createBlockingHandler$0(QuarkusRequestFilter.java:82)
keycloak_1  |   at io.quarkus.vertx.core.runtime.VertxCoreRecorder$14.runWith(VertxCoreRecorder.java:564)
keycloak_1  |   at org.jboss.threads.EnhancedQueueExecutor$Task.run(EnhancedQueueExecutor.java:2449)
keycloak_1  |   at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1478)
keycloak_1  |   at org.jboss.threads.DelegatingRunnable.run(DelegatingRunnable.java:29)
keycloak_1  |   at org.jboss.threads.ThreadLocalResettingRunnable.run(ThreadLocalResettingRunnable.java:29)
keycloak_1  |   at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
keycloak_1  |   at java.base/java.lang.Thread.run(Thread.java:829)
keycloak_1  | Caused by: java.lang.IllegalArgumentException: password empty
keycloak_1  |   at org.bouncycastle.jcajce.provider.symmetric.PBEPBKDF2$BasePBKDF2.engineGenerateSecret(Unknown Source)
keycloak_1  |   at java.base/javax.crypto.SecretKeyFactory.generateSecret(SecretKeyFactory.java:338)
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.encodedCredential(Pbkdf2PasswordHashProvider.java:111)
keycloak_1  |   ... 72 more
```",invokeOnTarget$2(:474,https://github.com/keycloak/keycloak
4,15336.0,"### Area

oidc

### Describe the bug

On my Dev server if I'm trying to authenicate an existing user with empty password (the user exists but the password is incorrect) I'm getting a weird error, server shows me `500 Internal Server Error`.

### Version

20.0.0

### Expected behavior

401 Unauthorized

### Actual behavior

500 Internal Server Error

### How to Reproduce?

Basically if I'm trying to do something like this (use correct client credentials in Basic Authorization):
```
curl -v --location --request POST 'http://192.168.99.104:8180/realms/quiz/protocol/openid-connect/token' \
--header 'Authorization: Basic cXVpei1zZXJ2aWNlOjdjNzk1MzlkLTBkYTktNDk0YS05OWIyLTVkZjdlN2M2NTBhZg==' \
--header 'Content-Type: application/x-www-form-urlencoded' \
--data-urlencode 'client_id=quiz-service' \
--data-urlencode 'username=quiz_user' \
--data-urlencode 'password=' \
--data-urlencode 'grant_type=password'
```
I'm getting this:
```
< HTTP/1.1 500 Internal Server Error
< Referrer-Policy: no-referrer
< X-Frame-Options: SAMEORIGIN
< Strict-Transport-Security: max-age=31536000; includeSubDomains
< Cache-Control: no-store
< X-Content-Type-Options: nosniff
< Pragma: no-cache
< X-XSS-Protection: 1; mode=block
< Content-Type: application/json
< content-length: 25
<
{""error"":""unknown_error""}
```

### Anything else?

Stack Trace from console log:
```
keycloak_1  | 2022-11-03 22:06:40,520 ERROR [org.keycloak.services.error.KeycloakErrorHandler] (executor-thread-0) Uncaught server error: java.lang.RuntimeException: java.lang.IllegalArgumentException: password empty
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.encodedCredential(Pbkdf2PasswordHashProvider.java:116)
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.verify(Pbkdf2PasswordHashProvider.java:92)
keycloak_1  |   at org.keycloak.credential.PasswordCredentialProvider.isValid(PasswordCredentialProvider.java:177)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.lambda$validate$11(LegacyUserCredentialManager.java:255)
keycloak_1  |   at java.base/java.util.Collection.removeIf(Collection.java:544)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.validate(LegacyUserCredentialManager.java:255)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.lambda$isValid$0(LegacyUserCredentialManager.java:76)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
keycloak_1  |   at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1693)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.isValid(LegacyUserCredentialManager.java:76)
keycloak_1  |   at org.keycloak.models.SubjectCredentialManager.isValid(SubjectCredentialManager.java:45)
keycloak_1  |   at org.keycloak.authentication.authenticators.directgrant.ValidatePassword.authenticate(ValidatePassword.java:47)
keycloak_1  |   at org.keycloak.authentication.DefaultAuthenticationFlow.processSingleFlowExecutionModel(DefaultAuthenticationFlow.java:446)
keycloak_1  |   at org.keycloak.authentication.DefaultAuthenticationFlow.processFlow(DefaultAuthenticationFlow.java:250)
keycloak_1  |   at org.keycloak.authentication.AuthenticationProcessor.authenticateOnly(AuthenticationProcessor.java:1017)
keycloak_1  |   at org.keycloak.protocol.oidc.endpoints.TokenEndpoint.resourceOwnerPasswordCredentialsGrant(TokenEndpoint.java:627)
keycloak_1  |   at org.keycloak.protocol.oidc.endpoints.TokenEndpoint.processGrantRequest(TokenEndpoint.java:208)
keycloak_1  |   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
keycloak_1  |   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
keycloak_1  |   at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
keycloak_1  |   at java.base/java.lang.reflect.Method.invoke(Method.java:566)
keycloak_1  |   at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:170)
keycloak_1  |   at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:130)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.internalInvokeOnTarget(ResourceMethodInvoker.java:660)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTargetAfterFilter(ResourceMethodInvoker.java:524)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.lambda$invokeOnTarget$2(ResourceMethodInvoker.java:474)
keycloak_1  |   at org.jboss.resteasy.core.interception.jaxrs.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:364)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTarget(ResourceMethodInvoker.java:476)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:434)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:192)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:152)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:183)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:141)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:32)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:492)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.lambda$invoke$4(SynchronousDispatcher.java:261)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.lambda$preprocess$0(SynchronousDispatcher.java:161)
keycloak_1  |   at org.jboss.resteasy.core.interception.jaxrs.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:364)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.preprocess(SynchronousDispatcher.java:164)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:247)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.RequestDispatcher.service(RequestDispatcher.java:73)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.dispatch(VertxRequestHandler.java:151)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.handle(VertxRequestHandler.java:82)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.handle(VertxRequestHandler.java:42)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at io.quarkus.vertx.http.runtime.StaticResourcesRecorder$2.handle(StaticResourcesRecorder.java:84)
keycloak_1  |   at io.quarkus.vertx.http.runtime.StaticResourcesRecorder$2.handle(StaticResourcesRecorder.java:71)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at io.quarkus.vertx.http.runtime.VertxHttpRecorder$6.handle(VertxHttpRecorder.java:430)
keycloak_1  |   at io.quarkus.vertx.http.runtime.VertxHttpRecorder$6.handle(VertxHttpRecorder.java:408)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at org.keycloak.quarkus.runtime.integration.web.QuarkusRequestFilter.lambda$createBlockingHandler$0(QuarkusRequestFilter.java:82)
keycloak_1  |   at io.quarkus.vertx.core.runtime.VertxCoreRecorder$14.runWith(VertxCoreRecorder.java:564)
keycloak_1  |   at org.jboss.threads.EnhancedQueueExecutor$Task.run(EnhancedQueueExecutor.java:2449)
keycloak_1  |   at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1478)
keycloak_1  |   at org.jboss.threads.DelegatingRunnable.run(DelegatingRunnable.java:29)
keycloak_1  |   at org.jboss.threads.ThreadLocalResettingRunnable.run(ThreadLocalResettingRunnable.java:29)
keycloak_1  |   at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
keycloak_1  |   at java.base/java.lang.Thread.run(Thread.java:829)
keycloak_1  | Caused by: java.lang.IllegalArgumentException: password empty
keycloak_1  |   at org.bouncycastle.jcajce.provider.symmetric.PBEPBKDF2$BasePBKDF2.engineGenerateSecret(Unknown Source)
keycloak_1  |   at java.base/javax.crypto.SecretKeyFactory.generateSecret(SecretKeyFactory.java:338)
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.encodedCredential(Pbkdf2PasswordHashProvider.java:111)
keycloak_1  |   ... 72 more
```",preprocess$0(:161,https://github.com/keycloak/keycloak
5,15336.0,"### Area

oidc

### Describe the bug

On my Dev server if I'm trying to authenicate an existing user with empty password (the user exists but the password is incorrect) I'm getting a weird error, server shows me `500 Internal Server Error`.

### Version

20.0.0

### Expected behavior

401 Unauthorized

### Actual behavior

500 Internal Server Error

### How to Reproduce?

Basically if I'm trying to do something like this (use correct client credentials in Basic Authorization):
```
curl -v --location --request POST 'http://192.168.99.104:8180/realms/quiz/protocol/openid-connect/token' \
--header 'Authorization: Basic cXVpei1zZXJ2aWNlOjdjNzk1MzlkLTBkYTktNDk0YS05OWIyLTVkZjdlN2M2NTBhZg==' \
--header 'Content-Type: application/x-www-form-urlencoded' \
--data-urlencode 'client_id=quiz-service' \
--data-urlencode 'username=quiz_user' \
--data-urlencode 'password=' \
--data-urlencode 'grant_type=password'
```
I'm getting this:
```
< HTTP/1.1 500 Internal Server Error
< Referrer-Policy: no-referrer
< X-Frame-Options: SAMEORIGIN
< Strict-Transport-Security: max-age=31536000; includeSubDomains
< Cache-Control: no-store
< X-Content-Type-Options: nosniff
< Pragma: no-cache
< X-XSS-Protection: 1; mode=block
< Content-Type: application/json
< content-length: 25
<
{""error"":""unknown_error""}
```

### Anything else?

Stack Trace from console log:
```
keycloak_1  | 2022-11-03 22:06:40,520 ERROR [org.keycloak.services.error.KeycloakErrorHandler] (executor-thread-0) Uncaught server error: java.lang.RuntimeException: java.lang.IllegalArgumentException: password empty
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.encodedCredential(Pbkdf2PasswordHashProvider.java:116)
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.verify(Pbkdf2PasswordHashProvider.java:92)
keycloak_1  |   at org.keycloak.credential.PasswordCredentialProvider.isValid(PasswordCredentialProvider.java:177)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.lambda$validate$11(LegacyUserCredentialManager.java:255)
keycloak_1  |   at java.base/java.util.Collection.removeIf(Collection.java:544)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.validate(LegacyUserCredentialManager.java:255)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.lambda$isValid$0(LegacyUserCredentialManager.java:76)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
keycloak_1  |   at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1693)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.isValid(LegacyUserCredentialManager.java:76)
keycloak_1  |   at org.keycloak.models.SubjectCredentialManager.isValid(SubjectCredentialManager.java:45)
keycloak_1  |   at org.keycloak.authentication.authenticators.directgrant.ValidatePassword.authenticate(ValidatePassword.java:47)
keycloak_1  |   at org.keycloak.authentication.DefaultAuthenticationFlow.processSingleFlowExecutionModel(DefaultAuthenticationFlow.java:446)
keycloak_1  |   at org.keycloak.authentication.DefaultAuthenticationFlow.processFlow(DefaultAuthenticationFlow.java:250)
keycloak_1  |   at org.keycloak.authentication.AuthenticationProcessor.authenticateOnly(AuthenticationProcessor.java:1017)
keycloak_1  |   at org.keycloak.protocol.oidc.endpoints.TokenEndpoint.resourceOwnerPasswordCredentialsGrant(TokenEndpoint.java:627)
keycloak_1  |   at org.keycloak.protocol.oidc.endpoints.TokenEndpoint.processGrantRequest(TokenEndpoint.java:208)
keycloak_1  |   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
keycloak_1  |   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
keycloak_1  |   at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
keycloak_1  |   at java.base/java.lang.reflect.Method.invoke(Method.java:566)
keycloak_1  |   at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:170)
keycloak_1  |   at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:130)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.internalInvokeOnTarget(ResourceMethodInvoker.java:660)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTargetAfterFilter(ResourceMethodInvoker.java:524)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.lambda$invokeOnTarget$2(ResourceMethodInvoker.java:474)
keycloak_1  |   at org.jboss.resteasy.core.interception.jaxrs.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:364)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTarget(ResourceMethodInvoker.java:476)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:434)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:192)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:152)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:183)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:141)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:32)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:492)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.lambda$invoke$4(SynchronousDispatcher.java:261)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.lambda$preprocess$0(SynchronousDispatcher.java:161)
keycloak_1  |   at org.jboss.resteasy.core.interception.jaxrs.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:364)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.preprocess(SynchronousDispatcher.java:164)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:247)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.RequestDispatcher.service(RequestDispatcher.java:73)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.dispatch(VertxRequestHandler.java:151)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.handle(VertxRequestHandler.java:82)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.handle(VertxRequestHandler.java:42)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at io.quarkus.vertx.http.runtime.StaticResourcesRecorder$2.handle(StaticResourcesRecorder.java:84)
keycloak_1  |   at io.quarkus.vertx.http.runtime.StaticResourcesRecorder$2.handle(StaticResourcesRecorder.java:71)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at io.quarkus.vertx.http.runtime.VertxHttpRecorder$6.handle(VertxHttpRecorder.java:430)
keycloak_1  |   at io.quarkus.vertx.http.runtime.VertxHttpRecorder$6.handle(VertxHttpRecorder.java:408)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at org.keycloak.quarkus.runtime.integration.web.QuarkusRequestFilter.lambda$createBlockingHandler$0(QuarkusRequestFilter.java:82)
keycloak_1  |   at io.quarkus.vertx.core.runtime.VertxCoreRecorder$14.runWith(VertxCoreRecorder.java:564)
keycloak_1  |   at org.jboss.threads.EnhancedQueueExecutor$Task.run(EnhancedQueueExecutor.java:2449)
keycloak_1  |   at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1478)
keycloak_1  |   at org.jboss.threads.DelegatingRunnable.run(DelegatingRunnable.java:29)
keycloak_1  |   at org.jboss.threads.ThreadLocalResettingRunnable.run(ThreadLocalResettingRunnable.java:29)
keycloak_1  |   at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
keycloak_1  |   at java.base/java.lang.Thread.run(Thread.java:829)
keycloak_1  | Caused by: java.lang.IllegalArgumentException: password empty
keycloak_1  |   at org.bouncycastle.jcajce.provider.symmetric.PBEPBKDF2$BasePBKDF2.engineGenerateSecret(Unknown Source)
keycloak_1  |   at java.base/javax.crypto.SecretKeyFactory.generateSecret(SecretKeyFactory.java:338)
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.encodedCredential(Pbkdf2PasswordHashProvider.java:111)
keycloak_1  |   ... 72 more
```",grant_type=password,https://github.com/keycloak/keycloak
6,15336.0,"### Area

oidc

### Describe the bug

On my Dev server if I'm trying to authenicate an existing user with empty password (the user exists but the password is incorrect) I'm getting a weird error, server shows me `500 Internal Server Error`.

### Version

20.0.0

### Expected behavior

401 Unauthorized

### Actual behavior

500 Internal Server Error

### How to Reproduce?

Basically if I'm trying to do something like this (use correct client credentials in Basic Authorization):
```
curl -v --location --request POST 'http://192.168.99.104:8180/realms/quiz/protocol/openid-connect/token' \
--header 'Authorization: Basic cXVpei1zZXJ2aWNlOjdjNzk1MzlkLTBkYTktNDk0YS05OWIyLTVkZjdlN2M2NTBhZg==' \
--header 'Content-Type: application/x-www-form-urlencoded' \
--data-urlencode 'client_id=quiz-service' \
--data-urlencode 'username=quiz_user' \
--data-urlencode 'password=' \
--data-urlencode 'grant_type=password'
```
I'm getting this:
```
< HTTP/1.1 500 Internal Server Error
< Referrer-Policy: no-referrer
< X-Frame-Options: SAMEORIGIN
< Strict-Transport-Security: max-age=31536000; includeSubDomains
< Cache-Control: no-store
< X-Content-Type-Options: nosniff
< Pragma: no-cache
< X-XSS-Protection: 1; mode=block
< Content-Type: application/json
< content-length: 25
<
{""error"":""unknown_error""}
```

### Anything else?

Stack Trace from console log:
```
keycloak_1  | 2022-11-03 22:06:40,520 ERROR [org.keycloak.services.error.KeycloakErrorHandler] (executor-thread-0) Uncaught server error: java.lang.RuntimeException: java.lang.IllegalArgumentException: password empty
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.encodedCredential(Pbkdf2PasswordHashProvider.java:116)
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.verify(Pbkdf2PasswordHashProvider.java:92)
keycloak_1  |   at org.keycloak.credential.PasswordCredentialProvider.isValid(PasswordCredentialProvider.java:177)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.lambda$validate$11(LegacyUserCredentialManager.java:255)
keycloak_1  |   at java.base/java.util.Collection.removeIf(Collection.java:544)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.validate(LegacyUserCredentialManager.java:255)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.lambda$isValid$0(LegacyUserCredentialManager.java:76)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
keycloak_1  |   at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1693)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.isValid(LegacyUserCredentialManager.java:76)
keycloak_1  |   at org.keycloak.models.SubjectCredentialManager.isValid(SubjectCredentialManager.java:45)
keycloak_1  |   at org.keycloak.authentication.authenticators.directgrant.ValidatePassword.authenticate(ValidatePassword.java:47)
keycloak_1  |   at org.keycloak.authentication.DefaultAuthenticationFlow.processSingleFlowExecutionModel(DefaultAuthenticationFlow.java:446)
keycloak_1  |   at org.keycloak.authentication.DefaultAuthenticationFlow.processFlow(DefaultAuthenticationFlow.java:250)
keycloak_1  |   at org.keycloak.authentication.AuthenticationProcessor.authenticateOnly(AuthenticationProcessor.java:1017)
keycloak_1  |   at org.keycloak.protocol.oidc.endpoints.TokenEndpoint.resourceOwnerPasswordCredentialsGrant(TokenEndpoint.java:627)
keycloak_1  |   at org.keycloak.protocol.oidc.endpoints.TokenEndpoint.processGrantRequest(TokenEndpoint.java:208)
keycloak_1  |   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
keycloak_1  |   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
keycloak_1  |   at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
keycloak_1  |   at java.base/java.lang.reflect.Method.invoke(Method.java:566)
keycloak_1  |   at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:170)
keycloak_1  |   at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:130)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.internalInvokeOnTarget(ResourceMethodInvoker.java:660)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTargetAfterFilter(ResourceMethodInvoker.java:524)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.lambda$invokeOnTarget$2(ResourceMethodInvoker.java:474)
keycloak_1  |   at org.jboss.resteasy.core.interception.jaxrs.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:364)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTarget(ResourceMethodInvoker.java:476)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:434)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:192)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:152)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:183)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:141)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:32)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:492)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.lambda$invoke$4(SynchronousDispatcher.java:261)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.lambda$preprocess$0(SynchronousDispatcher.java:161)
keycloak_1  |   at org.jboss.resteasy.core.interception.jaxrs.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:364)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.preprocess(SynchronousDispatcher.java:164)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:247)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.RequestDispatcher.service(RequestDispatcher.java:73)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.dispatch(VertxRequestHandler.java:151)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.handle(VertxRequestHandler.java:82)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.handle(VertxRequestHandler.java:42)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at io.quarkus.vertx.http.runtime.StaticResourcesRecorder$2.handle(StaticResourcesRecorder.java:84)
keycloak_1  |   at io.quarkus.vertx.http.runtime.StaticResourcesRecorder$2.handle(StaticResourcesRecorder.java:71)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at io.quarkus.vertx.http.runtime.VertxHttpRecorder$6.handle(VertxHttpRecorder.java:430)
keycloak_1  |   at io.quarkus.vertx.http.runtime.VertxHttpRecorder$6.handle(VertxHttpRecorder.java:408)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at org.keycloak.quarkus.runtime.integration.web.QuarkusRequestFilter.lambda$createBlockingHandler$0(QuarkusRequestFilter.java:82)
keycloak_1  |   at io.quarkus.vertx.core.runtime.VertxCoreRecorder$14.runWith(VertxCoreRecorder.java:564)
keycloak_1  |   at org.jboss.threads.EnhancedQueueExecutor$Task.run(EnhancedQueueExecutor.java:2449)
keycloak_1  |   at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1478)
keycloak_1  |   at org.jboss.threads.DelegatingRunnable.run(DelegatingRunnable.java:29)
keycloak_1  |   at org.jboss.threads.ThreadLocalResettingRunnable.run(ThreadLocalResettingRunnable.java:29)
keycloak_1  |   at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
keycloak_1  |   at java.base/java.lang.Thread.run(Thread.java:829)
keycloak_1  | Caused by: java.lang.IllegalArgumentException: password empty
keycloak_1  |   at org.bouncycastle.jcajce.provider.symmetric.PBEPBKDF2$BasePBKDF2.engineGenerateSecret(Unknown Source)
keycloak_1  |   at java.base/javax.crypto.SecretKeyFactory.generateSecret(SecretKeyFactory.java:338)
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.encodedCredential(Pbkdf2PasswordHashProvider.java:111)
keycloak_1  |   ... 72 more
```",password,https://github.com/keycloak/keycloak
7,15336.0,"### Area

oidc

### Describe the bug

On my Dev server if I'm trying to authenicate an existing user with empty password (the user exists but the password is incorrect) I'm getting a weird error, server shows me `500 Internal Server Error`.

### Version

20.0.0

### Expected behavior

401 Unauthorized

### Actual behavior

500 Internal Server Error

### How to Reproduce?

Basically if I'm trying to do something like this (use correct client credentials in Basic Authorization):
```
curl -v --location --request POST 'http://192.168.99.104:8180/realms/quiz/protocol/openid-connect/token' \
--header 'Authorization: Basic cXVpei1zZXJ2aWNlOjdjNzk1MzlkLTBkYTktNDk0YS05OWIyLTVkZjdlN2M2NTBhZg==' \
--header 'Content-Type: application/x-www-form-urlencoded' \
--data-urlencode 'client_id=quiz-service' \
--data-urlencode 'username=quiz_user' \
--data-urlencode 'password=' \
--data-urlencode 'grant_type=password'
```
I'm getting this:
```
< HTTP/1.1 500 Internal Server Error
< Referrer-Policy: no-referrer
< X-Frame-Options: SAMEORIGIN
< Strict-Transport-Security: max-age=31536000; includeSubDomains
< Cache-Control: no-store
< X-Content-Type-Options: nosniff
< Pragma: no-cache
< X-XSS-Protection: 1; mode=block
< Content-Type: application/json
< content-length: 25
<
{""error"":""unknown_error""}
```

### Anything else?

Stack Trace from console log:
```
keycloak_1  | 2022-11-03 22:06:40,520 ERROR [org.keycloak.services.error.KeycloakErrorHandler] (executor-thread-0) Uncaught server error: java.lang.RuntimeException: java.lang.IllegalArgumentException: password empty
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.encodedCredential(Pbkdf2PasswordHashProvider.java:116)
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.verify(Pbkdf2PasswordHashProvider.java:92)
keycloak_1  |   at org.keycloak.credential.PasswordCredentialProvider.isValid(PasswordCredentialProvider.java:177)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.lambda$validate$11(LegacyUserCredentialManager.java:255)
keycloak_1  |   at java.base/java.util.Collection.removeIf(Collection.java:544)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.validate(LegacyUserCredentialManager.java:255)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.lambda$isValid$0(LegacyUserCredentialManager.java:76)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
keycloak_1  |   at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1693)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.isValid(LegacyUserCredentialManager.java:76)
keycloak_1  |   at org.keycloak.models.SubjectCredentialManager.isValid(SubjectCredentialManager.java:45)
keycloak_1  |   at org.keycloak.authentication.authenticators.directgrant.ValidatePassword.authenticate(ValidatePassword.java:47)
keycloak_1  |   at org.keycloak.authentication.DefaultAuthenticationFlow.processSingleFlowExecutionModel(DefaultAuthenticationFlow.java:446)
keycloak_1  |   at org.keycloak.authentication.DefaultAuthenticationFlow.processFlow(DefaultAuthenticationFlow.java:250)
keycloak_1  |   at org.keycloak.authentication.AuthenticationProcessor.authenticateOnly(AuthenticationProcessor.java:1017)
keycloak_1  |   at org.keycloak.protocol.oidc.endpoints.TokenEndpoint.resourceOwnerPasswordCredentialsGrant(TokenEndpoint.java:627)
keycloak_1  |   at org.keycloak.protocol.oidc.endpoints.TokenEndpoint.processGrantRequest(TokenEndpoint.java:208)
keycloak_1  |   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
keycloak_1  |   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
keycloak_1  |   at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
keycloak_1  |   at java.base/java.lang.reflect.Method.invoke(Method.java:566)
keycloak_1  |   at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:170)
keycloak_1  |   at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:130)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.internalInvokeOnTarget(ResourceMethodInvoker.java:660)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTargetAfterFilter(ResourceMethodInvoker.java:524)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.lambda$invokeOnTarget$2(ResourceMethodInvoker.java:474)
keycloak_1  |   at org.jboss.resteasy.core.interception.jaxrs.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:364)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTarget(ResourceMethodInvoker.java:476)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:434)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:192)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:152)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:183)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:141)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:32)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:492)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.lambda$invoke$4(SynchronousDispatcher.java:261)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.lambda$preprocess$0(SynchronousDispatcher.java:161)
keycloak_1  |   at org.jboss.resteasy.core.interception.jaxrs.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:364)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.preprocess(SynchronousDispatcher.java:164)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:247)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.RequestDispatcher.service(RequestDispatcher.java:73)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.dispatch(VertxRequestHandler.java:151)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.handle(VertxRequestHandler.java:82)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.handle(VertxRequestHandler.java:42)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at io.quarkus.vertx.http.runtime.StaticResourcesRecorder$2.handle(StaticResourcesRecorder.java:84)
keycloak_1  |   at io.quarkus.vertx.http.runtime.StaticResourcesRecorder$2.handle(StaticResourcesRecorder.java:71)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at io.quarkus.vertx.http.runtime.VertxHttpRecorder$6.handle(VertxHttpRecorder.java:430)
keycloak_1  |   at io.quarkus.vertx.http.runtime.VertxHttpRecorder$6.handle(VertxHttpRecorder.java:408)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at org.keycloak.quarkus.runtime.integration.web.QuarkusRequestFilter.lambda$createBlockingHandler$0(QuarkusRequestFilter.java:82)
keycloak_1  |   at io.quarkus.vertx.core.runtime.VertxCoreRecorder$14.runWith(VertxCoreRecorder.java:564)
keycloak_1  |   at org.jboss.threads.EnhancedQueueExecutor$Task.run(EnhancedQueueExecutor.java:2449)
keycloak_1  |   at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1478)
keycloak_1  |   at org.jboss.threads.DelegatingRunnable.run(DelegatingRunnable.java:29)
keycloak_1  |   at org.jboss.threads.ThreadLocalResettingRunnable.run(ThreadLocalResettingRunnable.java:29)
keycloak_1  |   at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
keycloak_1  |   at java.base/java.lang.Thread.run(Thread.java:829)
keycloak_1  | Caused by: java.lang.IllegalArgumentException: password empty
keycloak_1  |   at org.bouncycastle.jcajce.provider.symmetric.PBEPBKDF2$BasePBKDF2.engineGenerateSecret(Unknown Source)
keycloak_1  |   at java.base/javax.crypto.SecretKeyFactory.generateSecret(SecretKeyFactory.java:338)
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.encodedCredential(Pbkdf2PasswordHashProvider.java:111)
keycloak_1  |   ... 72 more
```",incorrect,https://github.com/keycloak/keycloak
8,15336.0,"### Area

oidc

### Describe the bug

On my Dev server if I'm trying to authenicate an existing user with empty password (the user exists but the password is incorrect) I'm getting a weird error, server shows me `500 Internal Server Error`.

### Version

20.0.0

### Expected behavior

401 Unauthorized

### Actual behavior

500 Internal Server Error

### How to Reproduce?

Basically if I'm trying to do something like this (use correct client credentials in Basic Authorization):
```
curl -v --location --request POST 'http://192.168.99.104:8180/realms/quiz/protocol/openid-connect/token' \
--header 'Authorization: Basic cXVpei1zZXJ2aWNlOjdjNzk1MzlkLTBkYTktNDk0YS05OWIyLTVkZjdlN2M2NTBhZg==' \
--header 'Content-Type: application/x-www-form-urlencoded' \
--data-urlencode 'client_id=quiz-service' \
--data-urlencode 'username=quiz_user' \
--data-urlencode 'password=' \
--data-urlencode 'grant_type=password'
```
I'm getting this:
```
< HTTP/1.1 500 Internal Server Error
< Referrer-Policy: no-referrer
< X-Frame-Options: SAMEORIGIN
< Strict-Transport-Security: max-age=31536000; includeSubDomains
< Cache-Control: no-store
< X-Content-Type-Options: nosniff
< Pragma: no-cache
< X-XSS-Protection: 1; mode=block
< Content-Type: application/json
< content-length: 25
<
{""error"":""unknown_error""}
```

### Anything else?

Stack Trace from console log:
```
keycloak_1  | 2022-11-03 22:06:40,520 ERROR [org.keycloak.services.error.KeycloakErrorHandler] (executor-thread-0) Uncaught server error: java.lang.RuntimeException: java.lang.IllegalArgumentException: password empty
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.encodedCredential(Pbkdf2PasswordHashProvider.java:116)
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.verify(Pbkdf2PasswordHashProvider.java:92)
keycloak_1  |   at org.keycloak.credential.PasswordCredentialProvider.isValid(PasswordCredentialProvider.java:177)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.lambda$validate$11(LegacyUserCredentialManager.java:255)
keycloak_1  |   at java.base/java.util.Collection.removeIf(Collection.java:544)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.validate(LegacyUserCredentialManager.java:255)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.lambda$isValid$0(LegacyUserCredentialManager.java:76)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
keycloak_1  |   at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1693)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
keycloak_1  |   at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
keycloak_1  |   at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
keycloak_1  |   at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
keycloak_1  |   at org.keycloak.credential.LegacyUserCredentialManager.isValid(LegacyUserCredentialManager.java:76)
keycloak_1  |   at org.keycloak.models.SubjectCredentialManager.isValid(SubjectCredentialManager.java:45)
keycloak_1  |   at org.keycloak.authentication.authenticators.directgrant.ValidatePassword.authenticate(ValidatePassword.java:47)
keycloak_1  |   at org.keycloak.authentication.DefaultAuthenticationFlow.processSingleFlowExecutionModel(DefaultAuthenticationFlow.java:446)
keycloak_1  |   at org.keycloak.authentication.DefaultAuthenticationFlow.processFlow(DefaultAuthenticationFlow.java:250)
keycloak_1  |   at org.keycloak.authentication.AuthenticationProcessor.authenticateOnly(AuthenticationProcessor.java:1017)
keycloak_1  |   at org.keycloak.protocol.oidc.endpoints.TokenEndpoint.resourceOwnerPasswordCredentialsGrant(TokenEndpoint.java:627)
keycloak_1  |   at org.keycloak.protocol.oidc.endpoints.TokenEndpoint.processGrantRequest(TokenEndpoint.java:208)
keycloak_1  |   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
keycloak_1  |   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
keycloak_1  |   at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
keycloak_1  |   at java.base/java.lang.reflect.Method.invoke(Method.java:566)
keycloak_1  |   at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:170)
keycloak_1  |   at org.jboss.resteasy.core.MethodInjectorImpl.invoke(MethodInjectorImpl.java:130)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.internalInvokeOnTarget(ResourceMethodInvoker.java:660)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTargetAfterFilter(ResourceMethodInvoker.java:524)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.lambda$invokeOnTarget$2(ResourceMethodInvoker.java:474)
keycloak_1  |   at org.jboss.resteasy.core.interception.jaxrs.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:364)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTarget(ResourceMethodInvoker.java:476)
keycloak_1  |   at org.jboss.resteasy.core.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:434)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:192)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:152)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invokeOnTargetObject(ResourceLocatorInvoker.java:183)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:141)
keycloak_1  |   at org.jboss.resteasy.core.ResourceLocatorInvoker.invoke(ResourceLocatorInvoker.java:32)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:492)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.lambda$invoke$4(SynchronousDispatcher.java:261)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.lambda$preprocess$0(SynchronousDispatcher.java:161)
keycloak_1  |   at org.jboss.resteasy.core.interception.jaxrs.PreMatchContainerRequestContext.filter(PreMatchContainerRequestContext.java:364)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.preprocess(SynchronousDispatcher.java:164)
keycloak_1  |   at org.jboss.resteasy.core.SynchronousDispatcher.invoke(SynchronousDispatcher.java:247)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.RequestDispatcher.service(RequestDispatcher.java:73)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.dispatch(VertxRequestHandler.java:151)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.handle(VertxRequestHandler.java:82)
keycloak_1  |   at io.quarkus.resteasy.runtime.standalone.VertxRequestHandler.handle(VertxRequestHandler.java:42)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at io.quarkus.vertx.http.runtime.StaticResourcesRecorder$2.handle(StaticResourcesRecorder.java:84)
keycloak_1  |   at io.quarkus.vertx.http.runtime.StaticResourcesRecorder$2.handle(StaticResourcesRecorder.java:71)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at io.quarkus.vertx.http.runtime.VertxHttpRecorder$6.handle(VertxHttpRecorder.java:430)
keycloak_1  |   at io.quarkus.vertx.http.runtime.VertxHttpRecorder$6.handle(VertxHttpRecorder.java:408)
keycloak_1  |   at io.vertx.ext.web.impl.RouteState.handleContext(RouteState.java:1284)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImplBase.iterateNext(RoutingContextImplBase.java:173)
keycloak_1  |   at io.vertx.ext.web.impl.RoutingContextImpl.next(RoutingContextImpl.java:140)
keycloak_1  |   at org.keycloak.quarkus.runtime.integration.web.QuarkusRequestFilter.lambda$createBlockingHandler$0(QuarkusRequestFilter.java:82)
keycloak_1  |   at io.quarkus.vertx.core.runtime.VertxCoreRecorder$14.runWith(VertxCoreRecorder.java:564)
keycloak_1  |   at org.jboss.threads.EnhancedQueueExecutor$Task.run(EnhancedQueueExecutor.java:2449)
keycloak_1  |   at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1478)
keycloak_1  |   at org.jboss.threads.DelegatingRunnable.run(DelegatingRunnable.java:29)
keycloak_1  |   at org.jboss.threads.ThreadLocalResettingRunnable.run(ThreadLocalResettingRunnable.java:29)
keycloak_1  |   at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
keycloak_1  |   at java.base/java.lang.Thread.run(Thread.java:829)
keycloak_1  | Caused by: java.lang.IllegalArgumentException: password empty
keycloak_1  |   at org.bouncycastle.jcajce.provider.symmetric.PBEPBKDF2$BasePBKDF2.engineGenerateSecret(Unknown Source)
keycloak_1  |   at java.base/javax.crypto.SecretKeyFactory.generateSecret(SecretKeyFactory.java:338)
keycloak_1  |   at org.keycloak.credential.hash.Pbkdf2PasswordHashProvider.encodedCredential(Pbkdf2PasswordHashProvider.java:111)
keycloak_1  |   ... 72 more
```",keycloak,https://github.com/keycloak/keycloak
16,12141.0,"### Describe the bug

We are configuring a IdentityProvider in Keycloak with PKCE enabled and PKCE method ""SH256"". In the token request Keycloak sends the parameter ""code_challenge_method"" although it's non-standard.  The Identity-Provider we configure cancels the request because the request body contains a non-standard parameter. For security reasons all requests with non-standard parameters are blocked.

Specs: 
- https://openid.net/specs/openid-connect-core-1_0.html#TokenRequest
- https://datatracker.ietf.org/doc/html/rfc7636#section-6.1


Can the parameter ""code_challenge_method"" be removed in one of the next keycloak versions?

### Version

18.0.0

### Expected behavior

Token Request should not contain parameter ""code_challenge_method""

 ```
 POST /token HTTP/1.1
Host: server.example.com
Content-Type: application/x-www-form-urlencoded
Authorization: Basic czZCaGRSa3F0MzpnWDFmQmF0M2JW

grant_type=authorization_code
&code=SplxlOBeZQQYbYS6WxSbIA
&redirect_uri=https%3A%2F%2Fclient.example.org%2Fcb
&code_verifier=random_code_verifier
```

### Actual behavior

Token Request contains parameter ""code_challenge_method""

 ```
 POST /token HTTP/1.1
Host: server.example.com
Content-Type: application/x-www-form-urlencoded
Authorization: Basic czZCaGRSa3F0MzpnWDFmQmF0M2JW

grant_type=authorization_code
&code=SplxlOBeZQQYbYS6WxSbIA
&redirect_uri=https%3A%2F%2Fclient.example.org%2Fcb
&code_verifier=random_code_verifier
&code_challenge_method=S256
```

### How to Reproduce?

_No response_

### Anything else?

_No response_",code_challenge_method,https://github.com/keycloak/keycloak
17,11897.0,"### Describe the bug

Hi,

Before version 13.0.0 a client registration through the API returned the client-secret in the response. Now, while not documented, the secret is no longer part of the registration response. The JSON field in question is called ""secret"". As you can imagine, this breaks client automatisation and forces changes to the API consumer code.

 

Can, preferably, the previous behaviour be restored? If it's not a bug, the change should be documented and certainly included in the release notes.

curl command:

 ```
curl -sSL -X POST -H 'Content-Type:application/json' -H ""Authorization: bearer $INIT_TOKEN"" \
-d ""{ \""clientId\"": \""$CLIENT_ID\"", \""serviceAccountsEnabled\"": true, \""redirectUris\"": [\""https://localhost/\""]}"" \
$KEYCLOAK_REGISTRATION_URL | jq
 ```

output in 12.0.4:

 
```json
{
  ""id"": ""4e1dd8d9-aeab-4a9f-94db-45aad46dc2d4"",
  ""clientId"": ""rapi_ccis"",
  ""surrogateAuthRequired"": false,
  ""enabled"": true,
  ""alwaysDisplayInConsole"": false,
  ""clientAuthenticatorType"": ""client-secret"",
  ""secret"": ""ca42e775-75a3-4eb4-ac91-56eb1991dee8"",
  ""registrationAccessToken"": ""eyJhbGciOiJIUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJjZWJmNTJhNy04MmRlLTQ4OTEtYmZmZC1lNmE2MDI4ZGI4ZWUifQ.eyJleHAiOjAsImlhdCI6MTYyMTgxNjEwMSwianRpIjoiMzRhZTA5YmMtNmI2MC00ODUzLThhMjUtNmFkZWQxNDUyMjJhIiwiaXNzIjoiaHR0cDovL2xvY2FsaG9zdDo4MDgwL2F1dGgvcmVhbG1zL21hc3RlciIsImF1ZCI6Imh0dHA6Ly9sb2NhbGhvc3Q6ODA4MC9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJ0eXAiOiJSZWdpc3RyYXRpb25BY2Nlc3NUb2tlbiIsInJlZ2lzdHJhdGlvbl9hdXRoIjoiYXV0aGVudGljYXRlZCJ9.eyRQXCXZLo1rMIc34asoD-i9VDYtF5mniZSezc6Kjxg"",
  ""redirectUris"": [
    ""https://localhost/""
  ],
  ""webOrigins"": [
    ""https://localhost""
  ],
  ""notBefore"": 0,
  ""bearerOnly"": false,
  ""consentRequired"": false,
  ""standardFlowEnabled"": true,
  ""implicitFlowEnabled"": false,
  ""directAccessGrantsEnabled"": false,
  ""serviceAccountsEnabled"": true,
  ""publicClient"": false,
  ""frontchannelLogout"": false,
  ""protocol"": ""openid-connect"",
  ""attributes"": {},
  ""authenticationFlowBindingOverrides"": {},
  ""fullScopeAllowed"": true,
  ""nodeReRegistrationTimeout"": -1,
  ""defaultClientScopes"": [
    ""web-origins"",
    ""role_list"",
    ""roles"",
    ""profile"",
    ""email""
  ],
  ""optionalClientScopes"": [
    ""address"",
    ""phone"",
    ""openid"",
    ""offline_access"",
    ""microprofile-jwt"",
  ]
}
```

output in 13.0.0:

```json
{
  ""id"": ""80672982-e3a5-4f9c-a213-f401e5d68127"",
  ""clientId"": ""rapi_ccis"",
  ""surrogateAuthRequired"": false,
  ""enabled"": true,
  ""alwaysDisplayInConsole"": false,
  ""clientAuthenticatorType"": ""client-secret"",
  ""registrationAccessToken"": ""eyJhbGciOiJIUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICIwZmI3NjA1Zi01Y2Y3LTQwODItYjcwYy04NWVjNTVjNTZlYWUifQ.eyJleHAiOjAsImlhdCI6MTYyMTgxNjM2OSwianRpIjoiZDI3YTJmMGItNjY3NS00YWRhLWJiYWQtZmZmYWQ2YTU0YTIwIiwiaXNzIjoiaHR0cDovL2xvY2FsaG9zdDo4MDgwL2F1dGgvcmVhbG1zL21hc3RlciIsImF1ZCI6Imh0dHA6Ly9sb2NhbGhvc3Q6ODA4MC9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJ0eXAiOiJSZWdpc3RyYXRpb25BY2Nlc3NUb2tlbiIsInJlZ2lzdHJhdGlvbl9hdXRoIjoiYXV0aGVudGljYXRlZCJ9.DE2Tx0Z94EC72R4d8hTdAQwbdKXJ9NqV5kYdq1rY8qM"",
  ""redirectUris"": [
    ""https://localhost/""
  ],
  ""webOrigins"": [
    ""https://localhost""
  ],
  ""notBefore"": 0,
  ""bearerOnly"": false,
  ""consentRequired"": false,
  ""standardFlowEnabled"": true,
  ""implicitFlowEnabled"": false,
  ""directAccessGrantsEnabled"": false,
  ""serviceAccountsEnabled"": true,
  ""publicClient"": false,
  ""frontchannelLogout"": false,
  ""protocol"": ""openid-connect"",
  ""attributes"": {},
  ""authenticationFlowBindingOverrides"": {},
  ""fullScopeAllowed"": true,
  ""nodeReRegistrationTimeout"": -1,
  ""defaultClientScopes"": [
    ""web-origins"",
    ""roles"",
    ""profile"",
    ""email""
  ],
  ""optionalClientScopes"": [
    ""address"",
    ""phone"",
    ""openid"",
    ""offline_access"",
    ""microprofile-jwt"",
  ]
}
```

Thank you.

### Version

 13.0.0

### Expected behavior

_No response_

### Actual behavior

_No response_

### How to Reproduce?

_No response_

### Anything else?

https://issues.redhat.com/browse/KEYCLOAK-18257",ca42e775-75a3-4eb4-ac91-56eb1991dee8,https://github.com/keycloak/keycloak
18,11897.0,"### Describe the bug

Hi,

Before version 13.0.0 a client registration through the API returned the client-secret in the response. Now, while not documented, the secret is no longer part of the registration response. The JSON field in question is called ""secret"". As you can imagine, this breaks client automatisation and forces changes to the API consumer code.

 

Can, preferably, the previous behaviour be restored? If it's not a bug, the change should be documented and certainly included in the release notes.

curl command:

 ```
curl -sSL -X POST -H 'Content-Type:application/json' -H ""Authorization: bearer $INIT_TOKEN"" \
-d ""{ \""clientId\"": \""$CLIENT_ID\"", \""serviceAccountsEnabled\"": true, \""redirectUris\"": [\""https://localhost/\""]}"" \
$KEYCLOAK_REGISTRATION_URL | jq
 ```

output in 12.0.4:

 
```json
{
  ""id"": ""4e1dd8d9-aeab-4a9f-94db-45aad46dc2d4"",
  ""clientId"": ""rapi_ccis"",
  ""surrogateAuthRequired"": false,
  ""enabled"": true,
  ""alwaysDisplayInConsole"": false,
  ""clientAuthenticatorType"": ""client-secret"",
  ""secret"": ""ca42e775-75a3-4eb4-ac91-56eb1991dee8"",
  ""registrationAccessToken"": ""eyJhbGciOiJIUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJjZWJmNTJhNy04MmRlLTQ4OTEtYmZmZC1lNmE2MDI4ZGI4ZWUifQ.eyJleHAiOjAsImlhdCI6MTYyMTgxNjEwMSwianRpIjoiMzRhZTA5YmMtNmI2MC00ODUzLThhMjUtNmFkZWQxNDUyMjJhIiwiaXNzIjoiaHR0cDovL2xvY2FsaG9zdDo4MDgwL2F1dGgvcmVhbG1zL21hc3RlciIsImF1ZCI6Imh0dHA6Ly9sb2NhbGhvc3Q6ODA4MC9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJ0eXAiOiJSZWdpc3RyYXRpb25BY2Nlc3NUb2tlbiIsInJlZ2lzdHJhdGlvbl9hdXRoIjoiYXV0aGVudGljYXRlZCJ9.eyRQXCXZLo1rMIc34asoD-i9VDYtF5mniZSezc6Kjxg"",
  ""redirectUris"": [
    ""https://localhost/""
  ],
  ""webOrigins"": [
    ""https://localhost""
  ],
  ""notBefore"": 0,
  ""bearerOnly"": false,
  ""consentRequired"": false,
  ""standardFlowEnabled"": true,
  ""implicitFlowEnabled"": false,
  ""directAccessGrantsEnabled"": false,
  ""serviceAccountsEnabled"": true,
  ""publicClient"": false,
  ""frontchannelLogout"": false,
  ""protocol"": ""openid-connect"",
  ""attributes"": {},
  ""authenticationFlowBindingOverrides"": {},
  ""fullScopeAllowed"": true,
  ""nodeReRegistrationTimeout"": -1,
  ""defaultClientScopes"": [
    ""web-origins"",
    ""role_list"",
    ""roles"",
    ""profile"",
    ""email""
  ],
  ""optionalClientScopes"": [
    ""address"",
    ""phone"",
    ""openid"",
    ""offline_access"",
    ""microprofile-jwt"",
  ]
}
```

output in 13.0.0:

```json
{
  ""id"": ""80672982-e3a5-4f9c-a213-f401e5d68127"",
  ""clientId"": ""rapi_ccis"",
  ""surrogateAuthRequired"": false,
  ""enabled"": true,
  ""alwaysDisplayInConsole"": false,
  ""clientAuthenticatorType"": ""client-secret"",
  ""registrationAccessToken"": ""eyJhbGciOiJIUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICIwZmI3NjA1Zi01Y2Y3LTQwODItYjcwYy04NWVjNTVjNTZlYWUifQ.eyJleHAiOjAsImlhdCI6MTYyMTgxNjM2OSwianRpIjoiZDI3YTJmMGItNjY3NS00YWRhLWJiYWQtZmZmYWQ2YTU0YTIwIiwiaXNzIjoiaHR0cDovL2xvY2FsaG9zdDo4MDgwL2F1dGgvcmVhbG1zL21hc3RlciIsImF1ZCI6Imh0dHA6Ly9sb2NhbGhvc3Q6ODA4MC9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJ0eXAiOiJSZWdpc3RyYXRpb25BY2Nlc3NUb2tlbiIsInJlZ2lzdHJhdGlvbl9hdXRoIjoiYXV0aGVudGljYXRlZCJ9.DE2Tx0Z94EC72R4d8hTdAQwbdKXJ9NqV5kYdq1rY8qM"",
  ""redirectUris"": [
    ""https://localhost/""
  ],
  ""webOrigins"": [
    ""https://localhost""
  ],
  ""notBefore"": 0,
  ""bearerOnly"": false,
  ""consentRequired"": false,
  ""standardFlowEnabled"": true,
  ""implicitFlowEnabled"": false,
  ""directAccessGrantsEnabled"": false,
  ""serviceAccountsEnabled"": true,
  ""publicClient"": false,
  ""frontchannelLogout"": false,
  ""protocol"": ""openid-connect"",
  ""attributes"": {},
  ""authenticationFlowBindingOverrides"": {},
  ""fullScopeAllowed"": true,
  ""nodeReRegistrationTimeout"": -1,
  ""defaultClientScopes"": [
    ""web-origins"",
    ""roles"",
    ""profile"",
    ""email""
  ],
  ""optionalClientScopes"": [
    ""address"",
    ""phone"",
    ""openid"",
    ""offline_access"",
    ""microprofile-jwt"",
  ]
}
```

Thank you.

### Version

 13.0.0

### Expected behavior

_No response_

### Actual behavior

_No response_

### How to Reproduce?

_No response_

### Anything else?

https://issues.redhat.com/browse/KEYCLOAK-18257",serviceAccountsEnabled,https://github.com/keycloak/keycloak
19,11897.0,"### Describe the bug

Hi,

Before version 13.0.0 a client registration through the API returned the client-secret in the response. Now, while not documented, the secret is no longer part of the registration response. The JSON field in question is called ""secret"". As you can imagine, this breaks client automatisation and forces changes to the API consumer code.

 

Can, preferably, the previous behaviour be restored? If it's not a bug, the change should be documented and certainly included in the release notes.

curl command:

 ```
curl -sSL -X POST -H 'Content-Type:application/json' -H ""Authorization: bearer $INIT_TOKEN"" \
-d ""{ \""clientId\"": \""$CLIENT_ID\"", \""serviceAccountsEnabled\"": true, \""redirectUris\"": [\""https://localhost/\""]}"" \
$KEYCLOAK_REGISTRATION_URL | jq
 ```

output in 12.0.4:

 
```json
{
  ""id"": ""4e1dd8d9-aeab-4a9f-94db-45aad46dc2d4"",
  ""clientId"": ""rapi_ccis"",
  ""surrogateAuthRequired"": false,
  ""enabled"": true,
  ""alwaysDisplayInConsole"": false,
  ""clientAuthenticatorType"": ""client-secret"",
  ""secret"": ""ca42e775-75a3-4eb4-ac91-56eb1991dee8"",
  ""registrationAccessToken"": ""eyJhbGciOiJIUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJjZWJmNTJhNy04MmRlLTQ4OTEtYmZmZC1lNmE2MDI4ZGI4ZWUifQ.eyJleHAiOjAsImlhdCI6MTYyMTgxNjEwMSwianRpIjoiMzRhZTA5YmMtNmI2MC00ODUzLThhMjUtNmFkZWQxNDUyMjJhIiwiaXNzIjoiaHR0cDovL2xvY2FsaG9zdDo4MDgwL2F1dGgvcmVhbG1zL21hc3RlciIsImF1ZCI6Imh0dHA6Ly9sb2NhbGhvc3Q6ODA4MC9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJ0eXAiOiJSZWdpc3RyYXRpb25BY2Nlc3NUb2tlbiIsInJlZ2lzdHJhdGlvbl9hdXRoIjoiYXV0aGVudGljYXRlZCJ9.eyRQXCXZLo1rMIc34asoD-i9VDYtF5mniZSezc6Kjxg"",
  ""redirectUris"": [
    ""https://localhost/""
  ],
  ""webOrigins"": [
    ""https://localhost""
  ],
  ""notBefore"": 0,
  ""bearerOnly"": false,
  ""consentRequired"": false,
  ""standardFlowEnabled"": true,
  ""implicitFlowEnabled"": false,
  ""directAccessGrantsEnabled"": false,
  ""serviceAccountsEnabled"": true,
  ""publicClient"": false,
  ""frontchannelLogout"": false,
  ""protocol"": ""openid-connect"",
  ""attributes"": {},
  ""authenticationFlowBindingOverrides"": {},
  ""fullScopeAllowed"": true,
  ""nodeReRegistrationTimeout"": -1,
  ""defaultClientScopes"": [
    ""web-origins"",
    ""role_list"",
    ""roles"",
    ""profile"",
    ""email""
  ],
  ""optionalClientScopes"": [
    ""address"",
    ""phone"",
    ""openid"",
    ""offline_access"",
    ""microprofile-jwt"",
  ]
}
```

output in 13.0.0:

```json
{
  ""id"": ""80672982-e3a5-4f9c-a213-f401e5d68127"",
  ""clientId"": ""rapi_ccis"",
  ""surrogateAuthRequired"": false,
  ""enabled"": true,
  ""alwaysDisplayInConsole"": false,
  ""clientAuthenticatorType"": ""client-secret"",
  ""registrationAccessToken"": ""eyJhbGciOiJIUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICIwZmI3NjA1Zi01Y2Y3LTQwODItYjcwYy04NWVjNTVjNTZlYWUifQ.eyJleHAiOjAsImlhdCI6MTYyMTgxNjM2OSwianRpIjoiZDI3YTJmMGItNjY3NS00YWRhLWJiYWQtZmZmYWQ2YTU0YTIwIiwiaXNzIjoiaHR0cDovL2xvY2FsaG9zdDo4MDgwL2F1dGgvcmVhbG1zL21hc3RlciIsImF1ZCI6Imh0dHA6Ly9sb2NhbGhvc3Q6ODA4MC9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJ0eXAiOiJSZWdpc3RyYXRpb25BY2Nlc3NUb2tlbiIsInJlZ2lzdHJhdGlvbl9hdXRoIjoiYXV0aGVudGljYXRlZCJ9.DE2Tx0Z94EC72R4d8hTdAQwbdKXJ9NqV5kYdq1rY8qM"",
  ""redirectUris"": [
    ""https://localhost/""
  ],
  ""webOrigins"": [
    ""https://localhost""
  ],
  ""notBefore"": 0,
  ""bearerOnly"": false,
  ""consentRequired"": false,
  ""standardFlowEnabled"": true,
  ""implicitFlowEnabled"": false,
  ""directAccessGrantsEnabled"": false,
  ""serviceAccountsEnabled"": true,
  ""publicClient"": false,
  ""frontchannelLogout"": false,
  ""protocol"": ""openid-connect"",
  ""attributes"": {},
  ""authenticationFlowBindingOverrides"": {},
  ""fullScopeAllowed"": true,
  ""nodeReRegistrationTimeout"": -1,
  ""defaultClientScopes"": [
    ""web-origins"",
    ""roles"",
    ""profile"",
    ""email""
  ],
  ""optionalClientScopes"": [
    ""address"",
    ""phone"",
    ""openid"",
    ""offline_access"",
    ""microprofile-jwt"",
  ]
}
```

Thank you.

### Version

 13.0.0

### Expected behavior

_No response_

### Actual behavior

_No response_

### How to Reproduce?

_No response_

### Anything else?

https://issues.redhat.com/browse/KEYCLOAK-18257",i9VDYtF5mniZSezc6Kjxg,https://github.com/keycloak/keycloak
20,11897.0,"### Describe the bug

Hi,

Before version 13.0.0 a client registration through the API returned the client-secret in the response. Now, while not documented, the secret is no longer part of the registration response. The JSON field in question is called ""secret"". As you can imagine, this breaks client automatisation and forces changes to the API consumer code.

 

Can, preferably, the previous behaviour be restored? If it's not a bug, the change should be documented and certainly included in the release notes.

curl command:

 ```
curl -sSL -X POST -H 'Content-Type:application/json' -H ""Authorization: bearer $INIT_TOKEN"" \
-d ""{ \""clientId\"": \""$CLIENT_ID\"", \""serviceAccountsEnabled\"": true, \""redirectUris\"": [\""https://localhost/\""]}"" \
$KEYCLOAK_REGISTRATION_URL | jq
 ```

output in 12.0.4:

 
```json
{
  ""id"": ""4e1dd8d9-aeab-4a9f-94db-45aad46dc2d4"",
  ""clientId"": ""rapi_ccis"",
  ""surrogateAuthRequired"": false,
  ""enabled"": true,
  ""alwaysDisplayInConsole"": false,
  ""clientAuthenticatorType"": ""client-secret"",
  ""secret"": ""ca42e775-75a3-4eb4-ac91-56eb1991dee8"",
  ""registrationAccessToken"": ""eyJhbGciOiJIUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJjZWJmNTJhNy04MmRlLTQ4OTEtYmZmZC1lNmE2MDI4ZGI4ZWUifQ.eyJleHAiOjAsImlhdCI6MTYyMTgxNjEwMSwianRpIjoiMzRhZTA5YmMtNmI2MC00ODUzLThhMjUtNmFkZWQxNDUyMjJhIiwiaXNzIjoiaHR0cDovL2xvY2FsaG9zdDo4MDgwL2F1dGgvcmVhbG1zL21hc3RlciIsImF1ZCI6Imh0dHA6Ly9sb2NhbGhvc3Q6ODA4MC9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJ0eXAiOiJSZWdpc3RyYXRpb25BY2Nlc3NUb2tlbiIsInJlZ2lzdHJhdGlvbl9hdXRoIjoiYXV0aGVudGljYXRlZCJ9.eyRQXCXZLo1rMIc34asoD-i9VDYtF5mniZSezc6Kjxg"",
  ""redirectUris"": [
    ""https://localhost/""
  ],
  ""webOrigins"": [
    ""https://localhost""
  ],
  ""notBefore"": 0,
  ""bearerOnly"": false,
  ""consentRequired"": false,
  ""standardFlowEnabled"": true,
  ""implicitFlowEnabled"": false,
  ""directAccessGrantsEnabled"": false,
  ""serviceAccountsEnabled"": true,
  ""publicClient"": false,
  ""frontchannelLogout"": false,
  ""protocol"": ""openid-connect"",
  ""attributes"": {},
  ""authenticationFlowBindingOverrides"": {},
  ""fullScopeAllowed"": true,
  ""nodeReRegistrationTimeout"": -1,
  ""defaultClientScopes"": [
    ""web-origins"",
    ""role_list"",
    ""roles"",
    ""profile"",
    ""email""
  ],
  ""optionalClientScopes"": [
    ""address"",
    ""phone"",
    ""openid"",
    ""offline_access"",
    ""microprofile-jwt"",
  ]
}
```

output in 13.0.0:

```json
{
  ""id"": ""80672982-e3a5-4f9c-a213-f401e5d68127"",
  ""clientId"": ""rapi_ccis"",
  ""surrogateAuthRequired"": false,
  ""enabled"": true,
  ""alwaysDisplayInConsole"": false,
  ""clientAuthenticatorType"": ""client-secret"",
  ""registrationAccessToken"": ""eyJhbGciOiJIUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICIwZmI3NjA1Zi01Y2Y3LTQwODItYjcwYy04NWVjNTVjNTZlYWUifQ.eyJleHAiOjAsImlhdCI6MTYyMTgxNjM2OSwianRpIjoiZDI3YTJmMGItNjY3NS00YWRhLWJiYWQtZmZmYWQ2YTU0YTIwIiwiaXNzIjoiaHR0cDovL2xvY2FsaG9zdDo4MDgwL2F1dGgvcmVhbG1zL21hc3RlciIsImF1ZCI6Imh0dHA6Ly9sb2NhbGhvc3Q6ODA4MC9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJ0eXAiOiJSZWdpc3RyYXRpb25BY2Nlc3NUb2tlbiIsInJlZ2lzdHJhdGlvbl9hdXRoIjoiYXV0aGVudGljYXRlZCJ9.DE2Tx0Z94EC72R4d8hTdAQwbdKXJ9NqV5kYdq1rY8qM"",
  ""redirectUris"": [
    ""https://localhost/""
  ],
  ""webOrigins"": [
    ""https://localhost""
  ],
  ""notBefore"": 0,
  ""bearerOnly"": false,
  ""consentRequired"": false,
  ""standardFlowEnabled"": true,
  ""implicitFlowEnabled"": false,
  ""directAccessGrantsEnabled"": false,
  ""serviceAccountsEnabled"": true,
  ""publicClient"": false,
  ""frontchannelLogout"": false,
  ""protocol"": ""openid-connect"",
  ""attributes"": {},
  ""authenticationFlowBindingOverrides"": {},
  ""fullScopeAllowed"": true,
  ""nodeReRegistrationTimeout"": -1,
  ""defaultClientScopes"": [
    ""web-origins"",
    ""roles"",
    ""profile"",
    ""email""
  ],
  ""optionalClientScopes"": [
    ""address"",
    ""phone"",
    ""openid"",
    ""offline_access"",
    ""microprofile-jwt"",
  ]
}
```

Thank you.

### Version

 13.0.0

### Expected behavior

_No response_

### Actual behavior

_No response_

### How to Reproduce?

_No response_

### Anything else?

https://issues.redhat.com/browse/KEYCLOAK-18257",ac91-56eb1991dee8,https://github.com/keycloak/keycloak
21,11897.0,"### Describe the bug

Hi,

Before version 13.0.0 a client registration through the API returned the client-secret in the response. Now, while not documented, the secret is no longer part of the registration response. The JSON field in question is called ""secret"". As you can imagine, this breaks client automatisation and forces changes to the API consumer code.

 

Can, preferably, the previous behaviour be restored? If it's not a bug, the change should be documented and certainly included in the release notes.

curl command:

 ```
curl -sSL -X POST -H 'Content-Type:application/json' -H ""Authorization: bearer $INIT_TOKEN"" \
-d ""{ \""clientId\"": \""$CLIENT_ID\"", \""serviceAccountsEnabled\"": true, \""redirectUris\"": [\""https://localhost/\""]}"" \
$KEYCLOAK_REGISTRATION_URL | jq
 ```

output in 12.0.4:

 
```json
{
  ""id"": ""4e1dd8d9-aeab-4a9f-94db-45aad46dc2d4"",
  ""clientId"": ""rapi_ccis"",
  ""surrogateAuthRequired"": false,
  ""enabled"": true,
  ""alwaysDisplayInConsole"": false,
  ""clientAuthenticatorType"": ""client-secret"",
  ""secret"": ""ca42e775-75a3-4eb4-ac91-56eb1991dee8"",
  ""registrationAccessToken"": ""eyJhbGciOiJIUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJjZWJmNTJhNy04MmRlLTQ4OTEtYmZmZC1lNmE2MDI4ZGI4ZWUifQ.eyJleHAiOjAsImlhdCI6MTYyMTgxNjEwMSwianRpIjoiMzRhZTA5YmMtNmI2MC00ODUzLThhMjUtNmFkZWQxNDUyMjJhIiwiaXNzIjoiaHR0cDovL2xvY2FsaG9zdDo4MDgwL2F1dGgvcmVhbG1zL21hc3RlciIsImF1ZCI6Imh0dHA6Ly9sb2NhbGhvc3Q6ODA4MC9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJ0eXAiOiJSZWdpc3RyYXRpb25BY2Nlc3NUb2tlbiIsInJlZ2lzdHJhdGlvbl9hdXRoIjoiYXV0aGVudGljYXRlZCJ9.eyRQXCXZLo1rMIc34asoD-i9VDYtF5mniZSezc6Kjxg"",
  ""redirectUris"": [
    ""https://localhost/""
  ],
  ""webOrigins"": [
    ""https://localhost""
  ],
  ""notBefore"": 0,
  ""bearerOnly"": false,
  ""consentRequired"": false,
  ""standardFlowEnabled"": true,
  ""implicitFlowEnabled"": false,
  ""directAccessGrantsEnabled"": false,
  ""serviceAccountsEnabled"": true,
  ""publicClient"": false,
  ""frontchannelLogout"": false,
  ""protocol"": ""openid-connect"",
  ""attributes"": {},
  ""authenticationFlowBindingOverrides"": {},
  ""fullScopeAllowed"": true,
  ""nodeReRegistrationTimeout"": -1,
  ""defaultClientScopes"": [
    ""web-origins"",
    ""role_list"",
    ""roles"",
    ""profile"",
    ""email""
  ],
  ""optionalClientScopes"": [
    ""address"",
    ""phone"",
    ""openid"",
    ""offline_access"",
    ""microprofile-jwt"",
  ]
}
```

output in 13.0.0:

```json
{
  ""id"": ""80672982-e3a5-4f9c-a213-f401e5d68127"",
  ""clientId"": ""rapi_ccis"",
  ""surrogateAuthRequired"": false,
  ""enabled"": true,
  ""alwaysDisplayInConsole"": false,
  ""clientAuthenticatorType"": ""client-secret"",
  ""registrationAccessToken"": ""eyJhbGciOiJIUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICIwZmI3NjA1Zi01Y2Y3LTQwODItYjcwYy04NWVjNTVjNTZlYWUifQ.eyJleHAiOjAsImlhdCI6MTYyMTgxNjM2OSwianRpIjoiZDI3YTJmMGItNjY3NS00YWRhLWJiYWQtZmZmYWQ2YTU0YTIwIiwiaXNzIjoiaHR0cDovL2xvY2FsaG9zdDo4MDgwL2F1dGgvcmVhbG1zL21hc3RlciIsImF1ZCI6Imh0dHA6Ly9sb2NhbGhvc3Q6ODA4MC9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJ0eXAiOiJSZWdpc3RyYXRpb25BY2Nlc3NUb2tlbiIsInJlZ2lzdHJhdGlvbl9hdXRoIjoiYXV0aGVudGljYXRlZCJ9.DE2Tx0Z94EC72R4d8hTdAQwbdKXJ9NqV5kYdq1rY8qM"",
  ""redirectUris"": [
    ""https://localhost/""
  ],
  ""webOrigins"": [
    ""https://localhost""
  ],
  ""notBefore"": 0,
  ""bearerOnly"": false,
  ""consentRequired"": false,
  ""standardFlowEnabled"": true,
  ""implicitFlowEnabled"": false,
  ""directAccessGrantsEnabled"": false,
  ""serviceAccountsEnabled"": true,
  ""publicClient"": false,
  ""frontchannelLogout"": false,
  ""protocol"": ""openid-connect"",
  ""attributes"": {},
  ""authenticationFlowBindingOverrides"": {},
  ""fullScopeAllowed"": true,
  ""nodeReRegistrationTimeout"": -1,
  ""defaultClientScopes"": [
    ""web-origins"",
    ""roles"",
    ""profile"",
    ""email""
  ],
  ""optionalClientScopes"": [
    ""address"",
    ""phone"",
    ""openid"",
    ""offline_access"",
    ""microprofile-jwt"",
  ]
}
```

Thank you.

### Version

 13.0.0

### Expected behavior

_No response_

### Actual behavior

_No response_

### How to Reproduce?

_No response_

### Anything else?

https://issues.redhat.com/browse/KEYCLOAK-18257",registrationAccessToken,https://github.com/keycloak/keycloak
25,7650.0,"<!---
Please read https://github.com/keycloak/keycloak/blob/master/CONTRIBUTING.md and follow these guidelines when contributing to Keycloak
-->
Allow using Client Java for token exchanging. E.g.
```lang-kotlin
fun main() {
    val authzClient = AuthzClient.create()
    val request = TokenExchangeRequest()
    request.clientId = ""training-service""
    request.clientSecret = ""b56e9bb4-5037-32aa-b823-3f59f231e418""
    request.subjectToken = ""eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJMZ3pHN1dBOWZWNXE4Wl94TEVmOFNkRl94LWx3NWF6cnY0SUxoNjdTNEdZIn0.eyJleHAiOjE2MDY3NzE3ODksImlhdCI6MTYwNjczNTc5MCwiYXV0aF90aW1lIjoxNjA2NzM1Nzg5LCJqdGkiOiI4NDdjMTJlNi04M2Y1LTQ5MmEtYTI1My05OTVkYWM0ZDA0MWUiLCJpc3MiOiJodHRwOi8vbG9jYWxob3N0L2F1dGgvcmVhbG1zL2Jhc2tldG1hc3RlciIsImF1ZCI6ImFjY291bnQiLCJzdWIiOiI4MzgxYjYyOS01ZjEwLTQwMWMtYWU5MC1iYjM3NzY5ZTVmNzAiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJ0ZWFtcy1mcm9udGVuZCIsIm5vbmNlIjoiMmExYzRkZGItYTk2YS00YjRkLWE0ZDUtNmY4ZmVkOGIzOGI4Iiwic2Vzc2lvbl9zdGF0ZSI6IjUwMjJhYTE3LTlkZjEtNDc2My1hMmY4LWMxY2JkNGQ2ODYwMSIsImFjciI6IjEiLCJhbGxvd2VkLW9yaWdpbnMiOlsiKiJdLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsib2ZmbGluZV9hY2Nlc3MiLCJ1bWFfYXV0aG9yaXphdGlvbiJdfSwicmVzb3VyY2VfYWNjZXNzIjp7ImFjY291bnQiOnsicm9sZXMiOlsibWFuYWdlLWFjY291bnQiLCJtYW5hZ2UtYWNjb3VudC1saW5rcyIsInZpZXctcHJvZmlsZSJdfX0sInNjb3BlIjoib3BlbmlkIGVtYWlsIHByb2ZpbGUiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwibmFtZSI6IlRlc3QgRmlyc3QiLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJ0ZXN0IiwiZ2l2ZW5fbmFtZSI6IlRlc3QiLCJmYW1pbHlfbmFtZSI6IkZpcnN0IiwiZW1haWwiOiJ0ZXN0QGludmVudC5jb20ifQ.gIYCFqoW9qQ-BKlOJbOslYuGmC0Fhncwu868tKwV8tk_GX06p39Xh6lf6r3_zDI7QBKmZeZZzQFIpsYWv9JvvAFzLm2js0BFOHPTHIxsx74rgHi-Q9p4S1b7P0jMoK4A3vgRtywC2djhWfs639A2kcBRM2PDIr-TWAsCs9CAML3-ilPb6_LGiCRKkO28BCZFxLQ0XEpg2h_0Z_vD93y1HQOZtEK9kxLrzxPFFNcV1t03x3JxcHrJzsNTtmHkOFqakpBcLNddbsMISusFo-HkOefs4tpaUZpaqukoIPYjcWSQf_UN1n3AVga2SSSLO77WuG8Bu6Pnc_iZqxt7aY1s_A""
    request.audience = ""training-service""
    val tokenResponse = authzClient.exchange().exchange(request)
    println(tokenResponse.token)
}
```",b56e9bb4-5037-32aa-b823-3f59f231e418,https://github.com/keycloak/keycloak
26,7650.0,"<!---
Please read https://github.com/keycloak/keycloak/blob/master/CONTRIBUTING.md and follow these guidelines when contributing to Keycloak
-->
Allow using Client Java for token exchanging. E.g.
```lang-kotlin
fun main() {
    val authzClient = AuthzClient.create()
    val request = TokenExchangeRequest()
    request.clientId = ""training-service""
    request.clientSecret = ""b56e9bb4-5037-32aa-b823-3f59f231e418""
    request.subjectToken = ""eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJMZ3pHN1dBOWZWNXE4Wl94TEVmOFNkRl94LWx3NWF6cnY0SUxoNjdTNEdZIn0.eyJleHAiOjE2MDY3NzE3ODksImlhdCI6MTYwNjczNTc5MCwiYXV0aF90aW1lIjoxNjA2NzM1Nzg5LCJqdGkiOiI4NDdjMTJlNi04M2Y1LTQ5MmEtYTI1My05OTVkYWM0ZDA0MWUiLCJpc3MiOiJodHRwOi8vbG9jYWxob3N0L2F1dGgvcmVhbG1zL2Jhc2tldG1hc3RlciIsImF1ZCI6ImFjY291bnQiLCJzdWIiOiI4MzgxYjYyOS01ZjEwLTQwMWMtYWU5MC1iYjM3NzY5ZTVmNzAiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJ0ZWFtcy1mcm9udGVuZCIsIm5vbmNlIjoiMmExYzRkZGItYTk2YS00YjRkLWE0ZDUtNmY4ZmVkOGIzOGI4Iiwic2Vzc2lvbl9zdGF0ZSI6IjUwMjJhYTE3LTlkZjEtNDc2My1hMmY4LWMxY2JkNGQ2ODYwMSIsImFjciI6IjEiLCJhbGxvd2VkLW9yaWdpbnMiOlsiKiJdLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsib2ZmbGluZV9hY2Nlc3MiLCJ1bWFfYXV0aG9yaXphdGlvbiJdfSwicmVzb3VyY2VfYWNjZXNzIjp7ImFjY291bnQiOnsicm9sZXMiOlsibWFuYWdlLWFjY291bnQiLCJtYW5hZ2UtYWNjb3VudC1saW5rcyIsInZpZXctcHJvZmlsZSJdfX0sInNjb3BlIjoib3BlbmlkIGVtYWlsIHByb2ZpbGUiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwibmFtZSI6IlRlc3QgRmlyc3QiLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJ0ZXN0IiwiZ2l2ZW5fbmFtZSI6IlRlc3QiLCJmYW1pbHlfbmFtZSI6IkZpcnN0IiwiZW1haWwiOiJ0ZXN0QGludmVudC5jb20ifQ.gIYCFqoW9qQ-BKlOJbOslYuGmC0Fhncwu868tKwV8tk_GX06p39Xh6lf6r3_zDI7QBKmZeZZzQFIpsYWv9JvvAFzLm2js0BFOHPTHIxsx74rgHi-Q9p4S1b7P0jMoK4A3vgRtywC2djhWfs639A2kcBRM2PDIr-TWAsCs9CAML3-ilPb6_LGiCRKkO28BCZFxLQ0XEpg2h_0Z_vD93y1HQOZtEK9kxLrzxPFFNcV1t03x3JxcHrJzsNTtmHkOFqakpBcLNddbsMISusFo-HkOefs4tpaUZpaqukoIPYjcWSQf_UN1n3AVga2SSSLO77WuG8Bu6Pnc_iZqxt7aY1s_A""
    request.audience = ""training-service""
    val tokenResponse = authzClient.exchange().exchange(request)
    println(tokenResponse.token)
}
```",BKlOJbOslYuGmC0Fhncwu868tKwV8tk_GX06p39Xh6lf6r3_zDI7QBKmZeZZzQFIpsYWv9JvvAFzLm2js0BFOHPTHIxsx74rgHi-Q9p4S1b7P0jMoK4A3vgRtywC2djhWfs639A2kcBRM2PDIr-TWAsCs9CAML3-ilPb6_LGiCRKkO28BCZFxLQ0XEpg2h_0Z_vD93y1HQOZtEK9kxLrzxPFFNcV1t03x3JxcHrJzsNTtmHkOFqakpBcLNddbsMISusFo-HkOefs4tpaUZpaqukoIPYjcWSQf_UN1n3AVga2SSSLO77WuG8Bu6Pnc_iZqxt7aY1s_A,https://github.com/keycloak/keycloak
27,7650.0,"<!---
Please read https://github.com/keycloak/keycloak/blob/master/CONTRIBUTING.md and follow these guidelines when contributing to Keycloak
-->
Allow using Client Java for token exchanging. E.g.
```lang-kotlin
fun main() {
    val authzClient = AuthzClient.create()
    val request = TokenExchangeRequest()
    request.clientId = ""training-service""
    request.clientSecret = ""b56e9bb4-5037-32aa-b823-3f59f231e418""
    request.subjectToken = ""eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJMZ3pHN1dBOWZWNXE4Wl94TEVmOFNkRl94LWx3NWF6cnY0SUxoNjdTNEdZIn0.eyJleHAiOjE2MDY3NzE3ODksImlhdCI6MTYwNjczNTc5MCwiYXV0aF90aW1lIjoxNjA2NzM1Nzg5LCJqdGkiOiI4NDdjMTJlNi04M2Y1LTQ5MmEtYTI1My05OTVkYWM0ZDA0MWUiLCJpc3MiOiJodHRwOi8vbG9jYWxob3N0L2F1dGgvcmVhbG1zL2Jhc2tldG1hc3RlciIsImF1ZCI6ImFjY291bnQiLCJzdWIiOiI4MzgxYjYyOS01ZjEwLTQwMWMtYWU5MC1iYjM3NzY5ZTVmNzAiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJ0ZWFtcy1mcm9udGVuZCIsIm5vbmNlIjoiMmExYzRkZGItYTk2YS00YjRkLWE0ZDUtNmY4ZmVkOGIzOGI4Iiwic2Vzc2lvbl9zdGF0ZSI6IjUwMjJhYTE3LTlkZjEtNDc2My1hMmY4LWMxY2JkNGQ2ODYwMSIsImFjciI6IjEiLCJhbGxvd2VkLW9yaWdpbnMiOlsiKiJdLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsib2ZmbGluZV9hY2Nlc3MiLCJ1bWFfYXV0aG9yaXphdGlvbiJdfSwicmVzb3VyY2VfYWNjZXNzIjp7ImFjY291bnQiOnsicm9sZXMiOlsibWFuYWdlLWFjY291bnQiLCJtYW5hZ2UtYWNjb3VudC1saW5rcyIsInZpZXctcHJvZmlsZSJdfX0sInNjb3BlIjoib3BlbmlkIGVtYWlsIHByb2ZpbGUiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwibmFtZSI6IlRlc3QgRmlyc3QiLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJ0ZXN0IiwiZ2l2ZW5fbmFtZSI6IlRlc3QiLCJmYW1pbHlfbmFtZSI6IkZpcnN0IiwiZW1haWwiOiJ0ZXN0QGludmVudC5jb20ifQ.gIYCFqoW9qQ-BKlOJbOslYuGmC0Fhncwu868tKwV8tk_GX06p39Xh6lf6r3_zDI7QBKmZeZZzQFIpsYWv9JvvAFzLm2js0BFOHPTHIxsx74rgHi-Q9p4S1b7P0jMoK4A3vgRtywC2djhWfs639A2kcBRM2PDIr-TWAsCs9CAML3-ilPb6_LGiCRKkO28BCZFxLQ0XEpg2h_0Z_vD93y1HQOZtEK9kxLrzxPFFNcV1t03x3JxcHrJzsNTtmHkOFqakpBcLNddbsMISusFo-HkOefs4tpaUZpaqukoIPYjcWSQf_UN1n3AVga2SSSLO77WuG8Bu6Pnc_iZqxt7aY1s_A""
    request.audience = ""training-service""
    val tokenResponse = authzClient.exchange().exchange(request)
    println(tokenResponse.token)
}
```",exchange(request,https://github.com/keycloak/keycloak
28,7650.0,"<!---
Please read https://github.com/keycloak/keycloak/blob/master/CONTRIBUTING.md and follow these guidelines when contributing to Keycloak
-->
Allow using Client Java for token exchanging. E.g.
```lang-kotlin
fun main() {
    val authzClient = AuthzClient.create()
    val request = TokenExchangeRequest()
    request.clientId = ""training-service""
    request.clientSecret = ""b56e9bb4-5037-32aa-b823-3f59f231e418""
    request.subjectToken = ""eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJMZ3pHN1dBOWZWNXE4Wl94TEVmOFNkRl94LWx3NWF6cnY0SUxoNjdTNEdZIn0.eyJleHAiOjE2MDY3NzE3ODksImlhdCI6MTYwNjczNTc5MCwiYXV0aF90aW1lIjoxNjA2NzM1Nzg5LCJqdGkiOiI4NDdjMTJlNi04M2Y1LTQ5MmEtYTI1My05OTVkYWM0ZDA0MWUiLCJpc3MiOiJodHRwOi8vbG9jYWxob3N0L2F1dGgvcmVhbG1zL2Jhc2tldG1hc3RlciIsImF1ZCI6ImFjY291bnQiLCJzdWIiOiI4MzgxYjYyOS01ZjEwLTQwMWMtYWU5MC1iYjM3NzY5ZTVmNzAiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJ0ZWFtcy1mcm9udGVuZCIsIm5vbmNlIjoiMmExYzRkZGItYTk2YS00YjRkLWE0ZDUtNmY4ZmVkOGIzOGI4Iiwic2Vzc2lvbl9zdGF0ZSI6IjUwMjJhYTE3LTlkZjEtNDc2My1hMmY4LWMxY2JkNGQ2ODYwMSIsImFjciI6IjEiLCJhbGxvd2VkLW9yaWdpbnMiOlsiKiJdLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsib2ZmbGluZV9hY2Nlc3MiLCJ1bWFfYXV0aG9yaXphdGlvbiJdfSwicmVzb3VyY2VfYWNjZXNzIjp7ImFjY291bnQiOnsicm9sZXMiOlsibWFuYWdlLWFjY291bnQiLCJtYW5hZ2UtYWNjb3VudC1saW5rcyIsInZpZXctcHJvZmlsZSJdfX0sInNjb3BlIjoib3BlbmlkIGVtYWlsIHByb2ZpbGUiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwibmFtZSI6IlRlc3QgRmlyc3QiLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJ0ZXN0IiwiZ2l2ZW5fbmFtZSI6IlRlc3QiLCJmYW1pbHlfbmFtZSI6IkZpcnN0IiwiZW1haWwiOiJ0ZXN0QGludmVudC5jb20ifQ.gIYCFqoW9qQ-BKlOJbOslYuGmC0Fhncwu868tKwV8tk_GX06p39Xh6lf6r3_zDI7QBKmZeZZzQFIpsYWv9JvvAFzLm2js0BFOHPTHIxsx74rgHi-Q9p4S1b7P0jMoK4A3vgRtywC2djhWfs639A2kcBRM2PDIr-TWAsCs9CAML3-ilPb6_LGiCRKkO28BCZFxLQ0XEpg2h_0Z_vD93y1HQOZtEK9kxLrzxPFFNcV1t03x3JxcHrJzsNTtmHkOFqakpBcLNddbsMISusFo-HkOefs4tpaUZpaqukoIPYjcWSQf_UN1n3AVga2SSSLO77WuG8Bu6Pnc_iZqxt7aY1s_A""
    request.audience = ""training-service""
    val tokenResponse = authzClient.exchange().exchange(request)
    println(tokenResponse.token)
}
```",training-service,https://github.com/keycloak/keycloak
32,63648.0,"**What happened**:

When i use ceph rbd storageclass create persistent volume claim, it always in `pending` status, `kubectl describe ` get follow event:

```
Events:
  Type     Reason              Age   From                         Message
  ----     ------              ----  ----                         -------
  Warning  ProvisioningFailed  9s    persistentvolume-controller  Failed to provision volume with StorageClass ""ceph-storage"": failed to create rbd image: exit status 1, command output: 2018-05-10 08:33:15.897889 7fad4ac81780 -1 did not load config file, using default settings.
rbd: extraneous parameter --image-feature
```

ceph-storageclass-secret:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-storageclass-secret
type: kubernetes.io/rbd
data:
  key: QVFBOEt1QmE3Qm0vSGhBQWtFQlFZSkFKQmxYcFZoeXJSdTM2Vnc9PQ==
```

storageclass yaml config:

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-storage
provisioner: kubernetes.io/rbd
parameters:
  monitors: 10.244.4.148:6789,10.244.4.136:6789,10.244.4.143:6789
  adminId: admin
  adminSecretName: ceph-storageclass-secret
  pool: rbd
  userId: admin
  userSecretName: ceph-storageclass-secret
```

ceph-storage-pvc yaml:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
```


**Environment**:
- Kubernetes version: v1.8.2+coreos.0
- OS: CentOS 7.4
- Kernel: 3.10.0-693.11.6.el7.x86_64
- Install tools: kubespray

Thanks guys.",storageclass-secret,https://github.com/kubernetes/kubernetes
33,63648.0,"**What happened**:

When i use ceph rbd storageclass create persistent volume claim, it always in `pending` status, `kubectl describe ` get follow event:

```
Events:
  Type     Reason              Age   From                         Message
  ----     ------              ----  ----                         -------
  Warning  ProvisioningFailed  9s    persistentvolume-controller  Failed to provision volume with StorageClass ""ceph-storage"": failed to create rbd image: exit status 1, command output: 2018-05-10 08:33:15.897889 7fad4ac81780 -1 did not load config file, using default settings.
rbd: extraneous parameter --image-feature
```

ceph-storageclass-secret:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-storageclass-secret
type: kubernetes.io/rbd
data:
  key: QVFBOEt1QmE3Qm0vSGhBQWtFQlFZSkFKQmxYcFZoeXJSdTM2Vnc9PQ==
```

storageclass yaml config:

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-storage
provisioner: kubernetes.io/rbd
parameters:
  monitors: 10.244.4.148:6789,10.244.4.136:6789,10.244.4.143:6789
  adminId: admin
  adminSecretName: ceph-storageclass-secret
  pool: rbd
  userId: admin
  userSecretName: ceph-storageclass-secret
```

ceph-storage-pvc yaml:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
```


**Environment**:
- Kubernetes version: v1.8.2+coreos.0
- OS: CentOS 7.4
- Kernel: 3.10.0-693.11.6.el7.x86_64
- Install tools: kubespray

Thanks guys.",QVFBOEt1QmE3Qm0vSGhBQWtFQlFZSkFKQmxYcFZoeXJSdTM2Vnc9PQ,https://github.com/kubernetes/kubernetes
34,63648.0,"**What happened**:

When i use ceph rbd storageclass create persistent volume claim, it always in `pending` status, `kubectl describe ` get follow event:

```
Events:
  Type     Reason              Age   From                         Message
  ----     ------              ----  ----                         -------
  Warning  ProvisioningFailed  9s    persistentvolume-controller  Failed to provision volume with StorageClass ""ceph-storage"": failed to create rbd image: exit status 1, command output: 2018-05-10 08:33:15.897889 7fad4ac81780 -1 did not load config file, using default settings.
rbd: extraneous parameter --image-feature
```

ceph-storageclass-secret:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-storageclass-secret
type: kubernetes.io/rbd
data:
  key: QVFBOEt1QmE3Qm0vSGhBQWtFQlFZSkFKQmxYcFZoeXJSdTM2Vnc9PQ==
```

storageclass yaml config:

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-storage
provisioner: kubernetes.io/rbd
parameters:
  monitors: 10.244.4.148:6789,10.244.4.136:6789,10.244.4.143:6789
  adminId: admin
  adminSecretName: ceph-storageclass-secret
  pool: rbd
  userId: admin
  userSecretName: ceph-storageclass-secret
```

ceph-storage-pvc yaml:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
```


**Environment**:
- Kubernetes version: v1.8.2+coreos.0
- OS: CentOS 7.4
- Kernel: 3.10.0-693.11.6.el7.x86_64
- Install tools: kubespray

Thanks guys.",ceph-storage-pvc,https://github.com/kubernetes/kubernetes
35,63648.0,"**What happened**:

When i use ceph rbd storageclass create persistent volume claim, it always in `pending` status, `kubectl describe ` get follow event:

```
Events:
  Type     Reason              Age   From                         Message
  ----     ------              ----  ----                         -------
  Warning  ProvisioningFailed  9s    persistentvolume-controller  Failed to provision volume with StorageClass ""ceph-storage"": failed to create rbd image: exit status 1, command output: 2018-05-10 08:33:15.897889 7fad4ac81780 -1 did not load config file, using default settings.
rbd: extraneous parameter --image-feature
```

ceph-storageclass-secret:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-storageclass-secret
type: kubernetes.io/rbd
data:
  key: QVFBOEt1QmE3Qm0vSGhBQWtFQlFZSkFKQmxYcFZoeXJSdTM2Vnc9PQ==
```

storageclass yaml config:

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-storage
provisioner: kubernetes.io/rbd
parameters:
  monitors: 10.244.4.148:6789,10.244.4.136:6789,10.244.4.143:6789
  adminId: admin
  adminSecretName: ceph-storageclass-secret
  pool: rbd
  userId: admin
  userSecretName: ceph-storageclass-secret
```

ceph-storage-pvc yaml:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
```


**Environment**:
- Kubernetes version: v1.8.2+coreos.0
- OS: CentOS 7.4
- Kernel: 3.10.0-693.11.6.el7.x86_64
- Install tools: kubespray

Thanks guys.",QVFBOEt1QmE3Qm0vSGhBQWtFQlFZSkFKQmxYcFZoeXJSdTM2Vn,https://github.com/kubernetes/kubernetes
41,31548.0,"This seems like a strange sequence of events. I am attempting to create secrets in our cluster to contain database user passwords. The passwords are generated each time a secret is created. This worked great for 'root'.

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-root-secret
type: Opaque
data:
  password: [32 chars of stuff]
  username: root

secret ""mysqldb-root-secret"" created
```

These definitions are being specialized and then piped to 'kubectl create -f -', so I will just show the definition yaml and the result.

I then tried creating one for a user with a longer name, used in one of our services. Let's call him 'billy'.

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-billysservice-secret
type: Opaque
data:
  password: b64377d7043139fc99ddb8a693f9a709
  username: billy

unable to decode ""STDIN"": [pos 91]: json: error decoding base64 binary 'billy': illegal base64 data at input byte 4
```

So I thought perhaps if we call him 'bill'?

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-paymentservice-secret
type: Opaque
data:
  password: 6120fcf7640f052861fcac52087a0c72
  username: bill

secret ""mysqldb-paymentservice-secret"" created
```

It likes 'bill'. Would it also like 'bil'?

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-paymentservice-secret
type: Opaque
data:
  password: a1a6d51971227fe0a592c5da881954fe
  username: bil

unable to decode ""STDIN"": [pos 89]: json: error decoding base64 binary 'bil': illegal base64 data at input byte 0
```

Nope. In reality the name I want is even longer than ""billy"" and I don't want to change it. Am I doing something wrong here?
",billysservice-secret,https://github.com/kubernetes/kubernetes
42,31548.0,"This seems like a strange sequence of events. I am attempting to create secrets in our cluster to contain database user passwords. The passwords are generated each time a secret is created. This worked great for 'root'.

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-root-secret
type: Opaque
data:
  password: [32 chars of stuff]
  username: root

secret ""mysqldb-root-secret"" created
```

These definitions are being specialized and then piped to 'kubectl create -f -', so I will just show the definition yaml and the result.

I then tried creating one for a user with a longer name, used in one of our services. Let's call him 'billy'.

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-billysservice-secret
type: Opaque
data:
  password: b64377d7043139fc99ddb8a693f9a709
  username: billy

unable to decode ""STDIN"": [pos 91]: json: error decoding base64 binary 'billy': illegal base64 data at input byte 4
```

So I thought perhaps if we call him 'bill'?

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-paymentservice-secret
type: Opaque
data:
  password: 6120fcf7640f052861fcac52087a0c72
  username: bill

secret ""mysqldb-paymentservice-secret"" created
```

It likes 'bill'. Would it also like 'bil'?

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-paymentservice-secret
type: Opaque
data:
  password: a1a6d51971227fe0a592c5da881954fe
  username: bil

unable to decode ""STDIN"": [pos 89]: json: error decoding base64 binary 'bil': illegal base64 data at input byte 0
```

Nope. In reality the name I want is even longer than ""billy"" and I don't want to change it. Am I doing something wrong here?
",paymentservice-secret,https://github.com/kubernetes/kubernetes
43,31548.0,"This seems like a strange sequence of events. I am attempting to create secrets in our cluster to contain database user passwords. The passwords are generated each time a secret is created. This worked great for 'root'.

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-root-secret
type: Opaque
data:
  password: [32 chars of stuff]
  username: root

secret ""mysqldb-root-secret"" created
```

These definitions are being specialized and then piped to 'kubectl create -f -', so I will just show the definition yaml and the result.

I then tried creating one for a user with a longer name, used in one of our services. Let's call him 'billy'.

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-billysservice-secret
type: Opaque
data:
  password: b64377d7043139fc99ddb8a693f9a709
  username: billy

unable to decode ""STDIN"": [pos 91]: json: error decoding base64 binary 'billy': illegal base64 data at input byte 4
```

So I thought perhaps if we call him 'bill'?

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-paymentservice-secret
type: Opaque
data:
  password: 6120fcf7640f052861fcac52087a0c72
  username: bill

secret ""mysqldb-paymentservice-secret"" created
```

It likes 'bill'. Would it also like 'bil'?

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-paymentservice-secret
type: Opaque
data:
  password: a1a6d51971227fe0a592c5da881954fe
  username: bil

unable to decode ""STDIN"": [pos 89]: json: error decoding base64 binary 'bil': illegal base64 data at input byte 0
```

Nope. In reality the name I want is even longer than ""billy"" and I don't want to change it. Am I doing something wrong here?
",6120fcf7640f052861fcac52087a0c72,https://github.com/kubernetes/kubernetes
44,31548.0,"This seems like a strange sequence of events. I am attempting to create secrets in our cluster to contain database user passwords. The passwords are generated each time a secret is created. This worked great for 'root'.

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-root-secret
type: Opaque
data:
  password: [32 chars of stuff]
  username: root

secret ""mysqldb-root-secret"" created
```

These definitions are being specialized and then piped to 'kubectl create -f -', so I will just show the definition yaml and the result.

I then tried creating one for a user with a longer name, used in one of our services. Let's call him 'billy'.

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-billysservice-secret
type: Opaque
data:
  password: b64377d7043139fc99ddb8a693f9a709
  username: billy

unable to decode ""STDIN"": [pos 91]: json: error decoding base64 binary 'billy': illegal base64 data at input byte 4
```

So I thought perhaps if we call him 'bill'?

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-paymentservice-secret
type: Opaque
data:
  password: 6120fcf7640f052861fcac52087a0c72
  username: bill

secret ""mysqldb-paymentservice-secret"" created
```

It likes 'bill'. Would it also like 'bil'?

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-paymentservice-secret
type: Opaque
data:
  password: a1a6d51971227fe0a592c5da881954fe
  username: bil

unable to decode ""STDIN"": [pos 89]: json: error decoding base64 binary 'bil': illegal base64 data at input byte 0
```

Nope. In reality the name I want is even longer than ""billy"" and I don't want to change it. Am I doing something wrong here?
",mysqldb-root-secret,https://github.com/kubernetes/kubernetes
45,31548.0,"This seems like a strange sequence of events. I am attempting to create secrets in our cluster to contain database user passwords. The passwords are generated each time a secret is created. This worked great for 'root'.

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-root-secret
type: Opaque
data:
  password: [32 chars of stuff]
  username: root

secret ""mysqldb-root-secret"" created
```

These definitions are being specialized and then piped to 'kubectl create -f -', so I will just show the definition yaml and the result.

I then tried creating one for a user with a longer name, used in one of our services. Let's call him 'billy'.

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-billysservice-secret
type: Opaque
data:
  password: b64377d7043139fc99ddb8a693f9a709
  username: billy

unable to decode ""STDIN"": [pos 91]: json: error decoding base64 binary 'billy': illegal base64 data at input byte 4
```

So I thought perhaps if we call him 'bill'?

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-paymentservice-secret
type: Opaque
data:
  password: 6120fcf7640f052861fcac52087a0c72
  username: bill

secret ""mysqldb-paymentservice-secret"" created
```

It likes 'bill'. Would it also like 'bil'?

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-paymentservice-secret
type: Opaque
data:
  password: a1a6d51971227fe0a592c5da881954fe
  username: bil

unable to decode ""STDIN"": [pos 89]: json: error decoding base64 binary 'bil': illegal base64 data at input byte 0
```

Nope. In reality the name I want is even longer than ""billy"" and I don't want to change it. Am I doing something wrong here?
",a1a6d51971227fe0a592c5da881954fe,https://github.com/kubernetes/kubernetes
46,31548.0,"This seems like a strange sequence of events. I am attempting to create secrets in our cluster to contain database user passwords. The passwords are generated each time a secret is created. This worked great for 'root'.

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-root-secret
type: Opaque
data:
  password: [32 chars of stuff]
  username: root

secret ""mysqldb-root-secret"" created
```

These definitions are being specialized and then piped to 'kubectl create -f -', so I will just show the definition yaml and the result.

I then tried creating one for a user with a longer name, used in one of our services. Let's call him 'billy'.

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-billysservice-secret
type: Opaque
data:
  password: b64377d7043139fc99ddb8a693f9a709
  username: billy

unable to decode ""STDIN"": [pos 91]: json: error decoding base64 binary 'billy': illegal base64 data at input byte 4
```

So I thought perhaps if we call him 'bill'?

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-paymentservice-secret
type: Opaque
data:
  password: 6120fcf7640f052861fcac52087a0c72
  username: bill

secret ""mysqldb-paymentservice-secret"" created
```

It likes 'bill'. Would it also like 'bil'?

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-paymentservice-secret
type: Opaque
data:
  password: a1a6d51971227fe0a592c5da881954fe
  username: bil

unable to decode ""STDIN"": [pos 89]: json: error decoding base64 binary 'bil': illegal base64 data at input byte 0
```

Nope. In reality the name I want is even longer than ""billy"" and I don't want to change it. Am I doing something wrong here?
",b64377d7043139fc99ddb8a693f9a709,https://github.com/kubernetes/kubernetes
47,31548.0,"This seems like a strange sequence of events. I am attempting to create secrets in our cluster to contain database user passwords. The passwords are generated each time a secret is created. This worked great for 'root'.

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-root-secret
type: Opaque
data:
  password: [32 chars of stuff]
  username: root

secret ""mysqldb-root-secret"" created
```

These definitions are being specialized and then piped to 'kubectl create -f -', so I will just show the definition yaml and the result.

I then tried creating one for a user with a longer name, used in one of our services. Let's call him 'billy'.

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-billysservice-secret
type: Opaque
data:
  password: b64377d7043139fc99ddb8a693f9a709
  username: billy

unable to decode ""STDIN"": [pos 91]: json: error decoding base64 binary 'billy': illegal base64 data at input byte 4
```

So I thought perhaps if we call him 'bill'?

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-paymentservice-secret
type: Opaque
data:
  password: 6120fcf7640f052861fcac52087a0c72
  username: bill

secret ""mysqldb-paymentservice-secret"" created
```

It likes 'bill'. Would it also like 'bil'?

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-paymentservice-secret
type: Opaque
data:
  password: a1a6d51971227fe0a592c5da881954fe
  username: bil

unable to decode ""STDIN"": [pos 89]: json: error decoding base64 binary 'bil': illegal base64 data at input byte 0
```

Nope. In reality the name I want is even longer than ""billy"" and I don't want to change it. Am I doing something wrong here?
",generated,https://github.com/kubernetes/kubernetes
48,31548.0,"This seems like a strange sequence of events. I am attempting to create secrets in our cluster to contain database user passwords. The passwords are generated each time a secret is created. This worked great for 'root'.

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-root-secret
type: Opaque
data:
  password: [32 chars of stuff]
  username: root

secret ""mysqldb-root-secret"" created
```

These definitions are being specialized and then piped to 'kubectl create -f -', so I will just show the definition yaml and the result.

I then tried creating one for a user with a longer name, used in one of our services. Let's call him 'billy'.

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-billysservice-secret
type: Opaque
data:
  password: b64377d7043139fc99ddb8a693f9a709
  username: billy

unable to decode ""STDIN"": [pos 91]: json: error decoding base64 binary 'billy': illegal base64 data at input byte 4
```

So I thought perhaps if we call him 'bill'?

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-paymentservice-secret
type: Opaque
data:
  password: 6120fcf7640f052861fcac52087a0c72
  username: bill

secret ""mysqldb-paymentservice-secret"" created
```

It likes 'bill'. Would it also like 'bil'?

```
apiVersion: v1
kind: Secret
metadata:
  name: mysqldb-paymentservice-secret
type: Opaque
data:
  password: a1a6d51971227fe0a592c5da881954fe
  username: bil

unable to decode ""STDIN"": [pos 89]: json: error decoding base64 binary 'bil': illegal base64 data at input byte 0
```

Nope. In reality the name I want is even longer than ""billy"" and I don't want to change it. Am I doing something wrong here?
",username,https://github.com/kubernetes/kubernetes
49,28263.0,"This PR allows specifying non-binary data values in `Secret` objects as `""stringData"":{""key"":""string value""}`, in addition to the existing base64 []byte serializations in the `data` field.

On write, the keys and values in the `stringData` field are merged to the `data` map, overwriting any values already present in the `data` map. The move is one-way, the `stringData` field is never output when reading from the API.

A Secret can be created like this:

```
{
  ""kind"":""Secret"",
  ""apiVersion"":""v1"",
  ""metadata"":{""name"":""mysecret""},
  ""data"":{
    ""image"":""<base64-encoded-jpg>""
  },
  ""stringData"":{
    ""username"": ""myuser"",
    ""password"": ""mypassword""
  }
}
```

and when read from the API would look like this:

```
{
  ""kind"":""Secret"",
  ""apiVersion"":""v1"",
  ""metadata"":{""name"":""mysecret"",...},
  ""data"":{
    ""image"":""<base64-encoded-jpg>""
    ""username"": ""bXl1c2Vy"",
    ""password"": ""bXlwYXNzd29yZA==""
  }
}
```
",stringData:{key:string,https://github.com/kubernetes/kubernetes
50,28263.0,"This PR allows specifying non-binary data values in `Secret` objects as `""stringData"":{""key"":""string value""}`, in addition to the existing base64 []byte serializations in the `data` field.

On write, the keys and values in the `stringData` field are merged to the `data` map, overwriting any values already present in the `data` map. The move is one-way, the `stringData` field is never output when reading from the API.

A Secret can be created like this:

```
{
  ""kind"":""Secret"",
  ""apiVersion"":""v1"",
  ""metadata"":{""name"":""mysecret""},
  ""data"":{
    ""image"":""<base64-encoded-jpg>""
  },
  ""stringData"":{
    ""username"": ""myuser"",
    ""password"": ""mypassword""
  }
}
```

and when read from the API would look like this:

```
{
  ""kind"":""Secret"",
  ""apiVersion"":""v1"",
  ""metadata"":{""name"":""mysecret"",...},
  ""data"":{
    ""image"":""<base64-encoded-jpg>""
    ""username"": ""bXl1c2Vy"",
    ""password"": ""bXlwYXNzd29yZA==""
  }
}
```
",metadata:{name:mysecret,https://github.com/kubernetes/kubernetes
51,28263.0,"This PR allows specifying non-binary data values in `Secret` objects as `""stringData"":{""key"":""string value""}`, in addition to the existing base64 []byte serializations in the `data` field.

On write, the keys and values in the `stringData` field are merged to the `data` map, overwriting any values already present in the `data` map. The move is one-way, the `stringData` field is never output when reading from the API.

A Secret can be created like this:

```
{
  ""kind"":""Secret"",
  ""apiVersion"":""v1"",
  ""metadata"":{""name"":""mysecret""},
  ""data"":{
    ""image"":""<base64-encoded-jpg>""
  },
  ""stringData"":{
    ""username"": ""myuser"",
    ""password"": ""mypassword""
  }
}
```

and when read from the API would look like this:

```
{
  ""kind"":""Secret"",
  ""apiVersion"":""v1"",
  ""metadata"":{""name"":""mysecret"",...},
  ""data"":{
    ""image"":""<base64-encoded-jpg>""
    ""username"": ""bXl1c2Vy"",
    ""password"": ""bXlwYXNzd29yZA==""
  }
}
```
",mypassword,https://github.com/kubernetes/kubernetes
52,28263.0,"This PR allows specifying non-binary data values in `Secret` objects as `""stringData"":{""key"":""string value""}`, in addition to the existing base64 []byte serializations in the `data` field.

On write, the keys and values in the `stringData` field are merged to the `data` map, overwriting any values already present in the `data` map. The move is one-way, the `stringData` field is never output when reading from the API.

A Secret can be created like this:

```
{
  ""kind"":""Secret"",
  ""apiVersion"":""v1"",
  ""metadata"":{""name"":""mysecret""},
  ""data"":{
    ""image"":""<base64-encoded-jpg>""
  },
  ""stringData"":{
    ""username"": ""myuser"",
    ""password"": ""mypassword""
  }
}
```

and when read from the API would look like this:

```
{
  ""kind"":""Secret"",
  ""apiVersion"":""v1"",
  ""metadata"":{""name"":""mysecret"",...},
  ""data"":{
    ""image"":""<base64-encoded-jpg>""
    ""username"": ""bXl1c2Vy"",
    ""password"": ""bXlwYXNzd29yZA==""
  }
}
```
",bXlwYXNzd29yZA==,https://github.com/kubernetes/kubernetes
53,21840.0,"• Failure [5.649 seconds]
SSH
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:92
  should SSH to all nodes and run commands [It]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:91

  Feb 23 13:29:02.551: Ran echo ""Hello"" on 104.154.89.141:22, got error error getting signer for provider gce: 'error parsing SSH key -----BEGIN RSA PRIVATE KEY-----
  Proc-Type: 4,ENCRYPTED
  DEK-Info: AES-128-CBC,0B3908B11A8909BDEBB0FBD991D12FE5

  8fgALzSe3UWdWf0BgQJ24tUBPjJduvoGkfdrzE6g8fwx5VY/l1px4HCUsJp1Wt4i
  10lhvIBKfwmPWZxU3r0LH1I5WTn4uw5hl9BxnA33ATTZbhuCuRGa2M19mH8ZSUkV
  AJ55OYpxKzfeoFzfYtuvgttPc9ZW0uaY7+Hc8xKRukPBI+n0bysirRnA3wrDyV8x
  +KMyznMcNoviYZiYAYx3vVB5Rii2Ew9IqzrcYE1b8j64wIfsP80UWNp0kjUL1/5V
  FpZdeCskDThFSgZYJ5XswM+ncAjnL1vRpyWDRcZgbczvF9ino3DlQ9fJi45GSfwa
  PdSIqJ9h0RmBYeQSQ0Ax21RddBIzO/ez1oa4RYR0JjHNsiIvV82lNtA/T2DXNatZ
  cwn3oBX+kML/8wxbIMM3Q0n+vengaaJ3w/LeJLFNx/KL+hShQwquqISWUPJuTKuC
  maHlhB0uP/dlg2XYfQOk1oDZ+Pz/MBq+D2HmWpyH861Z+4S2bjiml8pp256Owq+1
  a8nxI+makbeSk9preXCCvd71FrdDgAe7VBHxUMFL/M77Ytt9N3Vy8kF4mroF2Y2S
  O1MhK0bSYYaPS27nb8hvpQCoLKa0ytsgco7Nb2/6g4ZG81Sqy4VuIwt4+sKQGeV7
  O8sUzJfuoawni4wNXSxF5ZnjpZ7igNkzfBVAolFoygyTO1oVsOrjJtcOOyPwMQKc
  sGGqLahD6fDaFQ5iuBef8PG9U8h7g0oVi4qpkute/6oMMh+aRQK75PBWvkxpyqe5
  NMDnOEob/D/9p5mKLXbuSetAkKSthiibESl1O43P07kpTpsMRJ2WKbxy3u4pSVXJ
  032HssmVtv8UCQGoBQvxDUdJqKE2g9Kl/o+vJenMUL/Vgu9N9PhX92XxhQw/9yKN
  18nh6/7Xp03TxllDPD3A2gRWlNuzFNXBNNytlrZcrsd85DSdJH4VLDKGK4Kd+AWQ
  FD9njiJPhtB0j4Me326tJ8346M636b8obceuRL/jyxhOuk4Fh9em5ounMvfa0NWR
  zATq379Bxa9beo+lkrwHPjfzIqkMOTGH3PZy0ANqs55uL4yrTCtG46oU3JAfOwwU
  SwW3MEcrzCeDKqQYNgsz8xcZySPDUMbEw60dX5294Kyk5/jaLe9ueozkxnAZ7vqp
  fMu75YyXcei3q2g9H8rM2AtSEqJ0UH84xUz+efYuigoABiqV2C3komlYYFsuP73N
  krDu1XOBywpNlgPWIyGgjXsFYpMuYZjDW4QmB2bgMC4IJq8X8Lns8Sl141Pr2lhZ
  AysEBd5LiY2XkTuHrg18TEq0ojcOP4co/p5NCTzen6YzuHukWxTJoHgDjX9Mv/5g
  wmVqNc9VQyZa73jVKIZjeCh8QhRd2vmK5MueADQnMAzcSLAOeppZFW3oX8QrZbCr
  SmHAHZzIdhFavK+jmpgsxYGe83g1iOQqG0Coyhjqr61GqJkDIaoDzoYQxf0baoz/
  kO687U5aGcveNbK6SLyYE71fvXKDhAe5uj0banXixmmWfrM/QHmcJll6PD6LxTdM
  3UqmC3N5zL3Ub2sUNL3MbhJIz3bSMa9hN6M6FoG1lxwT+ySEMFzjlWlxjuaaXW85
  -----END RSA PRIVATE KEY-----
  : 'asn1: structure error: superfluous leading zeros in length'', expected <nil>

  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:65
",wmVqNc9VQyZa73jVKIZjeCh8QhRd2vmK5MueADQnMAzcSLAOeppZFW3oX8QrZbCr,https://github.com/kubernetes/kubernetes
54,21840.0,"• Failure [5.649 seconds]
SSH
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:92
  should SSH to all nodes and run commands [It]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:91

  Feb 23 13:29:02.551: Ran echo ""Hello"" on 104.154.89.141:22, got error error getting signer for provider gce: 'error parsing SSH key -----BEGIN RSA PRIVATE KEY-----
  Proc-Type: 4,ENCRYPTED
  DEK-Info: AES-128-CBC,0B3908B11A8909BDEBB0FBD991D12FE5

  8fgALzSe3UWdWf0BgQJ24tUBPjJduvoGkfdrzE6g8fwx5VY/l1px4HCUsJp1Wt4i
  10lhvIBKfwmPWZxU3r0LH1I5WTn4uw5hl9BxnA33ATTZbhuCuRGa2M19mH8ZSUkV
  AJ55OYpxKzfeoFzfYtuvgttPc9ZW0uaY7+Hc8xKRukPBI+n0bysirRnA3wrDyV8x
  +KMyznMcNoviYZiYAYx3vVB5Rii2Ew9IqzrcYE1b8j64wIfsP80UWNp0kjUL1/5V
  FpZdeCskDThFSgZYJ5XswM+ncAjnL1vRpyWDRcZgbczvF9ino3DlQ9fJi45GSfwa
  PdSIqJ9h0RmBYeQSQ0Ax21RddBIzO/ez1oa4RYR0JjHNsiIvV82lNtA/T2DXNatZ
  cwn3oBX+kML/8wxbIMM3Q0n+vengaaJ3w/LeJLFNx/KL+hShQwquqISWUPJuTKuC
  maHlhB0uP/dlg2XYfQOk1oDZ+Pz/MBq+D2HmWpyH861Z+4S2bjiml8pp256Owq+1
  a8nxI+makbeSk9preXCCvd71FrdDgAe7VBHxUMFL/M77Ytt9N3Vy8kF4mroF2Y2S
  O1MhK0bSYYaPS27nb8hvpQCoLKa0ytsgco7Nb2/6g4ZG81Sqy4VuIwt4+sKQGeV7
  O8sUzJfuoawni4wNXSxF5ZnjpZ7igNkzfBVAolFoygyTO1oVsOrjJtcOOyPwMQKc
  sGGqLahD6fDaFQ5iuBef8PG9U8h7g0oVi4qpkute/6oMMh+aRQK75PBWvkxpyqe5
  NMDnOEob/D/9p5mKLXbuSetAkKSthiibESl1O43P07kpTpsMRJ2WKbxy3u4pSVXJ
  032HssmVtv8UCQGoBQvxDUdJqKE2g9Kl/o+vJenMUL/Vgu9N9PhX92XxhQw/9yKN
  18nh6/7Xp03TxllDPD3A2gRWlNuzFNXBNNytlrZcrsd85DSdJH4VLDKGK4Kd+AWQ
  FD9njiJPhtB0j4Me326tJ8346M636b8obceuRL/jyxhOuk4Fh9em5ounMvfa0NWR
  zATq379Bxa9beo+lkrwHPjfzIqkMOTGH3PZy0ANqs55uL4yrTCtG46oU3JAfOwwU
  SwW3MEcrzCeDKqQYNgsz8xcZySPDUMbEw60dX5294Kyk5/jaLe9ueozkxnAZ7vqp
  fMu75YyXcei3q2g9H8rM2AtSEqJ0UH84xUz+efYuigoABiqV2C3komlYYFsuP73N
  krDu1XOBywpNlgPWIyGgjXsFYpMuYZjDW4QmB2bgMC4IJq8X8Lns8Sl141Pr2lhZ
  AysEBd5LiY2XkTuHrg18TEq0ojcOP4co/p5NCTzen6YzuHukWxTJoHgDjX9Mv/5g
  wmVqNc9VQyZa73jVKIZjeCh8QhRd2vmK5MueADQnMAzcSLAOeppZFW3oX8QrZbCr
  SmHAHZzIdhFavK+jmpgsxYGe83g1iOQqG0Coyhjqr61GqJkDIaoDzoYQxf0baoz/
  kO687U5aGcveNbK6SLyYE71fvXKDhAe5uj0banXixmmWfrM/QHmcJll6PD6LxTdM
  3UqmC3N5zL3Ub2sUNL3MbhJIz3bSMa9hN6M6FoG1lxwT+ySEMFzjlWlxjuaaXW85
  -----END RSA PRIVATE KEY-----
  : 'asn1: structure error: superfluous leading zeros in length'', expected <nil>

  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:65
",O8sUzJfuoawni4wNXSxF5ZnjpZ7igNkzfBVAolFoygyTO1oVsOrjJtcOOyPwMQKc,https://github.com/kubernetes/kubernetes
55,21840.0,"• Failure [5.649 seconds]
SSH
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:92
  should SSH to all nodes and run commands [It]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:91

  Feb 23 13:29:02.551: Ran echo ""Hello"" on 104.154.89.141:22, got error error getting signer for provider gce: 'error parsing SSH key -----BEGIN RSA PRIVATE KEY-----
  Proc-Type: 4,ENCRYPTED
  DEK-Info: AES-128-CBC,0B3908B11A8909BDEBB0FBD991D12FE5

  8fgALzSe3UWdWf0BgQJ24tUBPjJduvoGkfdrzE6g8fwx5VY/l1px4HCUsJp1Wt4i
  10lhvIBKfwmPWZxU3r0LH1I5WTn4uw5hl9BxnA33ATTZbhuCuRGa2M19mH8ZSUkV
  AJ55OYpxKzfeoFzfYtuvgttPc9ZW0uaY7+Hc8xKRukPBI+n0bysirRnA3wrDyV8x
  +KMyznMcNoviYZiYAYx3vVB5Rii2Ew9IqzrcYE1b8j64wIfsP80UWNp0kjUL1/5V
  FpZdeCskDThFSgZYJ5XswM+ncAjnL1vRpyWDRcZgbczvF9ino3DlQ9fJi45GSfwa
  PdSIqJ9h0RmBYeQSQ0Ax21RddBIzO/ez1oa4RYR0JjHNsiIvV82lNtA/T2DXNatZ
  cwn3oBX+kML/8wxbIMM3Q0n+vengaaJ3w/LeJLFNx/KL+hShQwquqISWUPJuTKuC
  maHlhB0uP/dlg2XYfQOk1oDZ+Pz/MBq+D2HmWpyH861Z+4S2bjiml8pp256Owq+1
  a8nxI+makbeSk9preXCCvd71FrdDgAe7VBHxUMFL/M77Ytt9N3Vy8kF4mroF2Y2S
  O1MhK0bSYYaPS27nb8hvpQCoLKa0ytsgco7Nb2/6g4ZG81Sqy4VuIwt4+sKQGeV7
  O8sUzJfuoawni4wNXSxF5ZnjpZ7igNkzfBVAolFoygyTO1oVsOrjJtcOOyPwMQKc
  sGGqLahD6fDaFQ5iuBef8PG9U8h7g0oVi4qpkute/6oMMh+aRQK75PBWvkxpyqe5
  NMDnOEob/D/9p5mKLXbuSetAkKSthiibESl1O43P07kpTpsMRJ2WKbxy3u4pSVXJ
  032HssmVtv8UCQGoBQvxDUdJqKE2g9Kl/o+vJenMUL/Vgu9N9PhX92XxhQw/9yKN
  18nh6/7Xp03TxllDPD3A2gRWlNuzFNXBNNytlrZcrsd85DSdJH4VLDKGK4Kd+AWQ
  FD9njiJPhtB0j4Me326tJ8346M636b8obceuRL/jyxhOuk4Fh9em5ounMvfa0NWR
  zATq379Bxa9beo+lkrwHPjfzIqkMOTGH3PZy0ANqs55uL4yrTCtG46oU3JAfOwwU
  SwW3MEcrzCeDKqQYNgsz8xcZySPDUMbEw60dX5294Kyk5/jaLe9ueozkxnAZ7vqp
  fMu75YyXcei3q2g9H8rM2AtSEqJ0UH84xUz+efYuigoABiqV2C3komlYYFsuP73N
  krDu1XOBywpNlgPWIyGgjXsFYpMuYZjDW4QmB2bgMC4IJq8X8Lns8Sl141Pr2lhZ
  AysEBd5LiY2XkTuHrg18TEq0ojcOP4co/p5NCTzen6YzuHukWxTJoHgDjX9Mv/5g
  wmVqNc9VQyZa73jVKIZjeCh8QhRd2vmK5MueADQnMAzcSLAOeppZFW3oX8QrZbCr
  SmHAHZzIdhFavK+jmpgsxYGe83g1iOQqG0Coyhjqr61GqJkDIaoDzoYQxf0baoz/
  kO687U5aGcveNbK6SLyYE71fvXKDhAe5uj0banXixmmWfrM/QHmcJll6PD6LxTdM
  3UqmC3N5zL3Ub2sUNL3MbhJIz3bSMa9hN6M6FoG1lxwT+ySEMFzjlWlxjuaaXW85
  -----END RSA PRIVATE KEY-----
  : 'asn1: structure error: superfluous leading zeros in length'', expected <nil>

  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:65
",krDu1XOBywpNlgPWIyGgjXsFYpMuYZjDW4QmB2bgMC4IJq8X8Lns8Sl141Pr2lhZ,https://github.com/kubernetes/kubernetes
56,21840.0,"• Failure [5.649 seconds]
SSH
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:92
  should SSH to all nodes and run commands [It]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:91

  Feb 23 13:29:02.551: Ran echo ""Hello"" on 104.154.89.141:22, got error error getting signer for provider gce: 'error parsing SSH key -----BEGIN RSA PRIVATE KEY-----
  Proc-Type: 4,ENCRYPTED
  DEK-Info: AES-128-CBC,0B3908B11A8909BDEBB0FBD991D12FE5

  8fgALzSe3UWdWf0BgQJ24tUBPjJduvoGkfdrzE6g8fwx5VY/l1px4HCUsJp1Wt4i
  10lhvIBKfwmPWZxU3r0LH1I5WTn4uw5hl9BxnA33ATTZbhuCuRGa2M19mH8ZSUkV
  AJ55OYpxKzfeoFzfYtuvgttPc9ZW0uaY7+Hc8xKRukPBI+n0bysirRnA3wrDyV8x
  +KMyznMcNoviYZiYAYx3vVB5Rii2Ew9IqzrcYE1b8j64wIfsP80UWNp0kjUL1/5V
  FpZdeCskDThFSgZYJ5XswM+ncAjnL1vRpyWDRcZgbczvF9ino3DlQ9fJi45GSfwa
  PdSIqJ9h0RmBYeQSQ0Ax21RddBIzO/ez1oa4RYR0JjHNsiIvV82lNtA/T2DXNatZ
  cwn3oBX+kML/8wxbIMM3Q0n+vengaaJ3w/LeJLFNx/KL+hShQwquqISWUPJuTKuC
  maHlhB0uP/dlg2XYfQOk1oDZ+Pz/MBq+D2HmWpyH861Z+4S2bjiml8pp256Owq+1
  a8nxI+makbeSk9preXCCvd71FrdDgAe7VBHxUMFL/M77Ytt9N3Vy8kF4mroF2Y2S
  O1MhK0bSYYaPS27nb8hvpQCoLKa0ytsgco7Nb2/6g4ZG81Sqy4VuIwt4+sKQGeV7
  O8sUzJfuoawni4wNXSxF5ZnjpZ7igNkzfBVAolFoygyTO1oVsOrjJtcOOyPwMQKc
  sGGqLahD6fDaFQ5iuBef8PG9U8h7g0oVi4qpkute/6oMMh+aRQK75PBWvkxpyqe5
  NMDnOEob/D/9p5mKLXbuSetAkKSthiibESl1O43P07kpTpsMRJ2WKbxy3u4pSVXJ
  032HssmVtv8UCQGoBQvxDUdJqKE2g9Kl/o+vJenMUL/Vgu9N9PhX92XxhQw/9yKN
  18nh6/7Xp03TxllDPD3A2gRWlNuzFNXBNNytlrZcrsd85DSdJH4VLDKGK4Kd+AWQ
  FD9njiJPhtB0j4Me326tJ8346M636b8obceuRL/jyxhOuk4Fh9em5ounMvfa0NWR
  zATq379Bxa9beo+lkrwHPjfzIqkMOTGH3PZy0ANqs55uL4yrTCtG46oU3JAfOwwU
  SwW3MEcrzCeDKqQYNgsz8xcZySPDUMbEw60dX5294Kyk5/jaLe9ueozkxnAZ7vqp
  fMu75YyXcei3q2g9H8rM2AtSEqJ0UH84xUz+efYuigoABiqV2C3komlYYFsuP73N
  krDu1XOBywpNlgPWIyGgjXsFYpMuYZjDW4QmB2bgMC4IJq8X8Lns8Sl141Pr2lhZ
  AysEBd5LiY2XkTuHrg18TEq0ojcOP4co/p5NCTzen6YzuHukWxTJoHgDjX9Mv/5g
  wmVqNc9VQyZa73jVKIZjeCh8QhRd2vmK5MueADQnMAzcSLAOeppZFW3oX8QrZbCr
  SmHAHZzIdhFavK+jmpgsxYGe83g1iOQqG0Coyhjqr61GqJkDIaoDzoYQxf0baoz/
  kO687U5aGcveNbK6SLyYE71fvXKDhAe5uj0banXixmmWfrM/QHmcJll6PD6LxTdM
  3UqmC3N5zL3Ub2sUNL3MbhJIz3bSMa9hN6M6FoG1lxwT+ySEMFzjlWlxjuaaXW85
  -----END RSA PRIVATE KEY-----
  : 'asn1: structure error: superfluous leading zeros in length'', expected <nil>

  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:65
",10lhvIBKfwmPWZxU3r0LH1I5WTn4uw5hl9BxnA33ATTZbhuCuRGa2M19mH8ZSUkV,https://github.com/kubernetes/kubernetes
57,21840.0,"• Failure [5.649 seconds]
SSH
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:92
  should SSH to all nodes and run commands [It]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:91

  Feb 23 13:29:02.551: Ran echo ""Hello"" on 104.154.89.141:22, got error error getting signer for provider gce: 'error parsing SSH key -----BEGIN RSA PRIVATE KEY-----
  Proc-Type: 4,ENCRYPTED
  DEK-Info: AES-128-CBC,0B3908B11A8909BDEBB0FBD991D12FE5

  8fgALzSe3UWdWf0BgQJ24tUBPjJduvoGkfdrzE6g8fwx5VY/l1px4HCUsJp1Wt4i
  10lhvIBKfwmPWZxU3r0LH1I5WTn4uw5hl9BxnA33ATTZbhuCuRGa2M19mH8ZSUkV
  AJ55OYpxKzfeoFzfYtuvgttPc9ZW0uaY7+Hc8xKRukPBI+n0bysirRnA3wrDyV8x
  +KMyznMcNoviYZiYAYx3vVB5Rii2Ew9IqzrcYE1b8j64wIfsP80UWNp0kjUL1/5V
  FpZdeCskDThFSgZYJ5XswM+ncAjnL1vRpyWDRcZgbczvF9ino3DlQ9fJi45GSfwa
  PdSIqJ9h0RmBYeQSQ0Ax21RddBIzO/ez1oa4RYR0JjHNsiIvV82lNtA/T2DXNatZ
  cwn3oBX+kML/8wxbIMM3Q0n+vengaaJ3w/LeJLFNx/KL+hShQwquqISWUPJuTKuC
  maHlhB0uP/dlg2XYfQOk1oDZ+Pz/MBq+D2HmWpyH861Z+4S2bjiml8pp256Owq+1
  a8nxI+makbeSk9preXCCvd71FrdDgAe7VBHxUMFL/M77Ytt9N3Vy8kF4mroF2Y2S
  O1MhK0bSYYaPS27nb8hvpQCoLKa0ytsgco7Nb2/6g4ZG81Sqy4VuIwt4+sKQGeV7
  O8sUzJfuoawni4wNXSxF5ZnjpZ7igNkzfBVAolFoygyTO1oVsOrjJtcOOyPwMQKc
  sGGqLahD6fDaFQ5iuBef8PG9U8h7g0oVi4qpkute/6oMMh+aRQK75PBWvkxpyqe5
  NMDnOEob/D/9p5mKLXbuSetAkKSthiibESl1O43P07kpTpsMRJ2WKbxy3u4pSVXJ
  032HssmVtv8UCQGoBQvxDUdJqKE2g9Kl/o+vJenMUL/Vgu9N9PhX92XxhQw/9yKN
  18nh6/7Xp03TxllDPD3A2gRWlNuzFNXBNNytlrZcrsd85DSdJH4VLDKGK4Kd+AWQ
  FD9njiJPhtB0j4Me326tJ8346M636b8obceuRL/jyxhOuk4Fh9em5ounMvfa0NWR
  zATq379Bxa9beo+lkrwHPjfzIqkMOTGH3PZy0ANqs55uL4yrTCtG46oU3JAfOwwU
  SwW3MEcrzCeDKqQYNgsz8xcZySPDUMbEw60dX5294Kyk5/jaLe9ueozkxnAZ7vqp
  fMu75YyXcei3q2g9H8rM2AtSEqJ0UH84xUz+efYuigoABiqV2C3komlYYFsuP73N
  krDu1XOBywpNlgPWIyGgjXsFYpMuYZjDW4QmB2bgMC4IJq8X8Lns8Sl141Pr2lhZ
  AysEBd5LiY2XkTuHrg18TEq0ojcOP4co/p5NCTzen6YzuHukWxTJoHgDjX9Mv/5g
  wmVqNc9VQyZa73jVKIZjeCh8QhRd2vmK5MueADQnMAzcSLAOeppZFW3oX8QrZbCr
  SmHAHZzIdhFavK+jmpgsxYGe83g1iOQqG0Coyhjqr61GqJkDIaoDzoYQxf0baoz/
  kO687U5aGcveNbK6SLyYE71fvXKDhAe5uj0banXixmmWfrM/QHmcJll6PD6LxTdM
  3UqmC3N5zL3Ub2sUNL3MbhJIz3bSMa9hN6M6FoG1lxwT+ySEMFzjlWlxjuaaXW85
  -----END RSA PRIVATE KEY-----
  : 'asn1: structure error: superfluous leading zeros in length'', expected <nil>

  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:65
",a8nxI+makbeSk9preXCCvd71FrdDgAe7VBHxUMFL,https://github.com/kubernetes/kubernetes
58,21840.0,"• Failure [5.649 seconds]
SSH
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:92
  should SSH to all nodes and run commands [It]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:91

  Feb 23 13:29:02.551: Ran echo ""Hello"" on 104.154.89.141:22, got error error getting signer for provider gce: 'error parsing SSH key -----BEGIN RSA PRIVATE KEY-----
  Proc-Type: 4,ENCRYPTED
  DEK-Info: AES-128-CBC,0B3908B11A8909BDEBB0FBD991D12FE5

  8fgALzSe3UWdWf0BgQJ24tUBPjJduvoGkfdrzE6g8fwx5VY/l1px4HCUsJp1Wt4i
  10lhvIBKfwmPWZxU3r0LH1I5WTn4uw5hl9BxnA33ATTZbhuCuRGa2M19mH8ZSUkV
  AJ55OYpxKzfeoFzfYtuvgttPc9ZW0uaY7+Hc8xKRukPBI+n0bysirRnA3wrDyV8x
  +KMyznMcNoviYZiYAYx3vVB5Rii2Ew9IqzrcYE1b8j64wIfsP80UWNp0kjUL1/5V
  FpZdeCskDThFSgZYJ5XswM+ncAjnL1vRpyWDRcZgbczvF9ino3DlQ9fJi45GSfwa
  PdSIqJ9h0RmBYeQSQ0Ax21RddBIzO/ez1oa4RYR0JjHNsiIvV82lNtA/T2DXNatZ
  cwn3oBX+kML/8wxbIMM3Q0n+vengaaJ3w/LeJLFNx/KL+hShQwquqISWUPJuTKuC
  maHlhB0uP/dlg2XYfQOk1oDZ+Pz/MBq+D2HmWpyH861Z+4S2bjiml8pp256Owq+1
  a8nxI+makbeSk9preXCCvd71FrdDgAe7VBHxUMFL/M77Ytt9N3Vy8kF4mroF2Y2S
  O1MhK0bSYYaPS27nb8hvpQCoLKa0ytsgco7Nb2/6g4ZG81Sqy4VuIwt4+sKQGeV7
  O8sUzJfuoawni4wNXSxF5ZnjpZ7igNkzfBVAolFoygyTO1oVsOrjJtcOOyPwMQKc
  sGGqLahD6fDaFQ5iuBef8PG9U8h7g0oVi4qpkute/6oMMh+aRQK75PBWvkxpyqe5
  NMDnOEob/D/9p5mKLXbuSetAkKSthiibESl1O43P07kpTpsMRJ2WKbxy3u4pSVXJ
  032HssmVtv8UCQGoBQvxDUdJqKE2g9Kl/o+vJenMUL/Vgu9N9PhX92XxhQw/9yKN
  18nh6/7Xp03TxllDPD3A2gRWlNuzFNXBNNytlrZcrsd85DSdJH4VLDKGK4Kd+AWQ
  FD9njiJPhtB0j4Me326tJ8346M636b8obceuRL/jyxhOuk4Fh9em5ounMvfa0NWR
  zATq379Bxa9beo+lkrwHPjfzIqkMOTGH3PZy0ANqs55uL4yrTCtG46oU3JAfOwwU
  SwW3MEcrzCeDKqQYNgsz8xcZySPDUMbEw60dX5294Kyk5/jaLe9ueozkxnAZ7vqp
  fMu75YyXcei3q2g9H8rM2AtSEqJ0UH84xUz+efYuigoABiqV2C3komlYYFsuP73N
  krDu1XOBywpNlgPWIyGgjXsFYpMuYZjDW4QmB2bgMC4IJq8X8Lns8Sl141Pr2lhZ
  AysEBd5LiY2XkTuHrg18TEq0ojcOP4co/p5NCTzen6YzuHukWxTJoHgDjX9Mv/5g
  wmVqNc9VQyZa73jVKIZjeCh8QhRd2vmK5MueADQnMAzcSLAOeppZFW3oX8QrZbCr
  SmHAHZzIdhFavK+jmpgsxYGe83g1iOQqG0Coyhjqr61GqJkDIaoDzoYQxf0baoz/
  kO687U5aGcveNbK6SLyYE71fvXKDhAe5uj0banXixmmWfrM/QHmcJll6PD6LxTdM
  3UqmC3N5zL3Ub2sUNL3MbhJIz3bSMa9hN6M6FoG1lxwT+ySEMFzjlWlxjuaaXW85
  -----END RSA PRIVATE KEY-----
  : 'asn1: structure error: superfluous leading zeros in length'', expected <nil>

  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:65
",032HssmVtv8UCQGoBQvxDUdJqKE2g9Kl+vJenMUL,https://github.com/kubernetes/kubernetes
59,21840.0,"• Failure [5.649 seconds]
SSH
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:92
  should SSH to all nodes and run commands [It]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:91

  Feb 23 13:29:02.551: Ran echo ""Hello"" on 104.154.89.141:22, got error error getting signer for provider gce: 'error parsing SSH key -----BEGIN RSA PRIVATE KEY-----
  Proc-Type: 4,ENCRYPTED
  DEK-Info: AES-128-CBC,0B3908B11A8909BDEBB0FBD991D12FE5

  8fgALzSe3UWdWf0BgQJ24tUBPjJduvoGkfdrzE6g8fwx5VY/l1px4HCUsJp1Wt4i
  10lhvIBKfwmPWZxU3r0LH1I5WTn4uw5hl9BxnA33ATTZbhuCuRGa2M19mH8ZSUkV
  AJ55OYpxKzfeoFzfYtuvgttPc9ZW0uaY7+Hc8xKRukPBI+n0bysirRnA3wrDyV8x
  +KMyznMcNoviYZiYAYx3vVB5Rii2Ew9IqzrcYE1b8j64wIfsP80UWNp0kjUL1/5V
  FpZdeCskDThFSgZYJ5XswM+ncAjnL1vRpyWDRcZgbczvF9ino3DlQ9fJi45GSfwa
  PdSIqJ9h0RmBYeQSQ0Ax21RddBIzO/ez1oa4RYR0JjHNsiIvV82lNtA/T2DXNatZ
  cwn3oBX+kML/8wxbIMM3Q0n+vengaaJ3w/LeJLFNx/KL+hShQwquqISWUPJuTKuC
  maHlhB0uP/dlg2XYfQOk1oDZ+Pz/MBq+D2HmWpyH861Z+4S2bjiml8pp256Owq+1
  a8nxI+makbeSk9preXCCvd71FrdDgAe7VBHxUMFL/M77Ytt9N3Vy8kF4mroF2Y2S
  O1MhK0bSYYaPS27nb8hvpQCoLKa0ytsgco7Nb2/6g4ZG81Sqy4VuIwt4+sKQGeV7
  O8sUzJfuoawni4wNXSxF5ZnjpZ7igNkzfBVAolFoygyTO1oVsOrjJtcOOyPwMQKc
  sGGqLahD6fDaFQ5iuBef8PG9U8h7g0oVi4qpkute/6oMMh+aRQK75PBWvkxpyqe5
  NMDnOEob/D/9p5mKLXbuSetAkKSthiibESl1O43P07kpTpsMRJ2WKbxy3u4pSVXJ
  032HssmVtv8UCQGoBQvxDUdJqKE2g9Kl/o+vJenMUL/Vgu9N9PhX92XxhQw/9yKN
  18nh6/7Xp03TxllDPD3A2gRWlNuzFNXBNNytlrZcrsd85DSdJH4VLDKGK4Kd+AWQ
  FD9njiJPhtB0j4Me326tJ8346M636b8obceuRL/jyxhOuk4Fh9em5ounMvfa0NWR
  zATq379Bxa9beo+lkrwHPjfzIqkMOTGH3PZy0ANqs55uL4yrTCtG46oU3JAfOwwU
  SwW3MEcrzCeDKqQYNgsz8xcZySPDUMbEw60dX5294Kyk5/jaLe9ueozkxnAZ7vqp
  fMu75YyXcei3q2g9H8rM2AtSEqJ0UH84xUz+efYuigoABiqV2C3komlYYFsuP73N
  krDu1XOBywpNlgPWIyGgjXsFYpMuYZjDW4QmB2bgMC4IJq8X8Lns8Sl141Pr2lhZ
  AysEBd5LiY2XkTuHrg18TEq0ojcOP4co/p5NCTzen6YzuHukWxTJoHgDjX9Mv/5g
  wmVqNc9VQyZa73jVKIZjeCh8QhRd2vmK5MueADQnMAzcSLAOeppZFW3oX8QrZbCr
  SmHAHZzIdhFavK+jmpgsxYGe83g1iOQqG0Coyhjqr61GqJkDIaoDzoYQxf0baoz/
  kO687U5aGcveNbK6SLyYE71fvXKDhAe5uj0banXixmmWfrM/QHmcJll6PD6LxTdM
  3UqmC3N5zL3Ub2sUNL3MbhJIz3bSMa9hN6M6FoG1lxwT+ySEMFzjlWlxjuaaXW85
  -----END RSA PRIVATE KEY-----
  : 'asn1: structure error: superfluous leading zeros in length'', expected <nil>

  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:65
",lkrwHPjfzIqkMOTGH3PZy0ANqs55uL4yrTCtG46oU3JAfOwwU,https://github.com/kubernetes/kubernetes
60,21840.0,"• Failure [5.649 seconds]
SSH
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:92
  should SSH to all nodes and run commands [It]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:91

  Feb 23 13:29:02.551: Ran echo ""Hello"" on 104.154.89.141:22, got error error getting signer for provider gce: 'error parsing SSH key -----BEGIN RSA PRIVATE KEY-----
  Proc-Type: 4,ENCRYPTED
  DEK-Info: AES-128-CBC,0B3908B11A8909BDEBB0FBD991D12FE5

  8fgALzSe3UWdWf0BgQJ24tUBPjJduvoGkfdrzE6g8fwx5VY/l1px4HCUsJp1Wt4i
  10lhvIBKfwmPWZxU3r0LH1I5WTn4uw5hl9BxnA33ATTZbhuCuRGa2M19mH8ZSUkV
  AJ55OYpxKzfeoFzfYtuvgttPc9ZW0uaY7+Hc8xKRukPBI+n0bysirRnA3wrDyV8x
  +KMyznMcNoviYZiYAYx3vVB5Rii2Ew9IqzrcYE1b8j64wIfsP80UWNp0kjUL1/5V
  FpZdeCskDThFSgZYJ5XswM+ncAjnL1vRpyWDRcZgbczvF9ino3DlQ9fJi45GSfwa
  PdSIqJ9h0RmBYeQSQ0Ax21RddBIzO/ez1oa4RYR0JjHNsiIvV82lNtA/T2DXNatZ
  cwn3oBX+kML/8wxbIMM3Q0n+vengaaJ3w/LeJLFNx/KL+hShQwquqISWUPJuTKuC
  maHlhB0uP/dlg2XYfQOk1oDZ+Pz/MBq+D2HmWpyH861Z+4S2bjiml8pp256Owq+1
  a8nxI+makbeSk9preXCCvd71FrdDgAe7VBHxUMFL/M77Ytt9N3Vy8kF4mroF2Y2S
  O1MhK0bSYYaPS27nb8hvpQCoLKa0ytsgco7Nb2/6g4ZG81Sqy4VuIwt4+sKQGeV7
  O8sUzJfuoawni4wNXSxF5ZnjpZ7igNkzfBVAolFoygyTO1oVsOrjJtcOOyPwMQKc
  sGGqLahD6fDaFQ5iuBef8PG9U8h7g0oVi4qpkute/6oMMh+aRQK75PBWvkxpyqe5
  NMDnOEob/D/9p5mKLXbuSetAkKSthiibESl1O43P07kpTpsMRJ2WKbxy3u4pSVXJ
  032HssmVtv8UCQGoBQvxDUdJqKE2g9Kl/o+vJenMUL/Vgu9N9PhX92XxhQw/9yKN
  18nh6/7Xp03TxllDPD3A2gRWlNuzFNXBNNytlrZcrsd85DSdJH4VLDKGK4Kd+AWQ
  FD9njiJPhtB0j4Me326tJ8346M636b8obceuRL/jyxhOuk4Fh9em5ounMvfa0NWR
  zATq379Bxa9beo+lkrwHPjfzIqkMOTGH3PZy0ANqs55uL4yrTCtG46oU3JAfOwwU
  SwW3MEcrzCeDKqQYNgsz8xcZySPDUMbEw60dX5294Kyk5/jaLe9ueozkxnAZ7vqp
  fMu75YyXcei3q2g9H8rM2AtSEqJ0UH84xUz+efYuigoABiqV2C3komlYYFsuP73N
  krDu1XOBywpNlgPWIyGgjXsFYpMuYZjDW4QmB2bgMC4IJq8X8Lns8Sl141Pr2lhZ
  AysEBd5LiY2XkTuHrg18TEq0ojcOP4co/p5NCTzen6YzuHukWxTJoHgDjX9Mv/5g
  wmVqNc9VQyZa73jVKIZjeCh8QhRd2vmK5MueADQnMAzcSLAOeppZFW3oX8QrZbCr
  SmHAHZzIdhFavK+jmpgsxYGe83g1iOQqG0Coyhjqr61GqJkDIaoDzoYQxf0baoz/
  kO687U5aGcveNbK6SLyYE71fvXKDhAe5uj0banXixmmWfrM/QHmcJll6PD6LxTdM
  3UqmC3N5zL3Ub2sUNL3MbhJIz3bSMa9hN6M6FoG1lxwT+ySEMFzjlWlxjuaaXW85
  -----END RSA PRIVATE KEY-----
  : 'asn1: structure error: superfluous leading zeros in length'', expected <nil>

  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/ssh.go:65
",sGGqLahD6fDaFQ5iuBef8PG9U8h7g0oVi4qpkute,https://github.com/kubernetes/kubernetes
62,17713.0,"i setup a k8s 2 nodes cluster on ubuntu, then i use kubectl to get nodes, it failed.
use k8s version:  v1.1.1
root@zgy:/home/zgy/code/master/src/kubernetes/cluster/ubuntu/binaries# ./kubectl -s http://127.0.0.1:8080 get nodes
error: couldn't read version from server: Get http://127.0.0.1:8080/api: dial tcp 127.0.0.1:8080: connection refused
root@zgy:/home/zgy/code/master/src/kubernetes/cluster/ubuntu/binaries# ./kubectl config view
apiVersion: v1
clusters:
- cluster:
  insecure-skip-tls-verify: true
  server: http://10.67.57.247:8080
  name: ubuntu
  contexts:
- context:
  cluster: ubuntu
  user: ubuntu
  name: ubuntu
  current-context: ubuntu
  kind: Config
  preferences: {}
  users:
- name: ubuntu
  user:
    password: cjh3dg8AO9AbPYaJ
    username: admin

root@zgy:/home/zgy/code/master/src/kubernetes/cluster/ubuntu/binaries# ./kubectl version
Client Version: version.Info{Major:""1"", Minor:""1"", GitVersion:""v1.1.1"", GitCommit:""92635e23dfafb2ddc828c8ac6c03c7a7205a84d8"", GitTreeState:""clean""}
^Croot@zgy:/home/zgy/code/master/src/kubernetes/cluster/ubuntu/binaries# 

Ref:  #10539
",cjh3dg8AO9AbPYaJ,https://github.com/kubernetes/kubernetes
63,17713.0,"i setup a k8s 2 nodes cluster on ubuntu, then i use kubectl to get nodes, it failed.
use k8s version:  v1.1.1
root@zgy:/home/zgy/code/master/src/kubernetes/cluster/ubuntu/binaries# ./kubectl -s http://127.0.0.1:8080 get nodes
error: couldn't read version from server: Get http://127.0.0.1:8080/api: dial tcp 127.0.0.1:8080: connection refused
root@zgy:/home/zgy/code/master/src/kubernetes/cluster/ubuntu/binaries# ./kubectl config view
apiVersion: v1
clusters:
- cluster:
  insecure-skip-tls-verify: true
  server: http://10.67.57.247:8080
  name: ubuntu
  contexts:
- context:
  cluster: ubuntu
  user: ubuntu
  name: ubuntu
  current-context: ubuntu
  kind: Config
  preferences: {}
  users:
- name: ubuntu
  user:
    password: cjh3dg8AO9AbPYaJ
    username: admin

root@zgy:/home/zgy/code/master/src/kubernetes/cluster/ubuntu/binaries# ./kubectl version
Client Version: version.Info{Major:""1"", Minor:""1"", GitVersion:""v1.1.1"", GitCommit:""92635e23dfafb2ddc828c8ac6c03c7a7205a84d8"", GitTreeState:""clean""}
^Croot@zgy:/home/zgy/code/master/src/kubernetes/cluster/ubuntu/binaries# 

Ref:  #10539
",root@zgy,https://github.com/kubernetes/kubernetes
75,9483.0,"It seems creating a secret is only allowed if it's base64 encoded?  I have a secret that's PEM encoded, e.g.

```
apiVersion: v1beta3
kind: Secret
metadata:
  name: kube-keypair
data:
  kube-registry.crt: ""-----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC95VjiVBvZCZLm kvGkVwkfj+gR8NomUZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wY XYrhLEM357BxFRM1/G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ ltdI07XEPIgrGr5wvRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7 PutHNg3TIPSM4F4pmmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNb oHUT2RArHR1zJV47+NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O2 1+NM4DsjAgMBAAECggEAWNpciaJDWX2CUAMHsBbNfQgv1V02JeHaUebr4LvDJMCw tl0j/zdVwtby4ZbKcX96UdnmshTosPuZTUlPt2anmRv+0hrDX2pbUxTrUZ7U4yXc WIggQLebgP5pAS2qJOWS5Sk5K+kszJVE3PJuj3QIQ+by7ABahZPnS14usOyi2QUg eDTyQc2HPxj5H813KShT1OtQclgc19FGJ3jfYdEHVqyqL65aORasRhno2kLY6SlB d7+9eypks7NA48hJt+NZZC9ZZXvNsRHdlVTueExtM9lJPZEDgkAUsd8wTx49pIs3 T0EoVLA7wNjqewkuMW2bGoGwAROlK07jJMgAgWGVYQKBgQD4NxTg2Jtv7ylnlglr 16cwb5Q38qu01o7R5pZ6KkigRt0FlMDn92hEnipPd/zJesoSTKGSeoTu7SsGF9IC kg7y6cO7RrWNQNwbM7dwN9jIq/kJN45oxClVNW73lxEwenYy4AIx5boeALoktToz tJ2Zj36b/LvDA//kyznqBa+JEwKBgQDD2gU8B46Jika6H2g+5Hmk9mNtHj6ZZIHc ijwkDXiq5uLLRwa+RX1OhhVFnStVoJ3QrhFJAGqJbz0GHiDnTu3yfQL7iQE/Oc0K HMk415o3pjk3VIYchOENlaYx6mfr5Og6+ZXlcgXUvN4lRdwDphwyM3L4EKrfnc3v C5TYv/9XsQKBgB7UWJeEz+mbPv1KuWjXpEBz22kHjyQq8hpIFbCex96AbrteuocV R7IeIwsz0AYGZkrXkcnrxc8XThy3oLIJCipJtrHwGmStBamrRsF9bkOeeJQYfmus 0aVvuZSvAseOrlerUxp3eiJ33Kbqi4wYhuqn1AXz4i5atNHqHAthIWhZAoGAf6EQ UDJMfO0Tci8NbA7QBuRZnr5a5PNOfT3IO2ZcAGM94GKEznHSWt2d9yOZHc/xiBAn S39t7eoDxnzTzGjvP1qgGvMJUP6F09U2fEhkQ2ebWadHTkZ5srSW/WhYGK2veN81 A7Tuf9gkm/2OhwI01bIZdfbGo7refKz7btSPvcECgYEApN3h9grnhhZ9lvrGjPgj XAS6oAKmRZtLvlmvbKlZNdQceHDBDzUEn7JA4ASgGaq+WPQPimfmzdoXmRfEltUA BEm9Q3nVULfqjaGUUUW0JJ6HBvtRYzN40DUILYUQt8xa9pT5s5cwjdLSfXi1FyR2 Y9QiC6fdz+koJqq33yMwCy0= -----END PRIVATE KEY-----""
  kube-registry.key: ""-----BEGIN CERTIFICATE----- MIIDUDCCAjigAwIBAgIJAK87AEpK+g1OMA0GCSqGSIb3DQEBCwUAMB8xHTAbBgNV BAMUFDEyNy4wLjAuMUAxNDMyODQwODMyMB4XDTE1MDUyODE5MjAzMloXDTI1MDUy NTE5MjAzMlowHzEdMBsGA1UEAxQUMTI3LjAuMC4xQDE0MzI4NDA4MzIwggEiMA0G CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC95VjiVBvZCZLmkvGkVwkfj+gR8Nom UZB7TDnvJnFbdDqHvea9t6l2k5hiuVlJihIX7ZJR9cnEx5wYXYrhLEM357BxFRM1 /G7gjdcyt5DzdXKd5z7cehZLgEm6g7aoHYSNQP5/mRadDGR/ltdI07XEPIgrGr5w vRzNx93dYeX74exl1Qje/lA8I3AY5MXQRLTAEC71ybtvGdj7PutHNg3TIPSM4F4p mmBh1c0Iw+9r99U6zozuMvcDdIRAPmlIjqSIDOC+Tz+GljNboHUT2RArHR1zJV47 +NqkC44Fh+B/Mx3z2FHDQ37orXaEuvrTe2Fa8ojHu0n0y3O21+NM4DsjAgMBAAGj gY4wgYswHQYDVR0OBBYEFBe5fmodCcmMKa3VU/dvLmf3AK7RME8GA1UdIwRIMEaA FBe5fmodCcmMKa3VU/dvLmf3AK7RoSOkITAfMR0wGwYDVQQDFBQxMjcuMC4wLjFA MTQzMjg0MDgzMoIJAK87AEpK+g1OMAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEG MA0GCSqGSIb3DQEBCwUAA4IBAQAQ2wCPmH9kTXC5JGNN/w6qOhVe4O48EnmI3ceL XNvb5aPHztZ73uP+NH5vrdJOr+86w3m6QeiAiivUlr5GmcDWutgaWhElrrwo9tY/ oJpJf3E8zzCGixONYlFQEFtDA1oUl8Nz8yvO3Eq58YJr5Gqwmbmf3CjxAuBA+GX5 YDPXr3YWu/AMekA6pcNZg92mYfDdNI8GzAkGXmGgSZfUELCXIDPpn2XIjuQvN+4J tg12xMOnDPAoFGXmqnQzwhHy68aMYzGUeHcDuGbYNUgd+AMey71uau6j76NW0JFl 9wZ2hsIRH18oZwAJFbOHPkHaRhXSvzFauKqYc6k629yUF0s+ -----END CERTIFICATE-----""
type: Opaque
```

kubectl create reports error:

```
error: unable to load file ""/tmp/secret.yaml"": unable to load ""/tmp/secret.yaml"": illegal base64 data at input byte 0
```

BTW, I'm not sure if puting certificate in secret is the right approach in k8s (or will work).  My use case is to mount certificates as secret volume, and set ssl_certificate directive in nginx to use that certificates. e.g.

```
      containers:
        - name: kube-nginx
          image: kube-nginx
          volumeMounts:
            - name: kube-keypair
              readOnly: true
              mountPath: /etc/nginx/ssl
      volumes:
        - name: kube-keypair
          secret:
            secretName: kube-keypair
```

and nginx conf

```
  ssl_certificate /etc/nginx/ssl/kube-registry.crt;
  ssl_certificate_key /etc/nginx/ssl/kube-registry.key;
```
",+g1OMAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEG,https://github.com/kubernetes/kubernetes
77,6264.0,"Forked from: https://github.com/GoogleCloudPlatform/kubernetes/pull/6175

I started a cluster on GCE and sent a simple GET request at `https://104.155.58.27/api/v1beta1/proxy/services/monitoring-grafana` via browser and printed the request received by proxy.go.
This is the output:

```
&{GET /api/v1beta1/proxy/services/monitoring-grafana 
HTTP/1.1 1 1 
map[Accept:[text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8] 
User-Agent:[Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.89 Safari/537.36] 
Accept-Encoding:[gzip, deflate, sdch] 
Accept-Language:[en-US,en;q=0.8] 
Connection:[upgrade] 
Authorization:[Basic YWRtaW46SGY4d1BNSHloU3Q0cFJsZg==]] 
0x1329250 0 [] false 127.0.0.1:8080 map[] map[] <nil> map[] 127.0.0.1:40439 /api/v1beta1/proxy/services/monitoring-grafana <nil>}
```

The request has a `Connection:[upgrade]` header even tough the original request did not have it.
Clearly someone is adding this header. @lavalamp suggested that this is unexpected. No one should be modifying request headers.
The header is not added when the cluster is running on localhost

The Request headers in Network tab of Chrome are:

```
Accept:text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8
Accept-Encoding:gzip, deflate, sdch
Accept-Language:en-US,en;q=0.8
Authorization:Basic YWRtaW46SGY4d1BNSHloU3Q0cFJsZg==
Cache-Control:max-age=0
Connection:keep-alive
Host:23.251.148.110
User-Agent:Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.89 Safari/537.36
```
",YWRtaW46SGY4d1BNSHloU3Q0cFJsZg,https://github.com/kubernetes/kubernetes
2,10469.0,"# Bug report

## Describe the bug
When including `emailRedirectTo` value in the `generateLink()` of type 'signup', the returned 'action_link' and 'redirect_to' values contain my site url instead of the specified `emailRedirectTo` value. Even when this url is whitelisted (listed in the Redirect URLs) in the dashboard.

## To Reproduce

Steps to reproduce the behavior, please provide code snippets or a repository:
```
  const { data, error } = await supabase.auth.admin.generateLink({
    type: ""signup"",
    email: email,
    options: {
      password: ""password"",
      data: {
        first_name: firstName,
        last_name: lastName,
      },
      emailRedirectTo: ""https://portal.my-domain.com"",
    },
  });
```

## Expected behavior

The response to use the emailRedirectTo value instead of my site url.

This is what I get:
```
generateLink data {
  properties: {
    action_link: 'https://alkdjflskdjflwemnfoijsnelfk.supabase.co/auth/v1/verify?token=c575asda4fa3sd54gad35f4ad35f1asd3f1a3d5f1a3sd5&type=signup&redirect_to=https://www.my-domain.com',
    email_otp: '313413584',
    hashed_token: 'c575asda4fa3sd54gad35f4ad35f1asd3f1a3d5f1a3sd5',
    redirect_to: 'https://www.my-domain.com',
    verification_type: 'signup'
  },
     ...
```

This is what I expect:
```
generateLink data {
  properties: {
    action_link: 'https://alkdjflskdjflwemnfoijsnelfk.supabase.co/auth/v1/verify?token=c575asda4fa3sd54gad35f4ad35f1asd3f1a3d5f1a3sd5&type=signup&redirect_to=https://portal.my-domain.com',
    email_otp: '313413584',
    hashed_token: 'c575asda4fa3sd54gad35f4ad35f1asd3f1a3d5f1a3sd5',
    redirect_to: 'https://portal.my-domain.com',
    verification_type: 'signup'
  },
     ...
```

## System information

- OS: macOS
- Version of supabase-js: ""@supabase/supabase-js"": ""^2.1.0"",
- Version of Node.js: 18.12.1

",c575asda4fa3sd54gad35f4ad35f1asd3f1a3d5f1a3sd5,https://github.com/supabase/supabase
3,10469.0,"# Bug report

## Describe the bug
When including `emailRedirectTo` value in the `generateLink()` of type 'signup', the returned 'action_link' and 'redirect_to' values contain my site url instead of the specified `emailRedirectTo` value. Even when this url is whitelisted (listed in the Redirect URLs) in the dashboard.

## To Reproduce

Steps to reproduce the behavior, please provide code snippets or a repository:
```
  const { data, error } = await supabase.auth.admin.generateLink({
    type: ""signup"",
    email: email,
    options: {
      password: ""password"",
      data: {
        first_name: firstName,
        last_name: lastName,
      },
      emailRedirectTo: ""https://portal.my-domain.com"",
    },
  });
```

## Expected behavior

The response to use the emailRedirectTo value instead of my site url.

This is what I get:
```
generateLink data {
  properties: {
    action_link: 'https://alkdjflskdjflwemnfoijsnelfk.supabase.co/auth/v1/verify?token=c575asda4fa3sd54gad35f4ad35f1asd3f1a3d5f1a3sd5&type=signup&redirect_to=https://www.my-domain.com',
    email_otp: '313413584',
    hashed_token: 'c575asda4fa3sd54gad35f4ad35f1asd3f1a3d5f1a3sd5',
    redirect_to: 'https://www.my-domain.com',
    verification_type: 'signup'
  },
     ...
```

This is what I expect:
```
generateLink data {
  properties: {
    action_link: 'https://alkdjflskdjflwemnfoijsnelfk.supabase.co/auth/v1/verify?token=c575asda4fa3sd54gad35f4ad35f1asd3f1a3d5f1a3sd5&type=signup&redirect_to=https://portal.my-domain.com',
    email_otp: '313413584',
    hashed_token: 'c575asda4fa3sd54gad35f4ad35f1asd3f1a3d5f1a3sd5',
    redirect_to: 'https://portal.my-domain.com',
    verification_type: 'signup'
  },
     ...
```

## System information

- OS: macOS
- Version of supabase-js: ""@supabase/supabase-js"": ""^2.1.0"",
- Version of Node.js: 18.12.1

",password,https://github.com/supabase/supabase
10,5798.0,"# Bug report

## Describe the bug

While self hosting I get 404 ""We couldn't find the page you're looking for"" on all rest api endpoints.

## To Reproduce

1. Setup locally using docker-compose: https://supabase.com/docs/guides/hosting/docker
2. `cp .env.example .env`
3. `docker-compose up`
4. run sql `create table foo (bar text);`
5. run:
```
curl -v 'http://localhost:3000/rest/v1/foo?select=*' \
  -H 'Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJzZXJ2aWNlX3JvbGUiLAogICAgImlzcyI6ICJzdXBhYmFzZS1kZW1vIiwKICAgICJpYXQiOiAxNjQxNzY5MjAwLAogICAgImV4cCI6IDE3OTk1MzU2MDAKfQ.DaYlNEoUrrEn2Ig7tqibS-PHK5vgusbcbo7X36XVt4Q' \
  -H 'apikey: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJzZXJ2aWNlX3JvbGUiLAogICAgImlzcyI6ICJzdXBhYmFzZS1kZW1vIiwKICAgICJpYXQiOiAxNjQxNzY5MjAwLAogICAgImV4cCI6IDE3OTk1MzU2MDAKfQ.DaYlNEoUrrEn2Ig7tqibS-PHK5vgusbcbo7X36XVt4Q' 
```

## Expected behavior

Should give the proper output to the query. Instead it looks like the request does not even reach the rest service but is 404ed by (probably) supabase-studio.
",PHK5vgusbcbo7X36XVt4Q,https://github.com/supabase/supabase
11,5096.0,"Hi,

I have multiple questions regarding authentication and cant really find anything that helps me.
Maybe its a bug maybe its missing documentation, an error on my part or just expected behavior, I don't know @kiwicopple 

**Token expiration and startup** 
If I set the expire token duration to 1 week I **can restore** the session within that week. 
If I set the expire token duration to 2 min and the user opens  the app after that duration no refresh is possible and **""Invalid JWT token""** error appears.

Thats my token with the Invalid JWT token error on my dev environment, not that critical to share.
`""{""currentSession"":{""access_token"":""eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJhdXRoZW50aWNhdGVkIiwiZXhwIjoxNjQyODA1ODIwLCJzdWIiOiIzMDc4ZjY2NS1jNjFiLTQ0Y2UtYjY3ZS1kNzIyOTNjMWFjMmMiLCJlbWFpbCI6InNlcmdlai5zYWNoc0BnbXguZGUiLCJwaG9uZSI6IiIsImFwcF9tZXRhZGF0YSI6eyJwcm92aWRlciI6ImVtYWlsIiwicHJvdmlkZXJzIjpbImVtYWlsIl19LCJ1c2VyX21ldGFkYXRhIjp7fSwicm9sZSI6ImF1dGhlbnRpY2F0ZWQifQ.QQaZpPDTy1urjbZNoMf88I15HIIHIoGRPJN152ht06U"",""expires_in"":120,""refresh_token"":""ybqC0YdCNlYxByvlQh5WQA"",""token_type"":""bearer"",""provider_token"":null,""user"":{""id"":""3078f665-c61b-44ce-b67e-d72293c1ac2c"",""app_metadata"":{""provider"":""email"",""providers"":[""email""]},""user_metadata"":{},""aud"":""authenticated"",""email"":""anymail@provider.com"",""created_at"":""2022-01-20T12:12:57.382314Z"",""confirmed_at"":""2022-01-20T12:12:57.387309Z"",""last_sign_in_at"":""2022-01-21T22:55:00.788920397Z"",""role"":""authenticated"",""updated_at"":""2022-01-21T22:55:00.790273Z""}},""expiresAt"":1642805820}""`

If I set the expiration to 2 min and close my app for that time, should I be able to refresh my session? Because one week would be short for logout on users that does not open an app in that timeframe.

**Expired token while app is running**
I saw a property called ""**autoRefreshTokens**"" but this don't happen if my app is running and my token expires.
If I want to call any database API I get ""**JWT token expired""** error back.

I tested it with 2 min JWT expiration duration and using the dart only library for this tests.","ybqC0YdCNlYxByvlQh5WQA,token_type:bearer,provider_token:null,",https://github.com/supabase/supabase
0,2324.0,"As pointed out in https://spectrum.chat/zeit/now/now-dev-without-credentials~1a755ccf-ec83-4c81-a5aa-7e79574c73a2?m=MTU1NzIyODYyMzkzNw== we should consider removing the authentication requirement for `now dev`.

Asking for ZEIT credentials when running locally is unexpected and might become a blocker when using `now dev` in continuous-integration environments.",1a755ccf-ec83-4c81-a5aa-7e79574c73a2,https://github.com/vercel/vercel
3,16350.0,"<!--- Please keep this note for the community --->

### Community Note

* Please vote on this issue by adding a 👍 [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to the original issue to help the community and maintainers prioritize this request
* Please do not leave ""+1"" or other comments that do not add relevant new information or questions, they generate extra noise for issue followers and do not help prioritize the request
* If you are interested in working on this issue or have submitted a pull request, please leave a comment

<!--- Thank you for keeping this note for the community --->

### Description

<!--- Please leave a helpful description of the feature request here. --->

The aws_networkfirewall_firewall resource needs to expose the VPC endpoints created by the firewall for use in routing tables in order to actually use it.  These endpoints are exposed in the FirewallStatus structure returned by the API.

For example:
```
$ aws --region us-east-1 network-firewall describe-firewall --firewall-name dhagan-2020-11-20-002
{
    ""UpdateToken"": ""29c6147b-e772-4c46-8d78-7aa5f2b71aaf"",
    ""Firewall"": {
        ""FirewallName"": ""dhagan-2020-11-20-002"",
        ""FirewallArn"": ""arn:aws:network-firewall:us-east-1:xxx:firewall/dhagan-2020-11-20-002"",
        ""FirewallPolicyArn"": ""arn:aws:network-firewall:us-east-1:xxxx:firewall-policy/example-inspection-vpc-policy"",
        ""VpcId"": ""vpc-0b857b1a3686ae36d"",
        ""SubnetMappings"": [
            {
                ""SubnetId"": ""subnet-0f96d5471222feb28""
            },
            {
                ""SubnetId"": ""subnet-0f55ba6bdcc7357ca""
            },
            {
                ""SubnetId"": ""subnet-03c8e80f3c807fd2e""
            },
            {
                ""SubnetId"": ""subnet-05d7d553643f36d0e""
            },
            {
                ""SubnetId"": ""subnet-02e8df98316b27054""
            }
        ],
        ""DeleteProtection"": false,
        ""SubnetChangeProtection"": false,
        ""FirewallPolicyChangeProtection"": false,
        ""FirewallId"": ""b70bc32e-9753-4d7e-bb82-febfb362b72b"",
        ""Tags"": []
    },
    ""FirewallStatus"": {
        ""Status"": ""READY"",
        ""ConfigurationSyncStateSummary"": ""IN_SYNC"",
        ""SyncStates"": {
            ""us-east-1a"": {
                ""Attachment"": {
                    ""SubnetId"": ""subnet-05d7d553643f36d0e"",
                    ""EndpointId"": ""vpce-0394425be976920e3"",
                    ""Status"": ""READY""
                },
                ""Config"": {
                    ""arn:aws:network-firewall:us-east-1:xxxx:firewall-policy/example-inspection-vpc-policy"": {
                        ""SyncStatus"": ""IN_SYNC""
                    }
                }
            },
            ""us-east-1b"": {
                ""Attachment"": {
                    ""SubnetId"": ""subnet-0f96d5471222feb28"",
                    ""EndpointId"": ""vpce-073e44a76dab0949e"",
                    ""Status"": ""READY""
                },
                ""Config"": {
                    ""arn:aws:network-firewall:us-east-1:xxxx:firewall-policy/example-inspection-vpc-policy"": {
                        ""SyncStatus"": ""IN_SYNC""
                    }
                }
            },
            ""us-east-1c"": {
                ""Attachment"": {
                    ""SubnetId"": ""subnet-0f55ba6bdcc7357ca"",
                    ""EndpointId"": ""vpce-0ae3a6d2b2c886215"",
                    ""Status"": ""READY""
                },
                ""Config"": {
                    ""arn:aws:network-firewall:us-east-1:xxxx:firewall-policy/example-inspection-vpc-policy"": {
                        ""SyncStatus"": ""IN_SYNC""
                    }
                }
            },
            ""us-east-1d"": {
                ""Attachment"": {
                    ""SubnetId"": ""subnet-02e8df98316b27054"",
                    ""EndpointId"": ""vpce-0006b35c0f00be77c"",
                    ""Status"": ""READY""
                },
                ""Config"": {
                    ""arn:aws:network-firewall:us-east-1:xxxxx:firewall-policy/example-inspection-vpc-policy"": {
                        ""SyncStatus"": ""IN_SYNC""
                    }
                }
            },
            ""us-east-1f"": {
                ""Attachment"": {
                    ""SubnetId"": ""subnet-03c8e80f3c807fd2e"",
                    ""EndpointId"": ""vpce-0552034c8ae2b92ce"",
                    ""Status"": ""READY""
                },
                ""Config"": {
                    ""arn:aws:network-firewall:us-east-1:xxxxx:firewall-policy/example-inspection-vpc-policy"": {
                        ""SyncStatus"": ""IN_SYNC""
                    }
                }
            }
        }
    }
}
```


### New or Affected Resource(s)

<!--- Please list the new or affected resources and data sources. --->

* aws_networkfirewall_firewall

### Potential Terraform Configuration

<!--- Information about code formatting: https://help.github.com/articles/basic-writing-and-formatting-syntax/#quoting-code --->

```hcl
# Copy-paste your Terraform configurations here - for large Terraform configs,
# please use a service like Dropbox and share a link to the ZIP file. For
# security, you can also encrypt the files using our GPG public key.
```

### References

<!---
Information about referencing Github Issues: https://help.github.com/articles/basic-writing-and-formatting-syntax/#referencing-issues-and-pull-requests

Are there any other GitHub issues (open or closed) or pull requests that should be linked here? Vendor blog posts or documentation? For example:

* https://aws.amazon.com/about-aws/whats-new/2018/04/introducing-amazon-ec2-fleet/
--->

* https://docs.aws.amazon.com/network-firewall/latest/APIReference/API_FirewallStatus.html
",FirewallPolicyChangeProtection,https://github.com/hashicorp/terraform-provider-aws
4,16350.0,"<!--- Please keep this note for the community --->

### Community Note

* Please vote on this issue by adding a 👍 [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to the original issue to help the community and maintainers prioritize this request
* Please do not leave ""+1"" or other comments that do not add relevant new information or questions, they generate extra noise for issue followers and do not help prioritize the request
* If you are interested in working on this issue or have submitted a pull request, please leave a comment

<!--- Thank you for keeping this note for the community --->

### Description

<!--- Please leave a helpful description of the feature request here. --->

The aws_networkfirewall_firewall resource needs to expose the VPC endpoints created by the firewall for use in routing tables in order to actually use it.  These endpoints are exposed in the FirewallStatus structure returned by the API.

For example:
```
$ aws --region us-east-1 network-firewall describe-firewall --firewall-name dhagan-2020-11-20-002
{
    ""UpdateToken"": ""29c6147b-e772-4c46-8d78-7aa5f2b71aaf"",
    ""Firewall"": {
        ""FirewallName"": ""dhagan-2020-11-20-002"",
        ""FirewallArn"": ""arn:aws:network-firewall:us-east-1:xxx:firewall/dhagan-2020-11-20-002"",
        ""FirewallPolicyArn"": ""arn:aws:network-firewall:us-east-1:xxxx:firewall-policy/example-inspection-vpc-policy"",
        ""VpcId"": ""vpc-0b857b1a3686ae36d"",
        ""SubnetMappings"": [
            {
                ""SubnetId"": ""subnet-0f96d5471222feb28""
            },
            {
                ""SubnetId"": ""subnet-0f55ba6bdcc7357ca""
            },
            {
                ""SubnetId"": ""subnet-03c8e80f3c807fd2e""
            },
            {
                ""SubnetId"": ""subnet-05d7d553643f36d0e""
            },
            {
                ""SubnetId"": ""subnet-02e8df98316b27054""
            }
        ],
        ""DeleteProtection"": false,
        ""SubnetChangeProtection"": false,
        ""FirewallPolicyChangeProtection"": false,
        ""FirewallId"": ""b70bc32e-9753-4d7e-bb82-febfb362b72b"",
        ""Tags"": []
    },
    ""FirewallStatus"": {
        ""Status"": ""READY"",
        ""ConfigurationSyncStateSummary"": ""IN_SYNC"",
        ""SyncStates"": {
            ""us-east-1a"": {
                ""Attachment"": {
                    ""SubnetId"": ""subnet-05d7d553643f36d0e"",
                    ""EndpointId"": ""vpce-0394425be976920e3"",
                    ""Status"": ""READY""
                },
                ""Config"": {
                    ""arn:aws:network-firewall:us-east-1:xxxx:firewall-policy/example-inspection-vpc-policy"": {
                        ""SyncStatus"": ""IN_SYNC""
                    }
                }
            },
            ""us-east-1b"": {
                ""Attachment"": {
                    ""SubnetId"": ""subnet-0f96d5471222feb28"",
                    ""EndpointId"": ""vpce-073e44a76dab0949e"",
                    ""Status"": ""READY""
                },
                ""Config"": {
                    ""arn:aws:network-firewall:us-east-1:xxxx:firewall-policy/example-inspection-vpc-policy"": {
                        ""SyncStatus"": ""IN_SYNC""
                    }
                }
            },
            ""us-east-1c"": {
                ""Attachment"": {
                    ""SubnetId"": ""subnet-0f55ba6bdcc7357ca"",
                    ""EndpointId"": ""vpce-0ae3a6d2b2c886215"",
                    ""Status"": ""READY""
                },
                ""Config"": {
                    ""arn:aws:network-firewall:us-east-1:xxxx:firewall-policy/example-inspection-vpc-policy"": {
                        ""SyncStatus"": ""IN_SYNC""
                    }
                }
            },
            ""us-east-1d"": {
                ""Attachment"": {
                    ""SubnetId"": ""subnet-02e8df98316b27054"",
                    ""EndpointId"": ""vpce-0006b35c0f00be77c"",
                    ""Status"": ""READY""
                },
                ""Config"": {
                    ""arn:aws:network-firewall:us-east-1:xxxxx:firewall-policy/example-inspection-vpc-policy"": {
                        ""SyncStatus"": ""IN_SYNC""
                    }
                }
            },
            ""us-east-1f"": {
                ""Attachment"": {
                    ""SubnetId"": ""subnet-03c8e80f3c807fd2e"",
                    ""EndpointId"": ""vpce-0552034c8ae2b92ce"",
                    ""Status"": ""READY""
                },
                ""Config"": {
                    ""arn:aws:network-firewall:us-east-1:xxxxx:firewall-policy/example-inspection-vpc-policy"": {
                        ""SyncStatus"": ""IN_SYNC""
                    }
                }
            }
        }
    }
}
```


### New or Affected Resource(s)

<!--- Please list the new or affected resources and data sources. --->

* aws_networkfirewall_firewall

### Potential Terraform Configuration

<!--- Information about code formatting: https://help.github.com/articles/basic-writing-and-formatting-syntax/#quoting-code --->

```hcl
# Copy-paste your Terraform configurations here - for large Terraform configs,
# please use a service like Dropbox and share a link to the ZIP file. For
# security, you can also encrypt the files using our GPG public key.
```

### References

<!---
Information about referencing Github Issues: https://help.github.com/articles/basic-writing-and-formatting-syntax/#referencing-issues-and-pull-requests

Are there any other GitHub issues (open or closed) or pull requests that should be linked here? Vendor blog posts or documentation? For example:

* https://aws.amazon.com/about-aws/whats-new/2018/04/introducing-amazon-ec2-fleet/
--->

* https://docs.aws.amazon.com/network-firewall/latest/APIReference/API_FirewallStatus.html
",29c6147b-e772-4c46-8d78-7aa5f2b71aaf,https://github.com/hashicorp/terraform-provider-aws
5,16350.0,"<!--- Please keep this note for the community --->

### Community Note

* Please vote on this issue by adding a 👍 [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to the original issue to help the community and maintainers prioritize this request
* Please do not leave ""+1"" or other comments that do not add relevant new information or questions, they generate extra noise for issue followers and do not help prioritize the request
* If you are interested in working on this issue or have submitted a pull request, please leave a comment

<!--- Thank you for keeping this note for the community --->

### Description

<!--- Please leave a helpful description of the feature request here. --->

The aws_networkfirewall_firewall resource needs to expose the VPC endpoints created by the firewall for use in routing tables in order to actually use it.  These endpoints are exposed in the FirewallStatus structure returned by the API.

For example:
```
$ aws --region us-east-1 network-firewall describe-firewall --firewall-name dhagan-2020-11-20-002
{
    ""UpdateToken"": ""29c6147b-e772-4c46-8d78-7aa5f2b71aaf"",
    ""Firewall"": {
        ""FirewallName"": ""dhagan-2020-11-20-002"",
        ""FirewallArn"": ""arn:aws:network-firewall:us-east-1:xxx:firewall/dhagan-2020-11-20-002"",
        ""FirewallPolicyArn"": ""arn:aws:network-firewall:us-east-1:xxxx:firewall-policy/example-inspection-vpc-policy"",
        ""VpcId"": ""vpc-0b857b1a3686ae36d"",
        ""SubnetMappings"": [
            {
                ""SubnetId"": ""subnet-0f96d5471222feb28""
            },
            {
                ""SubnetId"": ""subnet-0f55ba6bdcc7357ca""
            },
            {
                ""SubnetId"": ""subnet-03c8e80f3c807fd2e""
            },
            {
                ""SubnetId"": ""subnet-05d7d553643f36d0e""
            },
            {
                ""SubnetId"": ""subnet-02e8df98316b27054""
            }
        ],
        ""DeleteProtection"": false,
        ""SubnetChangeProtection"": false,
        ""FirewallPolicyChangeProtection"": false,
        ""FirewallId"": ""b70bc32e-9753-4d7e-bb82-febfb362b72b"",
        ""Tags"": []
    },
    ""FirewallStatus"": {
        ""Status"": ""READY"",
        ""ConfigurationSyncStateSummary"": ""IN_SYNC"",
        ""SyncStates"": {
            ""us-east-1a"": {
                ""Attachment"": {
                    ""SubnetId"": ""subnet-05d7d553643f36d0e"",
                    ""EndpointId"": ""vpce-0394425be976920e3"",
                    ""Status"": ""READY""
                },
                ""Config"": {
                    ""arn:aws:network-firewall:us-east-1:xxxx:firewall-policy/example-inspection-vpc-policy"": {
                        ""SyncStatus"": ""IN_SYNC""
                    }
                }
            },
            ""us-east-1b"": {
                ""Attachment"": {
                    ""SubnetId"": ""subnet-0f96d5471222feb28"",
                    ""EndpointId"": ""vpce-073e44a76dab0949e"",
                    ""Status"": ""READY""
                },
                ""Config"": {
                    ""arn:aws:network-firewall:us-east-1:xxxx:firewall-policy/example-inspection-vpc-policy"": {
                        ""SyncStatus"": ""IN_SYNC""
                    }
                }
            },
            ""us-east-1c"": {
                ""Attachment"": {
                    ""SubnetId"": ""subnet-0f55ba6bdcc7357ca"",
                    ""EndpointId"": ""vpce-0ae3a6d2b2c886215"",
                    ""Status"": ""READY""
                },
                ""Config"": {
                    ""arn:aws:network-firewall:us-east-1:xxxx:firewall-policy/example-inspection-vpc-policy"": {
                        ""SyncStatus"": ""IN_SYNC""
                    }
                }
            },
            ""us-east-1d"": {
                ""Attachment"": {
                    ""SubnetId"": ""subnet-02e8df98316b27054"",
                    ""EndpointId"": ""vpce-0006b35c0f00be77c"",
                    ""Status"": ""READY""
                },
                ""Config"": {
                    ""arn:aws:network-firewall:us-east-1:xxxxx:firewall-policy/example-inspection-vpc-policy"": {
                        ""SyncStatus"": ""IN_SYNC""
                    }
                }
            },
            ""us-east-1f"": {
                ""Attachment"": {
                    ""SubnetId"": ""subnet-03c8e80f3c807fd2e"",
                    ""EndpointId"": ""vpce-0552034c8ae2b92ce"",
                    ""Status"": ""READY""
                },
                ""Config"": {
                    ""arn:aws:network-firewall:us-east-1:xxxxx:firewall-policy/example-inspection-vpc-policy"": {
                        ""SyncStatus"": ""IN_SYNC""
                    }
                }
            }
        }
    }
}
```


### New or Affected Resource(s)

<!--- Please list the new or affected resources and data sources. --->

* aws_networkfirewall_firewall

### Potential Terraform Configuration

<!--- Information about code formatting: https://help.github.com/articles/basic-writing-and-formatting-syntax/#quoting-code --->

```hcl
# Copy-paste your Terraform configurations here - for large Terraform configs,
# please use a service like Dropbox and share a link to the ZIP file. For
# security, you can also encrypt the files using our GPG public key.
```

### References

<!---
Information about referencing Github Issues: https://help.github.com/articles/basic-writing-and-formatting-syntax/#referencing-issues-and-pull-requests

Are there any other GitHub issues (open or closed) or pull requests that should be linked here? Vendor blog posts or documentation? For example:

* https://aws.amazon.com/about-aws/whats-new/2018/04/introducing-amazon-ec2-fleet/
--->

* https://docs.aws.amazon.com/network-firewall/latest/APIReference/API_FirewallStatus.html
",8d78-7aa5f2b71aaf,https://github.com/hashicorp/terraform-provider-aws
14,2671.0,"Hi there,

Thank you for opening an issue. Please note that we try to keep the Terraform issue tracker reserved for bug reports and feature requests. For general usage questions, please see: https://www.terraform.io/community.html.

### Terraform Version
Terraform v0.11.1
+ provider.aws v1.5.0

### Affected Resource(s)
Please list the resources as a list, for example:
- aws_dms_replication_instance
- aws_efs_filesystem

### Expected Behavior
Terraform should be ""smart enough"" to realize that when the result of describe-replication-instances attribute kms_key_arn will always be a KeyID, even when the create-replication-instance call was done with a key alias.  Terraform should lookup what keyid the alias points to, and when equal, then don't consider this field to be ""forces new resource"".

### Actual Behavior
      kms_key_arn:                        ""arn:aws:kms:us-east-1:{Redacted}:key/b0c45cd7-29fa-4b01-a24b-f2bbacbc385f"" => ""arn:aws:kms:us-east-1:{Redacted}:alias/iamtesting"" (forces new resource)


### Steps to Reproduce
Please list the steps required to reproduce the issue, for example:
1. `terraform plan`
2. `terraform apply`
3. `terraform plan` without any changes, will still force a  new resource due to this issue 

### Important Factoids
None

### References
None
",:us-east-1:{Redacted}:key,https://github.com/hashicorp/terraform-provider-aws
15,2671.0,"Hi there,

Thank you for opening an issue. Please note that we try to keep the Terraform issue tracker reserved for bug reports and feature requests. For general usage questions, please see: https://www.terraform.io/community.html.

### Terraform Version
Terraform v0.11.1
+ provider.aws v1.5.0

### Affected Resource(s)
Please list the resources as a list, for example:
- aws_dms_replication_instance
- aws_efs_filesystem

### Expected Behavior
Terraform should be ""smart enough"" to realize that when the result of describe-replication-instances attribute kms_key_arn will always be a KeyID, even when the create-replication-instance call was done with a key alias.  Terraform should lookup what keyid the alias points to, and when equal, then don't consider this field to be ""forces new resource"".

### Actual Behavior
      kms_key_arn:                        ""arn:aws:kms:us-east-1:{Redacted}:key/b0c45cd7-29fa-4b01-a24b-f2bbacbc385f"" => ""arn:aws:kms:us-east-1:{Redacted}:alias/iamtesting"" (forces new resource)


### Steps to Reproduce
Please list the steps required to reproduce the issue, for example:
1. `terraform plan`
2. `terraform apply`
3. `terraform plan` without any changes, will still force a  new resource due to this issue 

### Important Factoids
None

### References
None
",replication-instance,https://github.com/hashicorp/terraform-provider-aws
16,2663.0,"When removing the key_name from an existing launch configuration terraform configuration, terraform doesn't recreate or change the launch config. This means that the launch configuration still contains the key that isn't specified in the terraform code. 

### Terraform Version

```
Terraform v0.11.1
+ provider.aws v1.5.0
```

### Terraform Configuration Files

```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_key_pair"" ""deployer"" {
  key_name   = ""deployer-key""
  public_key = ""ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQD3F6tyPEFEzV0LX3X8BsXdMsQz1x2cEikKDEY0aIj41qgxMCP/iteneqXSIFZBp5vizPvaoIR3Um9xK7PGoW8giupGn+EPuxIA4cDM4vzOqOkiMPhz5XK0whEjkVzTo4+S0puvDZuwIsdiW9mxhJc7tgBNL0cYlWSYVkz4G/fslNfRPW5mYAM49f4fhtxPb5ok4Q2Lg9dPKVHO/Bgeu5woMc7RY0p1ej6D4CKFE6lymSDJpW0YHX/wqE9+cfEauh7xZcG0q9t2ta6F6fmX0agvpFyZo8aFbXeUBr7osSCJNgvavWbM/06niWrOvYX2xwWdhXmXSrbX8ZbabVohBK41 email@example.com""
}

resource ""aws_launch_configuration"" ""as_conf"" {
  name_prefix   = ""terraform-lc-example-""
  image_id      = ""ami-79ee4f01""
  instance_type = ""t2.micro""
  key_name      = ""${aws_key_pair.deployer.key_name}""

  lifecycle {
    create_before_destroy = true
  }
}
```

### Expected Behavior
<!--
What should have happened?
-->
After deleting the key_name parameter from the launch configuration, I'd expect the launch configuration to be changed (or recreated) to not include the original key_name. 

### Actual Behavior
<!--
What actually happened?
-->
After deleting the key_name parameter terraform doesn't recognise any changes, so the launch configuration isn't updated. 

### Steps to Reproduce
<!--
Please list the full steps required to reproduce the issue, for example: 
-->
Initially create the launch configuration including a key with the terraform code pasted above by running:
1. `terraform init`
2. `terraform plan`
3. `terraform apply`

Then remove the key_name parameter, and run those three terraform commands again. Nothing changes.",AAAAB3NzaC1yc2EAAAADAQABAAABAQD3F6tyPEFEzV0LX3X8BsXdMsQz1x2cEikKDEY0aIj41,https://github.com/hashicorp/terraform-provider-aws
17,2663.0,"When removing the key_name from an existing launch configuration terraform configuration, terraform doesn't recreate or change the launch config. This means that the launch configuration still contains the key that isn't specified in the terraform code. 

### Terraform Version

```
Terraform v0.11.1
+ provider.aws v1.5.0
```

### Terraform Configuration Files

```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_key_pair"" ""deployer"" {
  key_name   = ""deployer-key""
  public_key = ""ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQD3F6tyPEFEzV0LX3X8BsXdMsQz1x2cEikKDEY0aIj41qgxMCP/iteneqXSIFZBp5vizPvaoIR3Um9xK7PGoW8giupGn+EPuxIA4cDM4vzOqOkiMPhz5XK0whEjkVzTo4+S0puvDZuwIsdiW9mxhJc7tgBNL0cYlWSYVkz4G/fslNfRPW5mYAM49f4fhtxPb5ok4Q2Lg9dPKVHO/Bgeu5woMc7RY0p1ej6D4CKFE6lymSDJpW0YHX/wqE9+cfEauh7xZcG0q9t2ta6F6fmX0agvpFyZo8aFbXeUBr7osSCJNgvavWbM/06niWrOvYX2xwWdhXmXSrbX8ZbabVohBK41 email@example.com""
}

resource ""aws_launch_configuration"" ""as_conf"" {
  name_prefix   = ""terraform-lc-example-""
  image_id      = ""ami-79ee4f01""
  instance_type = ""t2.micro""
  key_name      = ""${aws_key_pair.deployer.key_name}""

  lifecycle {
    create_before_destroy = true
  }
}
```

### Expected Behavior
<!--
What should have happened?
-->
After deleting the key_name parameter from the launch configuration, I'd expect the launch configuration to be changed (or recreated) to not include the original key_name. 

### Actual Behavior
<!--
What actually happened?
-->
After deleting the key_name parameter terraform doesn't recognise any changes, so the launch configuration isn't updated. 

### Steps to Reproduce
<!--
Please list the full steps required to reproduce the issue, for example: 
-->
Initially create the launch configuration including a key with the terraform code pasted above by running:
1. `terraform init`
2. `terraform plan`
3. `terraform apply`

Then remove the key_name parameter, and run those three terraform commands again. Nothing changes.",AAAAB3NzaC1yc2EAAAADAQABAAABAQD3F6tyPEFEzV0LX3X8BsXdMsQz1x2cEikKDEY0aIj41qgxMCP,https://github.com/hashicorp/terraform-provider-aws
18,2663.0,"When removing the key_name from an existing launch configuration terraform configuration, terraform doesn't recreate or change the launch config. This means that the launch configuration still contains the key that isn't specified in the terraform code. 

### Terraform Version

```
Terraform v0.11.1
+ provider.aws v1.5.0
```

### Terraform Configuration Files

```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_key_pair"" ""deployer"" {
  key_name   = ""deployer-key""
  public_key = ""ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQD3F6tyPEFEzV0LX3X8BsXdMsQz1x2cEikKDEY0aIj41qgxMCP/iteneqXSIFZBp5vizPvaoIR3Um9xK7PGoW8giupGn+EPuxIA4cDM4vzOqOkiMPhz5XK0whEjkVzTo4+S0puvDZuwIsdiW9mxhJc7tgBNL0cYlWSYVkz4G/fslNfRPW5mYAM49f4fhtxPb5ok4Q2Lg9dPKVHO/Bgeu5woMc7RY0p1ej6D4CKFE6lymSDJpW0YHX/wqE9+cfEauh7xZcG0q9t2ta6F6fmX0agvpFyZo8aFbXeUBr7osSCJNgvavWbM/06niWrOvYX2xwWdhXmXSrbX8ZbabVohBK41 email@example.com""
}

resource ""aws_launch_configuration"" ""as_conf"" {
  name_prefix   = ""terraform-lc-example-""
  image_id      = ""ami-79ee4f01""
  instance_type = ""t2.micro""
  key_name      = ""${aws_key_pair.deployer.key_name}""

  lifecycle {
    create_before_destroy = true
  }
}
```

### Expected Behavior
<!--
What should have happened?
-->
After deleting the key_name parameter from the launch configuration, I'd expect the launch configuration to be changed (or recreated) to not include the original key_name. 

### Actual Behavior
<!--
What actually happened?
-->
After deleting the key_name parameter terraform doesn't recognise any changes, so the launch configuration isn't updated. 

### Steps to Reproduce
<!--
Please list the full steps required to reproduce the issue, for example: 
-->
Initially create the launch configuration including a key with the terraform code pasted above by running:
1. `terraform init`
2. `terraform plan`
3. `terraform apply`

Then remove the key_name parameter, and run those three terraform commands again. Nothing changes.",create_before_destroy,https://github.com/hashicorp/terraform-provider-aws
19,2133.0,"This is to address the following test failure:

```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (22.41s)
    testing.go:492: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed setting KMS key rotation status to true: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/2be74763-8842-4a25-bdbe-5c7ad655cd57' does not exist
            status code: 400, request id: 94ba7372-bed8-11e7-b066-975b24c8fa34
```",TestAccAWSKmsKey_isEnabled,https://github.com/hashicorp/terraform-provider-aws
20,2133.0,"This is to address the following test failure:

```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (22.41s)
    testing.go:492: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed setting KMS key rotation status to true: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/2be74763-8842-4a25-bdbe-5c7ad655cd57' does not exist
            status code: 400, request id: 94ba7372-bed8-11e7-b066-975b24c8fa34
```",west-2:*******:key,https://github.com/hashicorp/terraform-provider-aws
21,2133.0,"This is to address the following test failure:

```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (22.41s)
    testing.go:492: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed setting KMS key rotation status to true: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/2be74763-8842-4a25-bdbe-5c7ad655cd57' does not exist
            status code: 400, request id: 94ba7372-bed8-11e7-b066-975b24c8fa34
```",NotFoundException,https://github.com/hashicorp/terraform-provider-aws
22,1896.0,"This is to address the following test failure:

```
=== RUN   TestAccAWSKmsAlias_name_prefix
--- FAIL: TestAccAWSKmsAlias_name_prefix (24.56s)
    testing.go:434: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_alias.name_prefix: 1 error(s) occurred:
        
        * aws_kms_alias.name_prefix: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/d009dff4-27f4-4e10-ac07-aef1b74204e0' does not exist
            status code: 400, request id: 5334f913-b106-11e7-9f19-a3bd400081f8
```

Snippet from the log:
```
2017/10/14 17:37:17 [DEBUG] [aws-sdk-go] DEBUG: Response kms/DescribeKey Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 387
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: 531d2abb-b106-11e7-9b02-a5418d0385e7


-----------------------------------------------------
2017/10/14 17:37:17 [DEBUG] [aws-sdk-go] {""KeyMetadata"":{""AWSAccountId"":""*REDACTED*"",""Arn"":""arn:aws:kms:us-west-2:*REDACTED*:key/d009dff4-27f4-4e10-ac07-aef1b74204e0"",""CreationDate"":1.508002637976E9,""Description"":""Terraform acc test One Sat, 14 Oct 2017 17:37:14 UTC"",""Enabled"":true,""KeyId"":""d009dff4-27f4-4e10-ac07-aef1b74204e0"",""KeyManager"":""CUSTOMER"",""KeyState"":""Enabled"",""KeyUsage"":""ENCRYPT_DECRYPT"",""Origin"":""AWS_KMS""}}

-----------------------------------------------------
2017/10/14 17:37:18 [DEBUG] [aws-sdk-go] DEBUG: Request kms/CreateAlias Details:
---[ REQUEST POST-SIGN ]-----------------------------
POST / HTTP/1.1
Host: kms.us-west-2.amazonaws.com
User-Agent: aws-sdk-go/1.12.8 (go1.9; linux; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.10.0-dev
Content-Length: 101
Authorization: AWS4-HMAC-SHA256 Credential=*REDACTED*/20171014/us-west-2/kms/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-target, Signature=*REDACTED*
Content-Type: application/x-amz-json-1.1
X-Amz-Date: 20171014T173718Z
X-Amz-Target: TrentService.CreateAlias
Accept-Encoding: gzip

{""AliasName"":""alias/20171014173718010800000001"",""TargetKeyId"":""d009dff4-27f4-4e10-ac07-aef1b74204e0""}
-----------------------------------------------------

-----------------------------------------------------
2017/10/14 17:37:18 [DEBUG] [aws-sdk-go] DEBUG: Response kms/CreateAlias Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 400 Bad Request
Connection: close
Content-Length: 139
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: 5334f913-b106-11e7-9f19-a3bd400081f8


-----------------------------------------------------
2017/10/14 17:37:18 [DEBUG] [aws-sdk-go] {""__type"":""NotFoundException"",""message"":""Key 'arn:aws:kms:us-west-2:187416307283:key/d009dff4-27f4-4e10-ac07-aef1b74204e0' does not exist""}
```",187416307283:key,https://github.com/hashicorp/terraform-provider-aws
23,1896.0,"This is to address the following test failure:

```
=== RUN   TestAccAWSKmsAlias_name_prefix
--- FAIL: TestAccAWSKmsAlias_name_prefix (24.56s)
    testing.go:434: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_alias.name_prefix: 1 error(s) occurred:
        
        * aws_kms_alias.name_prefix: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/d009dff4-27f4-4e10-ac07-aef1b74204e0' does not exist
            status code: 400, request id: 5334f913-b106-11e7-9f19-a3bd400081f8
```

Snippet from the log:
```
2017/10/14 17:37:17 [DEBUG] [aws-sdk-go] DEBUG: Response kms/DescribeKey Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 387
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: 531d2abb-b106-11e7-9b02-a5418d0385e7


-----------------------------------------------------
2017/10/14 17:37:17 [DEBUG] [aws-sdk-go] {""KeyMetadata"":{""AWSAccountId"":""*REDACTED*"",""Arn"":""arn:aws:kms:us-west-2:*REDACTED*:key/d009dff4-27f4-4e10-ac07-aef1b74204e0"",""CreationDate"":1.508002637976E9,""Description"":""Terraform acc test One Sat, 14 Oct 2017 17:37:14 UTC"",""Enabled"":true,""KeyId"":""d009dff4-27f4-4e10-ac07-aef1b74204e0"",""KeyManager"":""CUSTOMER"",""KeyState"":""Enabled"",""KeyUsage"":""ENCRYPT_DECRYPT"",""Origin"":""AWS_KMS""}}

-----------------------------------------------------
2017/10/14 17:37:18 [DEBUG] [aws-sdk-go] DEBUG: Request kms/CreateAlias Details:
---[ REQUEST POST-SIGN ]-----------------------------
POST / HTTP/1.1
Host: kms.us-west-2.amazonaws.com
User-Agent: aws-sdk-go/1.12.8 (go1.9; linux; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.10.0-dev
Content-Length: 101
Authorization: AWS4-HMAC-SHA256 Credential=*REDACTED*/20171014/us-west-2/kms/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-target, Signature=*REDACTED*
Content-Type: application/x-amz-json-1.1
X-Amz-Date: 20171014T173718Z
X-Amz-Target: TrentService.CreateAlias
Accept-Encoding: gzip

{""AliasName"":""alias/20171014173718010800000001"",""TargetKeyId"":""d009dff4-27f4-4e10-ac07-aef1b74204e0""}
-----------------------------------------------------

-----------------------------------------------------
2017/10/14 17:37:18 [DEBUG] [aws-sdk-go] DEBUG: Response kms/CreateAlias Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 400 Bad Request
Connection: close
Content-Length: 139
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: 5334f913-b106-11e7-9f19-a3bd400081f8


-----------------------------------------------------
2017/10/14 17:37:18 [DEBUG] [aws-sdk-go] {""__type"":""NotFoundException"",""message"":""Key 'arn:aws:kms:us-west-2:187416307283:key/d009dff4-27f4-4e10-ac07-aef1b74204e0' does not exist""}
```","arn:aws:kms:us-west-2:*REDACTED*:key,CreationDate:,Description:",https://github.com/hashicorp/terraform-provider-aws
24,1896.0,"This is to address the following test failure:

```
=== RUN   TestAccAWSKmsAlias_name_prefix
--- FAIL: TestAccAWSKmsAlias_name_prefix (24.56s)
    testing.go:434: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_alias.name_prefix: 1 error(s) occurred:
        
        * aws_kms_alias.name_prefix: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/d009dff4-27f4-4e10-ac07-aef1b74204e0' does not exist
            status code: 400, request id: 5334f913-b106-11e7-9f19-a3bd400081f8
```

Snippet from the log:
```
2017/10/14 17:37:17 [DEBUG] [aws-sdk-go] DEBUG: Response kms/DescribeKey Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 387
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: 531d2abb-b106-11e7-9b02-a5418d0385e7


-----------------------------------------------------
2017/10/14 17:37:17 [DEBUG] [aws-sdk-go] {""KeyMetadata"":{""AWSAccountId"":""*REDACTED*"",""Arn"":""arn:aws:kms:us-west-2:*REDACTED*:key/d009dff4-27f4-4e10-ac07-aef1b74204e0"",""CreationDate"":1.508002637976E9,""Description"":""Terraform acc test One Sat, 14 Oct 2017 17:37:14 UTC"",""Enabled"":true,""KeyId"":""d009dff4-27f4-4e10-ac07-aef1b74204e0"",""KeyManager"":""CUSTOMER"",""KeyState"":""Enabled"",""KeyUsage"":""ENCRYPT_DECRYPT"",""Origin"":""AWS_KMS""}}

-----------------------------------------------------
2017/10/14 17:37:18 [DEBUG] [aws-sdk-go] DEBUG: Request kms/CreateAlias Details:
---[ REQUEST POST-SIGN ]-----------------------------
POST / HTTP/1.1
Host: kms.us-west-2.amazonaws.com
User-Agent: aws-sdk-go/1.12.8 (go1.9; linux; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.10.0-dev
Content-Length: 101
Authorization: AWS4-HMAC-SHA256 Credential=*REDACTED*/20171014/us-west-2/kms/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-target, Signature=*REDACTED*
Content-Type: application/x-amz-json-1.1
X-Amz-Date: 20171014T173718Z
X-Amz-Target: TrentService.CreateAlias
Accept-Encoding: gzip

{""AliasName"":""alias/20171014173718010800000001"",""TargetKeyId"":""d009dff4-27f4-4e10-ac07-aef1b74204e0""}
-----------------------------------------------------

-----------------------------------------------------
2017/10/14 17:37:18 [DEBUG] [aws-sdk-go] DEBUG: Response kms/CreateAlias Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 400 Bad Request
Connection: close
Content-Length: 139
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: 5334f913-b106-11e7-9f19-a3bd400081f8


-----------------------------------------------------
2017/10/14 17:37:18 [DEBUG] [aws-sdk-go] {""__type"":""NotFoundException"",""message"":""Key 'arn:aws:kms:us-west-2:187416307283:key/d009dff4-27f4-4e10-ac07-aef1b74204e0' does not exist""}
```",west-2:*******:key,https://github.com/hashicorp/terraform-provider-aws
25,1896.0,"This is to address the following test failure:

```
=== RUN   TestAccAWSKmsAlias_name_prefix
--- FAIL: TestAccAWSKmsAlias_name_prefix (24.56s)
    testing.go:434: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_alias.name_prefix: 1 error(s) occurred:
        
        * aws_kms_alias.name_prefix: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/d009dff4-27f4-4e10-ac07-aef1b74204e0' does not exist
            status code: 400, request id: 5334f913-b106-11e7-9f19-a3bd400081f8
```

Snippet from the log:
```
2017/10/14 17:37:17 [DEBUG] [aws-sdk-go] DEBUG: Response kms/DescribeKey Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 387
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: 531d2abb-b106-11e7-9b02-a5418d0385e7


-----------------------------------------------------
2017/10/14 17:37:17 [DEBUG] [aws-sdk-go] {""KeyMetadata"":{""AWSAccountId"":""*REDACTED*"",""Arn"":""arn:aws:kms:us-west-2:*REDACTED*:key/d009dff4-27f4-4e10-ac07-aef1b74204e0"",""CreationDate"":1.508002637976E9,""Description"":""Terraform acc test One Sat, 14 Oct 2017 17:37:14 UTC"",""Enabled"":true,""KeyId"":""d009dff4-27f4-4e10-ac07-aef1b74204e0"",""KeyManager"":""CUSTOMER"",""KeyState"":""Enabled"",""KeyUsage"":""ENCRYPT_DECRYPT"",""Origin"":""AWS_KMS""}}

-----------------------------------------------------
2017/10/14 17:37:18 [DEBUG] [aws-sdk-go] DEBUG: Request kms/CreateAlias Details:
---[ REQUEST POST-SIGN ]-----------------------------
POST / HTTP/1.1
Host: kms.us-west-2.amazonaws.com
User-Agent: aws-sdk-go/1.12.8 (go1.9; linux; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.10.0-dev
Content-Length: 101
Authorization: AWS4-HMAC-SHA256 Credential=*REDACTED*/20171014/us-west-2/kms/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-target, Signature=*REDACTED*
Content-Type: application/x-amz-json-1.1
X-Amz-Date: 20171014T173718Z
X-Amz-Target: TrentService.CreateAlias
Accept-Encoding: gzip

{""AliasName"":""alias/20171014173718010800000001"",""TargetKeyId"":""d009dff4-27f4-4e10-ac07-aef1b74204e0""}
-----------------------------------------------------

-----------------------------------------------------
2017/10/14 17:37:18 [DEBUG] [aws-sdk-go] DEBUG: Response kms/CreateAlias Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 400 Bad Request
Connection: close
Content-Length: 139
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: 5334f913-b106-11e7-9f19-a3bd400081f8


-----------------------------------------------------
2017/10/14 17:37:18 [DEBUG] [aws-sdk-go] {""__type"":""NotFoundException"",""message"":""Key 'arn:aws:kms:us-west-2:187416307283:key/d009dff4-27f4-4e10-ac07-aef1b74204e0' does not exist""}
```",-length;content-type;host;x-amz-date;x-amz-target,https://github.com/hashicorp/terraform-provider-aws
26,1896.0,"This is to address the following test failure:

```
=== RUN   TestAccAWSKmsAlias_name_prefix
--- FAIL: TestAccAWSKmsAlias_name_prefix (24.56s)
    testing.go:434: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_alias.name_prefix: 1 error(s) occurred:
        
        * aws_kms_alias.name_prefix: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/d009dff4-27f4-4e10-ac07-aef1b74204e0' does not exist
            status code: 400, request id: 5334f913-b106-11e7-9f19-a3bd400081f8
```

Snippet from the log:
```
2017/10/14 17:37:17 [DEBUG] [aws-sdk-go] DEBUG: Response kms/DescribeKey Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 387
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: 531d2abb-b106-11e7-9b02-a5418d0385e7


-----------------------------------------------------
2017/10/14 17:37:17 [DEBUG] [aws-sdk-go] {""KeyMetadata"":{""AWSAccountId"":""*REDACTED*"",""Arn"":""arn:aws:kms:us-west-2:*REDACTED*:key/d009dff4-27f4-4e10-ac07-aef1b74204e0"",""CreationDate"":1.508002637976E9,""Description"":""Terraform acc test One Sat, 14 Oct 2017 17:37:14 UTC"",""Enabled"":true,""KeyId"":""d009dff4-27f4-4e10-ac07-aef1b74204e0"",""KeyManager"":""CUSTOMER"",""KeyState"":""Enabled"",""KeyUsage"":""ENCRYPT_DECRYPT"",""Origin"":""AWS_KMS""}}

-----------------------------------------------------
2017/10/14 17:37:18 [DEBUG] [aws-sdk-go] DEBUG: Request kms/CreateAlias Details:
---[ REQUEST POST-SIGN ]-----------------------------
POST / HTTP/1.1
Host: kms.us-west-2.amazonaws.com
User-Agent: aws-sdk-go/1.12.8 (go1.9; linux; amd64) APN/1.0 HashiCorp/1.0 Terraform/0.10.0-dev
Content-Length: 101
Authorization: AWS4-HMAC-SHA256 Credential=*REDACTED*/20171014/us-west-2/kms/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-target, Signature=*REDACTED*
Content-Type: application/x-amz-json-1.1
X-Amz-Date: 20171014T173718Z
X-Amz-Target: TrentService.CreateAlias
Accept-Encoding: gzip

{""AliasName"":""alias/20171014173718010800000001"",""TargetKeyId"":""d009dff4-27f4-4e10-ac07-aef1b74204e0""}
-----------------------------------------------------

-----------------------------------------------------
2017/10/14 17:37:18 [DEBUG] [aws-sdk-go] DEBUG: Response kms/CreateAlias Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 400 Bad Request
Connection: close
Content-Length: 139
Content-Type: application/x-amz-json-1.1
X-Amzn-Requestid: 5334f913-b106-11e7-9f19-a3bd400081f8


-----------------------------------------------------
2017/10/14 17:37:18 [DEBUG] [aws-sdk-go] {""__type"":""NotFoundException"",""message"":""Key 'arn:aws:kms:us-west-2:187416307283:key/d009dff4-27f4-4e10-ac07-aef1b74204e0' does not exist""}
```","KeyUsage:ENCRYPT_DECRYPT,Origin:AWS_KMS",https://github.com/hashicorp/terraform-provider-aws
34,1818.0,"This is to address the following test failure:
```
=== RUN   TestAccDataSourceAwsKmsCiphertext_basic
--- FAIL: TestAccDataSourceAwsKmsCiphertext_basic (31.03s)
    testing.go:434: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/20505b0e-764f-4ef5-8f1b-73a2686fd11b' does not exist
            status code: 400, request id: a92026f9-a994-11e7-bbbb-41668ba48bea
```

Snippet from debug log:

```
2017/10/05 06:16:00 [DEBUG] [aws-sdk-go] DEBUG: Response kms/DescribeKey Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 383
Content-Type: application/x-amz-json-1.1
Date: Thu, 05 Oct 2017 06:16:01 GMT
Server: Server
X-Amzn-Requestid: a91885e4-a994-11e7-9579-2da1021b9d1d


-----------------------------------------------------
2017/10/05 06:16:00 [DEBUG] [aws-sdk-go] {""KeyMetadata"":{""AWSAccountId"":""*REDACTED*"",""Arn"":""arn:aws:kms:us-west-2:*REDACTED*:key/20505b0e-764f-4ef5-8f1b-73a2686fd11b"",""CreationDate"":1.507184161473E9,""Description"":""tf-test-acc-data-source-aws-kms-ciphertext-basic"",""Enabled"":true,""KeyId"":""20505b0e-764f-4ef5-8f1b-73a2686fd11b"",""KeyManager"":""CUSTOMER"",""KeyState"":""Enabled"",""KeyUsage"":""ENCRYPT_DECRYPT"",""Origin"":""AWS_KMS""}}

2017/10/05 06:16:00 [DEBUG] [aws-sdk-go] DEBUG: Response kms/GetKeyRotationStatus Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 400 Bad Request
Connection: close
Content-Length: 139
Content-Type: application/x-amz-json-1.1
Date: Thu, 05 Oct 2017 06:16:01 GMT
Server: Server
X-Amzn-Requestid: a92026f9-a994-11e7-bbbb-41668ba48bea


-----------------------------------------------------
2017/10/05 06:16:00 [DEBUG] [aws-sdk-go] {""__type"":""NotFoundException"",""message"":""Key 'arn:aws:kms:us-west-2:*REDACTED*:key/20505b0e-764f-4ef5-8f1b-73a2686fd11b' does not exist""}
```","KeyUsage:ENCRYPT_DECRYPT,Origin:AWS_KMS",https://github.com/hashicorp/terraform-provider-aws
35,1818.0,"This is to address the following test failure:
```
=== RUN   TestAccDataSourceAwsKmsCiphertext_basic
--- FAIL: TestAccDataSourceAwsKmsCiphertext_basic (31.03s)
    testing.go:434: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/20505b0e-764f-4ef5-8f1b-73a2686fd11b' does not exist
            status code: 400, request id: a92026f9-a994-11e7-bbbb-41668ba48bea
```

Snippet from debug log:

```
2017/10/05 06:16:00 [DEBUG] [aws-sdk-go] DEBUG: Response kms/DescribeKey Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 383
Content-Type: application/x-amz-json-1.1
Date: Thu, 05 Oct 2017 06:16:01 GMT
Server: Server
X-Amzn-Requestid: a91885e4-a994-11e7-9579-2da1021b9d1d


-----------------------------------------------------
2017/10/05 06:16:00 [DEBUG] [aws-sdk-go] {""KeyMetadata"":{""AWSAccountId"":""*REDACTED*"",""Arn"":""arn:aws:kms:us-west-2:*REDACTED*:key/20505b0e-764f-4ef5-8f1b-73a2686fd11b"",""CreationDate"":1.507184161473E9,""Description"":""tf-test-acc-data-source-aws-kms-ciphertext-basic"",""Enabled"":true,""KeyId"":""20505b0e-764f-4ef5-8f1b-73a2686fd11b"",""KeyManager"":""CUSTOMER"",""KeyState"":""Enabled"",""KeyUsage"":""ENCRYPT_DECRYPT"",""Origin"":""AWS_KMS""}}

2017/10/05 06:16:00 [DEBUG] [aws-sdk-go] DEBUG: Response kms/GetKeyRotationStatus Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 400 Bad Request
Connection: close
Content-Length: 139
Content-Type: application/x-amz-json-1.1
Date: Thu, 05 Oct 2017 06:16:01 GMT
Server: Server
X-Amzn-Requestid: a92026f9-a994-11e7-bbbb-41668ba48bea


-----------------------------------------------------
2017/10/05 06:16:00 [DEBUG] [aws-sdk-go] {""__type"":""NotFoundException"",""message"":""Key 'arn:aws:kms:us-west-2:*REDACTED*:key/20505b0e-764f-4ef5-8f1b-73a2686fd11b' does not exist""}
```","arn:aws:kms:us-west-2:*REDACTED*:key,CreationDate:,Description:",https://github.com/hashicorp/terraform-provider-aws
36,1818.0,"This is to address the following test failure:
```
=== RUN   TestAccDataSourceAwsKmsCiphertext_basic
--- FAIL: TestAccDataSourceAwsKmsCiphertext_basic (31.03s)
    testing.go:434: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/20505b0e-764f-4ef5-8f1b-73a2686fd11b' does not exist
            status code: 400, request id: a92026f9-a994-11e7-bbbb-41668ba48bea
```

Snippet from debug log:

```
2017/10/05 06:16:00 [DEBUG] [aws-sdk-go] DEBUG: Response kms/DescribeKey Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 383
Content-Type: application/x-amz-json-1.1
Date: Thu, 05 Oct 2017 06:16:01 GMT
Server: Server
X-Amzn-Requestid: a91885e4-a994-11e7-9579-2da1021b9d1d


-----------------------------------------------------
2017/10/05 06:16:00 [DEBUG] [aws-sdk-go] {""KeyMetadata"":{""AWSAccountId"":""*REDACTED*"",""Arn"":""arn:aws:kms:us-west-2:*REDACTED*:key/20505b0e-764f-4ef5-8f1b-73a2686fd11b"",""CreationDate"":1.507184161473E9,""Description"":""tf-test-acc-data-source-aws-kms-ciphertext-basic"",""Enabled"":true,""KeyId"":""20505b0e-764f-4ef5-8f1b-73a2686fd11b"",""KeyManager"":""CUSTOMER"",""KeyState"":""Enabled"",""KeyUsage"":""ENCRYPT_DECRYPT"",""Origin"":""AWS_KMS""}}

2017/10/05 06:16:00 [DEBUG] [aws-sdk-go] DEBUG: Response kms/GetKeyRotationStatus Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 400 Bad Request
Connection: close
Content-Length: 139
Content-Type: application/x-amz-json-1.1
Date: Thu, 05 Oct 2017 06:16:01 GMT
Server: Server
X-Amzn-Requestid: a92026f9-a994-11e7-bbbb-41668ba48bea


-----------------------------------------------------
2017/10/05 06:16:00 [DEBUG] [aws-sdk-go] {""__type"":""NotFoundException"",""message"":""Key 'arn:aws:kms:us-west-2:*REDACTED*:key/20505b0e-764f-4ef5-8f1b-73a2686fd11b' does not exist""}
```",west-2:*******:key,https://github.com/hashicorp/terraform-provider-aws
37,1818.0,"This is to address the following test failure:
```
=== RUN   TestAccDataSourceAwsKmsCiphertext_basic
--- FAIL: TestAccDataSourceAwsKmsCiphertext_basic (31.03s)
    testing.go:434: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/20505b0e-764f-4ef5-8f1b-73a2686fd11b' does not exist
            status code: 400, request id: a92026f9-a994-11e7-bbbb-41668ba48bea
```

Snippet from debug log:

```
2017/10/05 06:16:00 [DEBUG] [aws-sdk-go] DEBUG: Response kms/DescribeKey Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 200 OK
Connection: close
Content-Length: 383
Content-Type: application/x-amz-json-1.1
Date: Thu, 05 Oct 2017 06:16:01 GMT
Server: Server
X-Amzn-Requestid: a91885e4-a994-11e7-9579-2da1021b9d1d


-----------------------------------------------------
2017/10/05 06:16:00 [DEBUG] [aws-sdk-go] {""KeyMetadata"":{""AWSAccountId"":""*REDACTED*"",""Arn"":""arn:aws:kms:us-west-2:*REDACTED*:key/20505b0e-764f-4ef5-8f1b-73a2686fd11b"",""CreationDate"":1.507184161473E9,""Description"":""tf-test-acc-data-source-aws-kms-ciphertext-basic"",""Enabled"":true,""KeyId"":""20505b0e-764f-4ef5-8f1b-73a2686fd11b"",""KeyManager"":""CUSTOMER"",""KeyState"":""Enabled"",""KeyUsage"":""ENCRYPT_DECRYPT"",""Origin"":""AWS_KMS""}}

2017/10/05 06:16:00 [DEBUG] [aws-sdk-go] DEBUG: Response kms/GetKeyRotationStatus Details:
---[ RESPONSE ]--------------------------------------
HTTP/1.1 400 Bad Request
Connection: close
Content-Length: 139
Content-Type: application/x-amz-json-1.1
Date: Thu, 05 Oct 2017 06:16:01 GMT
Server: Server
X-Amzn-Requestid: a92026f9-a994-11e7-bbbb-41668ba48bea


-----------------------------------------------------
2017/10/05 06:16:00 [DEBUG] [aws-sdk-go] {""__type"":""NotFoundException"",""message"":""Key 'arn:aws:kms:us-west-2:*REDACTED*:key/20505b0e-764f-4ef5-8f1b-73a2686fd11b' does not exist""}
```",2:*REDACTED*:key,https://github.com/hashicorp/terraform-provider-aws
38,1039.0,"This is to address the following test failures:

```
=== RUN   TestAccAWSKmsAlias_basic
--- FAIL: TestAccAWSKmsAlias_basic (24.96s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c857a00e-1276-4e08-8e08-d5b9d0b75e41' does not exist
            status code: 400, request id: 0eba259a-5a3b-11e7-b344-c7ea9798df62

=== RUN   TestAccAWSKmsAlias_importBasic
--- FAIL: TestAccAWSKmsAlias_importBasic (25.91s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.one: 1 error(s) occurred:
        
        * aws_kms_key.one: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/118a3530-5361-4649-8536-a93f2ede591a' does not exist
            status code: 400, request id: 9898277d-5c8a-11e7-abbd-b9503fe779e7

=== RUN   TestAccAWSKmsAlias_multiple
--- FAIL: TestAccAWSKmsAlias_multiple (22.69s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.single: 1 error(s) occurred:
        
        * aws_kms_key.single: Failed to get KMS key tags (key: e47b525d-38b2-4402-aee1-0be1292d9c4a): NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/e47b525d-38b2-4402-aee1-0be1292d9c4a' does not exist
            status code: 400, request id: 078d3a84-5329-11e7-a59b-71d56abfc9fc

=== RUN   TestAccAWSKmsAlias_name_prefix
--- FAIL: TestAccAWSKmsAlias_name_prefix (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/620c5864-ab1e-4f32-8615-a39552d0c6b8' does not exist
            status code: 400, request id: e963e5c2-50cf-11e7-9c90-87b19212a0de

=== RUN   TestAccAWSKmsAlias_no_name
--- FAIL: TestAccAWSKmsAlias_no_name (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/03d28cdc-f477-43d0-9605-d52de2e0ff76' does not exist
            status code: 400, request id: fb98019c-54bb-11e7-8cee-9f3a8216e097

=== RUN   TestAccAWSKmsKey_basic
--- FAIL: TestAccAWSKmsKey_basic (22.54s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/adeb6095-616a-4b50-958d-0325dac6ae4f' does not exist
            status code: 400, request id: 9fcce70a-45cc-11e7-87aa-cb3194a8108b

=== RUN   TestAccAWSKmsKey_disappears
--- FAIL: TestAccAWSKmsKey_disappears (3.09s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/2520d4af-7071-4b39-9bec-8c75e3b46032' does not exist
            status code: 400, request id: 5ec12c47-32ef-11e7-ac0c-1feea159ca97

=== RUN   TestAccAWSKmsKey_importBasic
--- FAIL: TestAccAWSKmsKey_importBasic (23.50s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/161bfac9-279f-470b-8c4d-c8eed3783c3a' does not exist
            status code: 400, request id: 6d7093c7-4a76-11e7-b6df-cb1b052b0f43

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (21.54s)
    testing.go:265: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""0750e54a-1bd6-4ae8-830b-a019ad0601ec"" to true: ""NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/0750e54a-1bd6-4ae8-830b-a019ad0601ec' does not exist\n\tstatus code: 400, request id: 8c1c8664-f5a4-11e6-bd72-9d8ac691c38b""

=== RUN   TestAccAWSKmsKey_policy
--- FAIL: TestAccAWSKmsKey_policy (22.99s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/b0bc93e2-b846-4d38-afa0-5c41a23fd3b3' does not exist
            status code: 400, request id: eeafefd9-5f41-11e7-932d-7b4776770aa1

=== RUN   TestAccAWSKmsKey_tags
--- FAIL: TestAccAWSKmsKey_tags (23.16s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c0468528-75cd-46a4-941d-38cbab46cda9' does not exist
            status code: 400, request id: 00e0034e-50d0-11e7-b0df-ed08666dd37a
```

this one

```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (100.82s)
    testing.go:428: Step 1 error: After applying this step and refreshing, the plan was not empty:
        
        DIFF:
        
        UPDATE: aws_kms_key.bar
          is_enabled: ""true"" => ""false""
        
        STATE:
        
        aws_kms_key.bar:
          ID = 331666bc-e480-4bbd-8e94-19457be08dcc
          arn = arn:aws:kms:us-west-2:*******:key/331666bc-e480-4bbd-8e94-19457be08dcc
          deletion_window_in_days = 7
          description = Terraform acc test is_enabled Sun, 25 Jun 2017 06:53:24 UTC
          enable_key_rotation = false
          is_enabled = true
          key_id = 331666bc-e480-4bbd-8e94-19457be08dcc
          key_usage = ENCRYPT_DECRYPT
          policy = {""Id"":""key-default-1"",""Statement"":[{""Action"":""kms:*"",""Effect"":""Allow"",""Principal"":{""AWS"":""arn:aws:iam::*******:root""},""Resource"":""*"",""Sid"":""Enable IAM User Permissions""}],""Version"":""2012-10-17""}
          tags.% = 0

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (76.05s)
    testing.go:265: Step 1 error: Check failed: Check 2/4 error: aws_kms_key.bar: Attribute 'is_enabled' expected ""false"", got ""true""
```
and this one
```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (229.51s)
    testing.go:280: Step 2 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""c98e0539-7d2a-4bdf-9380-0a5258a69b44"" to true: ""DisabledException: arn:aws:kms:us-west-2:*******:key/c98e0539-7d2a-4bdf-9380-0a5258a69b44 is disabled.\n\tstatus code: 400, request id: 6a240630-2b16-11e7-b764-abbc11cb3f70""
```

and maybe a some more caused by the same error code.",c98e0539-7d2a-4bdf-9380-0a5258a69b44,https://github.com/hashicorp/terraform-provider-aws
39,1039.0,"This is to address the following test failures:

```
=== RUN   TestAccAWSKmsAlias_basic
--- FAIL: TestAccAWSKmsAlias_basic (24.96s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c857a00e-1276-4e08-8e08-d5b9d0b75e41' does not exist
            status code: 400, request id: 0eba259a-5a3b-11e7-b344-c7ea9798df62

=== RUN   TestAccAWSKmsAlias_importBasic
--- FAIL: TestAccAWSKmsAlias_importBasic (25.91s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.one: 1 error(s) occurred:
        
        * aws_kms_key.one: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/118a3530-5361-4649-8536-a93f2ede591a' does not exist
            status code: 400, request id: 9898277d-5c8a-11e7-abbd-b9503fe779e7

=== RUN   TestAccAWSKmsAlias_multiple
--- FAIL: TestAccAWSKmsAlias_multiple (22.69s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.single: 1 error(s) occurred:
        
        * aws_kms_key.single: Failed to get KMS key tags (key: e47b525d-38b2-4402-aee1-0be1292d9c4a): NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/e47b525d-38b2-4402-aee1-0be1292d9c4a' does not exist
            status code: 400, request id: 078d3a84-5329-11e7-a59b-71d56abfc9fc

=== RUN   TestAccAWSKmsAlias_name_prefix
--- FAIL: TestAccAWSKmsAlias_name_prefix (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/620c5864-ab1e-4f32-8615-a39552d0c6b8' does not exist
            status code: 400, request id: e963e5c2-50cf-11e7-9c90-87b19212a0de

=== RUN   TestAccAWSKmsAlias_no_name
--- FAIL: TestAccAWSKmsAlias_no_name (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/03d28cdc-f477-43d0-9605-d52de2e0ff76' does not exist
            status code: 400, request id: fb98019c-54bb-11e7-8cee-9f3a8216e097

=== RUN   TestAccAWSKmsKey_basic
--- FAIL: TestAccAWSKmsKey_basic (22.54s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/adeb6095-616a-4b50-958d-0325dac6ae4f' does not exist
            status code: 400, request id: 9fcce70a-45cc-11e7-87aa-cb3194a8108b

=== RUN   TestAccAWSKmsKey_disappears
--- FAIL: TestAccAWSKmsKey_disappears (3.09s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/2520d4af-7071-4b39-9bec-8c75e3b46032' does not exist
            status code: 400, request id: 5ec12c47-32ef-11e7-ac0c-1feea159ca97

=== RUN   TestAccAWSKmsKey_importBasic
--- FAIL: TestAccAWSKmsKey_importBasic (23.50s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/161bfac9-279f-470b-8c4d-c8eed3783c3a' does not exist
            status code: 400, request id: 6d7093c7-4a76-11e7-b6df-cb1b052b0f43

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (21.54s)
    testing.go:265: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""0750e54a-1bd6-4ae8-830b-a019ad0601ec"" to true: ""NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/0750e54a-1bd6-4ae8-830b-a019ad0601ec' does not exist\n\tstatus code: 400, request id: 8c1c8664-f5a4-11e6-bd72-9d8ac691c38b""

=== RUN   TestAccAWSKmsKey_policy
--- FAIL: TestAccAWSKmsKey_policy (22.99s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/b0bc93e2-b846-4d38-afa0-5c41a23fd3b3' does not exist
            status code: 400, request id: eeafefd9-5f41-11e7-932d-7b4776770aa1

=== RUN   TestAccAWSKmsKey_tags
--- FAIL: TestAccAWSKmsKey_tags (23.16s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c0468528-75cd-46a4-941d-38cbab46cda9' does not exist
            status code: 400, request id: 00e0034e-50d0-11e7-b0df-ed08666dd37a
```

this one

```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (100.82s)
    testing.go:428: Step 1 error: After applying this step and refreshing, the plan was not empty:
        
        DIFF:
        
        UPDATE: aws_kms_key.bar
          is_enabled: ""true"" => ""false""
        
        STATE:
        
        aws_kms_key.bar:
          ID = 331666bc-e480-4bbd-8e94-19457be08dcc
          arn = arn:aws:kms:us-west-2:*******:key/331666bc-e480-4bbd-8e94-19457be08dcc
          deletion_window_in_days = 7
          description = Terraform acc test is_enabled Sun, 25 Jun 2017 06:53:24 UTC
          enable_key_rotation = false
          is_enabled = true
          key_id = 331666bc-e480-4bbd-8e94-19457be08dcc
          key_usage = ENCRYPT_DECRYPT
          policy = {""Id"":""key-default-1"",""Statement"":[{""Action"":""kms:*"",""Effect"":""Allow"",""Principal"":{""AWS"":""arn:aws:iam::*******:root""},""Resource"":""*"",""Sid"":""Enable IAM User Permissions""}],""Version"":""2012-10-17""}
          tags.% = 0

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (76.05s)
    testing.go:265: Step 1 error: Check failed: Check 2/4 error: aws_kms_key.bar: Attribute 'is_enabled' expected ""false"", got ""true""
```
and this one
```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (229.51s)
    testing.go:280: Step 2 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""c98e0539-7d2a-4bdf-9380-0a5258a69b44"" to true: ""DisabledException: arn:aws:kms:us-west-2:*******:key/c98e0539-7d2a-4bdf-9380-0a5258a69b44 is disabled.\n\tstatus code: 400, request id: 6a240630-2b16-11e7-b764-abbc11cb3f70""
```

and maybe a some more caused by the same error code.",0750e54a-1bd6-4ae8-830b-a019ad0601ec,https://github.com/hashicorp/terraform-provider-aws
40,1039.0,"This is to address the following test failures:

```
=== RUN   TestAccAWSKmsAlias_basic
--- FAIL: TestAccAWSKmsAlias_basic (24.96s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c857a00e-1276-4e08-8e08-d5b9d0b75e41' does not exist
            status code: 400, request id: 0eba259a-5a3b-11e7-b344-c7ea9798df62

=== RUN   TestAccAWSKmsAlias_importBasic
--- FAIL: TestAccAWSKmsAlias_importBasic (25.91s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.one: 1 error(s) occurred:
        
        * aws_kms_key.one: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/118a3530-5361-4649-8536-a93f2ede591a' does not exist
            status code: 400, request id: 9898277d-5c8a-11e7-abbd-b9503fe779e7

=== RUN   TestAccAWSKmsAlias_multiple
--- FAIL: TestAccAWSKmsAlias_multiple (22.69s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.single: 1 error(s) occurred:
        
        * aws_kms_key.single: Failed to get KMS key tags (key: e47b525d-38b2-4402-aee1-0be1292d9c4a): NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/e47b525d-38b2-4402-aee1-0be1292d9c4a' does not exist
            status code: 400, request id: 078d3a84-5329-11e7-a59b-71d56abfc9fc

=== RUN   TestAccAWSKmsAlias_name_prefix
--- FAIL: TestAccAWSKmsAlias_name_prefix (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/620c5864-ab1e-4f32-8615-a39552d0c6b8' does not exist
            status code: 400, request id: e963e5c2-50cf-11e7-9c90-87b19212a0de

=== RUN   TestAccAWSKmsAlias_no_name
--- FAIL: TestAccAWSKmsAlias_no_name (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/03d28cdc-f477-43d0-9605-d52de2e0ff76' does not exist
            status code: 400, request id: fb98019c-54bb-11e7-8cee-9f3a8216e097

=== RUN   TestAccAWSKmsKey_basic
--- FAIL: TestAccAWSKmsKey_basic (22.54s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/adeb6095-616a-4b50-958d-0325dac6ae4f' does not exist
            status code: 400, request id: 9fcce70a-45cc-11e7-87aa-cb3194a8108b

=== RUN   TestAccAWSKmsKey_disappears
--- FAIL: TestAccAWSKmsKey_disappears (3.09s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/2520d4af-7071-4b39-9bec-8c75e3b46032' does not exist
            status code: 400, request id: 5ec12c47-32ef-11e7-ac0c-1feea159ca97

=== RUN   TestAccAWSKmsKey_importBasic
--- FAIL: TestAccAWSKmsKey_importBasic (23.50s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/161bfac9-279f-470b-8c4d-c8eed3783c3a' does not exist
            status code: 400, request id: 6d7093c7-4a76-11e7-b6df-cb1b052b0f43

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (21.54s)
    testing.go:265: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""0750e54a-1bd6-4ae8-830b-a019ad0601ec"" to true: ""NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/0750e54a-1bd6-4ae8-830b-a019ad0601ec' does not exist\n\tstatus code: 400, request id: 8c1c8664-f5a4-11e6-bd72-9d8ac691c38b""

=== RUN   TestAccAWSKmsKey_policy
--- FAIL: TestAccAWSKmsKey_policy (22.99s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/b0bc93e2-b846-4d38-afa0-5c41a23fd3b3' does not exist
            status code: 400, request id: eeafefd9-5f41-11e7-932d-7b4776770aa1

=== RUN   TestAccAWSKmsKey_tags
--- FAIL: TestAccAWSKmsKey_tags (23.16s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c0468528-75cd-46a4-941d-38cbab46cda9' does not exist
            status code: 400, request id: 00e0034e-50d0-11e7-b0df-ed08666dd37a
```

this one

```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (100.82s)
    testing.go:428: Step 1 error: After applying this step and refreshing, the plan was not empty:
        
        DIFF:
        
        UPDATE: aws_kms_key.bar
          is_enabled: ""true"" => ""false""
        
        STATE:
        
        aws_kms_key.bar:
          ID = 331666bc-e480-4bbd-8e94-19457be08dcc
          arn = arn:aws:kms:us-west-2:*******:key/331666bc-e480-4bbd-8e94-19457be08dcc
          deletion_window_in_days = 7
          description = Terraform acc test is_enabled Sun, 25 Jun 2017 06:53:24 UTC
          enable_key_rotation = false
          is_enabled = true
          key_id = 331666bc-e480-4bbd-8e94-19457be08dcc
          key_usage = ENCRYPT_DECRYPT
          policy = {""Id"":""key-default-1"",""Statement"":[{""Action"":""kms:*"",""Effect"":""Allow"",""Principal"":{""AWS"":""arn:aws:iam::*******:root""},""Resource"":""*"",""Sid"":""Enable IAM User Permissions""}],""Version"":""2012-10-17""}
          tags.% = 0

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (76.05s)
    testing.go:265: Step 1 error: Check failed: Check 2/4 error: aws_kms_key.bar: Attribute 'is_enabled' expected ""false"", got ""true""
```
and this one
```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (229.51s)
    testing.go:280: Step 2 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""c98e0539-7d2a-4bdf-9380-0a5258a69b44"" to true: ""DisabledException: arn:aws:kms:us-west-2:*******:key/c98e0539-7d2a-4bdf-9380-0a5258a69b44 is disabled.\n\tstatus code: 400, request id: 6a240630-2b16-11e7-b764-abbc11cb3f70""
```

and maybe a some more caused by the same error code.",e47b525d-38b2-4402-aee1-0be1292d9c4a,https://github.com/hashicorp/terraform-provider-aws
41,1039.0,"This is to address the following test failures:

```
=== RUN   TestAccAWSKmsAlias_basic
--- FAIL: TestAccAWSKmsAlias_basic (24.96s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c857a00e-1276-4e08-8e08-d5b9d0b75e41' does not exist
            status code: 400, request id: 0eba259a-5a3b-11e7-b344-c7ea9798df62

=== RUN   TestAccAWSKmsAlias_importBasic
--- FAIL: TestAccAWSKmsAlias_importBasic (25.91s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.one: 1 error(s) occurred:
        
        * aws_kms_key.one: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/118a3530-5361-4649-8536-a93f2ede591a' does not exist
            status code: 400, request id: 9898277d-5c8a-11e7-abbd-b9503fe779e7

=== RUN   TestAccAWSKmsAlias_multiple
--- FAIL: TestAccAWSKmsAlias_multiple (22.69s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.single: 1 error(s) occurred:
        
        * aws_kms_key.single: Failed to get KMS key tags (key: e47b525d-38b2-4402-aee1-0be1292d9c4a): NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/e47b525d-38b2-4402-aee1-0be1292d9c4a' does not exist
            status code: 400, request id: 078d3a84-5329-11e7-a59b-71d56abfc9fc

=== RUN   TestAccAWSKmsAlias_name_prefix
--- FAIL: TestAccAWSKmsAlias_name_prefix (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/620c5864-ab1e-4f32-8615-a39552d0c6b8' does not exist
            status code: 400, request id: e963e5c2-50cf-11e7-9c90-87b19212a0de

=== RUN   TestAccAWSKmsAlias_no_name
--- FAIL: TestAccAWSKmsAlias_no_name (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/03d28cdc-f477-43d0-9605-d52de2e0ff76' does not exist
            status code: 400, request id: fb98019c-54bb-11e7-8cee-9f3a8216e097

=== RUN   TestAccAWSKmsKey_basic
--- FAIL: TestAccAWSKmsKey_basic (22.54s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/adeb6095-616a-4b50-958d-0325dac6ae4f' does not exist
            status code: 400, request id: 9fcce70a-45cc-11e7-87aa-cb3194a8108b

=== RUN   TestAccAWSKmsKey_disappears
--- FAIL: TestAccAWSKmsKey_disappears (3.09s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/2520d4af-7071-4b39-9bec-8c75e3b46032' does not exist
            status code: 400, request id: 5ec12c47-32ef-11e7-ac0c-1feea159ca97

=== RUN   TestAccAWSKmsKey_importBasic
--- FAIL: TestAccAWSKmsKey_importBasic (23.50s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/161bfac9-279f-470b-8c4d-c8eed3783c3a' does not exist
            status code: 400, request id: 6d7093c7-4a76-11e7-b6df-cb1b052b0f43

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (21.54s)
    testing.go:265: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""0750e54a-1bd6-4ae8-830b-a019ad0601ec"" to true: ""NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/0750e54a-1bd6-4ae8-830b-a019ad0601ec' does not exist\n\tstatus code: 400, request id: 8c1c8664-f5a4-11e6-bd72-9d8ac691c38b""

=== RUN   TestAccAWSKmsKey_policy
--- FAIL: TestAccAWSKmsKey_policy (22.99s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/b0bc93e2-b846-4d38-afa0-5c41a23fd3b3' does not exist
            status code: 400, request id: eeafefd9-5f41-11e7-932d-7b4776770aa1

=== RUN   TestAccAWSKmsKey_tags
--- FAIL: TestAccAWSKmsKey_tags (23.16s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c0468528-75cd-46a4-941d-38cbab46cda9' does not exist
            status code: 400, request id: 00e0034e-50d0-11e7-b0df-ed08666dd37a
```

this one

```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (100.82s)
    testing.go:428: Step 1 error: After applying this step and refreshing, the plan was not empty:
        
        DIFF:
        
        UPDATE: aws_kms_key.bar
          is_enabled: ""true"" => ""false""
        
        STATE:
        
        aws_kms_key.bar:
          ID = 331666bc-e480-4bbd-8e94-19457be08dcc
          arn = arn:aws:kms:us-west-2:*******:key/331666bc-e480-4bbd-8e94-19457be08dcc
          deletion_window_in_days = 7
          description = Terraform acc test is_enabled Sun, 25 Jun 2017 06:53:24 UTC
          enable_key_rotation = false
          is_enabled = true
          key_id = 331666bc-e480-4bbd-8e94-19457be08dcc
          key_usage = ENCRYPT_DECRYPT
          policy = {""Id"":""key-default-1"",""Statement"":[{""Action"":""kms:*"",""Effect"":""Allow"",""Principal"":{""AWS"":""arn:aws:iam::*******:root""},""Resource"":""*"",""Sid"":""Enable IAM User Permissions""}],""Version"":""2012-10-17""}
          tags.% = 0

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (76.05s)
    testing.go:265: Step 1 error: Check failed: Check 2/4 error: aws_kms_key.bar: Attribute 'is_enabled' expected ""false"", got ""true""
```
and this one
```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (229.51s)
    testing.go:280: Step 2 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""c98e0539-7d2a-4bdf-9380-0a5258a69b44"" to true: ""DisabledException: arn:aws:kms:us-west-2:*******:key/c98e0539-7d2a-4bdf-9380-0a5258a69b44 is disabled.\n\tstatus code: 400, request id: 6a240630-2b16-11e7-b764-abbc11cb3f70""
```

and maybe a some more caused by the same error code.",aee1-0be1292d9c4a,https://github.com/hashicorp/terraform-provider-aws
42,1039.0,"This is to address the following test failures:

```
=== RUN   TestAccAWSKmsAlias_basic
--- FAIL: TestAccAWSKmsAlias_basic (24.96s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c857a00e-1276-4e08-8e08-d5b9d0b75e41' does not exist
            status code: 400, request id: 0eba259a-5a3b-11e7-b344-c7ea9798df62

=== RUN   TestAccAWSKmsAlias_importBasic
--- FAIL: TestAccAWSKmsAlias_importBasic (25.91s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.one: 1 error(s) occurred:
        
        * aws_kms_key.one: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/118a3530-5361-4649-8536-a93f2ede591a' does not exist
            status code: 400, request id: 9898277d-5c8a-11e7-abbd-b9503fe779e7

=== RUN   TestAccAWSKmsAlias_multiple
--- FAIL: TestAccAWSKmsAlias_multiple (22.69s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.single: 1 error(s) occurred:
        
        * aws_kms_key.single: Failed to get KMS key tags (key: e47b525d-38b2-4402-aee1-0be1292d9c4a): NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/e47b525d-38b2-4402-aee1-0be1292d9c4a' does not exist
            status code: 400, request id: 078d3a84-5329-11e7-a59b-71d56abfc9fc

=== RUN   TestAccAWSKmsAlias_name_prefix
--- FAIL: TestAccAWSKmsAlias_name_prefix (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/620c5864-ab1e-4f32-8615-a39552d0c6b8' does not exist
            status code: 400, request id: e963e5c2-50cf-11e7-9c90-87b19212a0de

=== RUN   TestAccAWSKmsAlias_no_name
--- FAIL: TestAccAWSKmsAlias_no_name (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/03d28cdc-f477-43d0-9605-d52de2e0ff76' does not exist
            status code: 400, request id: fb98019c-54bb-11e7-8cee-9f3a8216e097

=== RUN   TestAccAWSKmsKey_basic
--- FAIL: TestAccAWSKmsKey_basic (22.54s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/adeb6095-616a-4b50-958d-0325dac6ae4f' does not exist
            status code: 400, request id: 9fcce70a-45cc-11e7-87aa-cb3194a8108b

=== RUN   TestAccAWSKmsKey_disappears
--- FAIL: TestAccAWSKmsKey_disappears (3.09s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/2520d4af-7071-4b39-9bec-8c75e3b46032' does not exist
            status code: 400, request id: 5ec12c47-32ef-11e7-ac0c-1feea159ca97

=== RUN   TestAccAWSKmsKey_importBasic
--- FAIL: TestAccAWSKmsKey_importBasic (23.50s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/161bfac9-279f-470b-8c4d-c8eed3783c3a' does not exist
            status code: 400, request id: 6d7093c7-4a76-11e7-b6df-cb1b052b0f43

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (21.54s)
    testing.go:265: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""0750e54a-1bd6-4ae8-830b-a019ad0601ec"" to true: ""NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/0750e54a-1bd6-4ae8-830b-a019ad0601ec' does not exist\n\tstatus code: 400, request id: 8c1c8664-f5a4-11e6-bd72-9d8ac691c38b""

=== RUN   TestAccAWSKmsKey_policy
--- FAIL: TestAccAWSKmsKey_policy (22.99s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/b0bc93e2-b846-4d38-afa0-5c41a23fd3b3' does not exist
            status code: 400, request id: eeafefd9-5f41-11e7-932d-7b4776770aa1

=== RUN   TestAccAWSKmsKey_tags
--- FAIL: TestAccAWSKmsKey_tags (23.16s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c0468528-75cd-46a4-941d-38cbab46cda9' does not exist
            status code: 400, request id: 00e0034e-50d0-11e7-b0df-ed08666dd37a
```

this one

```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (100.82s)
    testing.go:428: Step 1 error: After applying this step and refreshing, the plan was not empty:
        
        DIFF:
        
        UPDATE: aws_kms_key.bar
          is_enabled: ""true"" => ""false""
        
        STATE:
        
        aws_kms_key.bar:
          ID = 331666bc-e480-4bbd-8e94-19457be08dcc
          arn = arn:aws:kms:us-west-2:*******:key/331666bc-e480-4bbd-8e94-19457be08dcc
          deletion_window_in_days = 7
          description = Terraform acc test is_enabled Sun, 25 Jun 2017 06:53:24 UTC
          enable_key_rotation = false
          is_enabled = true
          key_id = 331666bc-e480-4bbd-8e94-19457be08dcc
          key_usage = ENCRYPT_DECRYPT
          policy = {""Id"":""key-default-1"",""Statement"":[{""Action"":""kms:*"",""Effect"":""Allow"",""Principal"":{""AWS"":""arn:aws:iam::*******:root""},""Resource"":""*"",""Sid"":""Enable IAM User Permissions""}],""Version"":""2012-10-17""}
          tags.% = 0

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (76.05s)
    testing.go:265: Step 1 error: Check failed: Check 2/4 error: aws_kms_key.bar: Attribute 'is_enabled' expected ""false"", got ""true""
```
and this one
```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (229.51s)
    testing.go:280: Step 2 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""c98e0539-7d2a-4bdf-9380-0a5258a69b44"" to true: ""DisabledException: arn:aws:kms:us-west-2:*******:key/c98e0539-7d2a-4bdf-9380-0a5258a69b44 is disabled.\n\tstatus code: 400, request id: 6a240630-2b16-11e7-b764-abbc11cb3f70""
```

and maybe a some more caused by the same error code.",deletion_window_in_days,https://github.com/hashicorp/terraform-provider-aws
43,1039.0,"This is to address the following test failures:

```
=== RUN   TestAccAWSKmsAlias_basic
--- FAIL: TestAccAWSKmsAlias_basic (24.96s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c857a00e-1276-4e08-8e08-d5b9d0b75e41' does not exist
            status code: 400, request id: 0eba259a-5a3b-11e7-b344-c7ea9798df62

=== RUN   TestAccAWSKmsAlias_importBasic
--- FAIL: TestAccAWSKmsAlias_importBasic (25.91s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.one: 1 error(s) occurred:
        
        * aws_kms_key.one: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/118a3530-5361-4649-8536-a93f2ede591a' does not exist
            status code: 400, request id: 9898277d-5c8a-11e7-abbd-b9503fe779e7

=== RUN   TestAccAWSKmsAlias_multiple
--- FAIL: TestAccAWSKmsAlias_multiple (22.69s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.single: 1 error(s) occurred:
        
        * aws_kms_key.single: Failed to get KMS key tags (key: e47b525d-38b2-4402-aee1-0be1292d9c4a): NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/e47b525d-38b2-4402-aee1-0be1292d9c4a' does not exist
            status code: 400, request id: 078d3a84-5329-11e7-a59b-71d56abfc9fc

=== RUN   TestAccAWSKmsAlias_name_prefix
--- FAIL: TestAccAWSKmsAlias_name_prefix (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/620c5864-ab1e-4f32-8615-a39552d0c6b8' does not exist
            status code: 400, request id: e963e5c2-50cf-11e7-9c90-87b19212a0de

=== RUN   TestAccAWSKmsAlias_no_name
--- FAIL: TestAccAWSKmsAlias_no_name (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/03d28cdc-f477-43d0-9605-d52de2e0ff76' does not exist
            status code: 400, request id: fb98019c-54bb-11e7-8cee-9f3a8216e097

=== RUN   TestAccAWSKmsKey_basic
--- FAIL: TestAccAWSKmsKey_basic (22.54s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/adeb6095-616a-4b50-958d-0325dac6ae4f' does not exist
            status code: 400, request id: 9fcce70a-45cc-11e7-87aa-cb3194a8108b

=== RUN   TestAccAWSKmsKey_disappears
--- FAIL: TestAccAWSKmsKey_disappears (3.09s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/2520d4af-7071-4b39-9bec-8c75e3b46032' does not exist
            status code: 400, request id: 5ec12c47-32ef-11e7-ac0c-1feea159ca97

=== RUN   TestAccAWSKmsKey_importBasic
--- FAIL: TestAccAWSKmsKey_importBasic (23.50s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/161bfac9-279f-470b-8c4d-c8eed3783c3a' does not exist
            status code: 400, request id: 6d7093c7-4a76-11e7-b6df-cb1b052b0f43

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (21.54s)
    testing.go:265: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""0750e54a-1bd6-4ae8-830b-a019ad0601ec"" to true: ""NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/0750e54a-1bd6-4ae8-830b-a019ad0601ec' does not exist\n\tstatus code: 400, request id: 8c1c8664-f5a4-11e6-bd72-9d8ac691c38b""

=== RUN   TestAccAWSKmsKey_policy
--- FAIL: TestAccAWSKmsKey_policy (22.99s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/b0bc93e2-b846-4d38-afa0-5c41a23fd3b3' does not exist
            status code: 400, request id: eeafefd9-5f41-11e7-932d-7b4776770aa1

=== RUN   TestAccAWSKmsKey_tags
--- FAIL: TestAccAWSKmsKey_tags (23.16s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c0468528-75cd-46a4-941d-38cbab46cda9' does not exist
            status code: 400, request id: 00e0034e-50d0-11e7-b0df-ed08666dd37a
```

this one

```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (100.82s)
    testing.go:428: Step 1 error: After applying this step and refreshing, the plan was not empty:
        
        DIFF:
        
        UPDATE: aws_kms_key.bar
          is_enabled: ""true"" => ""false""
        
        STATE:
        
        aws_kms_key.bar:
          ID = 331666bc-e480-4bbd-8e94-19457be08dcc
          arn = arn:aws:kms:us-west-2:*******:key/331666bc-e480-4bbd-8e94-19457be08dcc
          deletion_window_in_days = 7
          description = Terraform acc test is_enabled Sun, 25 Jun 2017 06:53:24 UTC
          enable_key_rotation = false
          is_enabled = true
          key_id = 331666bc-e480-4bbd-8e94-19457be08dcc
          key_usage = ENCRYPT_DECRYPT
          policy = {""Id"":""key-default-1"",""Statement"":[{""Action"":""kms:*"",""Effect"":""Allow"",""Principal"":{""AWS"":""arn:aws:iam::*******:root""},""Resource"":""*"",""Sid"":""Enable IAM User Permissions""}],""Version"":""2012-10-17""}
          tags.% = 0

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (76.05s)
    testing.go:265: Step 1 error: Check failed: Check 2/4 error: aws_kms_key.bar: Attribute 'is_enabled' expected ""false"", got ""true""
```
and this one
```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (229.51s)
    testing.go:280: Step 2 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""c98e0539-7d2a-4bdf-9380-0a5258a69b44"" to true: ""DisabledException: arn:aws:kms:us-west-2:*******:key/c98e0539-7d2a-4bdf-9380-0a5258a69b44 is disabled.\n\tstatus code: 400, request id: 6a240630-2b16-11e7-b764-abbc11cb3f70""
```

and maybe a some more caused by the same error code.",TestAccAWSKmsKey_policy,https://github.com/hashicorp/terraform-provider-aws
44,1039.0,"This is to address the following test failures:

```
=== RUN   TestAccAWSKmsAlias_basic
--- FAIL: TestAccAWSKmsAlias_basic (24.96s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c857a00e-1276-4e08-8e08-d5b9d0b75e41' does not exist
            status code: 400, request id: 0eba259a-5a3b-11e7-b344-c7ea9798df62

=== RUN   TestAccAWSKmsAlias_importBasic
--- FAIL: TestAccAWSKmsAlias_importBasic (25.91s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.one: 1 error(s) occurred:
        
        * aws_kms_key.one: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/118a3530-5361-4649-8536-a93f2ede591a' does not exist
            status code: 400, request id: 9898277d-5c8a-11e7-abbd-b9503fe779e7

=== RUN   TestAccAWSKmsAlias_multiple
--- FAIL: TestAccAWSKmsAlias_multiple (22.69s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.single: 1 error(s) occurred:
        
        * aws_kms_key.single: Failed to get KMS key tags (key: e47b525d-38b2-4402-aee1-0be1292d9c4a): NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/e47b525d-38b2-4402-aee1-0be1292d9c4a' does not exist
            status code: 400, request id: 078d3a84-5329-11e7-a59b-71d56abfc9fc

=== RUN   TestAccAWSKmsAlias_name_prefix
--- FAIL: TestAccAWSKmsAlias_name_prefix (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/620c5864-ab1e-4f32-8615-a39552d0c6b8' does not exist
            status code: 400, request id: e963e5c2-50cf-11e7-9c90-87b19212a0de

=== RUN   TestAccAWSKmsAlias_no_name
--- FAIL: TestAccAWSKmsAlias_no_name (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/03d28cdc-f477-43d0-9605-d52de2e0ff76' does not exist
            status code: 400, request id: fb98019c-54bb-11e7-8cee-9f3a8216e097

=== RUN   TestAccAWSKmsKey_basic
--- FAIL: TestAccAWSKmsKey_basic (22.54s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/adeb6095-616a-4b50-958d-0325dac6ae4f' does not exist
            status code: 400, request id: 9fcce70a-45cc-11e7-87aa-cb3194a8108b

=== RUN   TestAccAWSKmsKey_disappears
--- FAIL: TestAccAWSKmsKey_disappears (3.09s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/2520d4af-7071-4b39-9bec-8c75e3b46032' does not exist
            status code: 400, request id: 5ec12c47-32ef-11e7-ac0c-1feea159ca97

=== RUN   TestAccAWSKmsKey_importBasic
--- FAIL: TestAccAWSKmsKey_importBasic (23.50s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/161bfac9-279f-470b-8c4d-c8eed3783c3a' does not exist
            status code: 400, request id: 6d7093c7-4a76-11e7-b6df-cb1b052b0f43

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (21.54s)
    testing.go:265: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""0750e54a-1bd6-4ae8-830b-a019ad0601ec"" to true: ""NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/0750e54a-1bd6-4ae8-830b-a019ad0601ec' does not exist\n\tstatus code: 400, request id: 8c1c8664-f5a4-11e6-bd72-9d8ac691c38b""

=== RUN   TestAccAWSKmsKey_policy
--- FAIL: TestAccAWSKmsKey_policy (22.99s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/b0bc93e2-b846-4d38-afa0-5c41a23fd3b3' does not exist
            status code: 400, request id: eeafefd9-5f41-11e7-932d-7b4776770aa1

=== RUN   TestAccAWSKmsKey_tags
--- FAIL: TestAccAWSKmsKey_tags (23.16s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c0468528-75cd-46a4-941d-38cbab46cda9' does not exist
            status code: 400, request id: 00e0034e-50d0-11e7-b0df-ed08666dd37a
```

this one

```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (100.82s)
    testing.go:428: Step 1 error: After applying this step and refreshing, the plan was not empty:
        
        DIFF:
        
        UPDATE: aws_kms_key.bar
          is_enabled: ""true"" => ""false""
        
        STATE:
        
        aws_kms_key.bar:
          ID = 331666bc-e480-4bbd-8e94-19457be08dcc
          arn = arn:aws:kms:us-west-2:*******:key/331666bc-e480-4bbd-8e94-19457be08dcc
          deletion_window_in_days = 7
          description = Terraform acc test is_enabled Sun, 25 Jun 2017 06:53:24 UTC
          enable_key_rotation = false
          is_enabled = true
          key_id = 331666bc-e480-4bbd-8e94-19457be08dcc
          key_usage = ENCRYPT_DECRYPT
          policy = {""Id"":""key-default-1"",""Statement"":[{""Action"":""kms:*"",""Effect"":""Allow"",""Principal"":{""AWS"":""arn:aws:iam::*******:root""},""Resource"":""*"",""Sid"":""Enable IAM User Permissions""}],""Version"":""2012-10-17""}
          tags.% = 0

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (76.05s)
    testing.go:265: Step 1 error: Check failed: Check 2/4 error: aws_kms_key.bar: Attribute 'is_enabled' expected ""false"", got ""true""
```
and this one
```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (229.51s)
    testing.go:280: Step 2 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""c98e0539-7d2a-4bdf-9380-0a5258a69b44"" to true: ""DisabledException: arn:aws:kms:us-west-2:*******:key/c98e0539-7d2a-4bdf-9380-0a5258a69b44 is disabled.\n\tstatus code: 400, request id: 6a240630-2b16-11e7-b764-abbc11cb3f70""
```

and maybe a some more caused by the same error code.",TestAccAWSKmsKey_disappears,https://github.com/hashicorp/terraform-provider-aws
45,1039.0,"This is to address the following test failures:

```
=== RUN   TestAccAWSKmsAlias_basic
--- FAIL: TestAccAWSKmsAlias_basic (24.96s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c857a00e-1276-4e08-8e08-d5b9d0b75e41' does not exist
            status code: 400, request id: 0eba259a-5a3b-11e7-b344-c7ea9798df62

=== RUN   TestAccAWSKmsAlias_importBasic
--- FAIL: TestAccAWSKmsAlias_importBasic (25.91s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.one: 1 error(s) occurred:
        
        * aws_kms_key.one: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/118a3530-5361-4649-8536-a93f2ede591a' does not exist
            status code: 400, request id: 9898277d-5c8a-11e7-abbd-b9503fe779e7

=== RUN   TestAccAWSKmsAlias_multiple
--- FAIL: TestAccAWSKmsAlias_multiple (22.69s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.single: 1 error(s) occurred:
        
        * aws_kms_key.single: Failed to get KMS key tags (key: e47b525d-38b2-4402-aee1-0be1292d9c4a): NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/e47b525d-38b2-4402-aee1-0be1292d9c4a' does not exist
            status code: 400, request id: 078d3a84-5329-11e7-a59b-71d56abfc9fc

=== RUN   TestAccAWSKmsAlias_name_prefix
--- FAIL: TestAccAWSKmsAlias_name_prefix (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/620c5864-ab1e-4f32-8615-a39552d0c6b8' does not exist
            status code: 400, request id: e963e5c2-50cf-11e7-9c90-87b19212a0de

=== RUN   TestAccAWSKmsAlias_no_name
--- FAIL: TestAccAWSKmsAlias_no_name (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/03d28cdc-f477-43d0-9605-d52de2e0ff76' does not exist
            status code: 400, request id: fb98019c-54bb-11e7-8cee-9f3a8216e097

=== RUN   TestAccAWSKmsKey_basic
--- FAIL: TestAccAWSKmsKey_basic (22.54s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/adeb6095-616a-4b50-958d-0325dac6ae4f' does not exist
            status code: 400, request id: 9fcce70a-45cc-11e7-87aa-cb3194a8108b

=== RUN   TestAccAWSKmsKey_disappears
--- FAIL: TestAccAWSKmsKey_disappears (3.09s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/2520d4af-7071-4b39-9bec-8c75e3b46032' does not exist
            status code: 400, request id: 5ec12c47-32ef-11e7-ac0c-1feea159ca97

=== RUN   TestAccAWSKmsKey_importBasic
--- FAIL: TestAccAWSKmsKey_importBasic (23.50s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/161bfac9-279f-470b-8c4d-c8eed3783c3a' does not exist
            status code: 400, request id: 6d7093c7-4a76-11e7-b6df-cb1b052b0f43

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (21.54s)
    testing.go:265: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""0750e54a-1bd6-4ae8-830b-a019ad0601ec"" to true: ""NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/0750e54a-1bd6-4ae8-830b-a019ad0601ec' does not exist\n\tstatus code: 400, request id: 8c1c8664-f5a4-11e6-bd72-9d8ac691c38b""

=== RUN   TestAccAWSKmsKey_policy
--- FAIL: TestAccAWSKmsKey_policy (22.99s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/b0bc93e2-b846-4d38-afa0-5c41a23fd3b3' does not exist
            status code: 400, request id: eeafefd9-5f41-11e7-932d-7b4776770aa1

=== RUN   TestAccAWSKmsKey_tags
--- FAIL: TestAccAWSKmsKey_tags (23.16s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c0468528-75cd-46a4-941d-38cbab46cda9' does not exist
            status code: 400, request id: 00e0034e-50d0-11e7-b0df-ed08666dd37a
```

this one

```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (100.82s)
    testing.go:428: Step 1 error: After applying this step and refreshing, the plan was not empty:
        
        DIFF:
        
        UPDATE: aws_kms_key.bar
          is_enabled: ""true"" => ""false""
        
        STATE:
        
        aws_kms_key.bar:
          ID = 331666bc-e480-4bbd-8e94-19457be08dcc
          arn = arn:aws:kms:us-west-2:*******:key/331666bc-e480-4bbd-8e94-19457be08dcc
          deletion_window_in_days = 7
          description = Terraform acc test is_enabled Sun, 25 Jun 2017 06:53:24 UTC
          enable_key_rotation = false
          is_enabled = true
          key_id = 331666bc-e480-4bbd-8e94-19457be08dcc
          key_usage = ENCRYPT_DECRYPT
          policy = {""Id"":""key-default-1"",""Statement"":[{""Action"":""kms:*"",""Effect"":""Allow"",""Principal"":{""AWS"":""arn:aws:iam::*******:root""},""Resource"":""*"",""Sid"":""Enable IAM User Permissions""}],""Version"":""2012-10-17""}
          tags.% = 0

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (76.05s)
    testing.go:265: Step 1 error: Check failed: Check 2/4 error: aws_kms_key.bar: Attribute 'is_enabled' expected ""false"", got ""true""
```
and this one
```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (229.51s)
    testing.go:280: Step 2 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""c98e0539-7d2a-4bdf-9380-0a5258a69b44"" to true: ""DisabledException: arn:aws:kms:us-west-2:*******:key/c98e0539-7d2a-4bdf-9380-0a5258a69b44 is disabled.\n\tstatus code: 400, request id: 6a240630-2b16-11e7-b764-abbc11cb3f70""
```

and maybe a some more caused by the same error code.",TestAccAWSKmsKey_importBasic,https://github.com/hashicorp/terraform-provider-aws
46,1039.0,"This is to address the following test failures:

```
=== RUN   TestAccAWSKmsAlias_basic
--- FAIL: TestAccAWSKmsAlias_basic (24.96s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c857a00e-1276-4e08-8e08-d5b9d0b75e41' does not exist
            status code: 400, request id: 0eba259a-5a3b-11e7-b344-c7ea9798df62

=== RUN   TestAccAWSKmsAlias_importBasic
--- FAIL: TestAccAWSKmsAlias_importBasic (25.91s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.one: 1 error(s) occurred:
        
        * aws_kms_key.one: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/118a3530-5361-4649-8536-a93f2ede591a' does not exist
            status code: 400, request id: 9898277d-5c8a-11e7-abbd-b9503fe779e7

=== RUN   TestAccAWSKmsAlias_multiple
--- FAIL: TestAccAWSKmsAlias_multiple (22.69s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.single: 1 error(s) occurred:
        
        * aws_kms_key.single: Failed to get KMS key tags (key: e47b525d-38b2-4402-aee1-0be1292d9c4a): NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/e47b525d-38b2-4402-aee1-0be1292d9c4a' does not exist
            status code: 400, request id: 078d3a84-5329-11e7-a59b-71d56abfc9fc

=== RUN   TestAccAWSKmsAlias_name_prefix
--- FAIL: TestAccAWSKmsAlias_name_prefix (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/620c5864-ab1e-4f32-8615-a39552d0c6b8' does not exist
            status code: 400, request id: e963e5c2-50cf-11e7-9c90-87b19212a0de

=== RUN   TestAccAWSKmsAlias_no_name
--- FAIL: TestAccAWSKmsAlias_no_name (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/03d28cdc-f477-43d0-9605-d52de2e0ff76' does not exist
            status code: 400, request id: fb98019c-54bb-11e7-8cee-9f3a8216e097

=== RUN   TestAccAWSKmsKey_basic
--- FAIL: TestAccAWSKmsKey_basic (22.54s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/adeb6095-616a-4b50-958d-0325dac6ae4f' does not exist
            status code: 400, request id: 9fcce70a-45cc-11e7-87aa-cb3194a8108b

=== RUN   TestAccAWSKmsKey_disappears
--- FAIL: TestAccAWSKmsKey_disappears (3.09s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/2520d4af-7071-4b39-9bec-8c75e3b46032' does not exist
            status code: 400, request id: 5ec12c47-32ef-11e7-ac0c-1feea159ca97

=== RUN   TestAccAWSKmsKey_importBasic
--- FAIL: TestAccAWSKmsKey_importBasic (23.50s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/161bfac9-279f-470b-8c4d-c8eed3783c3a' does not exist
            status code: 400, request id: 6d7093c7-4a76-11e7-b6df-cb1b052b0f43

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (21.54s)
    testing.go:265: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""0750e54a-1bd6-4ae8-830b-a019ad0601ec"" to true: ""NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/0750e54a-1bd6-4ae8-830b-a019ad0601ec' does not exist\n\tstatus code: 400, request id: 8c1c8664-f5a4-11e6-bd72-9d8ac691c38b""

=== RUN   TestAccAWSKmsKey_policy
--- FAIL: TestAccAWSKmsKey_policy (22.99s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/b0bc93e2-b846-4d38-afa0-5c41a23fd3b3' does not exist
            status code: 400, request id: eeafefd9-5f41-11e7-932d-7b4776770aa1

=== RUN   TestAccAWSKmsKey_tags
--- FAIL: TestAccAWSKmsKey_tags (23.16s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c0468528-75cd-46a4-941d-38cbab46cda9' does not exist
            status code: 400, request id: 00e0034e-50d0-11e7-b0df-ed08666dd37a
```

this one

```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (100.82s)
    testing.go:428: Step 1 error: After applying this step and refreshing, the plan was not empty:
        
        DIFF:
        
        UPDATE: aws_kms_key.bar
          is_enabled: ""true"" => ""false""
        
        STATE:
        
        aws_kms_key.bar:
          ID = 331666bc-e480-4bbd-8e94-19457be08dcc
          arn = arn:aws:kms:us-west-2:*******:key/331666bc-e480-4bbd-8e94-19457be08dcc
          deletion_window_in_days = 7
          description = Terraform acc test is_enabled Sun, 25 Jun 2017 06:53:24 UTC
          enable_key_rotation = false
          is_enabled = true
          key_id = 331666bc-e480-4bbd-8e94-19457be08dcc
          key_usage = ENCRYPT_DECRYPT
          policy = {""Id"":""key-default-1"",""Statement"":[{""Action"":""kms:*"",""Effect"":""Allow"",""Principal"":{""AWS"":""arn:aws:iam::*******:root""},""Resource"":""*"",""Sid"":""Enable IAM User Permissions""}],""Version"":""2012-10-17""}
          tags.% = 0

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (76.05s)
    testing.go:265: Step 1 error: Check failed: Check 2/4 error: aws_kms_key.bar: Attribute 'is_enabled' expected ""false"", got ""true""
```
and this one
```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (229.51s)
    testing.go:280: Step 2 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""c98e0539-7d2a-4bdf-9380-0a5258a69b44"" to true: ""DisabledException: arn:aws:kms:us-west-2:*******:key/c98e0539-7d2a-4bdf-9380-0a5258a69b44 is disabled.\n\tstatus code: 400, request id: 6a240630-2b16-11e7-b764-abbc11cb3f70""
```

and maybe a some more caused by the same error code.",TestAccAWSKmsKey_isEnabled,https://github.com/hashicorp/terraform-provider-aws
47,1039.0,"This is to address the following test failures:

```
=== RUN   TestAccAWSKmsAlias_basic
--- FAIL: TestAccAWSKmsAlias_basic (24.96s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c857a00e-1276-4e08-8e08-d5b9d0b75e41' does not exist
            status code: 400, request id: 0eba259a-5a3b-11e7-b344-c7ea9798df62

=== RUN   TestAccAWSKmsAlias_importBasic
--- FAIL: TestAccAWSKmsAlias_importBasic (25.91s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.one: 1 error(s) occurred:
        
        * aws_kms_key.one: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/118a3530-5361-4649-8536-a93f2ede591a' does not exist
            status code: 400, request id: 9898277d-5c8a-11e7-abbd-b9503fe779e7

=== RUN   TestAccAWSKmsAlias_multiple
--- FAIL: TestAccAWSKmsAlias_multiple (22.69s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.single: 1 error(s) occurred:
        
        * aws_kms_key.single: Failed to get KMS key tags (key: e47b525d-38b2-4402-aee1-0be1292d9c4a): NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/e47b525d-38b2-4402-aee1-0be1292d9c4a' does not exist
            status code: 400, request id: 078d3a84-5329-11e7-a59b-71d56abfc9fc

=== RUN   TestAccAWSKmsAlias_name_prefix
--- FAIL: TestAccAWSKmsAlias_name_prefix (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/620c5864-ab1e-4f32-8615-a39552d0c6b8' does not exist
            status code: 400, request id: e963e5c2-50cf-11e7-9c90-87b19212a0de

=== RUN   TestAccAWSKmsAlias_no_name
--- FAIL: TestAccAWSKmsAlias_no_name (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/03d28cdc-f477-43d0-9605-d52de2e0ff76' does not exist
            status code: 400, request id: fb98019c-54bb-11e7-8cee-9f3a8216e097

=== RUN   TestAccAWSKmsKey_basic
--- FAIL: TestAccAWSKmsKey_basic (22.54s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/adeb6095-616a-4b50-958d-0325dac6ae4f' does not exist
            status code: 400, request id: 9fcce70a-45cc-11e7-87aa-cb3194a8108b

=== RUN   TestAccAWSKmsKey_disappears
--- FAIL: TestAccAWSKmsKey_disappears (3.09s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/2520d4af-7071-4b39-9bec-8c75e3b46032' does not exist
            status code: 400, request id: 5ec12c47-32ef-11e7-ac0c-1feea159ca97

=== RUN   TestAccAWSKmsKey_importBasic
--- FAIL: TestAccAWSKmsKey_importBasic (23.50s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/161bfac9-279f-470b-8c4d-c8eed3783c3a' does not exist
            status code: 400, request id: 6d7093c7-4a76-11e7-b6df-cb1b052b0f43

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (21.54s)
    testing.go:265: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""0750e54a-1bd6-4ae8-830b-a019ad0601ec"" to true: ""NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/0750e54a-1bd6-4ae8-830b-a019ad0601ec' does not exist\n\tstatus code: 400, request id: 8c1c8664-f5a4-11e6-bd72-9d8ac691c38b""

=== RUN   TestAccAWSKmsKey_policy
--- FAIL: TestAccAWSKmsKey_policy (22.99s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/b0bc93e2-b846-4d38-afa0-5c41a23fd3b3' does not exist
            status code: 400, request id: eeafefd9-5f41-11e7-932d-7b4776770aa1

=== RUN   TestAccAWSKmsKey_tags
--- FAIL: TestAccAWSKmsKey_tags (23.16s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c0468528-75cd-46a4-941d-38cbab46cda9' does not exist
            status code: 400, request id: 00e0034e-50d0-11e7-b0df-ed08666dd37a
```

this one

```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (100.82s)
    testing.go:428: Step 1 error: After applying this step and refreshing, the plan was not empty:
        
        DIFF:
        
        UPDATE: aws_kms_key.bar
          is_enabled: ""true"" => ""false""
        
        STATE:
        
        aws_kms_key.bar:
          ID = 331666bc-e480-4bbd-8e94-19457be08dcc
          arn = arn:aws:kms:us-west-2:*******:key/331666bc-e480-4bbd-8e94-19457be08dcc
          deletion_window_in_days = 7
          description = Terraform acc test is_enabled Sun, 25 Jun 2017 06:53:24 UTC
          enable_key_rotation = false
          is_enabled = true
          key_id = 331666bc-e480-4bbd-8e94-19457be08dcc
          key_usage = ENCRYPT_DECRYPT
          policy = {""Id"":""key-default-1"",""Statement"":[{""Action"":""kms:*"",""Effect"":""Allow"",""Principal"":{""AWS"":""arn:aws:iam::*******:root""},""Resource"":""*"",""Sid"":""Enable IAM User Permissions""}],""Version"":""2012-10-17""}
          tags.% = 0

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (76.05s)
    testing.go:265: Step 1 error: Check failed: Check 2/4 error: aws_kms_key.bar: Attribute 'is_enabled' expected ""false"", got ""true""
```
and this one
```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (229.51s)
    testing.go:280: Step 2 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""c98e0539-7d2a-4bdf-9380-0a5258a69b44"" to true: ""DisabledException: arn:aws:kms:us-west-2:*******:key/c98e0539-7d2a-4bdf-9380-0a5258a69b44 is disabled.\n\tstatus code: 400, request id: 6a240630-2b16-11e7-b764-abbc11cb3f70""
```

and maybe a some more caused by the same error code.",9380-0a5258a69b44,https://github.com/hashicorp/terraform-provider-aws
48,1039.0,"This is to address the following test failures:

```
=== RUN   TestAccAWSKmsAlias_basic
--- FAIL: TestAccAWSKmsAlias_basic (24.96s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c857a00e-1276-4e08-8e08-d5b9d0b75e41' does not exist
            status code: 400, request id: 0eba259a-5a3b-11e7-b344-c7ea9798df62

=== RUN   TestAccAWSKmsAlias_importBasic
--- FAIL: TestAccAWSKmsAlias_importBasic (25.91s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.one: 1 error(s) occurred:
        
        * aws_kms_key.one: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/118a3530-5361-4649-8536-a93f2ede591a' does not exist
            status code: 400, request id: 9898277d-5c8a-11e7-abbd-b9503fe779e7

=== RUN   TestAccAWSKmsAlias_multiple
--- FAIL: TestAccAWSKmsAlias_multiple (22.69s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.single: 1 error(s) occurred:
        
        * aws_kms_key.single: Failed to get KMS key tags (key: e47b525d-38b2-4402-aee1-0be1292d9c4a): NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/e47b525d-38b2-4402-aee1-0be1292d9c4a' does not exist
            status code: 400, request id: 078d3a84-5329-11e7-a59b-71d56abfc9fc

=== RUN   TestAccAWSKmsAlias_name_prefix
--- FAIL: TestAccAWSKmsAlias_name_prefix (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/620c5864-ab1e-4f32-8615-a39552d0c6b8' does not exist
            status code: 400, request id: e963e5c2-50cf-11e7-9c90-87b19212a0de

=== RUN   TestAccAWSKmsAlias_no_name
--- FAIL: TestAccAWSKmsAlias_no_name (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/03d28cdc-f477-43d0-9605-d52de2e0ff76' does not exist
            status code: 400, request id: fb98019c-54bb-11e7-8cee-9f3a8216e097

=== RUN   TestAccAWSKmsKey_basic
--- FAIL: TestAccAWSKmsKey_basic (22.54s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/adeb6095-616a-4b50-958d-0325dac6ae4f' does not exist
            status code: 400, request id: 9fcce70a-45cc-11e7-87aa-cb3194a8108b

=== RUN   TestAccAWSKmsKey_disappears
--- FAIL: TestAccAWSKmsKey_disappears (3.09s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/2520d4af-7071-4b39-9bec-8c75e3b46032' does not exist
            status code: 400, request id: 5ec12c47-32ef-11e7-ac0c-1feea159ca97

=== RUN   TestAccAWSKmsKey_importBasic
--- FAIL: TestAccAWSKmsKey_importBasic (23.50s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/161bfac9-279f-470b-8c4d-c8eed3783c3a' does not exist
            status code: 400, request id: 6d7093c7-4a76-11e7-b6df-cb1b052b0f43

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (21.54s)
    testing.go:265: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""0750e54a-1bd6-4ae8-830b-a019ad0601ec"" to true: ""NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/0750e54a-1bd6-4ae8-830b-a019ad0601ec' does not exist\n\tstatus code: 400, request id: 8c1c8664-f5a4-11e6-bd72-9d8ac691c38b""

=== RUN   TestAccAWSKmsKey_policy
--- FAIL: TestAccAWSKmsKey_policy (22.99s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/b0bc93e2-b846-4d38-afa0-5c41a23fd3b3' does not exist
            status code: 400, request id: eeafefd9-5f41-11e7-932d-7b4776770aa1

=== RUN   TestAccAWSKmsKey_tags
--- FAIL: TestAccAWSKmsKey_tags (23.16s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c0468528-75cd-46a4-941d-38cbab46cda9' does not exist
            status code: 400, request id: 00e0034e-50d0-11e7-b0df-ed08666dd37a
```

this one

```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (100.82s)
    testing.go:428: Step 1 error: After applying this step and refreshing, the plan was not empty:
        
        DIFF:
        
        UPDATE: aws_kms_key.bar
          is_enabled: ""true"" => ""false""
        
        STATE:
        
        aws_kms_key.bar:
          ID = 331666bc-e480-4bbd-8e94-19457be08dcc
          arn = arn:aws:kms:us-west-2:*******:key/331666bc-e480-4bbd-8e94-19457be08dcc
          deletion_window_in_days = 7
          description = Terraform acc test is_enabled Sun, 25 Jun 2017 06:53:24 UTC
          enable_key_rotation = false
          is_enabled = true
          key_id = 331666bc-e480-4bbd-8e94-19457be08dcc
          key_usage = ENCRYPT_DECRYPT
          policy = {""Id"":""key-default-1"",""Statement"":[{""Action"":""kms:*"",""Effect"":""Allow"",""Principal"":{""AWS"":""arn:aws:iam::*******:root""},""Resource"":""*"",""Sid"":""Enable IAM User Permissions""}],""Version"":""2012-10-17""}
          tags.% = 0

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (76.05s)
    testing.go:265: Step 1 error: Check failed: Check 2/4 error: aws_kms_key.bar: Attribute 'is_enabled' expected ""false"", got ""true""
```
and this one
```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (229.51s)
    testing.go:280: Step 2 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""c98e0539-7d2a-4bdf-9380-0a5258a69b44"" to true: ""DisabledException: arn:aws:kms:us-west-2:*******:key/c98e0539-7d2a-4bdf-9380-0a5258a69b44 is disabled.\n\tstatus code: 400, request id: 6a240630-2b16-11e7-b764-abbc11cb3f70""
```

and maybe a some more caused by the same error code.",west-2:*******:key,https://github.com/hashicorp/terraform-provider-aws
49,1039.0,"This is to address the following test failures:

```
=== RUN   TestAccAWSKmsAlias_basic
--- FAIL: TestAccAWSKmsAlias_basic (24.96s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c857a00e-1276-4e08-8e08-d5b9d0b75e41' does not exist
            status code: 400, request id: 0eba259a-5a3b-11e7-b344-c7ea9798df62

=== RUN   TestAccAWSKmsAlias_importBasic
--- FAIL: TestAccAWSKmsAlias_importBasic (25.91s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.one: 1 error(s) occurred:
        
        * aws_kms_key.one: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/118a3530-5361-4649-8536-a93f2ede591a' does not exist
            status code: 400, request id: 9898277d-5c8a-11e7-abbd-b9503fe779e7

=== RUN   TestAccAWSKmsAlias_multiple
--- FAIL: TestAccAWSKmsAlias_multiple (22.69s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.single: 1 error(s) occurred:
        
        * aws_kms_key.single: Failed to get KMS key tags (key: e47b525d-38b2-4402-aee1-0be1292d9c4a): NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/e47b525d-38b2-4402-aee1-0be1292d9c4a' does not exist
            status code: 400, request id: 078d3a84-5329-11e7-a59b-71d56abfc9fc

=== RUN   TestAccAWSKmsAlias_name_prefix
--- FAIL: TestAccAWSKmsAlias_name_prefix (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/620c5864-ab1e-4f32-8615-a39552d0c6b8' does not exist
            status code: 400, request id: e963e5c2-50cf-11e7-9c90-87b19212a0de

=== RUN   TestAccAWSKmsAlias_no_name
--- FAIL: TestAccAWSKmsAlias_no_name (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/03d28cdc-f477-43d0-9605-d52de2e0ff76' does not exist
            status code: 400, request id: fb98019c-54bb-11e7-8cee-9f3a8216e097

=== RUN   TestAccAWSKmsKey_basic
--- FAIL: TestAccAWSKmsKey_basic (22.54s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/adeb6095-616a-4b50-958d-0325dac6ae4f' does not exist
            status code: 400, request id: 9fcce70a-45cc-11e7-87aa-cb3194a8108b

=== RUN   TestAccAWSKmsKey_disappears
--- FAIL: TestAccAWSKmsKey_disappears (3.09s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/2520d4af-7071-4b39-9bec-8c75e3b46032' does not exist
            status code: 400, request id: 5ec12c47-32ef-11e7-ac0c-1feea159ca97

=== RUN   TestAccAWSKmsKey_importBasic
--- FAIL: TestAccAWSKmsKey_importBasic (23.50s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/161bfac9-279f-470b-8c4d-c8eed3783c3a' does not exist
            status code: 400, request id: 6d7093c7-4a76-11e7-b6df-cb1b052b0f43

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (21.54s)
    testing.go:265: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""0750e54a-1bd6-4ae8-830b-a019ad0601ec"" to true: ""NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/0750e54a-1bd6-4ae8-830b-a019ad0601ec' does not exist\n\tstatus code: 400, request id: 8c1c8664-f5a4-11e6-bd72-9d8ac691c38b""

=== RUN   TestAccAWSKmsKey_policy
--- FAIL: TestAccAWSKmsKey_policy (22.99s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/b0bc93e2-b846-4d38-afa0-5c41a23fd3b3' does not exist
            status code: 400, request id: eeafefd9-5f41-11e7-932d-7b4776770aa1

=== RUN   TestAccAWSKmsKey_tags
--- FAIL: TestAccAWSKmsKey_tags (23.16s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c0468528-75cd-46a4-941d-38cbab46cda9' does not exist
            status code: 400, request id: 00e0034e-50d0-11e7-b0df-ed08666dd37a
```

this one

```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (100.82s)
    testing.go:428: Step 1 error: After applying this step and refreshing, the plan was not empty:
        
        DIFF:
        
        UPDATE: aws_kms_key.bar
          is_enabled: ""true"" => ""false""
        
        STATE:
        
        aws_kms_key.bar:
          ID = 331666bc-e480-4bbd-8e94-19457be08dcc
          arn = arn:aws:kms:us-west-2:*******:key/331666bc-e480-4bbd-8e94-19457be08dcc
          deletion_window_in_days = 7
          description = Terraform acc test is_enabled Sun, 25 Jun 2017 06:53:24 UTC
          enable_key_rotation = false
          is_enabled = true
          key_id = 331666bc-e480-4bbd-8e94-19457be08dcc
          key_usage = ENCRYPT_DECRYPT
          policy = {""Id"":""key-default-1"",""Statement"":[{""Action"":""kms:*"",""Effect"":""Allow"",""Principal"":{""AWS"":""arn:aws:iam::*******:root""},""Resource"":""*"",""Sid"":""Enable IAM User Permissions""}],""Version"":""2012-10-17""}
          tags.% = 0

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (76.05s)
    testing.go:265: Step 1 error: Check failed: Check 2/4 error: aws_kms_key.bar: Attribute 'is_enabled' expected ""false"", got ""true""
```
and this one
```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (229.51s)
    testing.go:280: Step 2 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""c98e0539-7d2a-4bdf-9380-0a5258a69b44"" to true: ""DisabledException: arn:aws:kms:us-west-2:*******:key/c98e0539-7d2a-4bdf-9380-0a5258a69b44 is disabled.\n\tstatus code: 400, request id: 6a240630-2b16-11e7-b764-abbc11cb3f70""
```

and maybe a some more caused by the same error code.",:*******:key\n\tstatus,https://github.com/hashicorp/terraform-provider-aws
50,1039.0,"This is to address the following test failures:

```
=== RUN   TestAccAWSKmsAlias_basic
--- FAIL: TestAccAWSKmsAlias_basic (24.96s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c857a00e-1276-4e08-8e08-d5b9d0b75e41' does not exist
            status code: 400, request id: 0eba259a-5a3b-11e7-b344-c7ea9798df62

=== RUN   TestAccAWSKmsAlias_importBasic
--- FAIL: TestAccAWSKmsAlias_importBasic (25.91s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.one: 1 error(s) occurred:
        
        * aws_kms_key.one: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/118a3530-5361-4649-8536-a93f2ede591a' does not exist
            status code: 400, request id: 9898277d-5c8a-11e7-abbd-b9503fe779e7

=== RUN   TestAccAWSKmsAlias_multiple
--- FAIL: TestAccAWSKmsAlias_multiple (22.69s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.single: 1 error(s) occurred:
        
        * aws_kms_key.single: Failed to get KMS key tags (key: e47b525d-38b2-4402-aee1-0be1292d9c4a): NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/e47b525d-38b2-4402-aee1-0be1292d9c4a' does not exist
            status code: 400, request id: 078d3a84-5329-11e7-a59b-71d56abfc9fc

=== RUN   TestAccAWSKmsAlias_name_prefix
--- FAIL: TestAccAWSKmsAlias_name_prefix (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/620c5864-ab1e-4f32-8615-a39552d0c6b8' does not exist
            status code: 400, request id: e963e5c2-50cf-11e7-9c90-87b19212a0de

=== RUN   TestAccAWSKmsAlias_no_name
--- FAIL: TestAccAWSKmsAlias_no_name (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/03d28cdc-f477-43d0-9605-d52de2e0ff76' does not exist
            status code: 400, request id: fb98019c-54bb-11e7-8cee-9f3a8216e097

=== RUN   TestAccAWSKmsKey_basic
--- FAIL: TestAccAWSKmsKey_basic (22.54s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/adeb6095-616a-4b50-958d-0325dac6ae4f' does not exist
            status code: 400, request id: 9fcce70a-45cc-11e7-87aa-cb3194a8108b

=== RUN   TestAccAWSKmsKey_disappears
--- FAIL: TestAccAWSKmsKey_disappears (3.09s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/2520d4af-7071-4b39-9bec-8c75e3b46032' does not exist
            status code: 400, request id: 5ec12c47-32ef-11e7-ac0c-1feea159ca97

=== RUN   TestAccAWSKmsKey_importBasic
--- FAIL: TestAccAWSKmsKey_importBasic (23.50s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/161bfac9-279f-470b-8c4d-c8eed3783c3a' does not exist
            status code: 400, request id: 6d7093c7-4a76-11e7-b6df-cb1b052b0f43

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (21.54s)
    testing.go:265: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""0750e54a-1bd6-4ae8-830b-a019ad0601ec"" to true: ""NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/0750e54a-1bd6-4ae8-830b-a019ad0601ec' does not exist\n\tstatus code: 400, request id: 8c1c8664-f5a4-11e6-bd72-9d8ac691c38b""

=== RUN   TestAccAWSKmsKey_policy
--- FAIL: TestAccAWSKmsKey_policy (22.99s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/b0bc93e2-b846-4d38-afa0-5c41a23fd3b3' does not exist
            status code: 400, request id: eeafefd9-5f41-11e7-932d-7b4776770aa1

=== RUN   TestAccAWSKmsKey_tags
--- FAIL: TestAccAWSKmsKey_tags (23.16s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c0468528-75cd-46a4-941d-38cbab46cda9' does not exist
            status code: 400, request id: 00e0034e-50d0-11e7-b0df-ed08666dd37a
```

this one

```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (100.82s)
    testing.go:428: Step 1 error: After applying this step and refreshing, the plan was not empty:
        
        DIFF:
        
        UPDATE: aws_kms_key.bar
          is_enabled: ""true"" => ""false""
        
        STATE:
        
        aws_kms_key.bar:
          ID = 331666bc-e480-4bbd-8e94-19457be08dcc
          arn = arn:aws:kms:us-west-2:*******:key/331666bc-e480-4bbd-8e94-19457be08dcc
          deletion_window_in_days = 7
          description = Terraform acc test is_enabled Sun, 25 Jun 2017 06:53:24 UTC
          enable_key_rotation = false
          is_enabled = true
          key_id = 331666bc-e480-4bbd-8e94-19457be08dcc
          key_usage = ENCRYPT_DECRYPT
          policy = {""Id"":""key-default-1"",""Statement"":[{""Action"":""kms:*"",""Effect"":""Allow"",""Principal"":{""AWS"":""arn:aws:iam::*******:root""},""Resource"":""*"",""Sid"":""Enable IAM User Permissions""}],""Version"":""2012-10-17""}
          tags.% = 0

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (76.05s)
    testing.go:265: Step 1 error: Check failed: Check 2/4 error: aws_kms_key.bar: Attribute 'is_enabled' expected ""false"", got ""true""
```
and this one
```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (229.51s)
    testing.go:280: Step 2 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""c98e0539-7d2a-4bdf-9380-0a5258a69b44"" to true: ""DisabledException: arn:aws:kms:us-west-2:*******:key/c98e0539-7d2a-4bdf-9380-0a5258a69b44 is disabled.\n\tstatus code: 400, request id: 6a240630-2b16-11e7-b764-abbc11cb3f70""
```

and maybe a some more caused by the same error code.","Effect:Allow,Principal:{AWS:arn:aws:iam::*******:root},Resource",https://github.com/hashicorp/terraform-provider-aws
51,1039.0,"This is to address the following test failures:

```
=== RUN   TestAccAWSKmsAlias_basic
--- FAIL: TestAccAWSKmsAlias_basic (24.96s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c857a00e-1276-4e08-8e08-d5b9d0b75e41' does not exist
            status code: 400, request id: 0eba259a-5a3b-11e7-b344-c7ea9798df62

=== RUN   TestAccAWSKmsAlias_importBasic
--- FAIL: TestAccAWSKmsAlias_importBasic (25.91s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.one: 1 error(s) occurred:
        
        * aws_kms_key.one: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/118a3530-5361-4649-8536-a93f2ede591a' does not exist
            status code: 400, request id: 9898277d-5c8a-11e7-abbd-b9503fe779e7

=== RUN   TestAccAWSKmsAlias_multiple
--- FAIL: TestAccAWSKmsAlias_multiple (22.69s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.single: 1 error(s) occurred:
        
        * aws_kms_key.single: Failed to get KMS key tags (key: e47b525d-38b2-4402-aee1-0be1292d9c4a): NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/e47b525d-38b2-4402-aee1-0be1292d9c4a' does not exist
            status code: 400, request id: 078d3a84-5329-11e7-a59b-71d56abfc9fc

=== RUN   TestAccAWSKmsAlias_name_prefix
--- FAIL: TestAccAWSKmsAlias_name_prefix (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/620c5864-ab1e-4f32-8615-a39552d0c6b8' does not exist
            status code: 400, request id: e963e5c2-50cf-11e7-9c90-87b19212a0de

=== RUN   TestAccAWSKmsAlias_no_name
--- FAIL: TestAccAWSKmsAlias_no_name (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/03d28cdc-f477-43d0-9605-d52de2e0ff76' does not exist
            status code: 400, request id: fb98019c-54bb-11e7-8cee-9f3a8216e097

=== RUN   TestAccAWSKmsKey_basic
--- FAIL: TestAccAWSKmsKey_basic (22.54s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/adeb6095-616a-4b50-958d-0325dac6ae4f' does not exist
            status code: 400, request id: 9fcce70a-45cc-11e7-87aa-cb3194a8108b

=== RUN   TestAccAWSKmsKey_disappears
--- FAIL: TestAccAWSKmsKey_disappears (3.09s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/2520d4af-7071-4b39-9bec-8c75e3b46032' does not exist
            status code: 400, request id: 5ec12c47-32ef-11e7-ac0c-1feea159ca97

=== RUN   TestAccAWSKmsKey_importBasic
--- FAIL: TestAccAWSKmsKey_importBasic (23.50s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/161bfac9-279f-470b-8c4d-c8eed3783c3a' does not exist
            status code: 400, request id: 6d7093c7-4a76-11e7-b6df-cb1b052b0f43

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (21.54s)
    testing.go:265: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""0750e54a-1bd6-4ae8-830b-a019ad0601ec"" to true: ""NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/0750e54a-1bd6-4ae8-830b-a019ad0601ec' does not exist\n\tstatus code: 400, request id: 8c1c8664-f5a4-11e6-bd72-9d8ac691c38b""

=== RUN   TestAccAWSKmsKey_policy
--- FAIL: TestAccAWSKmsKey_policy (22.99s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/b0bc93e2-b846-4d38-afa0-5c41a23fd3b3' does not exist
            status code: 400, request id: eeafefd9-5f41-11e7-932d-7b4776770aa1

=== RUN   TestAccAWSKmsKey_tags
--- FAIL: TestAccAWSKmsKey_tags (23.16s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c0468528-75cd-46a4-941d-38cbab46cda9' does not exist
            status code: 400, request id: 00e0034e-50d0-11e7-b0df-ed08666dd37a
```

this one

```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (100.82s)
    testing.go:428: Step 1 error: After applying this step and refreshing, the plan was not empty:
        
        DIFF:
        
        UPDATE: aws_kms_key.bar
          is_enabled: ""true"" => ""false""
        
        STATE:
        
        aws_kms_key.bar:
          ID = 331666bc-e480-4bbd-8e94-19457be08dcc
          arn = arn:aws:kms:us-west-2:*******:key/331666bc-e480-4bbd-8e94-19457be08dcc
          deletion_window_in_days = 7
          description = Terraform acc test is_enabled Sun, 25 Jun 2017 06:53:24 UTC
          enable_key_rotation = false
          is_enabled = true
          key_id = 331666bc-e480-4bbd-8e94-19457be08dcc
          key_usage = ENCRYPT_DECRYPT
          policy = {""Id"":""key-default-1"",""Statement"":[{""Action"":""kms:*"",""Effect"":""Allow"",""Principal"":{""AWS"":""arn:aws:iam::*******:root""},""Resource"":""*"",""Sid"":""Enable IAM User Permissions""}],""Version"":""2012-10-17""}
          tags.% = 0

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (76.05s)
    testing.go:265: Step 1 error: Check failed: Check 2/4 error: aws_kms_key.bar: Attribute 'is_enabled' expected ""false"", got ""true""
```
and this one
```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (229.51s)
    testing.go:280: Step 2 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""c98e0539-7d2a-4bdf-9380-0a5258a69b44"" to true: ""DisabledException: arn:aws:kms:us-west-2:*******:key/c98e0539-7d2a-4bdf-9380-0a5258a69b44 is disabled.\n\tstatus code: 400, request id: 6a240630-2b16-11e7-b764-abbc11cb3f70""
```

and maybe a some more caused by the same error code.",TestAccAWSKmsKey_basic,https://github.com/hashicorp/terraform-provider-aws
52,1039.0,"This is to address the following test failures:

```
=== RUN   TestAccAWSKmsAlias_basic
--- FAIL: TestAccAWSKmsAlias_basic (24.96s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c857a00e-1276-4e08-8e08-d5b9d0b75e41' does not exist
            status code: 400, request id: 0eba259a-5a3b-11e7-b344-c7ea9798df62

=== RUN   TestAccAWSKmsAlias_importBasic
--- FAIL: TestAccAWSKmsAlias_importBasic (25.91s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.one: 1 error(s) occurred:
        
        * aws_kms_key.one: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/118a3530-5361-4649-8536-a93f2ede591a' does not exist
            status code: 400, request id: 9898277d-5c8a-11e7-abbd-b9503fe779e7

=== RUN   TestAccAWSKmsAlias_multiple
--- FAIL: TestAccAWSKmsAlias_multiple (22.69s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.single: 1 error(s) occurred:
        
        * aws_kms_key.single: Failed to get KMS key tags (key: e47b525d-38b2-4402-aee1-0be1292d9c4a): NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/e47b525d-38b2-4402-aee1-0be1292d9c4a' does not exist
            status code: 400, request id: 078d3a84-5329-11e7-a59b-71d56abfc9fc

=== RUN   TestAccAWSKmsAlias_name_prefix
--- FAIL: TestAccAWSKmsAlias_name_prefix (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/620c5864-ab1e-4f32-8615-a39552d0c6b8' does not exist
            status code: 400, request id: e963e5c2-50cf-11e7-9c90-87b19212a0de

=== RUN   TestAccAWSKmsAlias_no_name
--- FAIL: TestAccAWSKmsAlias_no_name (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/03d28cdc-f477-43d0-9605-d52de2e0ff76' does not exist
            status code: 400, request id: fb98019c-54bb-11e7-8cee-9f3a8216e097

=== RUN   TestAccAWSKmsKey_basic
--- FAIL: TestAccAWSKmsKey_basic (22.54s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/adeb6095-616a-4b50-958d-0325dac6ae4f' does not exist
            status code: 400, request id: 9fcce70a-45cc-11e7-87aa-cb3194a8108b

=== RUN   TestAccAWSKmsKey_disappears
--- FAIL: TestAccAWSKmsKey_disappears (3.09s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/2520d4af-7071-4b39-9bec-8c75e3b46032' does not exist
            status code: 400, request id: 5ec12c47-32ef-11e7-ac0c-1feea159ca97

=== RUN   TestAccAWSKmsKey_importBasic
--- FAIL: TestAccAWSKmsKey_importBasic (23.50s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/161bfac9-279f-470b-8c4d-c8eed3783c3a' does not exist
            status code: 400, request id: 6d7093c7-4a76-11e7-b6df-cb1b052b0f43

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (21.54s)
    testing.go:265: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""0750e54a-1bd6-4ae8-830b-a019ad0601ec"" to true: ""NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/0750e54a-1bd6-4ae8-830b-a019ad0601ec' does not exist\n\tstatus code: 400, request id: 8c1c8664-f5a4-11e6-bd72-9d8ac691c38b""

=== RUN   TestAccAWSKmsKey_policy
--- FAIL: TestAccAWSKmsKey_policy (22.99s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/b0bc93e2-b846-4d38-afa0-5c41a23fd3b3' does not exist
            status code: 400, request id: eeafefd9-5f41-11e7-932d-7b4776770aa1

=== RUN   TestAccAWSKmsKey_tags
--- FAIL: TestAccAWSKmsKey_tags (23.16s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c0468528-75cd-46a4-941d-38cbab46cda9' does not exist
            status code: 400, request id: 00e0034e-50d0-11e7-b0df-ed08666dd37a
```

this one

```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (100.82s)
    testing.go:428: Step 1 error: After applying this step and refreshing, the plan was not empty:
        
        DIFF:
        
        UPDATE: aws_kms_key.bar
          is_enabled: ""true"" => ""false""
        
        STATE:
        
        aws_kms_key.bar:
          ID = 331666bc-e480-4bbd-8e94-19457be08dcc
          arn = arn:aws:kms:us-west-2:*******:key/331666bc-e480-4bbd-8e94-19457be08dcc
          deletion_window_in_days = 7
          description = Terraform acc test is_enabled Sun, 25 Jun 2017 06:53:24 UTC
          enable_key_rotation = false
          is_enabled = true
          key_id = 331666bc-e480-4bbd-8e94-19457be08dcc
          key_usage = ENCRYPT_DECRYPT
          policy = {""Id"":""key-default-1"",""Statement"":[{""Action"":""kms:*"",""Effect"":""Allow"",""Principal"":{""AWS"":""arn:aws:iam::*******:root""},""Resource"":""*"",""Sid"":""Enable IAM User Permissions""}],""Version"":""2012-10-17""}
          tags.% = 0

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (76.05s)
    testing.go:265: Step 1 error: Check failed: Check 2/4 error: aws_kms_key.bar: Attribute 'is_enabled' expected ""false"", got ""true""
```
and this one
```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (229.51s)
    testing.go:280: Step 2 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""c98e0539-7d2a-4bdf-9380-0a5258a69b44"" to true: ""DisabledException: arn:aws:kms:us-west-2:*******:key/c98e0539-7d2a-4bdf-9380-0a5258a69b44 is disabled.\n\tstatus code: 400, request id: 6a240630-2b16-11e7-b764-abbc11cb3f70""
```

and maybe a some more caused by the same error code.",830b-a019ad0601ec,https://github.com/hashicorp/terraform-provider-aws
53,1039.0,"This is to address the following test failures:

```
=== RUN   TestAccAWSKmsAlias_basic
--- FAIL: TestAccAWSKmsAlias_basic (24.96s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c857a00e-1276-4e08-8e08-d5b9d0b75e41' does not exist
            status code: 400, request id: 0eba259a-5a3b-11e7-b344-c7ea9798df62

=== RUN   TestAccAWSKmsAlias_importBasic
--- FAIL: TestAccAWSKmsAlias_importBasic (25.91s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.one: 1 error(s) occurred:
        
        * aws_kms_key.one: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/118a3530-5361-4649-8536-a93f2ede591a' does not exist
            status code: 400, request id: 9898277d-5c8a-11e7-abbd-b9503fe779e7

=== RUN   TestAccAWSKmsAlias_multiple
--- FAIL: TestAccAWSKmsAlias_multiple (22.69s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.single: 1 error(s) occurred:
        
        * aws_kms_key.single: Failed to get KMS key tags (key: e47b525d-38b2-4402-aee1-0be1292d9c4a): NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/e47b525d-38b2-4402-aee1-0be1292d9c4a' does not exist
            status code: 400, request id: 078d3a84-5329-11e7-a59b-71d56abfc9fc

=== RUN   TestAccAWSKmsAlias_name_prefix
--- FAIL: TestAccAWSKmsAlias_name_prefix (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/620c5864-ab1e-4f32-8615-a39552d0c6b8' does not exist
            status code: 400, request id: e963e5c2-50cf-11e7-9c90-87b19212a0de

=== RUN   TestAccAWSKmsAlias_no_name
--- FAIL: TestAccAWSKmsAlias_no_name (23.85s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.two: 1 error(s) occurred:
        
        * aws_kms_key.two: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/03d28cdc-f477-43d0-9605-d52de2e0ff76' does not exist
            status code: 400, request id: fb98019c-54bb-11e7-8cee-9f3a8216e097

=== RUN   TestAccAWSKmsKey_basic
--- FAIL: TestAccAWSKmsKey_basic (22.54s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/adeb6095-616a-4b50-958d-0325dac6ae4f' does not exist
            status code: 400, request id: 9fcce70a-45cc-11e7-87aa-cb3194a8108b

=== RUN   TestAccAWSKmsKey_disappears
--- FAIL: TestAccAWSKmsKey_disappears (3.09s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/2520d4af-7071-4b39-9bec-8c75e3b46032' does not exist
            status code: 400, request id: 5ec12c47-32ef-11e7-ac0c-1feea159ca97

=== RUN   TestAccAWSKmsKey_importBasic
--- FAIL: TestAccAWSKmsKey_importBasic (23.50s)
    testing.go:280: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/161bfac9-279f-470b-8c4d-c8eed3783c3a' does not exist
            status code: 400, request id: 6d7093c7-4a76-11e7-b6df-cb1b052b0f43

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (21.54s)
    testing.go:265: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""0750e54a-1bd6-4ae8-830b-a019ad0601ec"" to true: ""NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/0750e54a-1bd6-4ae8-830b-a019ad0601ec' does not exist\n\tstatus code: 400, request id: 8c1c8664-f5a4-11e6-bd72-9d8ac691c38b""

=== RUN   TestAccAWSKmsKey_policy
--- FAIL: TestAccAWSKmsKey_policy (22.99s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/b0bc93e2-b846-4d38-afa0-5c41a23fd3b3' does not exist
            status code: 400, request id: eeafefd9-5f41-11e7-932d-7b4776770aa1

=== RUN   TestAccAWSKmsKey_tags
--- FAIL: TestAccAWSKmsKey_tags (23.16s)
    testing.go:428: Step 0 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.foo: 1 error(s) occurred:
        
        * aws_kms_key.foo: NotFoundException: Key 'arn:aws:kms:us-west-2:*******:key/c0468528-75cd-46a4-941d-38cbab46cda9' does not exist
            status code: 400, request id: 00e0034e-50d0-11e7-b0df-ed08666dd37a
```

this one

```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (100.82s)
    testing.go:428: Step 1 error: After applying this step and refreshing, the plan was not empty:
        
        DIFF:
        
        UPDATE: aws_kms_key.bar
          is_enabled: ""true"" => ""false""
        
        STATE:
        
        aws_kms_key.bar:
          ID = 331666bc-e480-4bbd-8e94-19457be08dcc
          arn = arn:aws:kms:us-west-2:*******:key/331666bc-e480-4bbd-8e94-19457be08dcc
          deletion_window_in_days = 7
          description = Terraform acc test is_enabled Sun, 25 Jun 2017 06:53:24 UTC
          enable_key_rotation = false
          is_enabled = true
          key_id = 331666bc-e480-4bbd-8e94-19457be08dcc
          key_usage = ENCRYPT_DECRYPT
          policy = {""Id"":""key-default-1"",""Statement"":[{""Action"":""kms:*"",""Effect"":""Allow"",""Principal"":{""AWS"":""arn:aws:iam::*******:root""},""Resource"":""*"",""Sid"":""Enable IAM User Permissions""}],""Version"":""2012-10-17""}
          tags.% = 0

=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (76.05s)
    testing.go:265: Step 1 error: Check failed: Check 2/4 error: aws_kms_key.bar: Attribute 'is_enabled' expected ""false"", got ""true""
```
and this one
```
=== RUN   TestAccAWSKmsKey_isEnabled
--- FAIL: TestAccAWSKmsKey_isEnabled (229.51s)
    testing.go:280: Step 2 error: Error applying: 1 error(s) occurred:
        
        * aws_kms_key.bar: 1 error(s) occurred:
        
        * aws_kms_key.bar: Failed to set key rotation for ""c98e0539-7d2a-4bdf-9380-0a5258a69b44"" to true: ""DisabledException: arn:aws:kms:us-west-2:*******:key/c98e0539-7d2a-4bdf-9380-0a5258a69b44 is disabled.\n\tstatus code: 400, request id: 6a240630-2b16-11e7-b764-abbc11cb3f70""
```

and maybe a some more caused by the same error code.",TestAccAWSKmsKey_tags,https://github.com/hashicorp/terraform-provider-aws
54,436.0,"_This issue was originally opened by @morfien101 as hashicorp/terraform#10835. It was migrated here as part of the [provider split](https://www.hashicorp.com/blog/upcoming-provider-changes-in-terraform-0-10/). The original body of the issue is below._

<hr>

Hi Guys,

I am trying to make use of the new feature in Terraform 0.8.0.
https://github.com/hashicorp/terraform/pull/10615

The PGP key here is just a test key so there is no issue with it being compromised.

```
variable aws_region {}

provider ""aws"" {
    region = ""${var.aws_region}""
}

resource ""aws_iam_user"" ""ecr_user"" {
    name = ""ecr_user""
    force_destroy = true
}

resource ""aws_iam_access_key"" ""ecr_user"" {
    user = ""${aws_iam_user.ecr_user.name}""
    pgp_key = ""LS0tLS1CRUdJTiBQR1AgUFVCTElDIEtFWSBCTE9DSy0tLS0tDQpWZXJzaW9uOiBLZXliYXNlIE9wZW5QR1AgdjEuMC4wDQpDb21tZW50OiBodHRwczovL2tleWJhc2UuaW8vY3J5cHRvDQoNCnhtOEVXRmZVQ1JNRks0RUVBQ0lEQXdUbERnenlmYzNvSlAzV0lPcTFTaTRsUUpUNW9lUEszVEdqeGZHQU5KRVQNCjhFSkt2Y3g5MkMxUGFlTEVieEJhT2Q0S0xRWThkdFFYOHVNRndqTm95bUpDaE1KMmJGdTZGS3F2aG1FVUNRUWUNCnkrK2dVYWpRWFlRcTFVYmVNNjdyVmNyTkxYSmhibVI1SUdOdlluVnliaUE4Y21GdVpIa3VZMjlpZFhKdVFHNWwNCmQzWnZhV05sYldWa2FXRXVZMjl0UHNLUEJCTVRDZ0FYQlFKWVY5UUpBaHN2QXdzSkJ3TVZDZ2dDSGdFQ0Y0QUENCkNna1FrMER6MHZZYmc2dWZ0UUYvUW4vemoway9WOVdLY051aThWT3hlTFZpcWFxN1lGYXZ4NGhNRmp1S1BFRjMNCnM0SGd5NnJmV2FrNWlRbXd1Rkt6QVg5WFo1VEVZZmkyWlFsekIrSlRNOU1nTVJMSUpEWFdtL0xLZ3JWWUh3by8NCjZjZ3k5WFE2YTlQMk1QYkwvc05yc2dET1VnUllWOVFKRXdncWhrak9QUU1CQndJREJNMVlBNGt2b1BNRWttK2YNCkRCVlNyNE1Ybkgvd3JzdjZCc3FESXNSUW8ySjlzTVYyWEp2YzMxMndLdXc3KzlkR2xzS3VZR0Y2NzRLN3l3NGgNCi84NHhoRmJDd0NjRUdCTUtBQThGQWxoWDFBa0ZDUThKbkFBQ0d5NEFhZ2tRazBEejB2WWJnNnRmSUFRWkV3b0ENCkJnVUNXRmZVQ1FBS0NSRFN4cklOZ29nelM3UVFBUURZa0t5bEVMeU1wR2N2cFdOL0w4aUpjajd1c0dtK0NVQjMNCjBQOEhnVHJMSUFEL1MrRkpEM0lyN1NhSjJiMUFGWU1IOVgwZ2UweFhBM1dLQ1F0OEVMcjE1bFQ5cUFHQWh3SHkNCkowMFczUmkzRXVpZnFWQjJucHNPdVVNb3N6OHZzYXFoa1BDaXZ3bHpmKzBiZjUyaXRUL3lhTk9hN1ZnOEFZQ3ENCm13QVducFB3bnFaYnIzemx2RVBmVXZTYWJHc0RUQUZjQldmVGNFdUczSUIyU2dRbUFqdDFOZkdwQTN5Q3M4dk8NClVnUllWOVFKRXdncWhrak9QUU1CQndJREJDQ25Nc05HN05XSFQ2bTBlQ3lMLzEraDJVUE9mdWRoWittOGhqOFUNCkdCTzh6dEZ3dHJyeUdQczJNYStVZ1hRYnJ6ME1zR0NnRDRmNHpHY1Z2UkduVG8zQ3dDY0VHQk1LQUE4RkFsaFgNCjFBa0ZDUThKbkFBQ0d5NEFhZ2tRazBEejB2WWJnNnRmSUFRWkV3b0FCZ1VDV0ZmVUNRQUtDUkNQWlkyekw5R3oNCklUOFlBUUNQU0dreEdRNEpDcEhXc0dNVndXQVdYSkdaZjJyYnhseDh6ckMrOSs0T0R3RUEvZ3ZpT1orcXFhN0oNCitHZC9xanpsSGxjMzFZeEY1bmxrQW9YRGM1Vk5Zbk0zamdGL2FCclpGTkkzNmlvMHdYODVjdUtPU2hsRFk0TzUNCmhEWHdpNkMzVXQvZUVTQXJJQXF5QnpmTmx3cG83Y3lyQXhTTUFZRGVGcklMTElwM0VnbHF1N0NaNHlFbnVIZXYNCnFObGRrd3kvRHVMSStIM0hoQ2hqSzNwR3FnS3hkL2JQMTVGYVN4WT0NCj05NWE3DQotLS0tLUVORCBQR1AgUFVCTElDIEtFWSBCTE9DSy0tLS0tDQo=""
}

output ""user_secret"" {
    value = ""${aws_iam_access_key.ecr_user.encrypted_secret}""
}
```

Unfortunately when I use this I get the following error.
```
Error applying plan:
1 error(s) occurred:
* aws_iam_access_key.ecr_user: Error encrypting IAM Access Key Secret: Error parsing given PGP key: openpgp: invalid data: tag byte does not have MSB set
```

I made a key like this:

```
root@runner:~# gpg --gen-key
gpg (GnuPG) 1.4.16; Copyright (C) 2013 Free Software Foundation
, Inc.
This is free software: you are free to change and redistribute
it.
There is NO WARRANTY, to the extent permitted by law.

Please select what kind of key you want:
   (1) RSA and RSA (default)
   (2) DSA and Elgamal
   (3) DSA (sign only)
   (4) RSA (sign only)
Your selection? 1
RSA keys may be between 1024 and 4096 bits long.
What keysize do you want? (2048) 4096
Requested keysize is 4096 bits
Please specify how long the key should be valid.
         0 = key does not expire
      <n>  = key expires in n days
      <n>w = key expires in n weeks
      <n>m = key expires in n months
      <n>y = key expires in n years
Key is valid for? (0) 1y
Key expires at Tue 19 Dec 2017 11:48:23 AM UTC
Is this correct? (y/N) y

You need a user ID to identify your key; the software construct
s the user ID
from the Real Name, Comment and Email Address in this form:
    ""Heinrich Heine (Der Dichter) <heinrichh@duesseldorf.de>""

Real name: Randy Coburn
Email address: randy.coburn@email.com
Comment: Test Key
You selected this USER-ID:
    ""Randy Coburn (Test Key) <randy.coburn@email.com>""

Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O
You need a Passphrase to protect your secret key.

gpg: gpg-agent is not available in this session
We need to generate a lot of random bytes. It is a good idea to
 perform
some other action (type on the keyboard, move the mouse, utiliz
e the
disks) during the prime generation; this gives the random numbe
r
generator a better chance to gain enough entropy.

Not enough random bytes available.  Please do some other work t
o give
the OS a chance to collect more entropy! (Need 288 more bytes)
......+++++
.............................................+++++
We need to generate a lot of random bytes. It is a good idea to
 perform
some other action (type on the keyboard, move the mouse, utiliz
e the
disks) during the prime generation; this gives the random numbe
r
generator a better chance to gain enough entropy.
....................+++++
...................+++++
gpg: /root/.gnupg/trustdb.gpg: trustdb created
gpg: key DB74EF86 marked as ultimately trusted
public and secret key created and signed.

gpg: checking the trustdb
gpg: 3 marginal(s) needed, 1 complete(s) needed, PGP trust mode
l
gpg: depth: 0  valid:   1  signed:   0  trust: 0-, 0q, 0n, 0m,
0f, 1u
gpg: next trustdb check due at 2017-12-19
pub   4096R/DB74EF86 2016-12-19 [expires: 2017-12-19]
      Key fingerprint = FA8F 2A3D 9D1C 1AD6 1082  3C0B 1503 914
7 DB74 EF86
uid                  Randy Coburn (Test Key) <randy.coburn@email.com>
sub   4096R/822FDB0A 2016-12-19 [expires: 2017-12-19]
```

Then exported it like this:
```
gpg --export -a ""Randy Coburn"" | base64
```",LS0tLS1CRUdJTiBQR1AgUFVCTElDIEtFWSBCTE9DSy0tLS0tDQpWZXJzaW9uOiBLZXliYXNlI,https://github.com/hashicorp/terraform-provider-aws
55,436.0,"_This issue was originally opened by @morfien101 as hashicorp/terraform#10835. It was migrated here as part of the [provider split](https://www.hashicorp.com/blog/upcoming-provider-changes-in-terraform-0-10/). The original body of the issue is below._

<hr>

Hi Guys,

I am trying to make use of the new feature in Terraform 0.8.0.
https://github.com/hashicorp/terraform/pull/10615

The PGP key here is just a test key so there is no issue with it being compromised.

```
variable aws_region {}

provider ""aws"" {
    region = ""${var.aws_region}""
}

resource ""aws_iam_user"" ""ecr_user"" {
    name = ""ecr_user""
    force_destroy = true
}

resource ""aws_iam_access_key"" ""ecr_user"" {
    user = ""${aws_iam_user.ecr_user.name}""
    pgp_key = ""LS0tLS1CRUdJTiBQR1AgUFVCTElDIEtFWSBCTE9DSy0tLS0tDQpWZXJzaW9uOiBLZXliYXNlIE9wZW5QR1AgdjEuMC4wDQpDb21tZW50OiBodHRwczovL2tleWJhc2UuaW8vY3J5cHRvDQoNCnhtOEVXRmZVQ1JNRks0RUVBQ0lEQXdUbERnenlmYzNvSlAzV0lPcTFTaTRsUUpUNW9lUEszVEdqeGZHQU5KRVQNCjhFSkt2Y3g5MkMxUGFlTEVieEJhT2Q0S0xRWThkdFFYOHVNRndqTm95bUpDaE1KMmJGdTZGS3F2aG1FVUNRUWUNCnkrK2dVYWpRWFlRcTFVYmVNNjdyVmNyTkxYSmhibVI1SUdOdlluVnliaUE4Y21GdVpIa3VZMjlpZFhKdVFHNWwNCmQzWnZhV05sYldWa2FXRXVZMjl0UHNLUEJCTVRDZ0FYQlFKWVY5UUpBaHN2QXdzSkJ3TVZDZ2dDSGdFQ0Y0QUENCkNna1FrMER6MHZZYmc2dWZ0UUYvUW4vemoway9WOVdLY051aThWT3hlTFZpcWFxN1lGYXZ4NGhNRmp1S1BFRjMNCnM0SGd5NnJmV2FrNWlRbXd1Rkt6QVg5WFo1VEVZZmkyWlFsekIrSlRNOU1nTVJMSUpEWFdtL0xLZ3JWWUh3by8NCjZjZ3k5WFE2YTlQMk1QYkwvc05yc2dET1VnUllWOVFKRXdncWhrak9QUU1CQndJREJNMVlBNGt2b1BNRWttK2YNCkRCVlNyNE1Ybkgvd3JzdjZCc3FESXNSUW8ySjlzTVYyWEp2YzMxMndLdXc3KzlkR2xzS3VZR0Y2NzRLN3l3NGgNCi84NHhoRmJDd0NjRUdCTUtBQThGQWxoWDFBa0ZDUThKbkFBQ0d5NEFhZ2tRazBEejB2WWJnNnRmSUFRWkV3b0ENCkJnVUNXRmZVQ1FBS0NSRFN4cklOZ29nelM3UVFBUURZa0t5bEVMeU1wR2N2cFdOL0w4aUpjajd1c0dtK0NVQjMNCjBQOEhnVHJMSUFEL1MrRkpEM0lyN1NhSjJiMUFGWU1IOVgwZ2UweFhBM1dLQ1F0OEVMcjE1bFQ5cUFHQWh3SHkNCkowMFczUmkzRXVpZnFWQjJucHNPdVVNb3N6OHZzYXFoa1BDaXZ3bHpmKzBiZjUyaXRUL3lhTk9hN1ZnOEFZQ3ENCm13QVducFB3bnFaYnIzemx2RVBmVXZTYWJHc0RUQUZjQldmVGNFdUczSUIyU2dRbUFqdDFOZkdwQTN5Q3M4dk8NClVnUllWOVFKRXdncWhrak9QUU1CQndJREJDQ25Nc05HN05XSFQ2bTBlQ3lMLzEraDJVUE9mdWRoWittOGhqOFUNCkdCTzh6dEZ3dHJyeUdQczJNYStVZ1hRYnJ6ME1zR0NnRDRmNHpHY1Z2UkduVG8zQ3dDY0VHQk1LQUE4RkFsaFgNCjFBa0ZDUThKbkFBQ0d5NEFhZ2tRazBEejB2WWJnNnRmSUFRWkV3b0FCZ1VDV0ZmVUNRQUtDUkNQWlkyekw5R3oNCklUOFlBUUNQU0dreEdRNEpDcEhXc0dNVndXQVdYSkdaZjJyYnhseDh6ckMrOSs0T0R3RUEvZ3ZpT1orcXFhN0oNCitHZC9xanpsSGxjMzFZeEY1bmxrQW9YRGM1Vk5Zbk0zamdGL2FCclpGTkkzNmlvMHdYODVjdUtPU2hsRFk0TzUNCmhEWHdpNkMzVXQvZUVTQXJJQXF5QnpmTmx3cG83Y3lyQXhTTUFZRGVGcklMTElwM0VnbHF1N0NaNHlFbnVIZXYNCnFObGRrd3kvRHVMSStIM0hoQ2hqSzNwR3FnS3hkL2JQMTVGYVN4WT0NCj05NWE3DQotLS0tLUVORCBQR1AgUFVCTElDIEtFWSBCTE9DSy0tLS0tDQo=""
}

output ""user_secret"" {
    value = ""${aws_iam_access_key.ecr_user.encrypted_secret}""
}
```

Unfortunately when I use this I get the following error.
```
Error applying plan:
1 error(s) occurred:
* aws_iam_access_key.ecr_user: Error encrypting IAM Access Key Secret: Error parsing given PGP key: openpgp: invalid data: tag byte does not have MSB set
```

I made a key like this:

```
root@runner:~# gpg --gen-key
gpg (GnuPG) 1.4.16; Copyright (C) 2013 Free Software Foundation
, Inc.
This is free software: you are free to change and redistribute
it.
There is NO WARRANTY, to the extent permitted by law.

Please select what kind of key you want:
   (1) RSA and RSA (default)
   (2) DSA and Elgamal
   (3) DSA (sign only)
   (4) RSA (sign only)
Your selection? 1
RSA keys may be between 1024 and 4096 bits long.
What keysize do you want? (2048) 4096
Requested keysize is 4096 bits
Please specify how long the key should be valid.
         0 = key does not expire
      <n>  = key expires in n days
      <n>w = key expires in n weeks
      <n>m = key expires in n months
      <n>y = key expires in n years
Key is valid for? (0) 1y
Key expires at Tue 19 Dec 2017 11:48:23 AM UTC
Is this correct? (y/N) y

You need a user ID to identify your key; the software construct
s the user ID
from the Real Name, Comment and Email Address in this form:
    ""Heinrich Heine (Der Dichter) <heinrichh@duesseldorf.de>""

Real name: Randy Coburn
Email address: randy.coburn@email.com
Comment: Test Key
You selected this USER-ID:
    ""Randy Coburn (Test Key) <randy.coburn@email.com>""

Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O
You need a Passphrase to protect your secret key.

gpg: gpg-agent is not available in this session
We need to generate a lot of random bytes. It is a good idea to
 perform
some other action (type on the keyboard, move the mouse, utiliz
e the
disks) during the prime generation; this gives the random numbe
r
generator a better chance to gain enough entropy.

Not enough random bytes available.  Please do some other work t
o give
the OS a chance to collect more entropy! (Need 288 more bytes)
......+++++
.............................................+++++
We need to generate a lot of random bytes. It is a good idea to
 perform
some other action (type on the keyboard, move the mouse, utiliz
e the
disks) during the prime generation; this gives the random numbe
r
generator a better chance to gain enough entropy.
....................+++++
...................+++++
gpg: /root/.gnupg/trustdb.gpg: trustdb created
gpg: key DB74EF86 marked as ultimately trusted
public and secret key created and signed.

gpg: checking the trustdb
gpg: 3 marginal(s) needed, 1 complete(s) needed, PGP trust mode
l
gpg: depth: 0  valid:   1  signed:   0  trust: 0-, 0q, 0n, 0m,
0f, 1u
gpg: next trustdb check due at 2017-12-19
pub   4096R/DB74EF86 2016-12-19 [expires: 2017-12-19]
      Key fingerprint = FA8F 2A3D 9D1C 1AD6 1082  3C0B 1503 914
7 DB74 EF86
uid                  Randy Coburn (Test Key) <randy.coburn@email.com>
sub   4096R/822FDB0A 2016-12-19 [expires: 2017-12-19]
```

Then exported it like this:
```
gpg --export -a ""Randy Coburn"" | base64
```",LS0tLS1CRUdJTiBQR1AgUFVCTElDIEtFWSBCTE9DSy0tLS0tDQpWZXJzaW9uOiBLZXliYXNlIE9wZW5QR1AgdjEuMC4wDQpDb21tZW50OiBodHRwczovL2tleWJhc2UuaW8vY3J5cHRvDQoNCnhtOEVXRmZVQ1JNRks0RUVBQ0lEQXdUbERnenlmYzNvSlAzV0lPcTFTaTRsUUpUNW9lUEszVEdqeGZHQU5KRVQNCjhFSkt2Y3g5MkMxUGFlTEVieEJhT2Q0S0xRWThkdFFYOHVNRndqTm95bUpDaE1KMmJGdTZGS3F2aG1FVUNRUWUNCnkrK2dVYWpRWFlRcTFVYmVNNjdyVmNyTkxYSmhibVI1SUdOdlluVnliaUE4Y21GdVpIa3VZMjlpZFhKdVFHNWwNCmQzWnZhV05sYldWa2FXRXVZMjl0UHNLUEJCTVRDZ0FYQlFKWVY5UUpBaHN2QXdzSkJ3TVZDZ2dDSGdFQ0Y0QUENCkNna1FrMER6MHZZYmc2dWZ0UUYvUW4vemoway9WOVdLY051aThWT3hlTFZpcWFxN1lGYXZ4NGhNRmp1S1BFRjMNCnM0SGd5NnJmV2FrNWlRbXd1Rkt6QVg5WFo1VEVZZmkyWlFsekIrSlRNOU1nTVJMSUpEWFdtL0xLZ3JWWUh3by8NCjZjZ3k5WFE2YTlQMk1QYkwvc05yc2dET1VnUllWOVFKRXdncWhrak9QUU1CQndJREJNMVlBNGt2b1BNRWttK2YNCkRCVlNyNE1Ybkgvd3JzdjZCc3FESXNSUW8ySjlzTVYyWEp2YzMxMndLdXc3KzlkR2xzS3VZR0Y2NzRLN3l3NGgNCi84NHhoRmJDd0NjRUdCTUtBQThGQWxoWDFBa0ZDUThKbkFBQ0d5NEFhZ2tRazBEejB2WWJnNnRmSUFRWkV3b0ENCkJnVUNXRmZVQ1FBS0NSRFN4cklOZ29nelM3UVFBUURZa0t5bEVMeU1wR2N2cFdOL0w4aUpjajd1c0dtK0NVQjMNCjBQOEhnVHJMSUFEL1MrRkpEM0lyN1NhSjJiMUFGWU1IOVgwZ2UweFhBM1dLQ1F0OEVMcjE1bFQ5cUFHQWh3SHkNCkowMFczUmkzRXVpZnFWQjJucHNPdVVNb3N6OHZzYXFoa1BDaXZ3bHpmKzBiZjUyaXRUL3lhTk9hN1ZnOEFZQ3ENCm13QVducFB3bnFaYnIzemx2RVBmVXZTYWJHc0RUQUZjQldmVGNFdUczSUIyU2dRbUFqdDFOZkdwQTN5Q3M4dk8NClVnUllWOVFKRXdncWhrak9QUU1CQndJREJDQ25Nc05HN05XSFQ2bTBlQ3lMLzEraDJVUE9mdWRoWittOGhqOFUNCkdCTzh6dEZ3dHJyeUdQczJNYStVZ1hRYnJ6ME1zR0NnRDRmNHpHY1Z2UkduVG8zQ3dDY0VHQk1LQUE4RkFsaFgNCjFBa0ZDUThKbkFBQ0d5NEFhZ2tRazBEejB2WWJnNnRmSUFRWkV3b0FCZ1VDV0ZmVUNRQUtDUkNQWlkyekw5R3oNCklUOFlBUUNQU0dreEdRNEpDcEhXc0dNVndXQVdYSkdaZjJyYnhseDh6ckMrOSs0T0R3RUEvZ3ZpT1orcXFhN0oNCitHZC9xanpsSGxjMzFZeEY1bmxrQW9YRGM1Vk5Zbk0zamdGL2FCclpGTkkzNmlvMHdYODVjdUtPU2hsRFk0TzUNCmhEWHdpNkMzVXQvZUVTQXJJQXF5QnpmTmx3cG83Y3lyQXhTTUFZRGVGcklMTElwM0VnbHF1N0NaNHlFbnVIZXYNCnFObGRrd3kvRHVMSStIM0hoQ2hqSzNwR3FnS3hkL2JQMTVGYVN4WT0NCj05NWE3DQotLS0tLUVORCBQR1AgUFVCTElDIEtFWSBCTE9DSy0tLS0tDQo,https://github.com/hashicorp/terraform-provider-aws
56,436.0,"_This issue was originally opened by @morfien101 as hashicorp/terraform#10835. It was migrated here as part of the [provider split](https://www.hashicorp.com/blog/upcoming-provider-changes-in-terraform-0-10/). The original body of the issue is below._

<hr>

Hi Guys,

I am trying to make use of the new feature in Terraform 0.8.0.
https://github.com/hashicorp/terraform/pull/10615

The PGP key here is just a test key so there is no issue with it being compromised.

```
variable aws_region {}

provider ""aws"" {
    region = ""${var.aws_region}""
}

resource ""aws_iam_user"" ""ecr_user"" {
    name = ""ecr_user""
    force_destroy = true
}

resource ""aws_iam_access_key"" ""ecr_user"" {
    user = ""${aws_iam_user.ecr_user.name}""
    pgp_key = ""LS0tLS1CRUdJTiBQR1AgUFVCTElDIEtFWSBCTE9DSy0tLS0tDQpWZXJzaW9uOiBLZXliYXNlIE9wZW5QR1AgdjEuMC4wDQpDb21tZW50OiBodHRwczovL2tleWJhc2UuaW8vY3J5cHRvDQoNCnhtOEVXRmZVQ1JNRks0RUVBQ0lEQXdUbERnenlmYzNvSlAzV0lPcTFTaTRsUUpUNW9lUEszVEdqeGZHQU5KRVQNCjhFSkt2Y3g5MkMxUGFlTEVieEJhT2Q0S0xRWThkdFFYOHVNRndqTm95bUpDaE1KMmJGdTZGS3F2aG1FVUNRUWUNCnkrK2dVYWpRWFlRcTFVYmVNNjdyVmNyTkxYSmhibVI1SUdOdlluVnliaUE4Y21GdVpIa3VZMjlpZFhKdVFHNWwNCmQzWnZhV05sYldWa2FXRXVZMjl0UHNLUEJCTVRDZ0FYQlFKWVY5UUpBaHN2QXdzSkJ3TVZDZ2dDSGdFQ0Y0QUENCkNna1FrMER6MHZZYmc2dWZ0UUYvUW4vemoway9WOVdLY051aThWT3hlTFZpcWFxN1lGYXZ4NGhNRmp1S1BFRjMNCnM0SGd5NnJmV2FrNWlRbXd1Rkt6QVg5WFo1VEVZZmkyWlFsekIrSlRNOU1nTVJMSUpEWFdtL0xLZ3JWWUh3by8NCjZjZ3k5WFE2YTlQMk1QYkwvc05yc2dET1VnUllWOVFKRXdncWhrak9QUU1CQndJREJNMVlBNGt2b1BNRWttK2YNCkRCVlNyNE1Ybkgvd3JzdjZCc3FESXNSUW8ySjlzTVYyWEp2YzMxMndLdXc3KzlkR2xzS3VZR0Y2NzRLN3l3NGgNCi84NHhoRmJDd0NjRUdCTUtBQThGQWxoWDFBa0ZDUThKbkFBQ0d5NEFhZ2tRazBEejB2WWJnNnRmSUFRWkV3b0ENCkJnVUNXRmZVQ1FBS0NSRFN4cklOZ29nelM3UVFBUURZa0t5bEVMeU1wR2N2cFdOL0w4aUpjajd1c0dtK0NVQjMNCjBQOEhnVHJMSUFEL1MrRkpEM0lyN1NhSjJiMUFGWU1IOVgwZ2UweFhBM1dLQ1F0OEVMcjE1bFQ5cUFHQWh3SHkNCkowMFczUmkzRXVpZnFWQjJucHNPdVVNb3N6OHZzYXFoa1BDaXZ3bHpmKzBiZjUyaXRUL3lhTk9hN1ZnOEFZQ3ENCm13QVducFB3bnFaYnIzemx2RVBmVXZTYWJHc0RUQUZjQldmVGNFdUczSUIyU2dRbUFqdDFOZkdwQTN5Q3M4dk8NClVnUllWOVFKRXdncWhrak9QUU1CQndJREJDQ25Nc05HN05XSFQ2bTBlQ3lMLzEraDJVUE9mdWRoWittOGhqOFUNCkdCTzh6dEZ3dHJyeUdQczJNYStVZ1hRYnJ6ME1zR0NnRDRmNHpHY1Z2UkduVG8zQ3dDY0VHQk1LQUE4RkFsaFgNCjFBa0ZDUThKbkFBQ0d5NEFhZ2tRazBEejB2WWJnNnRmSUFRWkV3b0FCZ1VDV0ZmVUNRQUtDUkNQWlkyekw5R3oNCklUOFlBUUNQU0dreEdRNEpDcEhXc0dNVndXQVdYSkdaZjJyYnhseDh6ckMrOSs0T0R3RUEvZ3ZpT1orcXFhN0oNCitHZC9xanpsSGxjMzFZeEY1bmxrQW9YRGM1Vk5Zbk0zamdGL2FCclpGTkkzNmlvMHdYODVjdUtPU2hsRFk0TzUNCmhEWHdpNkMzVXQvZUVTQXJJQXF5QnpmTmx3cG83Y3lyQXhTTUFZRGVGcklMTElwM0VnbHF1N0NaNHlFbnVIZXYNCnFObGRrd3kvRHVMSStIM0hoQ2hqSzNwR3FnS3hkL2JQMTVGYVN4WT0NCj05NWE3DQotLS0tLUVORCBQR1AgUFVCTElDIEtFWSBCTE9DSy0tLS0tDQo=""
}

output ""user_secret"" {
    value = ""${aws_iam_access_key.ecr_user.encrypted_secret}""
}
```

Unfortunately when I use this I get the following error.
```
Error applying plan:
1 error(s) occurred:
* aws_iam_access_key.ecr_user: Error encrypting IAM Access Key Secret: Error parsing given PGP key: openpgp: invalid data: tag byte does not have MSB set
```

I made a key like this:

```
root@runner:~# gpg --gen-key
gpg (GnuPG) 1.4.16; Copyright (C) 2013 Free Software Foundation
, Inc.
This is free software: you are free to change and redistribute
it.
There is NO WARRANTY, to the extent permitted by law.

Please select what kind of key you want:
   (1) RSA and RSA (default)
   (2) DSA and Elgamal
   (3) DSA (sign only)
   (4) RSA (sign only)
Your selection? 1
RSA keys may be between 1024 and 4096 bits long.
What keysize do you want? (2048) 4096
Requested keysize is 4096 bits
Please specify how long the key should be valid.
         0 = key does not expire
      <n>  = key expires in n days
      <n>w = key expires in n weeks
      <n>m = key expires in n months
      <n>y = key expires in n years
Key is valid for? (0) 1y
Key expires at Tue 19 Dec 2017 11:48:23 AM UTC
Is this correct? (y/N) y

You need a user ID to identify your key; the software construct
s the user ID
from the Real Name, Comment and Email Address in this form:
    ""Heinrich Heine (Der Dichter) <heinrichh@duesseldorf.de>""

Real name: Randy Coburn
Email address: randy.coburn@email.com
Comment: Test Key
You selected this USER-ID:
    ""Randy Coburn (Test Key) <randy.coburn@email.com>""

Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O
You need a Passphrase to protect your secret key.

gpg: gpg-agent is not available in this session
We need to generate a lot of random bytes. It is a good idea to
 perform
some other action (type on the keyboard, move the mouse, utiliz
e the
disks) during the prime generation; this gives the random numbe
r
generator a better chance to gain enough entropy.

Not enough random bytes available.  Please do some other work t
o give
the OS a chance to collect more entropy! (Need 288 more bytes)
......+++++
.............................................+++++
We need to generate a lot of random bytes. It is a good idea to
 perform
some other action (type on the keyboard, move the mouse, utiliz
e the
disks) during the prime generation; this gives the random numbe
r
generator a better chance to gain enough entropy.
....................+++++
...................+++++
gpg: /root/.gnupg/trustdb.gpg: trustdb created
gpg: key DB74EF86 marked as ultimately trusted
public and secret key created and signed.

gpg: checking the trustdb
gpg: 3 marginal(s) needed, 1 complete(s) needed, PGP trust mode
l
gpg: depth: 0  valid:   1  signed:   0  trust: 0-, 0q, 0n, 0m,
0f, 1u
gpg: next trustdb check due at 2017-12-19
pub   4096R/DB74EF86 2016-12-19 [expires: 2017-12-19]
      Key fingerprint = FA8F 2A3D 9D1C 1AD6 1082  3C0B 1503 914
7 DB74 EF86
uid                  Randy Coburn (Test Key) <randy.coburn@email.com>
sub   4096R/822FDB0A 2016-12-19 [expires: 2017-12-19]
```

Then exported it like this:
```
gpg --export -a ""Randy Coburn"" | base64
```",LS0tLS1CRUdJTiBQR1AgUFVCTElDIEtFWSBCTE9DSy0tLS0tDQ,https://github.com/hashicorp/terraform-provider-aws
